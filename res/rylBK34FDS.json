{"notes": [{"id": "rylBK34FDS", "original": "HJlM6JmhUr", "number": 77, "cdate": 1569438845075, "ddate": null, "tcdate": 1569438845075, "tmdate": 1583912023924, "tddate": null, "forum": "rylBK34FDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "IdKl8YyaTq", "original": null, "number": 1, "cdate": 1576798686790, "ddate": null, "tcdate": 1576798686790, "tmdate": 1576800948242, "tddate": null, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "invitation": "ICLR.cc/2020/Conference/Paper77/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The authors propose a scale-invariant sparsity measure for deep networks. The experiments are extensive and convincing, according to reviewers. I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730484, "tmdate": 1576800283287, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper77/-/Decision"}}}, {"id": "S1gyncMZjH", "original": null, "number": 3, "cdate": 1573100198837, "ddate": null, "tcdate": 1573100198837, "tmdate": 1573100198837, "tddate": null, "forum": "rylBK34FDS", "replyto": "HkllzwGI5B", "invitation": "ICLR.cc/2020/Conference/Paper77/-/Official_Comment", "content": {"title": "Thanks for your positive feedback and addressing your concerns", "comment": "Thanks for your interest in our paper and your positive and constructive comments. Hopefully this reply can address all your concerns.\n\nFor the contribution, the main idea of this paper is to find a sparsity-inducing regularizer leveraging the desired properties of both the L0 regularizer (scale-invariant, minima along the axis) and the L1 regularizer (almost everywhere differentiable). With these requirements in mind, we find that Hoyer Square, the square of the traditional Hoyer regularizer, satisfies all the desired property and behaves as a differentiable approximation to the L0 norm. Extensive experiments are then performed to prove the desired property of Hoyer-Square is truly helpful for both element-wise and structural pruning of DNN models. We believe this paper is exciting as it proves that a simple differentiable approximation to the L0 norm can be used to guide the search of sparse DNNs and outperform much more complex methods that intent to directly optimize with L0 norm, like (Zhang et al., 2018) and (Louizos et al., 2017b).\n\nFor Figure 3, we compare our method with many (more than 5 in some figures, and 14 in total) previous works. The main goal of this figure is to show results achieved by DeepHoyer constantly stay above the Pareto frontier of all existing methods rather than some particular methods. We cannot list the names of all the previous methods in the figure due to the limited space. Instead, in the caption of Figure 3, we refer interested readers to Appendix C.3 where all the previous methods and detailed data are listed. Please take a look.\n\nFor the local minima problem, in this work we consider the minima along the axis as a desired property of DeepHoyer, because it mimics the minima structure of the L0 norm and can effective lead to sparsity without shrinking the parameter. As mentioned in the second-to-last paragraph on page 5, the recent advance of stochastic gradient descent optimizers provides satisfying performance on large-scale deep learning problem, which itself has a lot of local minima. From the observation in our experiments we believe the tiny bit nonconvexity induced by DeepHoyer does not affect the performance of DNNs. Since there are thousands or millions of parameters in a modern DNN, it\u2019s hard to guarantee which axis will generate better solution. This is why we let DeepHoyer to induce the \u201crotation\u201d of parameter towards axis but not enforcing a particular one. This will leave the flexibility to the training process to find a solution with optimal sparsity.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper77/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylBK34FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference/Paper77/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper77/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper77/Reviewers", "ICLR.cc/2020/Conference/Paper77/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper77/Authors|ICLR.cc/2020/Conference/Paper77/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176733, "tmdate": 1576860542628, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference/Paper77/Reviewers", "ICLR.cc/2020/Conference/Paper77/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper77/-/Official_Comment"}}}, {"id": "HyegG5GWsr", "original": null, "number": 2, "cdate": 1573100040092, "ddate": null, "tcdate": 1573100040092, "tmdate": 1573100040092, "tddate": null, "forum": "rylBK34FDS", "replyto": "Byx6q3uqtS", "invitation": "ICLR.cc/2020/Conference/Paper77/-/Official_Comment", "content": {"title": "Thanks for your positive feedback and addressing your concerns", "comment": "Thank you for your positive feedback to our paper. I\u2019d like to emphasize that the main idea of this paper is to find a sparsity-inducing regularizer leveraging the desired property of both the L0 regularizer (scale-invariant, minima along the axis) and the L1 regularizer (almost everywhere differentiable). With these requirements in consideration, we find that Hoyer Square, the square of the traditional Hoyer regularizer, satisfies all the desired properties and behaves as a differentiable approximation to the L0 norm. Extensive experiments are then performed to prove the desired property of Hoyer-Square is truly helpful for both element-wise and structural pruning of DNN models.\n\nThe three-stage pruning operations mentioned at the bottom of page 5 is a common practice for DNN pruning. Previous works like iterative pruning (Han et al., 2015b), regularization-based methods (Liu el al., 2015; Wen et al., 2016; Ma et al., 2019), and ADMM (Zhang et al., 2018) etc. all follow similar operations. Since the DeepHoyer regularizer is almost everywhere differentiable, it can be directly added to the original loss function of DNN training and be minimized with SGD (or other gradient-based) optimizers. Thus, the optimization process is as fast as applying L1 regularization, and a lot faster than ADMM which requires complex interplay between multiple objectives. Our experiment results show that DeepHoyer can achieve the lowest sparsity without introducing further complexity in the training process. Among all the pruning methods, SNIP (Lee et al., 2019) is the only one that does not require the 3-stage operations, as it prunes the model at initialization. SNIP might be faster than DeepHoyer, but the final solution it can achieve has much larger amount of parameters (2.5x on MNIST models). As mentioned in the second to last paragraph on page 5, the optimization of DeepHoyer well behaves under SGD. We do not observe any difficulties for training the DNN with DeepHoyer applied. Hope this explanation can address your concern.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper77/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylBK34FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference/Paper77/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper77/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper77/Reviewers", "ICLR.cc/2020/Conference/Paper77/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper77/Authors|ICLR.cc/2020/Conference/Paper77/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176733, "tmdate": 1576860542628, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference/Paper77/Reviewers", "ICLR.cc/2020/Conference/Paper77/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper77/-/Official_Comment"}}}, {"id": "HJxYKKfbsr", "original": null, "number": 1, "cdate": 1573099904576, "ddate": null, "tcdate": 1573099904576, "tmdate": 1573099904576, "tddate": null, "forum": "rylBK34FDS", "replyto": "HygbT3sqKH", "invitation": "ICLR.cc/2020/Conference/Paper77/-/Official_Comment", "content": {"title": "Thanks for your positive feedback", "comment": "Thank you for your positive feedback to our paper. We are glad that you consider the proposed method effective and find this work inspiring."}, "signatures": ["ICLR.cc/2020/Conference/Paper77/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylBK34FDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference/Paper77/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper77/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper77/Reviewers", "ICLR.cc/2020/Conference/Paper77/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper77/Authors|ICLR.cc/2020/Conference/Paper77/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176733, "tmdate": 1576860542628, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper77/Authors", "ICLR.cc/2020/Conference/Paper77/Reviewers", "ICLR.cc/2020/Conference/Paper77/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper77/-/Official_Comment"}}}, {"id": "Byx6q3uqtS", "original": null, "number": 1, "cdate": 1571617941126, "ddate": null, "tcdate": 1571617941126, "tmdate": 1572972641580, "tddate": null, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "invitation": "ICLR.cc/2020/Conference/Paper77/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper is written very nicely and the experiments are convincing (though you can always show more)\n\nIn terms of novelty, I shamelessly can say the idea is very simple and the basic form, the Hoyer regularization, was known. That said, I am all in for simple and efficient solutions so I am giving this paper a weak accept for now.\n\nThere is not much to ask here (unless I say I want to see how this would work on other backbones and problems). nevertheless, to improve this work, I think the authors need to compare which solution (referring to algorithms in Table1, 2 etc.) is faster/more well-behaved given the combo explained at the bottom of page 5 . This is basically my question/request for the rebuttal.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper77/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper77/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834052474, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper77/Reviewers"], "noninvitees": [], "tcdate": 1570237757413, "tmdate": 1575834052487, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper77/-/Official_Review"}}}, {"id": "HygbT3sqKH", "original": null, "number": 2, "cdate": 1571630264583, "ddate": null, "tcdate": 1571630264583, "tmdate": 1572972641545, "tddate": null, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "invitation": "ICLR.cc/2020/Conference/Paper77/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "To enforce sparsity in neural networks, the paper proposes a scale-invariant regularizer (DeepHoyer) inspired by the Hoyer measure. It is simply the ratio between l1 and l2 norm, which is almost everywhere differentiable, and enforces element-wise sparsity. It further proposes the Hoyer measure to quantify sparsity and applies the DeepHoyer in DNN training to train pruned models. The extension of Hoyer-Square is also straightforward. \n\nI generally enjoy simple yet effective ideas. The idea is very straightforward and well intuitive. The paper is well written and easy the follow. The discussion on the Hoyer measure is inspiring and the empirical studies on various different network architecture/datasets compared to several competitive baselines verify the effectiveness of the DeepHoyer model. \n\nTherefore I'm leaning to accept it. "}, "signatures": ["ICLR.cc/2020/Conference/Paper77/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper77/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834052474, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper77/Reviewers"], "noninvitees": [], "tcdate": 1570237757413, "tmdate": 1575834052487, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper77/-/Official_Review"}}}, {"id": "HkllzwGI5B", "original": null, "number": 3, "cdate": 1572378376437, "ddate": null, "tcdate": 1572378376437, "tmdate": 1572972641510, "tddate": null, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "invitation": "ICLR.cc/2020/Conference/Paper77/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper focuses on sparse neural networks. Typically, l1 regularization is the go-to strategy, however, it is not scale invariant. That is, all weights are affected by the regularization, not only those that are being driven to 0. l0 regularization is theoretically optimal, however, it is not smooth and has no gradients almost everywhere, so it cannot be used for training. As a compromise the paper proposes Hoyer regularization, that is the l1/l2 ratio. The Hoyer regularization has the same minima structure and leads to sparse solutions while being scale invariant, that is it does not affect all weights in the process. Additionally, the paper proposes structured Hoyer regularization. Last, it employs the said regularizations in deep networks: LeNet, AlexNet and ResNet on several datasets: MNIST, CIFAR, ImageNet.\n\nStrengths:\n+ The described method is simple, intuitive and straightforward. By applying the said regularization (~ \u03a3 |w_i|/sqrt(\u03a3 w_i^2)), one arrives at seemingly sparser solutions, which is verified in practice.\n\n+ The experiments are extensive and convincing. I particularly like that the authors have used their method with complex and deep models like ResNets, on large scale datasets like ImageNet.\n\n+ The presentation is generally clear and one can understand the paper straightaway.\n\nWeaknesses:\n+ The contributions of the paper are rather on the thin side. At the end of the day, Hoyer regularization is taken from another field (compressed sensing) and applied on deep networks. This is also witnessed by some moderate repetition in the writing, e.g., between the introduction and the related work.\n\n+ There are some points where the paper becomes unclear. For instance, in Figure 3 what are the \"other methods\"?\n\n+ In Figure 1 it is explained that the Hoyer regularization leads to minima along the axis. The gradients then push the models \"rotationally\". Could this lead to bad multiple local optimal problems? Is there any guarantee that any particular axis will generate better solutions than the other?\n\nAll in all, I would recommend for now weak accept. I find the work interesting and solid, although not that exciting."}, "signatures": ["ICLR.cc/2020/Conference/Paper77/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper77/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures", "authors": ["Huanrui Yang", "Wei Wen", "Hai Li"], "authorids": ["huanrui.yang@duke.edu", "wei.wen@duke.edu", "hai.li@duke.edu"], "keywords": ["Deep neural network", "Sparsity inducing regularizer", "Model compression"], "TL;DR": "We propose almost everywhere differentiable and scale invariant regularizers for DNN pruning, which can lead to supremum sparsity through standard SGD training.", "abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.", "pdf": "/pdf/1d5e3997e449727a4e2d2c3559ef81ec5d1b8fc0.pdf", "paperhash": "yang|deephoyer_learning_sparser_neural_network_with_differentiable_scaleinvariant_sparsity_measures", "code": "https://github.com/yanghr/DeepHoyer", "_bibtex": "@inproceedings{\nYang2020DeepHoyer:,\ntitle={DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},\nauthor={Huanrui Yang and Wei Wen and Hai Li},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylBK34FDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/84fdd392b326446bcd3c2def8f752a7e6ee8c063.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylBK34FDS", "replyto": "rylBK34FDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper77/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575834052474, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper77/Reviewers"], "noninvitees": [], "tcdate": 1570237757413, "tmdate": 1575834052487, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper77/-/Official_Review"}}}], "count": 8}