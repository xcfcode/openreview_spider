{"notes": [{"id": "B1gJOoRcYQ", "original": "HkeIkWO5KX", "number": 315, "cdate": 1538087782646, "ddate": null, "tcdate": 1538087782646, "tmdate": 1545355403719, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryllD7fgxV", "original": null, "number": 1, "cdate": 1544721240093, "ddate": null, "tcdate": 1544721240093, "tmdate": 1545354509274, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Meta_Review", "content": {"metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\nThe paper \n- tackles an interesting problem\n- makes a concerted effort to provide qualititative results that give insight into the models behaviour.\n- sufficiently cites related work.\n\n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- The model architecture lacks novelty.\n- There was also agreement that the contributions - (i) minor modifications of existing sequential attention-based models, and (ii) application to the RL domain - are minor.\n- A lot of space in the paper (section 4.2) is devoted to exploring the use of this model for image classification and video action recognition. However the proposed model performed poorly compared to SOTA methods for this task and no motivation was given for why the proposed model would be useful for such tasks.\n\nAll three points impacted the final decision.\n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it\u2019s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nThere was high agreement between the reviewers on the main drawbacks of the paper, before and after the rebuttal.\nThe AC considered the rebuttals by the authors (in which they argued that there was sufficient contribution) but, in the end, agreed with the reviewers' assessments.\n\n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be rejected.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "good analysis of the proposed method; lacks novelty"}, "signatures": ["ICLR.cc/2019/Conference/Paper315/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper315/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353260278, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper315/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper315/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper315/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353260278}}}, {"id": "B1xBJOreCm", "original": null, "number": 4, "cdate": 1542637533060, "ddate": null, "tcdate": 1542637533060, "tmdate": 1542637533060, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "S1gXvlv3pX", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "content": {"title": "new results for imagenet", "comment": "EDIT: We have a quick update regarding 3. A new model, also trained with a curriculum where we switch from sequence length 4 to 8 at 1e5 iterations, achieves Top-1 74.5%, Top-5 91.5%. This is now better than our sequence length 4 model and also our baseline."}, "signatures": ["ICLR.cc/2019/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614931, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1gJOoRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper315/Authors|ICLR.cc/2019/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614931}}}, {"id": "S1gXvlv3pX", "original": null, "number": 1, "cdate": 1542381658632, "ddate": null, "tcdate": 1542381658632, "tmdate": 1542623969052, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "BJlkSl8ahm", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "content": {"title": "Response", "comment": "Thank you for the comments and we are glad that you found the paper interesting. We address the concerns you raised below.\n\n1: While many works have been published with a similar theme (many of them we mention and cite in the paper), this specific configuration is novel, as well as the application to RL. Regarding the differences with MAC (which is indeed close to our own architecture) we replied this to reviewer 2 as well:\n\nIn terms of differences, first we would say that the existence of the question as guidance to the controller in MAC is a fundamental difference mainly because it serves as a very strong top-down bias for the vision (read) module - it\u2019s not just in the initialization of the reasoning, but throughout the whole process. In the RL case you could think of the reward as having a somewhat related role, but we feel this is a big difference. Second, our model is trained end-to-end so the vision can adapt and produce keys/values which are task relevant - MAC uses a pre-trained ImageNet module (though we don\u2019t argue MAC can\u2019t be learned end-to-end, it\u2019s just that it wasn\u2019t, and in the RL case, it\u2019s not clear what pre-trained network would be useful). Lastly, MAC does not use a spatial basis of any sort, so it can\u2019t reason about absolute positioning as well as limited relative positioning (trip wires, for example, wouldn\u2019t be possible without this).\n\n2: With NLP it is common to have the output of the attention model to be the same size as the input (because of the self-attention, all-to-all connectivity). This is not the case in this model since we have a severe spatial bottleneck from image size to single vector size. It\u2019s not clear how to build a multilayer attention model like this, but this is certainly an interesting research direction.\n\n3: We believe the issue here is not overfitting but rather optimization difficulties. It stems from running a high-capacity convnet over many time steps and propagating gradients. A new model trained with sequence length 8 achieves Top-1 70.1%, Top-5 88.6%. We have recently found a curriculum helps as well; a model that is first trained with sequence length 4 for 2e5 iterations and then further trained with sequence length 8 achieves Top-1 71.7%, Top-5 89.8% on sequence length 8.\n\nEDIT: We have a quick update regarding 3. A new model, also trained with a curriculum where we switch from sequence length 4 to 8 at 1e5 iterations, achieves Top-1 74.5%, Top-5 91.5%. This is now better than our sequence length 4 model and also our baseline.\n\n4: We have found that our method provides a more focused indication of the salient areas in the image for the agent.  In section 4.1.6, we compare with a saliency mapping method that blurs specific regions of the input and measures the change of the policy or value function.  Compared to this, our method produces sharper attention maps and more naturally matches the structure of the image (the alternate analysis can only produce roughly circular regions of interest).  \nWe have investigated the applications of GradCAM to our baseline model on ATARI.  This produces interesting visualizations and is more adapted to the structure of the problem than the saliency visualization.  The biggest advantage our model has over GradCAM is it shows all the information that the agent is extracting from the image, not just what is relevant to produce a certain action.  GradCAM is a very powerful tool for showing why a certain action is being taken, but it does not show the full information the agent is extracting from the environment.  Especially when there is an LSTM above the convolutional stack, information may be extracted at the current timestep that won\u2019t be used until several timesteps in the future, and so would not show up in a GradCAM visualization of the current frame.  One could imagine a modification of the GradCAM architecture which analyzes the gradient of the conv layer with respect to some future action, but this may quickly begin to overwhelm our ability to interpret the images.  Using our method, the agent cannot extract any information from the current frame that is not attended to, so we are sure we have a full picture of what is relevant for either the current action or any future action. \n\n5: We didn\u2019t focus too much on metrics for kinetics; we were more interested in understanding the attention maps qualitatively. The model has optimization issues on long time sequences, as seen in Imagenet sequence size 8.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614931, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1gJOoRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper315/Authors|ICLR.cc/2019/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614931}}}, {"id": "BylpAZvh6Q", "original": null, "number": 3, "cdate": 1542382037446, "ddate": null, "tcdate": 1542382037446, "tmdate": 1542382037446, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "ryeL9mBKhm", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your kind review and interest in the model, here is our response to concerns raised:\n\n* Relation to MAC\nAs we mentioned, MAC is indeed close to this architecture. In terms of differences, first we would say that the existence of the question as guidance to the controller in MAC is a fundamental difference mainly because it serves as a very strong top-down bias for the vision (read) module - it\u2019s not just in the initialization of the reasoning, but throughout the whole process. In the RL case you could think of the reward as having a somewhat related role, but we feel this is a big difference. Second, our model is trained end-to-end so the vision can adapt and produce keys/values which are task relevant - MAC uses a pre-trained ImageNet module (though we don\u2019t argue MAC can\u2019t be learned end-to-end, it\u2019s just that it wasn\u2019t, and in the RL tasks we use, it\u2019s not clear what pre-trained network would be useful). Lastly, MAC does not use a spatial basis of any sort, so it can\u2019t reason about absolute positioning as well as limited relative positioning (trip wires, for example, wouldn\u2019t be possible without this).  \n\n* Number of attention heads\nWe observe that having only a single head produces substantially lower scores across a number of levels on ATARI and other environments.  We also see noticeable dropoff in performance in several levels beyond 8 heads.  Beyond that the answer is strongly level-dependent.  For example, an agent with 2 heads takes 200M steps more to reach maximum performance than one with 4 heads on Enduro, but an agent with 4 heads performs exactly the same as one with 8 heads.  In Berzerk, agents with 2, 4, and 8 heads all achieve similar scores. We chose 4 heads on the basis that 4 heads was never worse than 2 and occasionally better, while 8 heads was always very similar in performance to 4 heads.  \n\n* Missing citation\nWe will add this to the next version.\n\n* Coarseness of attention\nWe checked on ImageNet, and the performance difference was negligible. The granularity isn\u2019t particularly important for this dataset because the objects to classify are quite large and there\u2019s no need to have a particularly fine level of attention. \n\n* Sampling of actions after every RNN iteration\nWe sample a model action after every iteration.  We did not try to run the RNN multiple times before sampling the action, but our general intuition is that the frame rate in ATARI is high enough that this is generally not needed.  Indeed, we effectively sample an action after every four frames, because our model uses action repeat to speed up training (this is mentioned briefly in appendix A.1.1)."}, "signatures": ["ICLR.cc/2019/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614931, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1gJOoRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper315/Authors|ICLR.cc/2019/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614931}}}, {"id": "rkejVWvn6Q", "original": null, "number": 2, "cdate": 1542381875245, "ddate": null, "tcdate": 1542381875245, "tmdate": 1542381875245, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "rJgHnIg93Q", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "content": {"title": "Response", "comment": "Thank you for you interest in the model and constructive comments. Below is our response.\n\n* Regarding Xu et al:\n\nIt is true that our attention mechanism shares a number of features with Show, Attend, Tell.  The main differences between our model and SAT are:\n1. We produce separate keys and values from the image.  In SAT, the attention weights are produced from the a vectors and the hidden state, and then the result is the weighted sum of the a vectors.  In our setup, the weights are produced from the k vectors and the hidden state, and then the result is the weighted sum of the v vectors.  Our method could, in principle, allow the system to specialize the information used to compute the weights compared to that used to produce the answers.  Furthermore, we are able to introduce an information asymmetry between the keys and values: the keys have only 8 channels, while the values have 120 channels, which is not possible in the SAT system\n\n2. The addition of the spatial basis allows the system to ask positional questions.  The analysis in section 4.1.5 suggest that (on some games, at least) the agents are actively using the ability to ask for position-dependent information.  The inclusion of the spatial basis in the answer allows the system to learn the location within the image of the feature it just queried for and then ask a subsequent question about features around that location.  This can be seen in Figure 5, where the attention puts a location-based probe in the area where a new car will appear, and the content-based channels light up when the car does indeed appear.  This would be very difficult to do without the spatial basis, since it would have to encode a unique feature for a location that has no distinct features other than its position in space.\n\n3. We compute multiple independent queries at each timestep.  This allows the system to track multiple threads at the same time.  This probably is not a vital feature for tasks such as image captioning, but for agents acting in an environment it is quite important.\n\n4. While not a direct difference in the mechanism, it\u2019s also worth emphasizing that the application of this sort of mechanism to the RL domain is novel and a substantial difference from the work presented in Xu et al.\n\nAs you point out\t, however, the attention mechanisms are quite similar and we will modify the text accordingly to reflect this.\n\n\n* Regarding pre-training of the ResNets:\nAll CNNs are trained from scratch alongside the rest of the model.\n\n* Comparisons to existing attention-based models:\n[1] Sharma et al., \u201cAction recognition using visual attention,\u201d ICLR workshop 2016.\n\nWhile a direct comparison here would be difficulty due to use of a different dataset, we can observe that the resulting attention maps in this work are much blurrier and diffuse than the ones we get with our model. \n\n[2] Sorokin et al., \u201cDeep Attention Recurrent Q-network,\u201d NIPS workshop 2015.\n\nCompared to Sorokin, our attention mechanism provides much sharper attention maps.  Our final scores are substantially better, but it is hard to differentiate between the effect of our IMPALA-style training and the differences in the attention mechanism.  \n\n[3] Choi et al., \u201cMulti-Focus Attention Network for Efficient Deep Reinforcement Learning,\u201d AAAI workshop 2017.\n\nLooking at the details in this paper, it appears that their attention mechanism is very similar to the \u201cfixed query\u201d baseline from section 4.1.1. They segment and process the image to arrive at a set of keys and values in an analogous way to the vision stack in our model.  The selectors from their paper are analogous to our learnable bias tensors.  There are differences: we use a ConvLSTM in the visual processing, a spatial basis, and an LSTM in the policy processing, but none of these affect the core attention mechanism.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614931, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1gJOoRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper315/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper315/Authors|ICLR.cc/2019/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper315/Reviewers", "ICLR.cc/2019/Conference/Paper315/Authors", "ICLR.cc/2019/Conference/Paper315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614931}}}, {"id": "BJlkSl8ahm", "original": null, "number": 3, "cdate": 1541394486671, "ddate": null, "tcdate": 1541394486671, "tmdate": 1541534099698, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Official_Review", "content": {"title": "Interesting model, however, the performance on the supervised task is not good. ", "review": "\n[Summary]\n\nThis paper proposed a soft, spatial, sequential, top-down attention model which enable the agent and classifier actively select important, task relative information to generate the appropriate output. Given the observations, the proposed method uses a convLSTM to produce the key and value tensor. Different from multi-head attention, the query vector is generated in a top-down fashion. The authors proposed to augment the spatial feature with Fourier bases which is similar to previous work. The authors verify the proposed method on both reinforcement learning and supervised learning. On reinforcement learning, the proposed method outperformed the feedforward baseline and LSTM baseline. On reinforcement learning task, the proposed method achieves compelling result with more interpretable attention map that shows the model's decision. \n\n[Strength]\n1: The proposed model is a straightforward extension of the multi-head attention to visual input. Compare to multi-head attention, it generates the query vector in a top-down manner instead of pure bottom up, and the authors verify the proposed choice is better than LSTM baseline empirically.\n\n2: The authors verify the proposed method by extensive experiments on reinforcement learning tasks and also try supervised learning tasks. The attention visualization and human normalized scores for experts on ATARI show the effectiveness of the proposed method. \n\n[Weakness]\n1: The soft spatial top-down attention is very common in vision and language domain, such as VQA. As the authors mentioned, the proposed method is very similar with MAC for CLEVER. The sequential attention is also explored in previous VQA work. Thus the novelty of the proposed method is limited. \n\n2: Multi-head attention for NLP tasks are usually composed with multiple layers. Will more layer of attention help the performance? The paper is less of this ablation study. \n\n3: The proposed method is worse compared with other baselines on supervised learning tasks, on both imagenet classification and kinetics. I wonder whether the recurrent process is required for those tasks? On table 2, we can observe that with sequence length 8, the performance is much worse,  this may be caused by overfitting. \n\n4: If the recurrent attention is more interpretable, given other visualization methods, such as gradcam, I wonder what is advantage?\n\n5: I would expect that the performance on Kinetics dataset is better since sequential attention is required on video dataset. However, the performance is much worse compare of the baseline in the dataset. I wonder what is the reason? is there ablation study or any other results on this dataset? ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper315/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Official_Review", "cdate": 1542234489754, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper315/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335696743, "tmdate": 1552335696743, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper315/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJgHnIg93Q", "original": null, "number": 2, "cdate": 1541174957101, "ddate": null, "tcdate": 1541174957101, "tmdate": 1541534099493, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Official_Review", "content": {"title": "An interesting visual attention approach.", "review": "Summary.\nThe paper proposes a variant model of existing recurrent attention models. The paper explores the use of query-based attention, spatial basis, and multiple attention modules running in parallel. The effectiveness of the proposed method is demonstrated with various tasks including RL (on Atari games), ImageNet image classification, and action recognition, and shows reasonable performance. \n\nStrengths.\n- An interesting problem in the current CV/RL community.\n- Well-surveyed related work.\n- Supplemental materials and figures were helpful in understanding the idea.\n\nvs. Existing recurrent attention models.\nIn Section 2, the proposed model is explained with emphasizing the differences from existing models, but there needs a careful clarification.\n\nIn this paper, attention weights are computed conditioned on a query vector (which solely depends on the RNN\u2019s state) and the Keys (which are generated by a visual encoder, called vision core). In the landmark work by Xu et al. (ICML \u201815, as already referenced), attention weights are computed very similarly - they used the hidden state of RNN followed by an additional layer (similar to the \u201cquery\u201d) and visual features from CNN followed by an additional layer (similar to the \u201ckeys\u201d). The only difference seems the use of element-wise multiplication vs. addition, but both are common units in building an attention module. Can authors clarify the main difference from the existing recurrent attention models?\n\nTraining details.\nIn the supervised learning tasks, are these CNN bases (ResNet-50 and ResNet-34) trained from scratch or pre-trained with another dataset?\n\nMissing comparison with existing attention-based models.\nThe main contribution claimed is the attention module, but the paper does not provide any quantitative/qualitative comparison from another attention-based model. This makes hard to determine its effectiveness over others. Notable works may include:\n\n[1] Sharma et al., \u201cAction recognition using visual attention,\u201d ICLR workshop 2016.\n[2] Sorokin et al., \u201cDeep Attention Recurrent Q-network,\u201d NIPS workshop 2015.\n[3] Choi et al., \u201cMulti-Focus Attention Network for Efficient Deep Reinforcement Learning,\u201d AAAI workshop 2017.\n\nMinor concerns.\nThe related work section would be helpful if it proceeds the current Section 2.\nTypos", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper315/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Official_Review", "cdate": 1542234489754, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper315/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335696743, "tmdate": 1552335696743, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper315/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeL9mBKhm", "original": null, "number": 1, "cdate": 1541129102459, "ddate": null, "tcdate": 1541129102459, "tmdate": 1541534099287, "tddate": null, "forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper315/Official_Review", "content": {"title": "Interesting results. Some more experimentation needed", "review": "This work presents a recurrent attention model as part of an RNN-based RL framework. The attention over the visual input is conditioned on the the model's state representation at time t. Notably, this work incorporated multiple attention heads, each with differing behavior.\n\nPros:\n-Paper was easy to understand\n-Detailed analysis of model behavior. The breakdown analysis between \"what\" and \"where\" was particularly interesting.\n-Attention results appear interpretable as claimed\n\nCons:\n-Compared to the recurrent mechanism in MAC, both methods generate intermediate query vectors conditioned on previous model state information. I would not consider the fact that MAC expects a guiding question to initialize its reasoning steps constitute a major difference in the overall method.\n-There should be an experiment demonstrating the effect of # of attention heads against model performance. How necessary is it to have multiple heads? At what point do we see diminishing returns?\n-I would also recommend including a citation for :\nSukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. \"End-to-end memory networks.\" NIPS 2015.\n\n\nGeneral questions:\n-Was there an effect of attention grid coarseness on performance?\n-For the atari experiments, is a model action sampled after each RNN iteration? If so, would there be any benefit to trying multiple RNN iterations between action sampling?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper315/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "S3TA: A Soft, Spatial, Sequential, Top-Down Attention Model", "abstract": "We present a soft, spatial, sequential, top-down attention model (S3TA). This model uses a soft attention mechanism to bottleneck its view of the input. A recurrent core is used to generate query vectors, which actively select information from the input by correlating the query with input- and space-dependent key maps at different spatial locations.\n\nWe demonstrate the power and interpretabilty of this model under two settings. First, we build an agent which uses this attention model in RL environments and show that we can achieve performance competitive with state-of-the-art models while producing attention maps that elucidate some of the strategies used to solve the task. Second, we use this model in supervised learning tasks and show that it also achieves competitive performance and provides interpretable attention maps that show some of the underlying logic in the model's decision making.", "keywords": ["Attention", "RL", "Top-Down", "Interpretability"], "authorids": ["alexmott@google.com", "danielzoran@google.com", "chrzanowskim@google.com", "wierstra@google.com", "danilor@google.com"], "authors": ["Alex Mott", "Daniel Zoran", "Mike Chrzanowski", "Daan Wierstra", "Danilo J. Rezende"], "TL;DR": "http://sites.google.com/view/s3ta", "pdf": "/pdf/1ba3a556b46a3c574f87f2a572add27de4cc0003.pdf", "paperhash": "mott|s3ta_a_soft_spatial_sequential_topdown_attention_model", "_bibtex": "@misc{\nmott2019sta,\ntitle={S3{TA}: A Soft, Spatial, Sequential, Top-Down Attention Model},\nauthor={Alex Mott and Daniel Zoran and Mike Chrzanowski and Daan Wierstra and Danilo J. Rezende},\nyear={2019},\nurl={https://openreview.net/forum?id=B1gJOoRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper315/Official_Review", "cdate": 1542234489754, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1gJOoRcYQ", "replyto": "B1gJOoRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper315/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335696743, "tmdate": 1552335696743, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper315/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}