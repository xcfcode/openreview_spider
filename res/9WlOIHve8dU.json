{"notes": [{"id": "9WlOIHve8dU", "original": "PXUSwBImBF9", "number": 3028, "cdate": 1601308335727, "ddate": null, "tcdate": 1601308335727, "tmdate": 1614985774354, "tddate": null, "forum": "9WlOIHve8dU", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "PHqFZVwDj_1", "original": null, "number": 1, "cdate": 1610040357047, "ddate": null, "tcdate": 1610040357047, "tmdate": 1610473946742, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The main problem as flagged by reviewers is the lack of formal evidence that the approach is a right one to carry out. Decision tree induction has early been the subject of formal studies in ML, whether in statistics (Friedman et al.) or ML (Kearns et al.). It is a bit sad that a new approach that relies on a much different standpoint on the problem and modelling of tree classification (Section 3, R2), with experimental results recognized by reviewers (R3, R4) is not accompanied by formal analyses on par with SOTA for related approaches (R3, R1). I would strongly suggest the authors fit in a few more Lemmata, either to follow up on specific problems (R1). The paper would tremendously benefit from extensive connections with the existing theory, be it from the generalization and overfitting standpoint (R2, remark #6) or the choice of the appropriate best contender using the boosting literature. Decision was taken not to accept the paper but I would very strongly encourage the authors to revise the draft.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040357033, "tmdate": 1610473946725, "id": "ICLR.cc/2021/Conference/Paper3028/-/Decision"}}}, {"id": "eEz4z4bpf-9", "original": null, "number": 10, "cdate": 1606303107764, "ddate": null, "tcdate": 1606303107764, "tmdate": 1606303107764, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "Comments on submitted revised paper", "comment": "We submitted a revised version of the paper with the following updates:\n1. We revised the related work as suggested by reviewers;\n2. We clarified our formulation and its derivation;\n3. We added a pseudocode for the overall optimization procedure in Algorithm 2, following reviewer 2\u2019s suggestion;\n4. We reported the results of Adaptive Neural Trees (ANT) on regression datasets in Table 1, to address reviewer 2\u2019s concerns about the limited number of baselines in this setting. We will update the paper with ANT\u2019s performance on classification datasets as well as soon as the tuning and training are complete (these have yet to finish after 7 days);\n5. We improved our method\u2019s results, by initializing the split functions so that points are equally spread over the tree at initialization as described in the appendices;\n6. We added a comparison with optimal tree baselines in Table 2, as suggested by reviewer 3;\n7. We reported training times vs error on the tabular datasets for each baseline in Figure 6 of the appendices, to show that our model is consistently faster to train than existing differentiable tree learning techniques (e.g. NDF, ANT) while achieving state-of-the-art performance;\n8. We reported model sizes in Table 4 in the appendices, following reviewer 2\u2019s remarks.\n\nAbout the comparison to optimal tree baselines of **Table 2**, the current version of the paper reports results only for Mushrooms dataset. As the previous version revealed, on **COMPAS** our method matches the performance of GOSDT with 13 times faster runtime. However **we chose to remove this dataset from our current version of the paper because of the deep ethical concerns posed by the recidivism prediction task**. This is especially important since the reviewers have already filled in ethics reports for our paper based on the initial version that didn\u2019t include this dataset. We are currently working on comparing on other small datasets instead.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "G2ocuQIZxr", "original": null, "number": 9, "cdate": 1606254402171, "ddate": null, "tcdate": 1606254402171, "tmdate": 1606254402171, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "Boosting vs MIP", "comment": "One question comes to mind for a Boosting person: the work of Kearns and Mansour (started w/ STOC 1996: On the Boosting Ability of Top-Down Decision Tree Learning Algorithms) and follow ups (see the citing papers) show that greediness has a purpose in the boosting framework -- it gives provable and fast convergence under lightweight assumptions about splits. However,  the convergence rate depends on the splitting criterion and CART's criterion displays the \"poorest\" of all. I understand the technical interest in picking Gini index in the paper's context. However, Kearns and Mansour give an optimal splitting criterion -- not the binary entropy of C4.5 but another one with possibly better technical appeal in the context of the paper. Could it be used ?\n\nI also suspect it would be possible, at least until section 3.1, to analyze the boosting abilities of the proposed method following Kearns and Mansour's  framework.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "SyewztWrr4C", "original": null, "number": 8, "cdate": 1606158741293, "ddate": null, "tcdate": 1606158741293, "tmdate": 1606158741293, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "Vb6pN6NHBxL", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "Thank you for the clarification", "comment": "More stuff become clear now. Thanks. Some further questions/comments:\n\n 4. The only relaxations we make are to input-traversals and tree-pruning decisions. The relaxation is sparse so most values will be either 0 or 1 and very few will fall within [0,1]. This way our method is different from most differentiable soft tree relaxations such as Irsoy et al., 2012; DNDT; NDF.\n\nAfaik, in soft trees, we can traverse each input to all nodes with some probability which is equivalent to having relaxed z in your case, isn't it? Thus I can see a strong similarity between these two methods. However, soft trees are trained end-to-end using gradient based techniques whereas here some sort of alternating optimization involved.\n\n 5. We have added additional comparisons to optimal tree baselines suggested by R3 in Table 3. We are also currently running [1] and will cite all of the above works.\n\nI believe that the experimental evaluation is still a weak point here: regression is compared against CART only, only 2 relatively simple classification datasets were used. I guess the choice of datasets mainly driven from (Popov et al., 2019). I suggest to include more relevant datasets (e.g. from [2,3], mnist could be a very nice benchmarks since it is a well studied) and compare against baselines therein.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "kW10XGAxSHQ", "original": null, "number": 3, "cdate": 1606039080008, "ddate": null, "tcdate": 1606039080008, "tmdate": 1606041406348, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "We thank all reviewers", "comment": "We would like to thank all reviewers for their time and thoughtful comments. We appreciated the suggestions for improving the paper and updated it accordingly. \nIn the official comments, we address each reviewer's concerns point by point.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "Vb6pN6NHBxL", "original": null, "number": 7, "cdate": 1606041018228, "ddate": null, "tcdate": 1606041018228, "tmdate": 1606041018228, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "YsYqIvhv_L", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "Clarification on formulation and advantages of proposed method", "comment": "1. However, in order to compute gradients w.r.t. \\theta, we need to compute q(x) which is, I believe, non-differentiable (it involves max function).\n\nThis is a common misconception, thank you for pointing it out, we have added a clarification at the end of Section 3.2. The max function is differentiable almost everywhere, this is not a problem; in fact, ReLU(z) = max(z, 0) is commonly used as an activation in neural nets; our computation is similar. The gradient of a minimum of a family of functions propagates along the minimizing branch, by Danskin's theorem. (Nonlinear Programming, Bertsekas, 1999, Proposition B.25)\n\n2. Are you solving the tree program (eq. 3) at each mini-batch update [...]? Providing a pseudocode of the overall algorithm would be beneficial.\n\nWe solve the tree program at each mini-batch, as we need to derive the tree representations $z_i$ for all points. Thanks for this suggestion, we have included algorithm pseudocode in the updated version (Algorithm 2).\n\n3. In conventional trees, a tree output is given by its leaf. However, here it seems the final prediction is obtained by f(.) and I don't understand why?\n\nWe agree that this aspect of our work should be highlighted more clearly: instead of making predictions using heuristic scores over the training points assigned to a leaf (e.g., majority class), our approach allows one to learn a prediction function f() that optimizes an overall objective. One way to think of this is that our approach embeds a decision tree as a layer of a deep neural network and optimizes the continuous parameters of the whole model by back-propagation. In our experiments, we considered a simple neural architecture, where z and a final predictor f(x, z) are learned end-to-end. Beyond the benefits of joint optimization, this final predictor is also useful for adapting the tree to certain settings. For example, in regression settings the labels often lie in a large (or even unknown) interval of the reals. The final predictor f() is particularly useful here as it can learn to map the traversals z, which lie in [0, 1], to arbitrary intervals.\n\n4. Given all relaxations in the final algorithm, my impression is that these trees are look like a regular soft trees but with tree structure learning capability.\n\nThe only relaxations we make are to input-traversals and tree-pruning decisions. The relaxation is sparse so most values will be either 0 or 1 and very few will fall within [0,1]. This way our method is different from most differentiable soft tree relaxations such as Irsoy et al., 2012; DNDT; NDF.\n\n5. Experiments lack sufficient evaluations (for regression and classification) [...] I strongly suggest authors to include more advanced baselines (e.g. [1,2,3]) for both classification and regression.\n\nWe have added additional comparisons to optimal tree baselines suggested by R3 in Table 3. We are also currently running [1] and will cite all of the above works.\n\n6. why depth of the tree for CART is limited to {2,...,6}? I believe that it heavily under-represents CART...\n\nThank you for catching this. In fact we searched CART depths in {2,...,10}, thank you for drawing attention to this typo! We found empirically that CART trees quickly start to overfit on these datasets as the depth increases. Specifically, for the considered supervised learning datasets, the tree depth selected by hyperparameter tuning was always smaller or equal to 7.\n\n7. resulting table should contain model sizes in terms of number of parameters[...]\n\nWe have updated the paper to include the number of parameters for single-tree methods on supervised learning datasets, in Table 2.. Apart from CART our model is consistently smaller than both optimal tree and differentiable tree methods.\n\n8. I believe that the method cannot be used to train a regular axis-algined trees[...]\n\nOur method does not learn axis-aligned trees but this is an interesting direction for future work. We believe we could use SparseMax (Martins & Astudillo, 2016) to learn approximately axis-aligned splits, or REINFORCE (Williams, 1988) to learn exact axis-aligned splits.\n\n9. NDF baseline is always performs better than the proposed approach. I believe ANT[1] will perform even better. Given all of them have similar model sizes (including the proposed method), I don't see any practical benefits of the proposed method over the others.\n\nAs pointed out by R3 and R4, our method can be applied to a wide variety of tasks, even to unsupervised problems, which is not the case for NDF and ANT. Moreover, as detailed by R4, it offers an efficient way for optimizing the tree structure (via the optimization of pruning vector a). In comparison, ANT needs to incrementally grow trees. We are currently running ANT on all tabular datasets and we have observed that this approach is significantly slower than ours. Finally, aside from CART, our approach uses fewer parameters than all other large-scale supervised tree learning methods as shown in Table 2.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "2eoxBG94jbt", "original": null, "number": 6, "cdate": 1606040252219, "ddate": null, "tcdate": 1606040252219, "tmdate": 1606040252219, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "aTNHdmykDIT", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "Scalability and clarification on comparison with CVXPY", "comment": "1.  I wonder if cvxpy is mostly implemented in Python, given that the authors implemented their method in C++, I would expect a better performance just from that. Although, the huge gap in runtime suggests that it wouldn't matter. \n\nThanks for bringing this up. CVXPY is based on three open source solvers that are implemented largely in C and Objective C, which is why we implemented our method in a similarly fast language for comparison. We have added this to the appendix to make this clearer.\n\nOverall we\u2019d like to thank the reviewer for emphasizing one of the major advantages of our method. Specifically, not only is the proposed relaxation and the derived algorithm for inducing tree traversals several magnitudes faster than state-of-the-art techniques, but it also makes the formulation differentiable, allowing our method to learn trees using extremely efficient automatic differentiation libraries. This further allows us to embed a decision tree as a module of a deep learning architecture and optimize it with arbitrary objectives, even in unsupervised settings.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "Q90UWro3RZ", "original": null, "number": 5, "cdate": 1606040075384, "ddate": null, "tcdate": 1606040075384, "tmdate": 1606040075384, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "sa6giDZLUDO", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "Clarification about formulation", "comment": "1. The problem definitions are not somewhat unclear. It seems that (1)-(7) is to find a pruning of the given tree, which are not explicitly mentioned.\n\nWe would like to clarify our formulation. The solution to Eq. (1) characterizes tree routing. As mentioned right above it, Eq. (2) extends (1) to jointly model tree routing and pruning. Eq. (3) is a relaxed and smooth (thus differentiable) version of (2). Eqs. (4-7) describe a customized way to solve the optimization problem (3).\n\n2. It is not nontrivial that the problem (1) leads to a tree...\n\nProblem (1) is essentially a simplification of the MIP in Bertsimas & Dunn, 2017 in Eq. (24). The proof that it yields a tree is as follows. For any x_i, fix a tree depth and all splitting functions $s_{\\theta_t}$. Given this tree there exists only one leaf node $l^*$ for which itself and all its parents have $s_{\\theta_t}$ values greater than $0$. This is because for any branch at node $t$, $x_i$ splits right if $s_{\\theta_t}(x_i) > 0$ and left if $-s_{\\theta_t}(x_i) > 0$. Thus, there is one path that $x_i$ can follow where $s_{\\theta_t}(x_i) > 0$ for all considered nodes $t$, until hitting the leaf node $l^*$. Finally for any $t$, $q_{it}$ records the minimum value along the path to reach $t$ for $x_i$. Thus, $q_{it}$ is negative for all nodes along all paths except the one leading to $l^*$. Thus $z$ will be positive only for the nodes leading to $l^*$ and $0$ otherwise.\n\n3. The obtained solutions in (3)-(7) are not discrete and how to round up continuous solutions to discrete ones are not shown...\n\nWe do not perform any rounding. If discrete solutions are necessary one could simply round to the nearest integer.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "erqHQuLwMYN", "original": null, "number": 4, "cdate": 1606039617551, "ddate": null, "tcdate": 1606039617551, "tmdate": 1606039617551, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "YiG1x_10a8q", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment", "content": {"title": "Relation and comparison with methods to train optimal decision trees", "comment": "1. Have you evaluated the optimality of solutions of your method with respect to the exact tree-fitting optimization problem \u2013 i.e., problem (10) where the tree is a solution to the MIP in (2)...\n\nWe agree this is an interesting question, but determining this optimality gap is highly non-trivial as it would require: 1. Determining the gap between an MIP similar to eq (2) which also includes the variables $\\theta$, $\\eta$, $\\phi$ and its sparse relaxation (which would be a variant of eq (3)); and 2. Arguing about how close the local minima found by stochastic gradient methods are to the global minimum when solving eq (10). While 1 seems possible (we believe there are arguments akin to total unimodularity that could be used for the MIP->QP setting), 2 is at the forefront of non-convex optimization theory and has no clear answers for our setting. We think this is an interesting direction for future work.\n\n2. FWIW, the statement that \"Bertsimas & Dunn (2017) phrased the objective of CART as a MIP that could be solved exactly\" is misleading...\n\nThank you for the clarification, we will rephrase.\n\n3. I am wondering why the authors did not consider the following methods to train optimal decision trees in their experiments...\n\nWhile this line of work is indeed an important influence, we stress that there is a significant difference in our motivation, as we compromise global tree optimality in order to allow learning within a neural network. We did try this comparison, and on large datasets it is not as easy as claimed, for computational reasons. On the datasets reported in the initial submission, GSOSDT (Lin et al., 2020) ran out of memory. At your suggestion, we have run OPTREE (Bertsimas & Dunn, 2017) and GOSDT on the smaller Mushrooms and COMPAS datasets, and reported the results in the updated version of the paper. Our method achieves error rates comparable to those of GOSDT and it is several times faster on these small-scale datasets. We haven\u2019t been able to obtain OPTREE\u2019s performance yet, either the MIP version (Bertsimas & Dunn, 2017) or the local search version (Dunn, 2018), after 3 days of training per run. This is currently running and we will report the results as soon as we get them. Overall, we want to stress that our method is aimed at a different setting than these methods: learning flexible trees for arbitrary objectives.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9WlOIHve8dU", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3028/Authors|ICLR.cc/2021/Conference/Paper3028/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841938, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Comment"}}}, {"id": "YsYqIvhv_L", "original": null, "number": 1, "cdate": 1603839320429, "ddate": null, "tcdate": 1603839320429, "tmdate": 1605024082974, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review", "content": {"title": "Quite generic but unclear tree learning algorithm", "review": "The given paper describes a novel approach of learning binary trees using a differentiable relaxation of MIP. For that matter, authors describe their approach of tree traversal for the given input x and a complete binary tree of depth D. This problem is formulated as MIP which is relaxed to make it differentiable. This MIP is then reformulated to learn a tree structure (i.e. tree pruning). Finally, a decision node parameters are optimized using backpropagation. The algorithm alternates between tree structure learning using relaxed MIP and tree parameters learning using SGD. \n\nMajor advantageous:\n\n  . The proposed method is interesting and addresses the tree learning algorithm from different perspective. I liked the idea of MIP relaxation and the proposed algorithm to solve the QP (using isotonic regression).\n\n  . It is nice that the method is quite generic and can be applied to both (semi)supervised/unsupervised (e.g. clustering) problems.\n\nMajor concerns:\n\n  . First of all, I found this paper is hard to follow and understand. Therefore, I have a couple of fundamental questions/comments regarding methodology:\n\n    - The overall obj function (eq. 10) is minimized using backpropagation over decision node parameters \\theta. However, in order to compute gradients w.r.t. \\theta, we need to compute q(x) which is, I believe, non-differentiable (it involves max function). \n\n    - Are you solving the tree program (eq. 3) at each mini-batch update or it is done only once at the beginning of the minimization problem 10? Providing a pseudocode of the overall algorithm would be beneficial.\n\n    - In conventional trees, a tree output is given by its leaf. However, here it seems the final prediction is obtained by f(.) and I don't understand why? What is the use of f(.)? I know that it is a linear function or MLP. But, why it takes z and x as arguments and makes the final prediction. It is not well described in the paper. By the way, figure 1 is not referenced anywhere...\n\n  . Given all relaxations in the final algorithm, my impression is that these trees are look like a regular soft trees but with tree structure learning capability. \n\n  . Experiments lack sufficient evaluations (for regression and classification). For regression, comparison is performed only against CART. Moreover, why depth of the tree for CART is limited to {2,...,6}? I believe that it heavily under-represents CART since it is known that the depth for such trees should be sufficiently large. Most importantly, I strongly suggest authors to include more advanced baselines (e.g. [1,2,3]) for both classification and regression. Moreover, resulting table should contain model sizes in terms of number of parameters, at least (e.g. I'm confident that CART trees will be much smaller).\n\n[1] Tanno, R., Arulkumaran, K., Alexander, D. C., Criminisi, A., and Nori, A. Adaptive neural trees. ICML 2019.\n\n[2] Carreira-Perpinan, M. A. and Tavallali, P. (2018). Alternating optimization of decision trees, with application to learning sparse oblique trees. NeurIPS 2018.\n\n[3] A. Zharmagambetov and M. A. Carreira-Perpinan (2020): Smaller, More Accurate Regression Forests Using Tree Alternating Optimization. ICML 2020\n\nMinor concerns: \n\n  . I believe that the method cannot be used to train a regular axis-algined trees, i.e. to force each node to use only one feature. \n\n  . NDF baseline is always performs better than the proposed approach. I believe ANT[1] will perform even better. Given all of them have similar model sizes (including the proposed method), I don't see any practical benefits of the proposed method over the others.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083792, "tmdate": 1606915760391, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review"}}}, {"id": "aTNHdmykDIT", "original": null, "number": 2, "cdate": 1603896537893, "ddate": null, "tcdate": 1603896537893, "tmdate": 1605024082913, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review", "content": {"title": "Good paper", "review": "Summary: The paper provides an interesting way to learn binary trees that is faster than generic solvers of MIP. The trick is to use gradient-based methods for a new sparse relaxation of MPI. Authors compare their method to generic solvers and find a substantial improvement on runtime. This is certainly important for scalability.\n\nThe paper overall is of good quality. The story of the work is well-written which makes the contributions easier to digest.\n\nIn terms of theoretical contributions, the work is \"weak\" in the sense that the main result follows from a simple relaxation. However, authors show strong empirical evidence that their method is faster and competitive to existing results. Thus, in terms of scalability, the results are relevant.\n\nIf my understanding is correct, the appealing aspect of their method is the runtime. In the experiments section, authors show a huge gap in runtime between cvxpy and their implementation. I wonder if cvxpy is mostly implemented in Python, given that the authors implemented their method in C++, I would expect a better performance just from that. Although, the huge gap in runtime suggests that it wouldn't matter.\n\nThe topic is also of good significance given that decision trees are still widely used in practice. \n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083792, "tmdate": 1606915760391, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review"}}}, {"id": "sa6giDZLUDO", "original": null, "number": 3, "cdate": 1604119893021, "ddate": null, "tcdate": 1604119893021, "tmdate": 1605024082856, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review", "content": {"title": "The experimental results are promising but writing lacks important information", "review": "The problem definitions are not somewhat unclear. It seems that (1)-(7) is to find a pruning of the given tree, which are not explicitly mentioned. \n\nIt is not nontrivial that the problem (1) leads to a tree. It would be necessary to have some proposition that ensures the desired properties (e.g., solution represents a tree) of the solution or cites such previously known ones.\n\nThe obtained solutions in (3)-(7) are not discrete and how to round up continuous solutions to discrete ones are not shown. \n\nAs a summary, the current writing lacks important information such as the correctness of the solutions, which significantly reduces the contribution of the paper, even if the experimental results look better than previous work.\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083792, "tmdate": 1606915760391, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review"}}}, {"id": "YiG1x_10a8q", "original": null, "number": 4, "cdate": 1604123982949, "ddate": null, "tcdate": 1604123982949, "tmdate": 1605024082793, "tddate": null, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "invitation": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review", "content": {"title": "Recommending Weak Accept.", "review": "## Summary\n\nThis paper presents a new approach to learn decision trees via sparse relaxation. The approach starts from a mixed-integer program that can simultaneously induce and prune optimal decision trees from data. The proposed technique aims to solve a continuous relaxation of this problem by combining (1) a tree induction routine, which uses isotonic optimization with (2) an efficient implementation that avoids the need for automatic differentiation. The paper includes experiments on publically available datasets, showing how the decision trees produced via sparse relaxation to decision trees produced using other tree induction approaches.\n\n## Pros\n\n- Interesting new technique for a canonical problem. \n- Proposed method produces trees that appear to perform well on classification, regression, and clustering problems.\n- Paper showcases a deft approach to modern algorithm design\n\n## Cons\n\n- No theory or empirical results to characterize the optimality of trees produced via sparse relaxation. \n\n## Rating\n\nI have awarded the paper a 6, though I am open to increasing my score if the authors can address some of the questions I include below. Overall, my chief concern about this work is that it does not include any evidence pertaining to the optimality for trees produced via sparse relaxation. In effect, the main argument for the proposed sparse relaxation technique is that it produces trees that perform well on five datasets. I believe that my concerns should be easy to address \u2013 either by reporting the optimality gap of the trees in the experimental results, or by including comparisons with methods that are guaranteed to find optimal trees. \n\n\n##  Questions / Comments\n\n1. Have you evaluated the optimality of solutions of your method with respect to the exact tree-fitting optimization problem \u2013 i.e., problem (10) where the tree is a solution to the MIP in (2). All decision trees should be feasible with respect to this problem. In turn, it should be possible to report their \"optimality gap.\" Reporting the optimality gap would provide some evidence to evaluate the integrity of the decisions you made in algorithm decision. It could also showcase the value of sparse relaxation as a feasibility heuristic for the MIP-based approach. \n\n2. In the experiments, what is the relative optimality gap of the solutions found by OPTREE? Is it finding certifiably optimal solutions (i.e., solutions with a relative optimality gap of 0%)? If not, it would be useful to include comparisons on a tabular dataset that is small enough for OPTREE to find a certifiably optimal solution. Two datasets to consider here are (1) UCI Mushrooms dataset (which is linearly separable and should be easy for all methods), and (2) the COMPAS dataset (also small, but not linearly separable). \n\nFWIW, the statement that \"Bertsimas & Dunn (2017) phrased the objective of CART as a MIP that could be solved exactly\" is misleading as it suggests that Bertsimas & Dunn (2017) are able to recover globally optimal solutions to the MIP. This is not the case. In many datasets, the proposed method can only find feasible solutions that perform well but have large optimality gaps. \n\n3. I am wondering why the authors did not consider the following methods to train optimal decision trees in their experiments:\n\n- [Optimal Sparse Decision Trees](https://papers.nips.cc/paper/8947-optimal-sparse-decision-trees)\n- [Generalized and Scalable Optimal Sparse Decision Trees](https://proceedings.icml.cc/static/paper_files/icml/2020/3364-Paper.pdf)\n\nThis seems like a weird oversight given that: (i) the paper includes references to both of these works; (ii) both works include reliable implementation that could easily be used to train decision trees in the experimental section (see e.g., https://github.com/Jimmy-Lin/GeneralizedOptimalSparseDecisionTrees). \n\nThe paper currently references both of these works in a way that suggests that they are MIP-based methods. This is misleading. These methods are able to fit certifiably optimal decision trees through specialized algorithms that do not require solving a MIP. I recognize that there has to be a limit to the number of experiments that one can perform. That being said, the trees produced by these methods are an important baseline to include in the experiments since they appear to outperform those produced by MIP-based approaches.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3028/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3028/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Binary Trees via Sparse Relaxation", "authorids": ["~Valentina_Zantedeschi2", "~Matt_Kusner1", "~Vlad_Niculae2"], "authors": ["Valentina Zantedeschi", "Matt Kusner", "Vlad Niculae"], "keywords": ["optimization", "binary trees"], "abstract": "One of the most classical problems in machine learning is how to learn binary trees that split data into meaningful partitions. From classification/regression via decision trees to hierarchical clustering, binary trees are useful because they (a) are often easy to visualize; (b) make computationally-efficient predictions; and (c) allow for flexible partitioning. Because of this there has been extensive research on how to learn such trees. Optimization generally falls into one of three categories: 1. greedy node-by-node optimization; 2. probabilistic relaxations for differentiability; 3. mixed-integer programming (MIP). Each of these have downsides: greedy can myopically choose poor splits, probabilistic relaxations do not have principled ways to prune trees, MIP methods can be slow on large problems and may not generalize. In this work we derive a novel sparse relaxation for binary tree learning. By sparsely relaxing a new MIP, our approach is able to learn tree splits and tree pruning using state-of-the-art gradient-based approaches. We demonstrate how our approach is easily visualizable, is efficient, and is competitive with current work in classification/regression and hierarchical clustering.", "one-sentence_summary": "We present a new sparse differentiable relaxation of mixed-integer programming methods for tree learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zantedeschi|learning_binary_trees_via_sparse_relaxation", "pdf": "/pdf/ab46ab28494b027bd02472ca30b4e348d7801c67.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UjwrAaCped", "_bibtex": "@misc{\nzantedeschi2021learning,\ntitle={Learning Binary Trees via Sparse Relaxation},\nauthor={Valentina Zantedeschi and Matt Kusner and Vlad Niculae},\nyear={2021},\nurl={https://openreview.net/forum?id=9WlOIHve8dU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9WlOIHve8dU", "replyto": "9WlOIHve8dU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3028/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083792, "tmdate": 1606915760391, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3028/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3028/-/Official_Review"}}}], "count": 14}