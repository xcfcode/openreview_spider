{"notes": [{"id": "ryeQmCVYPS", "original": "HkgoUvSOwB", "number": 1031, "cdate": 1569439258750, "ddate": null, "tcdate": 1569439258750, "tmdate": 1577168223328, "tddate": null, "forum": "ryeQmCVYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "d8vmM9LwzD", "original": null, "number": 1, "cdate": 1576798712724, "ddate": null, "tcdate": 1576798712724, "tmdate": 1576800923718, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers wondered about the practical application of this method, given that the performance was lower.  The reviewers were also surprised by some of your claims and wanted you to explore them more deeply.  \n\nOn the positive side, the reviewers found your experiments to be very thorough.  You also performed additional experiments during the rebuttal period.  We hope that those experiments will help you to build a better paper as you work towards publishing this work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708378, "tmdate": 1576800256788, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Decision"}}}, {"id": "S1x5zqVhiB", "original": null, "number": 4, "cdate": 1573829137950, "ddate": null, "tcdate": 1573829137950, "tmdate": 1573829137950, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "rylp1fFAtS", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank Reviewer #3 for the feedback and suggestions. The suggestions are really helpful in further improving our work.\n\nRegarding sampling of 4000 images in the patch experiment with confidence > 99%:\n\nThe phenomenon is similar between the images that are correctly predicted with confidence score > 99% and < 99%. We set that threshold for reducing the effects of test accuracy.\n\n\n\n\nRegarding the adversarial training:\n\nDuring the rebuttal, we adversarially trained a defective CNN under the same setting described in [1] and reach 51.6% successful defense rate against the default PGD attack (\\ell_\\infty = 8 and 7 steps) used in training, which outperforms the standard CNN (50.0%). And the test accuracy will drop from 87.3% to 83.3%.\n\n[1] Towards Deep Learning Models Resistant to Adversarial Attacks, https://arxiv.org/abs/1706.06083\n\n\n\n\nRegarding the experiments on ImageNet-C, Stylized-ImageNet and etc:\n\nThanks for the suggestion. In our paper, we want to prove this point by random shuffling experiments. We believe adding such experiments will strong the conclusions.\n\n\n\n\nRegarding the experiments of the original (neuron-wise) dropout:\n\nSince there are fewer people would adopt this kind of dropout into CNNs, we choose to presents the results of two popular ways. We will seriously consider this point in revision.\n\n\n\n\nRegarding a stronger attacker with a larger number of iterations and random restarts on white-box, more thorough  gray-box experiments, and visualizing the loss gradients:\n\nThanks for those valuable suggestions. We really appreciate it and will conduct them to strengthen the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1031/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeQmCVYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1031/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1031/Authors|ICLR.cc/2020/Conference/Paper1031/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162329, "tmdate": 1576860557881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment"}}}, {"id": "HylPFmN3iH", "original": null, "number": 3, "cdate": 1573827455297, "ddate": null, "tcdate": 1573827455297, "tmdate": 1573827455297, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "H1gO5pmhsS", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment", "content": {"title": "Response to Reviewer#3 [cont'd]", "comment": "Regarding the \u201ctexture\u201d and \u201cshape\u201d, and how does destroying \"texture\" affect objects with significantly lower size/scale in the image.\n\nThanks for pointing this out. In our paper, the shape means the boundaries of objects and the texture means the repeated patterns over objects. It is hard to explicitly find the effects of the small objects with destroying \"texture\". But, from the results of random shuffling experiments, you might also think such destroying would affect the recognization of small objects. Take Figure 3 as an example, our humans can still recognize the Ailurus fulgens according to the noses and ears, but the model would fail in this case.\n\n\n\n\nRegarding the suppress texture it would seem more effective to always use the same mask, but that apparently does not work:\n\nAccording to Table 4, the 0.5-Bottom_{SM} indeed outperforms 0.5-Bottom, which is aligned with the analysis. But the 0.1-Bottom_{SM} is worse than 0.1-Bottom. We indicate that the randomness in generating masks in different channels involves various topological structures for local feature extraction instead of learning. When the keep probability is very low, the way of using the same mask across channels will result in limited topological structures for local feature extraction.\n\n\n\n\nRegarding which networks were actually used to generate the adversarial examples:\n\nWe generate adversarial examples against the standard CNNs. For each experiment, we illustrated the specific setting in the corresponding descriptions. \n\n\n\n\nRegarding the incoherent mixture of datasets:\n\nIn our paper, we mainly show the results on CIFAR-10 and show the qualitative results on both CIFAR-10 and Tiny-ImageNet. To better evaluate our methods, we also perform experiments on MNIST and ImageNet. The inconsistent mainly due to the compared methods and computation cost. The improvements of robustness are consistent across datasets. But currently, we can not find the visual samples that can fool humans on ImageNet trained models.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1031/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeQmCVYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1031/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1031/Authors|ICLR.cc/2020/Conference/Paper1031/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162329, "tmdate": 1576860557881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment"}}}, {"id": "H1gO5pmhsS", "original": null, "number": 2, "cdate": 1573825935869, "ddate": null, "tcdate": 1573825935869, "tmdate": 1573826020773, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "SyxFZjSItB", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank Reviewer #3 for the thorough analysis and suggestions. We will try our best to address the concerns and are open to further discussions. \n\n\nRegarding the one can throw away 90% of the responses in the early conv layers with hardly any performance loss and simply reducing the resolution may also destroy the adversarial pattern:\n\nWe also find this point very surprising that freezing quite a lot of neurons in the network would not drastically degrade the performance, and validated it in five architectures (DenseNet-121, SENet18, VGG-19, ResNet-18, ResNet-50). To ensure this, we attach the codes in revision. Please refer to the link and have a check. \n\nBesides, there are three points of our findings that may help us understand this point. \nThe number of parameters in the network is not reduced after including defective neurons.\nTo obtain high test accuracy, it is very important to introduce various patterns for defective neurons arrangement across channels. According to Table 4, if the patterns are the same across channels, the test accuracy will significantly drop (from 0.1-Bottom 87.7 to 0.1-Bottom_{SM} 74.3).\nAccording to Table 4, applying the method on Bottom layers are sensitive than on Top layers about the test accuracy.\n\nDuring rebuttal, we downsample the images in CIFAR-10 to 8x8 and find the test accuracy will drop to 77.3% which is a very low value. Some related works ([1], [2]) have studied the effects of input transformations on adversarial robustness.  According to their results, those transformations will indeed destroy the adversarial pattern to some degree but also decrease the test accuracy a lot.\n\n[1] Countering Adversarial Images using Input Transformations, https://arxiv.org/abs/1711.00117\n[2] Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks, https://arxiv.org/abs/1704.01155\n\n\n\n\nRegarding if defective convolutions indeed become popular, then an attacker would obviously know about that and also use a defective network to generate his adversarial examples:\n\nWe have considered this point and conducted thorough experiments about the gray-box setting in Appendix A.7. For the proposed method, we actually found there are two kinds of gray-box settings. One way is to generate adversarial examples against one trained neural network and test those images on a network with the same structure but different initializations. The other way is specific to our defective models. We generate adversarial examples on one trained defective CNN and test them on a network with the same keep probability but different samplings of defective neurons. The results show the proposed model outperforms than standard models in the gray-box setting and, thus, alleviate the mentioned issues.\n\n\n\n\nRegarding the additive Gaussian noise:\n\nAccording to Figure 5, networks actually perform very well when the scale of Gaussian noise is small, which is aligned with your analysis. Indeed, it is not a weakness for networks cannot resist the Gaussian noise if adding noise of the wrong magnitude. But, as the noise scale changing from 0/255 to 64/255 in \\ell_\\infty, we can clearly see the performance of the proposed model is consistently higher than the standard one.  We believe this phenomenon is non-trivial. In addition, [3] uses empirical evidence to points out that a successful adversarial defend method should also effectively defense against images with additive Gaussian noise.\n\n[3] Adversarial examples are a natural consequence of test error in noise, https://arxiv.org/abs/1901.10513\n\n\n\n\nRegarding the adversarial noise in the paper looks a lot stronger than normal:\n\nThe noise for the visual examples of CIFAR-10 is 8/255 and 16/255 in \\ell_\\infty. The noise for the visual examples of Tiny-ImageNet is 16/255 and 32/255 in \\ell_\\infty. You can check this point by directly crop the images from the paper and compare it with the original images. We believe those scales are commonly used in the related papers. The reason why noise looks stronger than normal is probably that our model finds a suitable direction for those noises.\n\n\n\n\nRegarding these are in fact not adversarial examples of class A, but examples of a different class wrongly labeled as class A:\n\nPlease refer to Figure 1, the images in the first row are generated from the images in the second row. The names below the first row are the predicted class by our models, and the names below the second row are the ground-truth class. All the ground-truth labels are correct. The new predicted labels are corresponding to the images that demonstrate our claim that some adversarial examples generated by our model can even fool humans.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1031/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeQmCVYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1031/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1031/Authors|ICLR.cc/2020/Conference/Paper1031/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162329, "tmdate": 1576860557881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment"}}}, {"id": "HyesL3Q3ir", "original": null, "number": 1, "cdate": 1573825618595, "ddate": null, "tcdate": 1573825618595, "tmdate": 1573825988013, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "B1xJXVihYr", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank Reviewer #2 for the feedback and suggestions. The suggestions are helpful, and most concerns are already considered in the paper. \n\n\nRegarding the proposed method is simply to reduce the test accuracy and thereby increase the adversarial robustness:\n\nPlease refer to the first paragraph on Page 7. To eliminate this point, all numbers in our paper except Table 2 are calculated on the adversarial examples whose corresponding original images can be classified correctly by the tested model. \n\n\n\n\nRegarding the model they present achieves significantly lower test accuracy on clean data than a standard network:\n\nWe want to emphasize that the proposed method has the state-of-the-art performance on transfer-based black-box defense while maintaining the highest test accuracy. In our experiments, we also demonstrate the effectiveness of the proposed method against the decision-based attack which is a more realistic setting.\n\n\n\n\nRegarding the correlations between test accuracy and adversarial robustness:\n\nThe phenomena are specific to the proposed method. Different keep probabilities would bring different improvements on the robustness and declinations on test accuracy. It is hard to apply similar experiments on standard models.\n\n\n\n\nRegarding the claim that the white box attacks are semantically meaningful and so could fool a human, but no human evaluation is presented and the examples which are illustrated do not demonstrate this property:\n\nThis work is an attempt to discover a kind of CNNs which learn features that are more aligned with human perception. So far, there are no works can achieve better or the same effects on CIFAR-10 and Tiny-ImageNet by modifying the architecture of CNNs. Please compare the generated adversarial examples along with their corresponding predicted classes.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1031/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryeQmCVYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1031/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1031/Authors|ICLR.cc/2020/Conference/Paper1031/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162329, "tmdate": 1576860557881, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1031/Authors", "ICLR.cc/2020/Conference/Paper1031/Reviewers", "ICLR.cc/2020/Conference/Paper1031/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Official_Comment"}}}, {"id": "SyxFZjSItB", "original": null, "number": 1, "cdate": 1571343105022, "ddate": null, "tcdate": 1571343105022, "tmdate": 1572972521268, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper deals with robustness against adversarial attacks. It proposes to blank out large parts of the early convolution layers in a CNN, in an attempt to shift the focus from \"texture\" to \"shape\" features. This does seem to improve robustness against adversarial examples, with only a small decrease in general classification performance. The explanation for this, on the other hand, is not really convincing.\n\nThe idea is simple: adversarial noise introduces high-frequency texture patterns, so destroy those by blanking out a large portion of the neurons in a layer. Quite obviously, this can have an influence - when blanking out 90% of the pixels (as suggested in the paper), the effective sampling resolution goes down by a factor of 3 in each axis, and high-frequency patterns are a lot less likely to be picked up. It does, however, remain unclear why this approach is particularly useful, or why it even works at all. On the one hand, an easier way to surely get rid of those patterns is simply to blur the images accordingly before feeding them to the network. That baseline is missing. On the other hand it is quite an outrageous claim that one can throw away 90% of the responses in the early conv layers with hardly any performance loss. I don't doubt the experiments results, but if one discovers something like that, it needs to be explained. The network has  a lot lessless capacity, effectively loses a factor 3 in resolution, but performance seemingly stays almost the same!\n\nThis brings me to another point. It is never really defined what is meant by \"texture\" respectively \"shape\". By reverse-engineering from the paper text I gather that \"shape\" is simply texture at a significantly lower resolution. But then how does destroying \"texture\" affect objects with significantly lower size/scale in the image?\n\nA few technical questions remain unclear. First, the adversarial noise in the paper looks a lot stronger than normal. It is easily visible, and in that sense not \"adversarial\". In fact the paper openly states that they \"even fool humans\". So since the labels are human-annotated, these are in fact not adversarial examples of class A, but examples of a different class wrongly labeled as class A...\n\nAnother question is how much \"texture\" is lost, since the paper finds it important to use a different random mask for each filter in a layer. So does that really suppress so much texture? Almost all pixels will be seen by some non-defective filters, so it would seem that the hi-res information is implicitly still there. To really suppress texture it would seem more effective to always use the same mask, but that apparently does not work. Why?\n\nWhat completely confused me was which networks were actually used to generate the adversarial examples. Are these adversarial against the standard CNN or against the defective one? If defective convolutions indeed become popular, then an attacker would obviously know about that and also use a defective network to generate his adversarial examples. \n\nOne thing I did not understand is the incoherent mixture of datasets. The first experiment with the reshuffled tiles is done on ImageNet. But then the actual experiments regarding robustness against attacks are done only with Cifar-10. Why suddenly switch? And then, many of the visual examples are from TinyImageNet. Why switch again? And on that account, since apparently all of them were processed, is the behaviour consistent across datasets?\n\nA note aside, I am not sure it is a good idea to treat additive Gaussian noise in the same way as adversarial patterns. Some level of noise that is at least approximately Gaussian is present in almost all images. So it is actually a good thing if a network learns the magnitude of that noise, so as to separate it from the signal, i.e., the brightness variations that are informative and not Gaussian noise. In that view it is a good thing, not a weakness, if adding noise of the wrong magnitude misleads the network (although, ideally, it should of course flag the image as being out-of-distribution).\n\nIn summary, I find the results interesting - in particular also the one on tiled and reshuffled images. But I am at this point not convinced by the explanations. If one can indeed throw away 90% of all responses in the low layers then that would be a rather big thing that needs an explanation. Unless the task is easy enough to be solved with 3x lower resolution - but in that case I would expect that simply reducing the resolution would also destroy the adversarial pattern. I am on the fence, but  in its current state the paper leaves too many open questions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1031/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1031/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575346582883, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1031/Reviewers"], "noninvitees": [], "tcdate": 1570237743409, "tmdate": 1575346582896, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Official_Review"}}}, {"id": "B1xJXVihYr", "original": null, "number": 2, "cdate": 1571759127113, "ddate": null, "tcdate": 1571759127113, "tmdate": 1572972521232, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a technique for adversarial defense, employing what the authors refer to as \"defective  convolution layers\".  It attempts to use such layers to provide more adversarially robust models. The paper is easy to follow and well written, but the experimentation is lacking, and so the contribution is limited.\n\nDefective  Convolution  layers make use of a random fixed masking matrix, to fix some number of neurons to constant values, effectively removing the incoming weights of these neurons from the optimization problem. The outgoing weights of the masked neurons are effectively additional bias terms for the layer above. Applying this technique to a fully connected layer would be equivalent to training with a fully connected layer with a smaller size.\n\nThe paper presents the accuracy of such models on a variety of black box attacks. They focus on black box for two reasons, neither of which are particularly convincing. They claim that the white box attacks are semantically meaningful and so could fool a human, but no human evaluation is presented and the examples which are illustrated do not demonstrate this property.  They also claim to focus on black box attacks because is it more practical in a real world setting. However the model they present achieves significantly lower test accuracy on clean data than a standard network, so the practical deployment of such a model seems unlikely in its current implementation. \n\nThe paper presents thorough ablation studies on the architectural choices that go into this model. This is a positive quality of the work. However they do not compare standard networks with similar test accuracy to their defective models. As they note in the paper and in the appendix, there is a correlation between test accuracy and adversarial robustness. Based on the findings which they present, it is unclear if the effect of their \"Defective Convolution Layer\" is simply to reduce the test accuracy and thereby increase the adversarial robustness. This issue must be addressed for the work to contribute to the adversarial literature in a meaningful way. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1031/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1031/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575346582883, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1031/Reviewers"], "noninvitees": [], "tcdate": 1570237743409, "tmdate": 1575346582896, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Official_Review"}}}, {"id": "rylp1fFAtS", "original": null, "number": 3, "cdate": 1571881445057, "ddate": null, "tcdate": 1571881445057, "tmdate": 1572972521186, "tddate": null, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "invitation": "ICLR.cc/2020/Conference/Paper1031/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "* Summary *\nThe paper proposes defective convolutional layers as a measure of defense against adversarial attacks on deep neural networks. This layer sets the outputs of a randomly sampled but *fixed* set of neurons in the convolutional layers to zero during training and testing. The authors claim that defective convolutional layers encourage the model to pick up features other than local textures, e.g. shape information. The shape-vs-texture tradeoff is supported by experiments showing that defective CNNs perform worse than normal CNNs on images with permuted patches and that adversarial examples with larger epsilons exhibit more semantic shapes. The detailed experiment section evaluates the method on transfer-based, gray-box and black-box adversarial attacks,\tincluding Gaussian noise. Additionally, it provides ablation studies on the keep-probability and position of the defective layer.\n\t\nOverall I think this is a valuable contribution to a topic of high interest for ICLR and should be accepted. The method is simple to implement, has minor impact on the test accuracy and seems to increase robustness measures under the proposed settings across all of the tested architectures. However, the evaluation is lacking w.r.t. natural robustness, more detailed evaluation on the gray-box, black-box and white-box attacks. In the white-box setting (Table 8) a stronger attacker with a larger number of iterations and random restarts should be used in order to ensure that the difference in defense performance is real.\n\t\nThe experiment section should have a stronger focus on the gray-box attacks where the source network also has defective layers, since the method alters the network architecture and presumably learn a different set of features. However, the lack of transfer from \"normal models\",\tcan also be seen as supporting argument for the model picking up different, potentially robust, features, following the argument in [4].\n\t\nBecause the idea is motivated from the texture-vs-shape discussion [1], an evaluation on natural/empirical robustness under image corruptions [2], e.g. CIFAR10-C or ImageNet-C, and/or comparison\tto a network trained on Stylized-ImageNet, should be conducted.\n\t\n\t\nAdditional Feedback:\n- Since the method is closer to the original (neuron-wise) dropout than to SpatialDropout and DropBlock, including this in the evaluation would be appreciated.\n- To show the alignment of the adversarial gradients with the human-vision, the authors could visualize the loss gradients similar to [3] (Figure 2b)\n- For comparison in A.4, the median squared L2-distance of the adversarially trained Madry model under the decision-based attack would be helpful.\n- Why is the sampling of 4000 images in the patch experiment 3.2 done? What happens to the images that are correctly predicted but with <99% confidence?\n- What happens if this method is combined with adversarial training?\n\t\n[1] ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness, https://arxiv.org/abs/1811.12231\n[2] Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations, https://arxiv.org/abs/1807.01697\n[3] Robustness May Be at Odds with Accuracy, https://arxiv.org/abs/1805.12152\n[4] Adversarial Examples Are Not Bugs, They Are Features, https://arxiv.org/abs/1905.02175 "}, "signatures": ["ICLR.cc/2020/Conference/Paper1031/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1031/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defective Convolutional Layers Learn Robust CNNs", "authors": ["Tiange Luo", "Tianle Cai", "Xiaomeng Zhang", "Siyu Chen", "Di He", "Liwei Wang"], "authorids": ["luotg@pku.edu.cn", "caitianle1998@pku.edu.cn", "zhan147@usc.edu", "siyuchen@pku.edu.cn", "dihe@microsoft.com", "wanglw@cis.pku.edu.cn"], "keywords": ["adversarial examples", "robust machine learning", "cnn structure", "deep feature representations"], "TL;DR": "We propose a technique that modifies CNN structures to make predictions more relying on shape information and improve the defense ability against several types of attack.", "abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "pdf": "/pdf/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "paperhash": "luo|defective_convolutional_layers_learn_robust_cnns", "code": "https://drive.google.com/open?id=1ovRyQhW4jKKbxH8QeCOCOCcaaXA4eb8_", "original_pdf": "/attachment/696c255b36858e0908e0fdfed5e0ea303b62c35b.pdf", "_bibtex": "@misc{\nluo2020defective,\ntitle={Defective Convolutional Layers Learn Robust {\\{}CNN{\\}}s},\nauthor={Tiange Luo and Tianle Cai and Xiaomeng Zhang and Siyu Chen and Di He and Liwei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=ryeQmCVYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryeQmCVYPS", "replyto": "ryeQmCVYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1031/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575346582883, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1031/Reviewers"], "noninvitees": [], "tcdate": 1570237743409, "tmdate": 1575346582896, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1031/-/Official_Review"}}}], "count": 9}