{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1391837220000, "tcdate": 1391837220000, "number": 3, "id": "ybOCyBR3ijyWz", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DnsBnbl6TQD6t", "replyto": "DnsBnbl6TQD6t", "signatures": ["anonymous reviewer 0514"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "review": "This paper proposes a multi-task deep neural network approach to solve the task of image retrieval. A dataset of clickthrough  data is used to train the network.\r\n\r\nThe main contribution of this paper is the proposed multi-task DNN method for image retrieval and the ring training. This multi-task approach consists of adding weights specific to each query (basically a supplemental fully-connected layer) before binary classification. The authors compare their approach to binary DNNs and multi-class DNN. However, I would have like to see a multi-label DNN as well for comparison.\r\n\r\nQuality:\r\nI found the quality of the language passable. There were several syntactic, grammatical errors and typos which made the paper hard to understand. The paper could have used more polishing. In general, I have found the paper hard to read, and the authors did not get their points through clearly.\r\n\r\nGeneral comments:\r\nWhat loss function was used? I don't understand why the authors did not give it explicitly.\r\n\r\nIn the ring training pseudo code, it is not mentioned how j is defined, is there an iteration on all values, or is it chosen randomly? It makes a lot of difference how j is treated if the classes are unbalanced. This should be explained clearly.\r\n\r\nIn section 4, the architecture of the network is presented, but nothing is mentioned about which weights were query-specific in the multi-task case for the CIFAR-10 experiment.\r\n\r\nsection 4.2: 'In general, binary DNN performs consistently worse for the severe overfitting problem.' This sentence does not make a lot of sense, and does not reflect the results very well.\r\n\r\nI don't really see the point of using dataset 2. The difference in results between d1 and d2 are small, and do not, in my opinion, demonstrate what the authors claim it shows. I think it only clutters the paper.\r\n\r\nThe authors use the term significantly without showing confidence intervals. Perhaps another term should be used.\r\n\r\nSection 5.1: 'multi-class DNN is infeasible for such large number of queries'. This would require more explanation. Why does the multi-task approach scale better than the multi-class approach? Since the parameters are not shared, doesn't the multi-task DNN require even more parameters than the multi-class? If this is not the case, please explain more clearly your point.\r\n\r\nIn section 5.3, why not use the activation of the classification layer as an affinity measure instead of training SVMs"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "decision": "submitted, no decision", "abstract": "Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.", "pdf": "https://arxiv.org/abs/1312.4740", "paperhash": "ma|learning_highlevel_image_representation_for_image_retrieval_via_multitask_dnn_using_clickthrough_data", "keywords": [], "conflicts": [], "authors": ["Wei-Ying Ma", "Tiejun Zhao", "Kuiyuan Yang", "Wei Yu", "Yalong Bai"], "authorids": ["wyma@microsoft.com", "tjzhao@hit.edu.cn", "kuyang@microsoft.com", "w.yu@hit.edu.cn", "ylbai@mtlab.hit.edu.cn"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391806080000, "tcdate": 1391806080000, "number": 2, "id": "9M6x9jeOhSCO_", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DnsBnbl6TQD6t", "replyto": "DnsBnbl6TQD6t", "signatures": ["anonymous reviewer 2839"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "review": "Pros: \r\n\u2022\tThe use of multi-task DNNs for clickthrough data makes sense and provides good results\r\n\r\n\r\nCons: \r\n\u2022\tThe idea of multi-task DNNs has been explored before in the literature and is not new. For example, in speech in multi-lingual speech processing this has been explored before. \r\no\tThis paper does joint training of the shared and task-specific components: K. Vesley et al, \u201cThe Language-independent bottleneck features,\u201d in Proc. ICASSP 2012. \r\no\tThis paper also does joint training: Z. Tuske et al, \u201cInvestigation on Cross-and Multilingual MLP Features under Matched and Mismatched Acoustical Conditions,\u201d in Proc. ICASSP 2013.\r\no\tThis paper does separate training of shared and task-specific components: Samuel Thomas, Sriram Ganapathy and Hynek Hermansky, Multilingual MLP Features For Low-resource LVCSR Systems, ICASSP, Kyoto, Japan, March 2012\r\n\u2022\tDetails of and Ring training are not clearly described. Also, relating this to prior art and justifying why this is the best approach is missing.\r\n\u2022\tThe paper could be written much better. There are many spelling/grammar mistakes in the paper \r\n\r\nHere are my comments per section:\r\n\u2022\tSection 1, page 1: expand on SIFT, HOG and LBP\r\n\u2022\tSection 1, page 1: Your statement on CNNs being good doesn\u2019t fit with the rest of the sentence\r\n\u2022\tSection 1, page 2: exiting \u2192 existing\r\n\u2022\tSection 2: There needs to be a discussion on Multi-task DNNs relation to prior work and why your idea is novel. For example, see the papers listed above for speech processing.\r\n\u2022\tSection 2, page 3: What is the loss function that you use. \r\n\u2022\tSection 3, page 3: I found the discussion of Ring Training very confusing. You should describe the algorithm 1 in more detail in the text. Also, ring training is not the only way to train Multi-task DNNs. Why did you use this approach as opposed to other ideas in the literature, and why is it potentially better?\r\n\u2022\tSection 4, page 3 CIFRA\u2192 CIFAR\r\n\u2022\tSection 5.2, page 6: Why did you only add dropout to the first fully connect layer? Did you tune this optimally? Sometimes it helps to add dropout to multiple layers. You should state if it\u2019s tuned properly or not.\r\n\u2022\tSection 5.3 page 6: Provide a reference for DCG"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "decision": "submitted, no decision", "abstract": "Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.", "pdf": "https://arxiv.org/abs/1312.4740", "paperhash": "ma|learning_highlevel_image_representation_for_image_retrieval_via_multitask_dnn_using_clickthrough_data", "keywords": [], "conflicts": [], "authors": ["Wei-Ying Ma", "Tiejun Zhao", "Kuiyuan Yang", "Wei Yu", "Yalong Bai"], "authorids": ["wyma@microsoft.com", "tjzhao@hit.edu.cn", "kuyang@microsoft.com", "w.yu@hit.edu.cn", "ylbai@mtlab.hit.edu.cn"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390993380000, "tcdate": 1390993380000, "number": 3, "id": "pp6Q9YYzIx9Tn", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "DnsBnbl6TQD6t", "replyto": "3FowFUkEs_3Xh", "signatures": ["Wyvern Bai"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Sorry about my mistake. Error distributions of Multi-class DNN is shown in figure 5."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "decision": "submitted, no decision", "abstract": "Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.", "pdf": "https://arxiv.org/abs/1312.4740", "paperhash": "ma|learning_highlevel_image_representation_for_image_retrieval_via_multitask_dnn_using_clickthrough_data", "keywords": [], "conflicts": [], "authors": ["Wei-Ying Ma", "Tiejun Zhao", "Kuiyuan Yang", "Wei Yu", "Yalong Bai"], "authorids": ["wyma@microsoft.com", "tjzhao@hit.edu.cn", "kuyang@microsoft.com", "w.yu@hit.edu.cn", "ylbai@mtlab.hit.edu.cn"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390993140000, "tcdate": 1390993140000, "number": 1, "id": "x1YC1qLv58Zvi", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "DnsBnbl6TQD6t", "replyto": "3FowFUkEs_3Xh", "signatures": ["Wyvern Bai"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments. \r\n\r\nYou are right that the 2-way softmax is similar to sigmoid with 1 unit. Actually we used the activity function like sigmoid and tanh but did not get  better result than using 2-way softmax, thus we used 2-way softmax.\r\n\r\nOur experiments results on CIFAR-10 show that the multi-class DNN can not work well when the data is heavy tail. The experiments results in table1 showed Multi-task DNN + ring training was better than Multi-class DNN signally in dataset_1, which is a heavy tail distributed dataset. Error distributions of Multi-class DNN is shown in figure, it showed that the data distribution can effect predictions of multi-class DNN greatly. Meanwhile the experiments in dataset_2 show that trying to discriminate categories describing the same concept will hurt multi-class DNN, and there are many queries in practice are synonymous.\r\n\r\nFor mulit-class DNN, the number parameters of last full connect layer with millions outputs for million queries is too large, thus we described  in the paper that the multi-class DNN is infeasible for this situation. \r\n\r\nAbout why the SVM needed, we considering that the bag of word are trained by SVM,  to explain the feature learned by our method is better than bag of words we also used SVM to train the ranker.\r\n\r\nThank you again for your comments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "decision": "submitted, no decision", "abstract": "Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.", "pdf": "https://arxiv.org/abs/1312.4740", "paperhash": "ma|learning_highlevel_image_representation_for_image_retrieval_via_multitask_dnn_using_clickthrough_data", "keywords": [], "conflicts": [], "authors": ["Wei-Ying Ma", "Tiejun Zhao", "Kuiyuan Yang", "Wei Yu", "Yalong Bai"], "authorids": ["wyma@microsoft.com", "tjzhao@hit.edu.cn", "kuyang@microsoft.com", "w.yu@hit.edu.cn", "ylbai@mtlab.hit.edu.cn"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390993140000, "tcdate": 1390993140000, "number": 2, "id": "O9mROjhpZ_Lpw", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "DnsBnbl6TQD6t", "replyto": "3FowFUkEs_3Xh", "signatures": ["Wyvern Bai"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your comments. \r\n\r\nYou are right that the 2-way softmax is similar to sigmoid with 1 unit. Actually we used the activity function like sigmoid and tanh but did not get  better result than using 2-way softmax, thus we used 2-way softmax.\r\n\r\nOur experiments results on CIFAR-10 show that the multi-class DNN can not work well when the data is heavy tail. The experiments results in table1 showed Multi-task DNN + ring training was better than Multi-class DNN signally in dataset_1, which is a heavy tail distributed dataset. Error distributions of Multi-class DNN is shown in figure, it showed that the data distribution can effect predictions of multi-class DNN greatly. Meanwhile the experiments in dataset_2 show that trying to discriminate categories describing the same concept will hurt multi-class DNN, and there are many queries in practice are synonymous.\r\n\r\nFor mulit-class DNN, the number parameters of last full connect layer with millions outputs for million queries is too large, thus we described  in the paper that the multi-class DNN is infeasible for this situation. \r\n\r\nAbout why the SVM needed, we considering that the bag of word are trained by SVM,  to explain the feature learned by our method is better than bag of words we also used SVM to train the ranker.\r\n\r\nThank you again for your comments."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "decision": "submitted, no decision", "abstract": "Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.", "pdf": "https://arxiv.org/abs/1312.4740", "paperhash": "ma|learning_highlevel_image_representation_for_image_retrieval_via_multitask_dnn_using_clickthrough_data", "keywords": [], "conflicts": [], "authors": ["Wei-Ying Ma", "Tiejun Zhao", "Kuiyuan Yang", "Wei Yu", "Yalong Bai"], "authorids": ["wyma@microsoft.com", "tjzhao@hit.edu.cn", "kuyang@microsoft.com", "w.yu@hit.edu.cn", "ylbai@mtlab.hit.edu.cn"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390861260000, "tcdate": 1390861260000, "number": 1, "id": "3FowFUkEs_3Xh", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DnsBnbl6TQD6t", "replyto": "DnsBnbl6TQD6t", "signatures": ["anonymous reviewer e3ba"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "review": "The paper presents an architecture to learn simultaneously multiple image rankers, all of which sharing low-level layers of a deep neural network, while keeping their last layer separate for each ranking task. Since the distribution of images per query is far from uniform, this helps 'poor' queries learning good image representations from 'rich' queries. The query specific model is a 2-way softmax (good or bad image for that query). This is not exactly what I would call a ranker, but rather a classifier. In fact, I didn't see how this architecture differs from a DNN trained with logistic regression (so that each query has one output unit, which should be 1 if the image corresponds to the query, and 0 otherwise); it seems that a softmax with 2 units is similar to a single sigmoid unit, no?\r\nThe paper shows two series of experiments, one on CIFAR-10, and one on MSR-Bing Image Retrieval challenge dataset, which is a real image ranking problem.\r\nResults show that the proposed approach works better than the two compared approaches (the Binary DNN consists of one DNN per query, which cannot work when the data is heavy tail; the multi-class DNN, on the other hand, seems very similar to the proposed approach, and I did not understand why this one would not scale to millions of queries, while the proposed approach would). Finally, apparently, an SVM is trained after the features are fixed. I did not understand this part neither. Why is the SVM needed anyway?\r\nThe paper is not very well written, with a poor level of english, and several details missing, both in describing the algorithm and in the experimental section."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "decision": "submitted, no decision", "abstract": "Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.", "pdf": "https://arxiv.org/abs/1312.4740", "paperhash": "ma|learning_highlevel_image_representation_for_image_retrieval_via_multitask_dnn_using_clickthrough_data", "keywords": [], "conflicts": [], "authors": ["Wei-Ying Ma", "Tiejun Zhao", "Kuiyuan Yang", "Wei Yu", "Yalong Bai"], "authorids": ["wyma@microsoft.com", "tjzhao@hit.edu.cn", "kuyang@microsoft.com", "w.yu@hit.edu.cn", "ylbai@mtlab.hit.edu.cn"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387578900000, "tcdate": 1387578900000, "number": 20, "id": "DnsBnbl6TQD6t", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "DnsBnbl6TQD6t", "signatures": ["wyma@microsoft.com"], "readers": ["everyone"], "content": {"title": "Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data", "decision": "submitted, no decision", "abstract": "Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.", "pdf": "https://arxiv.org/abs/1312.4740", "paperhash": "ma|learning_highlevel_image_representation_for_image_retrieval_via_multitask_dnn_using_clickthrough_data", "keywords": [], "conflicts": [], "authors": ["Wei-Ying Ma", "Tiejun Zhao", "Kuiyuan Yang", "Wei Yu", "Yalong Bai"], "authorids": ["wyma@microsoft.com", "tjzhao@hit.edu.cn", "kuyang@microsoft.com", "w.yu@hit.edu.cn", "ylbai@mtlab.hit.edu.cn"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}