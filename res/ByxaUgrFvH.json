{"notes": [{"id": "ByxaUgrFvH", "original": "r1x5hcgFPB", "number": 2341, "cdate": 1569439829238, "ddate": null, "tcdate": 1569439829238, "tmdate": 1586751271852, "tddate": null, "forum": "ByxaUgrFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "YsrnjBMa3", "original": null, "number": 1, "cdate": 1576798746642, "ddate": null, "tcdate": 1576798746642, "tmdate": 1576800889454, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes the Mutual Information Gradient Estimator (MIGE) for estimating the gradient of the mutual information (MI), instead of calculating it directly. To build a tractable approximation to the gradient of MI, the authors make use of Stein's estimator followed by a random projection. The authors empirically evaluate the performance on representation learning tasks and show benefits over prior MI estimation methods.\nThe reviewers agree that the problem is important and challenging, and that the proposed approach is novel and principled. While there were some concerns about the empirical evaluation, most of the issues were addressed during the discussion phase. I will hence recommend acceptance of this paper. We ask the authors to update the manuscript as discussed.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707192, "tmdate": 1576800255372, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Decision"}}}, {"id": "HyeJ_8BnoB", "original": null, "number": 15, "cdate": 1573832295283, "ddate": null, "tcdate": 1573832295283, "tmdate": 1573832295283, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "rylHOHJsiS", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 1 ", "comment": "Thank you for recommence. \nWe will add the statement related to [1] to the appendix in the revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "rJxEhSHhoH", "original": null, "number": 14, "cdate": 1573832107667, "ddate": null, "tcdate": 1573832107667, "tmdate": 1573832107667, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "r1lRPLxjjr", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your response\n\n>> Q1Q2\nEven some empirical statistics would suffice. I find it misleading to plot only one realization of the gradient estimate. How about plotting 10000 realizations and calculate the variance per value of \\rho? If that number is lower than MINE, it would strengthen your argument.\n\nR\uff1aThank you for your constructive suggestion.We agree that the method of  quantitive measures of  proposed by reviewer  is effective to strengthen our argument. Due to the limited response time, we will add the quantitive measures for tightness and variance of MIGE  in the camera-ready.\n\n\n>> Q3\nOk. Then it\u2019s a fair comparison. If you used their code in that way, please consider citing it as such.\n\nR\uff1aYes, of course.\n\n>> Q4\nOk. Do you plan to include that table in the revision?\nYes, of course.\n\n>> Q5\nIn other comments in this thread you mentioned the concurrent work [1]. How does [1] relate to the statement you make in the paper? \nWe will add the statement related to [1] to the appendix\n\n>> Q6Q8Q9\nUnderstood. Please include in the revised version\n\nR\uff1aYes, of course.\n\n[1] https://openreview.net/forum?id=rkxoh24FPH\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "r1lRPLxjjr", "original": null, "number": 13, "cdate": 1573746278377, "ddate": null, "tcdate": 1573746278377, "tmdate": 1573746278377, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "HygathtKor", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thank you for responding\n\n>> On the copy-editor. I don\u2019t think hiring a copy editor is necessary. I recommend some off-the-shelf grammar and spelling checkers. Also, having the paper proofread by colleagues generally helps. They would point out missing definitions of variables, like \u2018AnonReviewer2\u2019 pointed out.\n\n>> Q1Q2\nEven some empirical statistics would suffice. I find it misleading to plot only one realization of the gradient estimate. How about plotting 10000 realizations and calculate the variance per value of \\rho? If that number is lower than MINE, it would strengthen your argument.\n\n>> Q3\nOk. Then it\u2019s a fair comparison. If you used their code in that way, please consider citing it as such.\n\n>> Q4\nOk. Do you plan to include that table in the revision?\n\n>> Q5\nIn other comments in this thread you mentioned the concurrent work [1]. How does [1] relate to the statement you make in the paper? \n\n>> Q6Q8Q9\nUnderstood. Please include in the revised version\n\n[1] https://openreview.net/forum?id=rkxoh24FPH"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer3"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "rylHOHJsiS", "original": null, "number": 12, "cdate": 1573741933407, "ddate": null, "tcdate": 1573741933407, "tmdate": 1573741933407, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "BylD9OC9oS", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for the response. It would be great if you could add the DIM(L) results to the paper and discuss the issues, e.g. in the appendix. \n\nYour paper is a piece of some really solid work. I strongly recommend acceptance!"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "H1ezeV1siH", "original": null, "number": 11, "cdate": 1573741546344, "ddate": null, "tcdate": 1573741546344, "tmdate": 1573741546344, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "B1glaFo5jS", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "R1 : We will revise our paper to include your valuable comments. \n\nR2: Note the performace of Random Project in different layers is different. And the perfomace of the last layer is quite invariant with different RP dimensons. We will   conduct more extensive expreiments with different dimensions, and will show the trend curve in the camera-ready upon acceptance. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "BylD9OC9oS", "original": null, "number": 10, "cdate": 1573738638523, "ddate": null, "tcdate": 1573738638523, "tmdate": 1573738638523, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "HyliJBZciS", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "\n\nR1: We have conducted experiments of applying MIGE for DIM(L) in the CIFAR dataset and we show the results in the following Table. Surprisingly, there is a significant gap to DIM(L).  To analyze this result, we find during training of CIFAR-10, the testing accuracy gets stable after reaching over 50%, while the training accuracy soon reaches 99%.  This may suggest that some regularization technique may be needed to the gradient estimation of DIM(L).\n\nTo our knowledge, the principle of DIM(L) is still  unclear.  As argued in [2],   the success of these methods cannot be attributed to the properties of MI alone, and they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parameterization of the employed MI estimators.  \n\u00a0\nFor MIGE, we are investigating the behind reason, e.g., to investigate the distribution of the patches.\n\n.---------------------------------------------------------------------------------------------------\n.\t\t\t\t\t\t\tCIFAR10\t\t\t\t\tCIFAR100\n.\t\t\t\t\tconv\tfc\t\tY\t\tconv\tfc\t\tY\n.---------------------------------------------------------------------------------------------------\n.DIM(L)(JSD)\t\t\t72.16\t67.99\t66.35\t41.65\t39.60\t39.66\n.DIM(L)(JSD+PM)\t\t73.25\t73.62\t66.96\t48.13\t45.92\t39.60\n.DIM(L)(infoNCE)\t\t75.05\t70.68\t69.24\t44.11\t42.97\t42.74\n.DIM(L)(infoNCE+PM)\t75.21\t75.57\t69.13\t49.74\t47.72\t41.61\n.---------------------------------------------------------------------------------------------------\n.MIGE(L)\t\t\t59.72\t56.14\t54.01\t30.00\t28.96\t27.65\n.---------------------------------------------------------------------------------------------------\n\n\n\n\nR2: Thanks for providing the insight. We can use MINE (as used in [1], and MINE is better than InfoNCE due to the high bias of InforNCE) for estimating MI stored in representations. Due to the limited response time, we will add the metric in the camera-ready.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "B1glaFo5jS", "original": null, "number": 9, "cdate": 1573726648124, "ddate": null, "tcdate": 1573726648124, "tmdate": 1573726648124, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "rkePFjKYor", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Authors", "comment": "\"For experiments, we follow the experiments of Deep InfoMax and Information Bottleneck to set the experimental setup as in [1, 2], and we also refer to their source code [3, 4]. Under these experimental settings, we use our MI Gradient Estimator to replace the MI estimator in Deep InfoMax and Information Bottleneck\"\n\nThanks - Please clarify this in the main text. Also i just looked over the PDF again and i could not find a link to the code you refer to?\n\n\"For definitions of q(z)_\\psi(z) and q(x,z)_\\psi, q(z)_\\psi(z) corresponds to the distribution of representation of x via the encoder E_\\psi as described in Equation (11). As mentioned in our paper, \"we can obtain the samples from the marginal distribution of z by pushing samples from the data empirical distribution p(x) through E\\psi(.) for representation learning.\" q_\\psi is an implicit distribution determined by the encoder parameters \\psi. q(x,z)_\\psi is the joint distribution of (x,z), which is determined by the encoder parameters \\psi.\"\n\nAgain thanks for the clarification - I would strongly encourage you to include this in the main text as well as it is not clear how E_psi, q_psi etc is related.\n\nThanks for providing the STL results - Can you give any intuition for why you see decreased performance when increasing the dimensionality of RP?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "B1xOHQzqiH", "original": null, "number": 8, "cdate": 1573688127551, "ddate": null, "tcdate": 1573688127551, "tmdate": 1573688127551, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "SyeM9cFYjB", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "proofreading", "comment": ">  We will carefully revise our manuscript and hire professional copy editors to proofread our paper.   \n\nThanks; I don't think professional service is necessary. Please just use spell-check and have someone proofread the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "HyliJBZciS", "original": null, "number": 7, "cdate": 1573684450820, "ddate": null, "tcdate": 1573684450820, "tmdate": 1573684450820, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "SyeM9cFYjB", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "response", "comment": "> In addition, we note that the literature has argued that maximizing tighter bounds in DIM(L) leads to worse results [2].\n\nThe above might be true for a specific setting when a more expressive critic is used to estimate the InfoNCE bound but has no significance to the paper. As you point out yourself, you are not estimating MI, but only its gradient. If using MIGE for DIM(L) leads to worse results than in the original paper, then it should be clearly stated and discussed -- it will not diminish the value of this paper. \n\n> We cannot provide the value of MI to evaluate the representation, because MIGE directly estimates the gradient of MI to optimize MI, rather than estimating the value of MI.\n\nWhat you can do is to train a separate critic to estimate InfoNCE as in [2], and evaluate InfoNCE with a large batch size. This result would be very helpful as it would shed light on how much information is actually stored in representations learned with MIGE.\n\nThanks for the experiments on STL-10 and further clarification. I'm hoping to read a response to the above comments before the end of the discussion period."}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "r1gDQ2YKiB", "original": null, "number": 3, "cdate": 1573653535263, "ddate": null, "tcdate": 1573653535263, "tmdate": 1573653847211, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "B1gYH_Xuqr", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (1)", "comment": "Thank you for acknowledging that our idea is interesting and the results are encouraging. We will revise our paper according to the comments and hire a copy-editor to carefully polish the writing.\n\n\nQ1:The estimator for the gradient is shown at first on a toy task. Take a correlated Gaussian distribution and estimate gradients of the MI. The correlated Gaussian has an analytical form of MI, which makes this a useful experiment. The paper claims that this estimator \u2018provides a tighter and smoother\u2019 gradient estimate. I don\u2019t see how this experiment and this claim tie together. \n\nR: In terms of \" tighter and smoother\u2019 gradient estimate\", Fig.1 shows that the proposed MI Gradient Estimator is the tighter and smoother in a high-dimensional and large MI setting than the competitors. MINE and MINE-f have high variance, and suffer from unstable estimation due to their dependence on the discriminator used to estimate the bound of mutual information. The toy experiment shows that the MIGE has a lower variance. \n\nQ2: Could the tightness or smoothness be quantified? It seems the MIGE has a lower variance, could empirical results or bounds on the variance be obtained?\nMoreover, this plot concerns random quantities, whereas we see only one realization. Both the MIGE and the MINE hold under expectation of samples from the data distribution. This is a toy example where we can sample infinitely from the data distribution. That means we can either a) plot more samples or b) obtain (empirical) error bounds on the gradient under these sampling distributions.\n\n\nR:The correlated Gaussian experiment is taken from [1] which did not provide quantitive measures. However, we agree with the reviewer that it is a very nice idea to quantify the tightness and smoothness. We will try to add the qualification of tightness and smoothness to make our conclusion more convincing. Thank you for your valuable suggestion again.\n\nQ3: The authors train a quote \u2018small fully connected neural network classifier\u2019. However, the work of Hjelm trains a linear SVM on the representations.Is it the improved representations (as obtained by using MIGE) or is it the change classifier?\n\nR:  For consistent comparison, the baseline of DIM and our proposed MIGE are all based on non-linear classification, which is also mentioned in [2]. We did not include any result of linear SVM for DIM or any other methods.\nThe same classifiers are used for all methods. Our baseline results are directly copied from [2] or by running the code in https://github.com/rdevon/DIM. We haven\u2019t changed the code of the non-linear classification. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "HkliG6Ktor", "original": null, "number": 5, "cdate": 1573653779235, "ddate": null, "tcdate": 1573653779235, "tmdate": 1573653779235, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "SJx1qthYqB", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "\nThank you for acknowledging that our idea is interesting and the results are encouraging. We will revise our paper according to the comments and hire a copy-editor to carefully polish the writing.\n\nQ1: No comparison on downstream tasks for more datasets except MNIST. In the end, a key question is a final accuracy on different datasets and how to maximize the information effect on it.\nR: Table 1 shows the classification accuracy (top 1) results of downstream tasks on CIFAR10 and CIFAR100 in Deep InfoMax experiments. Table 2 shows the experimental results on MNIST in Information Bottleneck (IB). \n\nQ2: There is no discussion about the effect of the random projection on the representation. For example, how it affects performance? How much the algorithm sensitive to this projection? \nR: Generally speaking, representation learning for big datasets is usually costly in storage and computation. For example, the dimension of images in STL-10 is 96 \\times 96 \\times 3 (i.e., the vector length is 27648). This makes it almost impossible to directly estimate the gradient of MI between the input and representation. Therefore, we introduce Random Projection to make MIGE applicable high dimensional settings. For exmaple on STL-10., when the dimension of images is reduced to 256 via RP, we can observe significant improvement over the baselines (as shown in the table below). More specific analyses will be added to the revised manuscript.\n\n\nTable: Classification accuracy (top 1) results on STL-10. \nRP denotes Random Projection.\n.----------------------------------------------------------------\n.\t\t\t\t\t\t\tSTL-10\t\n.\t\t\t\t\tconv\tfc\t\tY\t\n.----------------------------------------------------------------\n.DIM(JSD)\t\t\t42.03\t30.28\t28.09\t\n.DIM(infoNCE)\t\t43.13\t35.80\t34.44\t\n.----------------------------------------------------------------\n.MIGE\t\t\tunaffordable computational cost\t\n.MIGE+RP to 1024d\t49.08\t40.09\t38.95\t\n.MIGE+RP to 512d\t49.89\t41.05\t38.56\t\n.MIGE+RP to 256d\t49.91\t40.24\t38.83\t\n.----------------------------------------------------------------\n\nQ3: What is the performance of the MINE if it combined with random projection\n\nR: It is possible to apply RP for MINE.  However,  due to the dependence on the discriminator used to estimate the mutual information,  RP+MINE may lead to results with high variance. And we will investigate this issue in the future work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "HygathtKor", "original": null, "number": 4, "cdate": 1573653637206, "ddate": null, "tcdate": 1573653637206, "tmdate": 1573653637206, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "r1gDQ2YKiB", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 3  (2)", "comment": "\n\nQ4: One contribution of the paper is to make the gradient estimator work in high dimensions. To this end, the authors propose Random Projections. It is not clear how this approximation influences the results. An experiment regarding this topic would make the point clearer. Is RP used in the current experiments? Then how does this influence the results? Is the RP used for computational purposes? Then can we quantify the gain in computation?\n\nR: Generally speaking, representation learning for big datasets is usually costly in storage and computation. For example, the dimension of images in STL-10 is 96 \\times 96 \\times 3 (i.e., the vector length is 27648). This makes it almost impossible to directly estimate the gradient of MI between the input and representation. Therefore, we introduce Random Projection to make MIGE applicable high dimensional settings. For exmaple on STL-10., when the dimension of images is reduced to 256 via RP, we can observe significant improvement over the baselines (as shown in the table below). More specific analyses will be added to the revised manuscript.\n\nTable: Classification accuracy (top 1) results on STL-10. \nRP denotes Random Projection.\n.----------------------------------------------------------------\n.\t\t\t\t\t\t\tSTL-10\t\n.\t\t\t\t\tconv\tfc\t\tY\t\n.----------------------------------------------------------------\n.DIM(JSD)\t\t\t42.03\t30.28\t28.09\t\n.DIM(infoNCE)\t\t43.13\t35.80\t34.44\t\n.----------------------------------------------------------------\n.MIGE\t\t\tunaffordable computational cost\t\n.MIGE+RP to 1024d\t49.08\t40.09\t38.95\t\n.MIGE+RP to 512d\t49.89\t41.05\t38.56\t\n.MIGE+RP to 256d\t49.91\t40.24\t38.83\t\n.----------------------------------------------------------------\n\nQ5: In practice, we do not care about MI estimation\u2019. Please explain further or refer to previous work. *\n\nR:Sorry for the confusion. Existing deep models focusing on mutual information usually firstly estimate the MI, and then maximize/minimize it by gradient ascent/descent. Our concern is that directly estimating the gradient could be easier and more accurate since estimating the mutual information in itself is very difficult. We will revise this sentence. \n\nQ6: Two points 1. The information \u2018between z and z\u2019 is probably a typo? 2. How does sufficiency relate to an optimization problem? Doesn\u2019t sufficiency mean in this context I(X;Y)=I(Z;Y)?\n\nR:Yes, it is a typo. Here sufficiency means this context I(X;Y)=I(Z;Y). \n\nQ7: Equation 12: In the part . Why do we take gradient w.r.t. x? It seems to me that the reparametrization is a function of x only via. If not, then please explain what this tuple means.\n\nR: While x is unrelated to parameter \\psi, we cannot directly get the gradient of log p_\\psi(x,z) with z. p_\\psi(x,z) is an implicit distribution. Hence, we don\u2019t know the formation of p_\\psi(x,z). But based on Spectral Stein Gradient Estimator, we can estimate the gradient of log p_\\psi(x,z) w.r.t (x,z) by sampling (x,z). Then the gradient of log p_\\psi(x,z) w.r.t \\psi is directly computed according to the chain rule derivation. \n\nQ8: Table 1 has no units. How to interpret the numbers in this table?\n\nR: Table 1 shows the classification accuracy (top 1) results of downstream tasks on CIFAR10 and CIFAR100 in Deep InfoMax experiments. \"%\" will be added.\n\nQ9: Section 4.3, authors note their experiment is \u2018a little bit different\u2019 from other related research. How and what exactly is different?\nR: We use the same setting except that the initial learning rate of 2e-4 was set  for Adam optimizer, and exponential decay with decaying rate by a factor of 0.96 was set for every 2 epochs. \n\n[1] Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, ICML, 2018.\n[2] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.\n[3] Ben Poole, Sherjil Ozair, Aaron van den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "rkePFjKYor", "original": null, "number": 2, "cdate": 1573653375149, "ddate": null, "tcdate": 1573653375149, "tmdate": 1573653375149, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "BklSOaCu9S", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for acknowledging that our idea is interesting and the results are encouraging. We will revise our paper according to the comments and hire a copy-editor to carefully polish the writing.\n\nIn the following, we will answer the concerns of the reviewer.\n\n1) For experiments, we follow the experiments of Deep InfoMax and Information Bottleneck to set the experimental setup as in [1, 2], and we also refer to their source code [3, 4]. Under these experimental settings, we use our MI Gradient Estimator to replace the MI estimator in Deep InfoMax and Information Bottleneck.\nWe will provide an algorithm description in the revision, and we will release our code upon acceptance with more detailed settings.\n\n2) Due to the page limits (recommended 8 pages in ICLR CALL for Papers), we cannot provide more details of missing concepts and definitions. We will add an appendix and make our paper more self-contained in the revision.\n\nQ1: In section 3. Please define q(z)_psi(z), q(x,z)_psi and describe how they relate to E_psi.\n\nR: For definitions of q(z)_\\psi(z) and q(x,z)_\\psi, q(z)_\\psi(z) corresponds to the distribution of representation of x via the encoder E_\\psi as described in Equation (11). As mentioned in our paper, \"we can obtain the samples from the marginal distribution of z by pushing samples from the data empirical distribution p(x) through E\\psi(.) for representation learning.\" q_\\psi is an implicit distribution determined by the encoder parameters \\psi. q(x,z)_\\psi is the joint distribution of (x,z), which is determined by the encoder parameters \\psi.\n\nQ2: What exactly are the contributions by the authors wrt to spectral stein gradient descent (sec 2.1) e.g. is it the scalable approach based on random projections described in sec 3? Further i would like some discussion on the quality of this approximation?\n\nR: Generally, representation learning for big datasets is usually costly in storage and computation. For example, the dimension of images in STL-10 is 96 \\times 96 \\times 3 (the dimension is 27648). This makes it hard to directly estimate the gradient of MI between the input and representation by SSGE. Therefore, we introduce Random Projection to achieve the approximation of Equation (19) in the Nystr\u00f6m approximations of SSGE. \nThus with the aid of Random Projection, we could evaluate on bigger datasets, e.g., STL-10. Especially, when the dimension of images is reduced to 256, we can observe significant improvement over the baselines (as shown in the table below). More specific analyses will be added to the revised manuscript.\n\nTable: Classification accuracy (top 1) results on STL-10. \nRP denotes Random Projection.\n.----------------------------------------------------------------\n.\t\t\t\t\t\t\tSTL-10\t\n.\t\t\t\t\tconv\tfc\t\tY\t\n.----------------------------------------------------------------\n.DIM(JSD)\t\t\t42.03\t30.28\t28.09\t\n.DIM(infoNCE)\t\t43.13\t35.80\t34.44\t\n.----------------------------------------------------------------\n.MIGE\t\t\tunaffordable computational cost\t\n.MIGE+RP to 1024d\t49.08\t40.09\t38.95\t\n.MIGE+RP to 512d\t49.89\t41.05\t38.56\t\n.MIGE+RP to 256d\t49.91\t40.24\t38.83\t\n.----------------------------------------------------------------\n\nQ3: Please provide some more details on the DeepInfoMax and Information bottleneck experiments e.g. How exactly did you estimate the MI gradients in these settings? how is the downstream task setup and is it identical to prior work?\n\nR: We follow the experiments of Deep InfoMax and Information Bottleneck to set the experimental setup as in [1, 2], and we also refer to their source code [3, 4]. Under these experimental settings, we use our MI Gradient Estimator to replace the MI estimator in Deep InfoMax and Information Bottleneck.\nWe will provide an algorithm description in the revision, and we will release our code upon acceptance with more detailed settings.\n\nQ4: About writing style\n\nR: Thank you for your valuable suggestions about writing style, we will fix these problems in the revision.\n\n[1] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019. \n[2] Alemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. \n[3] https://github.com/rdevon/DIM\n[4] https://github.com/alexalemi/vib_demo \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "SyeM9cFYjB", "original": null, "number": 1, "cdate": 1573653130291, "ddate": null, "tcdate": 1573653130291, "tmdate": 1573653130291, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "BylvZbsnFB", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment", "content": {"title": "Response to Reviewer 1 ", "comment": "We thank the reviewer for the acknowledgment of our paper and the valuable suggestions. We are glad that the reviewer found the work to be novel and well-motivated. We will carefully revise our manuscript and hire professional copy editors to proofread our paper.   \n\n1) R: Thanks for pointing out the recommendation to apply MIGE on DIM(L) as this is a good suggestion. Due to limited responce time and computational resources, we are still in the process of experimenting and analyzing preliminary results.  In addition, we note that the literature has argued that maximizing tighter bounds in DIM(L) leads to worse results [2]. We will consider this as future work and discuss these new findings in a new paper.\n\nWe cannot provide the value of MI to evaluate the representation, because MIGE directly estimates the gradient of MI to optimize MI, rather than estimating the value of MI.\n\n2) R: For comments on evaluating on larger datasets, we have evaluated our method on a large scale dataset, i.e., STL-10. The results can be found in the following table.\n\nTable: Classification accuracy (top 1) results on STL-10.\nRP denotes Random Projection.\n.----------------------------------------------------------------\n.\t\t\t\t\t\t\tSTL-10\t\n.\t\t\t\t\tconv\tfc\t\tY\t\n.----------------------------------------------------------------\n.DIM(JSD)\t\t\t42.03\t30.28\t28.09\t\n.DIM(infoNCE)\t\t43.13\t35.80\t34.44\t\n.----------------------------------------------------------------\n.MIGE\t\t\tunaffordable computational cost\t\n.MIGE+RP to 1024d\t49.08\t40.09\t38.95\t\n.MIGE+RP to 512d\t49.89\t41.05\t38.56\t\n.MIGE+RP to 256d\t49.91\t40.24\t38.83\t\n.----------------------------------------------------------------\n\nR: The setting of our toy experiment follows the literature [3, 4], where the  highest-dimensions  is set to 20. We will add the evaluation on the toy experiment in  high dimensional settings (e.g., 256, 1024).\n\n\nQ1\uff1aCircumstances 2 and 3 can be quite easily derived from circumstance 1; also they are not evaluated empirically; it would be nice to have experiments for them, and they can be moved to the appendix in case of lack of space\n\nR: MIGE in Circumstances 3 is applied to the Information Bottleneck experiments in Section 4.3. We will add explanations in Section 4.3.\n\nQ2: Section 4.2 paragraph 2: \u201cshrinking\u201d for different layers wasn\u2019t mentioned before, and is not immediately clear what it means; the reader needs to be intimately familiar with the DIM paper to understand.\n\nR: We will add the detail description to make the reader easy to understand \"shrinking\".\n\nQ3: explain the difference between q_\\psi and p_\\psi, which seem to be used interchangeably.\n\nR: p_\\psi is a typo and should be instead of q_\\psi. p(.) means that this distribution is uncorrelated to the encoder parameters \\psi. q_\\psi(.) is determined by the encoder parameters \\psi(.) \n\nQ4: Section 4.3 mentions \u201cthreshold\u201d for stein gradient estimator, which was not mentioned before. Please explain what it is.\n\nR:\u201cthreshold\u201d is a hyperparameter of Spectral Stein Gradient Estimator (SSGE), and it is used to set the kernel bandwidth of RBF kernel.\n\nQ5: The authors talk about MINE, which optimizes the InfoNCE bound [1], which is also used in DIM and CPC [2]. I strongly encourage the authors to cite [1] and [2] and mention them in the related works. Additionally, it would be clear if Figure 1 and related references and description used \u201cInfoNCE\u201d instead of \u201cMINE\u201d as the name of the method since InfoNCE is \n\nR: Indeed, we have cited these two papers. MINE optimizes the Donsker-Varadhan representation rather than the InfoNCE bound. MINE-f optimizes the f-divergence representation. InfoNCE is a different estimator with high bias. For more details please refer to [3, 4].\n\n\n\n[1] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019.\n[2] On mutual inforamtion maximization for representations, https://openreview.net/forum?id=rkxoh24FPH.\n[3] Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, ICML, 2018.\n[4] Ben Poole, Sherjil Ozair, Aaron van den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxaUgrFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2341/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2341/Authors|ICLR.cc/2020/Conference/Paper2341/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142809, "tmdate": 1576860528510, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Authors", "ICLR.cc/2020/Conference/Paper2341/Reviewers", "ICLR.cc/2020/Conference/Paper2341/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Comment"}}}, {"id": "BylvZbsnFB", "original": null, "number": 1, "cdate": 1571758334541, "ddate": null, "tcdate": 1571758334541, "tmdate": 1572972351182, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes MIGE---a novel estimator of the mutual information (MI) gradient, based on estimating the score function of an implicit distribution. To this end, the authors employ the spectral Stein gradient estimator (SSGE) and propose its scalable version based on random projections of the original input. The theoretical advantages of the method are presented using a toy experiment with correlated Gaussian random variables, where both the mutual information and its gradient can be computed analytically. In this setting, MIGE provides gradient estimates that are less biased and smoother than baselines. The method is also evaluated on two more complicated tasks: unsupervised representation learning on Cifar-10 and CIfar-100 via DeepInfoMax (DIM) and classification on MNIST with Information Bottleneck (IB), where MIGE outperforms all baselines by a significant margin.\n\nI recommend ACCEPTing this paper. This work discusses a vital problem and proposes a novel, well-motivated, principled and very performant solution; additionally, it demonstrates the broad applicability of the introduced method. While the proposed technique consists of previously known building blocks (spectral Stein gradient estimator and random projections), it is cleverly applied in a novel context of estimating MI gradients.\n\nWhile the paper is solid, I believe that it could be improved in the following ways. Firstly, I would like to see 1) more extensive and 2) larger-scale evaluations. In the DIM experiment, 1) would correspond to trying the DIM(L) approach, which maximises patch-wise MI. In fact, I strongly recommend including this experiment as it corresponds to and could improve current state-of-the-art. If it turns out that MIGE does not work well on DIM(L), then this would correspond to a serious issue with the method. In this experiment, 1) would also include providing other metrics for learned representations. It would be much more convincing to include estimates of true mutual information (e.g. InfoNCE bound evaluated with a large number of samples [1]) and showing that MIGE can attain higher values than baselines. 2) would correspond to evaluation on bigger datasets: (tiny) ImageNet and STL-10 dataset. Also, the toy experiment would benefit from a higher-dimensional setting (e.g. d=256 to d=1024), since these are often used in practice. \nSecondly, the paper is sloppily-written, which quite a few grammar and stylistic mistakes (e.g. sentence in sec 3, paragraph 2: \u201cwe assume obtain to\u2026\u201d, which starts with a lower-case letter and doesn\u2019t make sense). Finally, the paper would benefit from the following clarifications: 1) explain what is the Nystr\\:om method, 2) provide either a proof or a citation for eq (19); also the error bound for SSGE should be provided for the paper to be self-contained, 3) explain the difference between q_\\psi and p_\\psi, which seem to be used interchangeably.\n\nAdditional remarks:\nSec 2.2, 2), \u201cstreamlining\u201d is unclear\nCircumstances 2 and 3 can be quite easily derived from circumstance 1; also they are not evaluated empirically; it would be nice to have experiments for them, and they can be moved to the appendix in case of lack of space\nEq (19) while nice, seem to bear no significance for the proposed method and the rest of the paper; consider removing it\nSection 4.2 paragraph 2: \u201cshrinking\u201d for different layers wasn\u2019t mentioned before, and is not immediately clear what it means; the reader needs to be intimately familiar with the DIM paper to understand.\nSection 4.3 mentions \u201cthreshold\u201d for stein gradient estimator, which was not mentioned before. Please explain what it is.\nEquations (8-10) are just simple derivations and are not necessary; it would be enough to provide Eq (10).\nThe authors talk about MINE, which optimizes the InfoNCE bound [1], which is also used in DIM and CPC [2]. I strongly encourage the authors to cite [1] and [2] and mention them in the related works. Additionally, it would be clear if Figure 1 and related references and description used \u201cInfoNCE\u201d instead of \u201cMINE\u201d as the name of the method since InfoNCE is an estimator and MINE is just a particular implementation of the method.\n \n[1] Poole et. al., \u201cOn variational bounds of mutual information\u201d, ICML 2019.\n[2] van den Oord et. al, \u201cRepresentation Learning with Contrastive Predictive Coding\u201c, arXiv 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575876377348, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Reviewers"], "noninvitees": [], "tcdate": 1570237724218, "tmdate": 1575876377362, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review"}}}, {"id": "B1gYH_Xuqr", "original": null, "number": 2, "cdate": 1572513856900, "ddate": null, "tcdate": 1572513856900, "tmdate": 1572972351131, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper works out estimators for the gradient of Mutual Information (MI). The focus is on its recent popular use for representation learning. The insight the authors provide is to see encoding the representation as a \u2018reparametrization\u2019 of the data. This insight enables mathematical tools from the literature on \u2018pathwise derivatives\u2019. With gradients on the MI, one can estimate models that aim to maximize this quantity. For example in unsupervised learning one can learn representations for downstream tasks. This is shown in Table 1. Another application in supervised learning is the Information Bottleneck. This shown in Table 2.\n\nThree points for review:\n1)\nThe estimator for the gradient is shown at first on a toy task. Take a correlated Gaussian distribution and estimate gradients of the MI. The correlated Gaussian has an analytical form of MI, which makes this a useful experiment. The paper claims that this estimator \u2018provides a tighter and smoother\u2019 gradient estimate. I don\u2019t see how this experiment and this claim tie together. Could the tightness or smoothness be quantified? It seems the MIGE has a lower variance, could empirical results or bounds on the variance be obtained? \n\nMoreover, this plot concerns random quantities, whereas we see only one realization. Both the MIGE and the MINE hold under expectation of samples from the data distribution. This is a toy example where we can sample infinitely from the data distribution. That means we can either a) plot more samples or b) obtain (empirical) error bounds on the gradient under these sampling distributions.\n\n2)\nThe major experimental result in the paper shows advantage of the gradient estimator in Transfer learning. Specifically, the authors compare against the recent DIM of Hjelm et al 2019. The authors train a quote \u2018small fully connected neural network classifier\u2019. However, the work of Hjelm trains a linear SVM on the representations. It is not clear where the increase in performance originates. Is it the improved representations (as obtained by using MIGE) or is it the change classifier? \n\n3)\nOne contribution of the paper is to make the gradient estimator work in high dimensions. To this end, the authors propose Random Projections. It is not clear how this approximation influences the results. An experiment regarding this topic would make the point clearer. Is RP used in the current experiments? Then how does this influence the results? Is the RP used for computational purposes? Then can we quantify the gain in computation?\n\nMinor comments: \n  *\u2019In practice, we do not care about MI estimation\u2019. Please explain further or refer to previous work.\n  *\u2019In optimization, it should be achieved by maximizing the information between z and z.\u2019 (Section 2). Two points\n    1. The information \u2018between z and z\u2019 is probably a typo?\n    2. How does sufficiency relate to an optimization problem? Doesn\u2019t sufficiency mean in this context I(X;Y)=I(Z;Y)?\n  * Equation 12: In the part $\\nabla_psi (x, E_\\psi(x))$. Why do we take gradient w.r.t. x? It seems to me that the reparametrization is a function of x only via $E_\\psi(x)$. If not, then please explain what this tuple means.\n  * Table 1 has no units. How to interpret the numbers in this table?\n  * Section 4.3, authors note their experiment is \u2018a little bit different\u2019 from other related research. How and what exactly is different?\n\nTypographic comments\n  *Just below eqn17, \u2018minibatche\u2019 => \u2018mini-batch\u2019 or \u2018mini batch\u2019\n  *Section 4.2 \u2018images classification\u2019 => \u2018image classification\u2019\n  *\u2019However A tractable density is\u2019 => \u2018However, a tractable density is\u2019\n  *\u2019Estimating gradients of MI than\u2019 -> \u2018Estimating gradients of MI rather than\u2019\n  *Section 3, circumstance 1 \u2018representation\u2019 => \u2018represent\u2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575876377348, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Reviewers"], "noninvitees": [], "tcdate": 1570237724218, "tmdate": 1575876377362, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review"}}}, {"id": "BklSOaCu9S", "original": null, "number": 3, "cdate": 1572560236824, "ddate": null, "tcdate": 1572560236824, "tmdate": 1572972351023, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper argues that directly estimating the intractable mutual information (MI) for representation learning is challenging in high dimensions. Instead the authors propose to estimate the needed MI gradients directly using a score function based approach. Using some identities of MI the authors arrive at an expression for the gradient of the mutual information between input and latent representation (eq 10) and proposes to use a generalization of the reparameterization trick and spectral stein gradient descent to approximate this gradient. In Toy experiment and MNIST/CIFAR10 experiments the authors demonstrate that their method produces latent representations that are more informative than competing MI methods for downstream classification tasks. I found the approach and content of the paper interesting and the results seems encouraging.  My main concern is that I did not find the that I did not find the method and experimental section to be fully comprehensive and further lacking many details which makes it hard to compare the results with prior work. \n\n\nPros:\n1) I find the approach taken by the authors interesting and different from current MI estimation approaches. The paper convincingly motivates their approach by describing the deficiencies of current MI estimators and why targeting the gradients directly might have merits.\n2) The authors propose to use SSGD and 'generalized' reparameterization in a (well motivated) new setting.\n3) The cifar10 experiments in table 1 are encouraging and the toy experiment in 2D is illustrates nicely the deficiencies of the current MI estimators \n\nCons\n1) The experimental section is lacking many details to fully understand how and what experiments were performed and how comparable they are to prior work\n 2) The paper would benefit greatly from a thorough editing to clarify the presentation - there are many missing concepts and definitions that makes it hard to follow without intimate knowledge of related literature. \n \n \nFurther suggestions / questions \n\n1. In section 3. Please define q(z)_psi(z),  q(x,z)_psi and describe how they relate to  E_psi.\n\n2) What exactly are the contributions by the authors wrt to spectral stein gradient descent (sec 2.1) e.g. is it the scalable approach based on random projections described in sec 3 ? Further i would like some discussion on the quality of this approximation?\n\n3) Please provide some more details on the DeepInfoMax and Information bottleneck experiments e.g. How exactly did you estimate the MI gradients in these settings? how is the downstream task setup and is it identical to prior work?\n\n\n4) About writing style:\nI think it would benefit the paper if you let the reader decide for them self what adjectives should be used to describe a result. A few concrete suggestions:\n - Use remarkable/y about your own findings a bit more sparingly (used 4x). \n - Consider deleting \u201cmuch\u201d and \u201cvast\u201d in a sentence like: \u201cour approach MIGE gives much more favorable gradient direction, and demonstrates more power in controlling information flows without vast loss\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575876377348, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Reviewers"], "noninvitees": [], "tcdate": 1570237724218, "tmdate": 1575876377362, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review"}}}, {"id": "SJx1qthYqB", "original": null, "number": 4, "cdate": 1572616583491, "ddate": null, "tcdate": 1572616583491, "tmdate": 1572972350977, "tddate": null, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "invitation": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes the Mutual Information Gradient Estimator (MIGE) for estimating the gradient of the mutual information (MI) instead of calculating it directly in learning representation. They are using Stein's estimator following by a random projection to build a tractable approximation to the gradient of the MI. \n The MIGE is evaluated on several of unsupervised and supervised tasks, and shown improvement over prior MI estimation approaches in maximize the MI and learning features for classification.\n\nIn general, I think that the idea of estimating the gradient of the MI instead of directly calculating it  is an exciting research direction, and this paper combines a few pieces together (As mentioned in the paper, there was a work of Li & Turner, 2017 that applied Stein's estimator for implicit models).\nHowever, the experimental part of this paper is lacking. My main concern is regarding the performance on downstream tasks. Although the experiments demonstrate wins over different models in maximizing MI for CIFAR10 and CIFAR100, the only comparison for downstream tasks is for Permutation-invariant MNIST. One more concern is regarding the random projection. It is not clear what is the effect of it on the representation, and how it impacts on the gradine's estimation.  \n\nStrengths:\n+ Interesting new model for representation learning based on an estimation of the MI gradients'.\n+ Good set experiments looking at MI maximization performance.\n+A well-written and well-organized paper.\n\nWeaknesses:\n No comparison on downstream tasks for more datasets except MNIST. In the end, a key question is a final accuracy on different datasets and how to maximize the information effect on it. \nThere is no discussion about the effect of the random projection on the representation. For example, how it affects performance? How much the algorithm sensitive to this projection? What is the performance of the MINE if it combined with random projection...\n\nMinor comments:\n\n-Typos and English mistakes - there are many typos. For example -\n    In the introduction - \"Another closely related work is the the Information\u2026\"\n    In section 2 - \u201cIn order to overcome this disadvantages\"  \n    In section  2.20  - \"In optimization, it should be achieved by maximizing the information between z and z.\"\n- There should be more detailed explanations of the experiments. For example - what is the projected dimension (for all the experiments). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2341/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wlj6816@gmail.com", "zhouyiji@outlook.com", "ronghe1217@gmail.com", "mingyuan.zhou@mccombs.utexas.edu", "zenglin@gmail.com"], "title": "Mutual Information Gradient Estimation for  Representation Learning", "authors": ["Liangjian Wen", "Yiji Zhou", "Lirong He", "Mingyuan Zhou", "Zenglin Xu"], "pdf": "/pdf/0b5d08511f43c8eea7a10da7a0ae88603c8ee064.pdf", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "keywords": ["Mutual Information", "Score Estimation", "Representation Learning", "Information Bottleneck"], "paperhash": "wen|mutual_information_gradient_estimation_for_representation_learning", "_bibtex": "@inproceedings{\nWen2020Mutual,\ntitle={Mutual Information Gradient Estimation for  Representation Learning},\nauthor={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxaUgrFvH}\n}", "original_pdf": "/attachment/39912406879177b92545e396c5e43caf4353279f.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxaUgrFvH", "replyto": "ByxaUgrFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2341/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575876377348, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2341/Reviewers"], "noninvitees": [], "tcdate": 1570237724218, "tmdate": 1575876377362, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2341/-/Official_Review"}}}], "count": 20}