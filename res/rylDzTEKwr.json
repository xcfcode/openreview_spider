{"notes": [{"id": "rylDzTEKwr", "original": "SkxGmbjLwr", "number": 416, "cdate": 1569438991190, "ddate": null, "tcdate": 1569438991190, "tmdate": 1577168281408, "tddate": null, "forum": "rylDzTEKwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "W8a02AliI7", "original": null, "number": 1, "cdate": 1576798695807, "ddate": null, "tcdate": 1576798695807, "tmdate": 1576800939811, "tddate": null, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Decision", "content": {"decision": "Reject", "comment": "There was a clear consensus amongst reviewers that the paper should not be accepted. This view was not changed by the rebuttal. Thus the paper is rejected. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715223, "tmdate": 1576800265085, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper416/-/Decision"}}}, {"id": "Sye7HBCqsS", "original": null, "number": 3, "cdate": 1573737787032, "ddate": null, "tcdate": 1573737787032, "tmdate": 1573761868796, "tddate": null, "forum": "rylDzTEKwr", "replyto": "HyevKp7TFr", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment", "content": {"title": "Author response part 2", "comment": "Q: \u201cTo realize the self-masking role, the paper proposed to use the function f(z_u, z_i)=g(Hamming_self-mask(z_u, z_i)), Equ. 12, and demonstrate the effectiveness of using this function by experiments. Since the necessity of using self-masking is not very convincing, I doubt whether this self-masking function f(z_u, z_i) is indispensable. Maybe, if some other functions that takes z_u and z_i as input are used, better results may be observed. \u201c\nA: The necessity of self-masking is due to the inability to *efficiently* compute a *weighted* Hamming distance, as it is only based on a hardware supported sum (popcount) and a Boolean XOR. Motivated by the hypothesis that not all bit dimensions are equally important for each user, we derive self-masking as a way to do binary weighting. The binary weighting is implemented by masking the item by the user hash code (i.e, using an AND operation), such that the user hash code determines which dimensions are important for the user. After the masking, the Hamming distance is now effectively only computed on the bit dimensions set to 1 in the user hash code (since all items have a value of -1 in dimensions that are -1 in the user hash code).\n    While in principle other functions taking z_u and z_i as input might exist, they would have to be described using only Boolean operations and without using other inputs (otherwise, it would no longer be fast to compute). Thus, this set of possible functions is extremely limited, and we have not been able to find any other meaningful functions than our self-masking. \n    Without self-masking, we are left with our VaH-CF model as seen in Table 2, where the experimental results clearly show the large performance improvement obtained by self-masking.\n\nWe hope the above comments clarify the uncertainties, and that the reviewer would reconsider their recommendation.\n\n[1] Chenghao Liu, Tao Lu, Xin Wang, Zhiyong Cheng, Jianling Sun, and Steven C.H. Hoi. 2019. Compositional Coding for Collaborative Filtering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). ACM, New York, NY, USA, 145-154.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylDzTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper416/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper416/Authors|ICLR.cc/2020/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171798, "tmdate": 1576860534894, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment"}}}, {"id": "Bke64EA9oS", "original": null, "number": 1, "cdate": 1573737524572, "ddate": null, "tcdate": 1573737524572, "tmdate": 1573738386622, "tddate": null, "forum": "rylDzTEKwr", "replyto": "SJgFWI0B9r", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for their insightful review, which we have taken into consideration for improving our submission. Below we detail each comment individually.\n\nQ: \u201cIn Table 2, the reported results of MF and its variants are lower than those of DCF. These results are very unreasonable. I have also conducted many similar experiments and the obtained results are not consistent with the results this paper reported.\u201d\nA:  We used the code as provided in the CCCF [1] GitHub repository [2]. However, after your comment, we found that a wrong distance measure had been used for MF, and this has now been correctly changed to the dot product. We have updated the MF baseline results in the revised paper, and the performance is now notably higher, as expected (thus reflecting the performance scores observed in previous work). This, however, does not change the performance comparison against the state-of-the-art hashing based approaches, which is the focus of the paper and where our approach still leads to a very substantial improvement.\n\nQ: \u201cIn addition, in the original paper, CCCF is reported to achieve the superior performance over DCF. However, in this paper, the results are also inconsistent.\u201d\nA: CCCF learns floating-point weights that are used for the weighted Hamming distance computation. In their original paper, the authors (mistakenly, we believe) do not count these floating-point weights when counting the total number of bits for each hash code, thus leading to an unfair experimental comparison. For example, they use 8 blocks for all hash code lengths, which corresponds to 128-512 (depending on the floating-point precision) additional unaccounted bits compared to the baselines (this is a very significant difference when using hash codes of length 32-64). \n    When we account for the floating-point weights, CCCF obtains similar performance to DCF, which is to be expected, since single-block CCCF has an almost identical formulation as DCF. As described in our Section 4.3, we experiment with block sizes of {8,16,32,64} and count each floating-point as only 16 bits \u2014 we try all combinations within the bit budget (32 and 64) and report the best performance (if a single block is chosen, we do not count the floating-point weight). We focus on hash code lengths of 32 and 64 as these correspond to the common machine word lengths, and thus the case where the Hamming distance is fastest to compute (compared to cases where the hash code spans multiple machine words).\n\nQ. \u201cTo sum up, the reported results in the paper are not convincing to me. I would doubt that there is no much accuracy improvement compared to the state-of-the-art methods.\u201d\nA: We have revised our paper in response to the reviewer\u2019s comments regarding the two questions above. Our results (see Table 2) show our approach does in fact provide a large improvement compared against the state-of-the-art hashing based methods. We hope our comments and paper changes related to the previous questions above clarify this in a satisfactory manner. \n\nQ: \u201cFinally, as I understand, using variational hashing to maximize the likelihood of all observed items and users will inevitably increase the training time. The presented experiments, however, include no results on the efficiency comparison.\u201d\nA: In Section 4.5 we analyze the convergence rate. Our self-mask method enables the model to converge extremely fast (often 5-20 epochs)  compared to the no self-masking case (up to 1000 epochs), which means it is fast to train.\n\nQ: \u201cIf there are no advantages on the recommendation accuracy and efficiency, the motivations of this designed hashing method are vague.\u201d\nA: We hope our changes and explanations above provide evidence of the advantages of our approach with regards to both effectiveness and (training/runtime) efficiency, and that the reviewer will revise their recommendation.\n\n[1] Chenghao Liu, Tao Lu, Xin Wang, Zhiyong Cheng, Jianling Sun, and Steven C.H. Hoi. 2019. Compositional Coding for Collaborative Filtering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). ACM, New York, NY, USA, 145-154.\n[2] https://github.com/3140102441/CCCF\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylDzTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper416/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper416/Authors|ICLR.cc/2020/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171798, "tmdate": 1576860534894, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment"}}}, {"id": "r1gVPH09jr", "original": null, "number": 4, "cdate": 1573737820425, "ddate": null, "tcdate": 1573737820425, "tmdate": 1573737959541, "tddate": null, "forum": "rylDzTEKwr", "replyto": "HyevKp7TFr", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment", "content": {"title": "Author response part 1", "comment": "We thank the reviewer for their insightful review, which we have taken into consideration for improving our submission. Below, we detail each comment individually.\n\nQ: \u201cI have concerns over using the VAE to model the ratings of each user and item pairs here. Essentially, you are building a VAE for every rating value R_u, i.  Although the parameters are shared, the VAE\u2019s have their own prior. For generative models like VAE, they need to learn from a lot of data, not just one data point. From the perspective of generating data looking similar to the training data, I don\u2019t think your model have learned anything. Only the reconstruction part is important to your model. \u201c\nA: It is correct that only the reconstruction part is important in our case, since we do not need the generative properties of the model for its use-case of collaborative filtering. As described in Section 4.1, and similarly done in related work, we require each user to have at least 10 ratings, and each item to be rated at least 10 times, such that the model does have data to learn the dynamics between users and items. While this amount of data may not be sufficient to utilize the generative properties, we find that it is sufficient for learning hash codes that enable highly efficient item recommendations. We have extended our revised paper with this information in Section 3.4.\n\nQ: \u201cThe idea of using discrete VAE for hashing tasks has been explored before, see \u201cNASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing\u201d.  It employed a very similar idea, although it is not used for CF, but for hashing directly. The novelties of the model is limited.\u201d\nA: Methods based on discrete VAE have indeed previously been used in the task of semantic hashing for both text and images, where text/images are transformed to short hash codes similarly to our work. In contrast to these, we are the first to design a discrete VAE for hashing-based collaborative filtering, whereas existing approaches are based on traditional non-neural setups. However, the most important contribution of our work is the introduction of self-masking in the Hamming distance. While this may appear to be minor, we are the first to consider changes to the Hamming distance which we experimentally show to significantly improve performance.\n\nQ: \u201cSince ratings are essentially ordinal data, using Gaussian distribution to model the rating data may be not appropriate.\u201d\nA: We use a Gaussian distribution to model the difference between the observed and estimated ratings. While one option would be to use different variances depending on the rating, this would effectively enforce different weights in the reconstruction. We make the choice of weighting all ratings equally and thus fix the variance to be the same across all ratings. As such, we do not agree that it is inappropriate, and in fact this leads to an MSE objective identical to our baselines (which implicitly make the same Gaussian assumption). We have updated Section 3.4 in our revised paper to reflect this.\n\nQ: \u201cThe whole paper, especially the model, is not presented well. The model is not presented in a rigorous way, and some sentences in this paper are difficult to follow.\u201d\nA: We have updated the paper and made small changes to improve the presentation.\n\nQ: \u201cThe runtime analysis is not sufficient. In addition to comparing with the methods using hamming distance, we also want to see the advantages of the hamming based method over the real-value based method on speed acceleration.\u201d\nA: We have now added the floating-point case using the dot product (see Section 4.6).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylDzTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper416/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper416/Authors|ICLR.cc/2020/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171798, "tmdate": 1576860534894, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment"}}}, {"id": "BylyTNC9oH", "original": null, "number": 2, "cdate": 1573737654975, "ddate": null, "tcdate": 1573737654975, "tmdate": 1573737654975, "tddate": null, "forum": "rylDzTEKwr", "replyto": "H1lGdIJEKH", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for their insightful review that we have taken into consideration for improving our submission. Below we detail each comment individually.\n\nQ: \u201c1 The studied problem (i.e., rating prediction) is well studied in the community of recommender systems, and there are many more accurate algorithms than the basic MF algorithm used in the empirical studies. I thus suggest the authors to include more such algorithms though the focus of this paper is for efficiency (but the authors also claim the accuracy of the proposed model).\u201d\nA: While this could in principle be included, the focus of our work is on efficient hashing based approaches, and similar to all related work in this and similar domains (described in Section 2, but also that of \u201csemantic hashing\u201d not part of our related work), this is outside the scope of the paper. Thus, we keep an experimental setup similar to related work, where the focus is on the performance among efficient hashing based approaches.\n\nQ: \u201c2 The authors are suggested to give a brief explanation on choosing those baseline methods in the context of other hashing-based CF methods.\u201d\nA: There has been fairly scant research done on hashing based approaches for collaborative filtering with explicit feedback without side information (e.g., content information such as item descriptions or user reviews). As such, DCF and CCCF represent the state-of-the-art in this domain. We have updated parts of section 4.2 to provide a better explanation for our choices.\n    An older two-stage rounding approach named BCCF has been used in related work, however, all current methods have been shown to outperform this approach, which is why we have not included it as part of our experimental setup. \n\nQ: \u201c3 Some details are missing, e.g., the number of latent dimensions in MF. And some presentation in the parameter setting are not clear, e.g., 'is chosen consistently across all data sets', 'was consistently chosen'. Does 'consistently' means 'exactly the same'?\u201d\nA: The latent dimension in MF is the same as the number of bits used, e.g., for hash codes of 32 bits the competing MF would use a latent dimension of 32 as well. \nYes, \u201cconsistently chosen\u201d means exactly the same. \nWe have updated the paper to reflect both points.\n\nWe hope the above comments clarify the uncertainties, and that the reviewer will reconsider their recommendation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper416/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylDzTEKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper416/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper416/Authors|ICLR.cc/2020/Conference/Paper416/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171798, "tmdate": 1576860534894, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper416/Authors", "ICLR.cc/2020/Conference/Paper416/Reviewers", "ICLR.cc/2020/Conference/Paper416/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper416/-/Official_Comment"}}}, {"id": "H1lGdIJEKH", "original": null, "number": 1, "cdate": 1571186282148, "ddate": null, "tcdate": 1571186282148, "tmdate": 1572972597953, "tddate": null, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors study a classical (and well-studied) problem called rating prediction from a new perspective, i.e., learning binary vector representations of the users' and items' latent representations for efficiency. In particular, the authors introduce a personalized self-masking shown in Eq.(2) and in Figure 1 (the 'AND' operation) in order to improve the previous hashing-based collaborative filtering methods without increasing the time cost much.\n\nEmpirical studies on four public datasets show the effectiveness of the proposed model, i.e., variational hashing-based collaborative filtering with self-masking (VaHSM-CF).\n\nSome comments/suggestions:\n\n1 The studied problem (i.e., rating prediction) is well studied in the community of recommender systems, and there are many more accurate algorithms than the basic MF algorithm used in the empirical studies. I thus suggest the authors to include more such algorithms though the focus of this paper is for efficiency (but the authors also claim the accuracy of the proposed model).\n\n2 The authors are suggested to give a brief explanation on choosing those baseline methods in the context of other hashing-based CF methods.\n\n3 Some details are missing, e.g., the number of latent dimensions in MF. And some presentation in the parameter setting are not clear, e.g., 'is chosen consistently across all data sets', 'was consistently chosen'. Does 'consistently' means 'exactly the same'?\n\nMinors:\nThere are some typos: 'a user, u, and', 'of e.g., restaurant and shopping malls', etc.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper416/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper416/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575403623606, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper416/Reviewers"], "noninvitees": [], "tcdate": 1570237752458, "tmdate": 1575403623617, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper416/-/Official_Review"}}}, {"id": "HyevKp7TFr", "original": null, "number": 2, "cdate": 1571794303166, "ddate": null, "tcdate": 1571794303166, "tmdate": 1572972597918, "tddate": null, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The author proposes a variational encoder (VAE) based approach to perform hashing-based collaborative filtering, which focuses on the weighting problem of each hash-code bit by applying the \"self-masking\" technique. This proposed technique modifies the encoded information in hash codes and avoids the additional storage requirements and floating point computation, while preserving the efficiency of bit manipulations thanks to hardware-based acceleration. An end-to-end training is achieved by resorting the well-known discrete gradient estimator, straight-through (ST) estimator. \n\nStrength:\nThe idea of employing the discrete VAE framework to perform hashing-based collaborative filtering is interesting. From the perspective of applications, I think it is somewhat novel.\n\nThe experimental results demonstrate the performance superiorities of the self-masking hashing-based collaborative filtering method. \n\n\n\nWeakness:\n\nI have concerns over using the VAE to model the ratings of each user and item pairs here. Essentially, you are building a VAE for every rating value R_u, i.  Although the parameters are shared, the VAE\u2019s have their own prior. For generative models like VAE, they need to learn from a lot of data, not just one data point. From the perspective of generating data looking similar to the training data, I don\u2019t think your model have learned anything. Only the reconstruction part is important to your model. \n\nThe idea of using discrete VAE for hashing tasks has been explored before, see \u201cNASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing\u201d.  It employed a very similar idea, although it is not used for CF, but for hashing directly. The novelties of the model is limited.\n\nSince ratings are essentially ordinal data, using Gaussian distribution to model the rating data may be not appropriate.\n\nThe whole paper, especially the model, is not presented well. The model is not presented in a rigorous way, and some sentences in this paper are difficult to follow.\n\nThe runtime analysis is not sufficient. In addition to comparing with the methods using hamming distance, we also want to see the advantages of the hamming based method over the real-value based method on speed acceleration.\n\n\nOther question:\n\nTo realize the self-masking role, the paper proposed to use the function f(z_u, z_i)=g(Hamming_self-mask(z_u, z_i)), Equ. 12, and demonstrate the effectiveness of using this function by experiments. Since the necessity of using self-masking is not very convincing, I doubt whether this self-masking function f(z_u, z_i) is indispensable. Maybe, if some other functions that takes z_u and z_i as input are used, better results may be observed.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper416/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper416/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575403623606, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper416/Reviewers"], "noninvitees": [], "tcdate": 1570237752458, "tmdate": 1575403623617, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper416/-/Official_Review"}}}, {"id": "SJgFWI0B9r", "original": null, "number": 3, "cdate": 1572361729461, "ddate": null, "tcdate": 1572361729461, "tmdate": 1572972597874, "tddate": null, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "invitation": "ICLR.cc/2020/Conference/Paper416/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors proposed an end-to-end variational hashing-based collaborative filtering scheme with self-masking to solve the efficiency of large-scale recommender systems. It mainly targets at addressing a problem in existing hashing-based collaborative filtering methods, where each bit is equally weighted in the Hamming distance computing process. To this end, the presented hashing scheme develops a self-masking technique to encode which bits are important to the user. The comparative experiments on several datasets demonstrate the superior performance of the presented hashing method.\n\nThe idea is interesting in the sense that an efficient self-masking technique is proposed to generate user-adaptive hash codes, by differentiating the importance of binary bits, for a fast recommendation system. However, the issues with the experimental results make me inclined to reject this paper.\n\nIn Table 2, the reported results of MF and its variants are lower than those of DCF. These results are very unreasonable. I have also conducted many similar experiments and the obtained results are not consistent with the results this paper reported.\n\nIn addition, in the original paper, CCCF is reported to achieve the superior performance over DCF. However, in this paper, the results are also inconsistent.\n\nTo sum up, the reported results in the paper are not convincing to me. I would doubt that there is no much accuracy improvement compared against the state-of-the-art methods.\n\nFinally, as I understand, using variational hashing to maximize the likelihood of all observed items and users will inevitably increase the training time. The presented experiments, however, include no results on the efficiency comparison.\n\nIf there are no advantages on the recommendation accuracy and efficiency, the motivations of this designed hashing method are vague.\n\nBased on the above reasons, I tend to reject this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper416/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper416/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Hashing-based Collaborative Filtering with Self-Masking", "authors": ["Casper Hansen", "Christian Hansen", "Jakob Grue Simonsen", "Stephen Alstrup", "Christina Lioma"], "authorids": ["c.hansen@di.ku.dk", "chrh@di.ku.dk", "simonsen@di.ku.dk", "s.alstrup@di.ku.dk", "c.lioma@di.ku.dk"], "keywords": ["hashing", "collaborative filtering", "information retrieval", "supervised learning"], "TL;DR": "We propose a new variational hashing-based collaborative filtering approach optimized for a novel self-mask variant of the Hamming distance, which outperforms state-of-the-art by up to 12% on NDCG.", "abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.", "pdf": "/pdf/f512a2ad40b5d496038e7c14a49ac8a0523dd445.pdf", "paperhash": "hansen|variational_hashingbased_collaborative_filtering_with_selfmasking", "original_pdf": "/attachment/d1aed55e44b9e2f9d87fecb8ad401af85f06c116.pdf", "_bibtex": "@misc{\nhansen2020variational,\ntitle={Variational Hashing-based Collaborative Filtering with Self-Masking},\nauthor={Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen Alstrup and Christina Lioma},\nyear={2020},\nurl={https://openreview.net/forum?id=rylDzTEKwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylDzTEKwr", "replyto": "rylDzTEKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575403623606, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper416/Reviewers"], "noninvitees": [], "tcdate": 1570237752458, "tmdate": 1575403623617, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper416/-/Official_Review"}}}], "count": 9}