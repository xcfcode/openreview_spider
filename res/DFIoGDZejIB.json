{"notes": [{"id": "DFIoGDZejIB", "original": "6c2k5JWQiqe", "number": 2050, "cdate": 1601308225871, "ddate": null, "tcdate": 1601308225871, "tmdate": 1614985749092, "tddate": null, "forum": "DFIoGDZejIB", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 28, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "gOoXgT7YG2", "original": null, "number": 1, "cdate": 1610040386713, "ddate": null, "tcdate": 1610040386713, "tmdate": 1610473980438, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This is a well written paper, outlining a class of assistive algorithms. Being more or less a survey paper, it could do a better job of discussing 'inverse reinforcement learning' and 'collaborative inverse reinforcement learning'. It could also be slightly more general: for example the human dewcision function need not be known if we model the interaction as a Bayesian game (then the human might have a latent type, which can be inferred together with the reward function). The active reward learning problem is sometimes referred to as 'preference elicitation'. In the end, it was not clear that the discussion in this paper had any actionable insights for future models or algorithms in this area.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040386699, "tmdate": 1610473980420, "id": "ICLR.cc/2021/Conference/Paper2050/-/Decision"}}}, {"id": "IsJJ_J7rcBM", "original": null, "number": 6, "cdate": 1604858578860, "ddate": null, "tcdate": 1604858578860, "tmdate": 1606918160647, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review", "content": {"title": "The paper provides a comparative analysis of reward learning and assistance. While the discussion in the paper is certainly interesting, I'm uncertain about what are the contributions of the paper.", "review": "### Overview\n\nThe paper addresses the problem of learning from human feedback. It provides an analysis of reward learning---where human feedback is used to extract a task description in the form of a reward---and assistance---where the learning agent and human co-exist in the environment and both perform actions; the agent seeks to select its actions to optimize the (unknown) that is implicit in the human's actions. \n\nThe paper shows that reward learning problems can be converted to assistance problems, turning queries from reward learning to communicative actions in a two-phase communicative assistance problem. Conversely, two-phase communicative assistance problems can be converted to active reward learning problems.\n\n### Comments\n\nIn my opinion, the paper reads very well, and the discussion in the paper is quite interesting. \n\nIn spite of an interesting discussion, I am not certain about the contribution of the paper. The derivations, although they seem technically sound, are not particularly surprising; similarly, the qualitative differences in behavior pointed out in the paper are hardly surprising---assistance learning, being a more general problem than reward learning (as follows from the discussion of the paper), will lead to more diverse behaviors. \n\nThe particular behaviors observed are also a consequence of the POMDP formulation, as the agent will act to strike an adequate tradeoff between information gathering and goal optimization.\n\n### Post-rebuttal update\n\nI thank the authors for the clarifications and discussion. However, and admitting that I may have missed something, I remain unconvinced regarding the contributions of the paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105154, "tmdate": 1606915768652, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2050/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review"}}}, {"id": "DcvDLwyij8r", "original": null, "number": 5, "cdate": 1603942785255, "ddate": null, "tcdate": 1603942785255, "tmdate": 1606796983277, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review", "content": {"title": "Official Blind Review | Reviewer #4", "review": "#### Summary\n\nThe submission provides a survey of two paradigms for \u2018agents learning from human feedback.\u2019 The two paradigms are unified under a new formalism (assistance games), which subsumes them as its special cases. Further, a taxonomy of different problems resulting from the formalism is provided (communicative games, two-phase games, etc.), along with illustrative examples of resulting agent behaviors. Based on the survey and taxonomy, the authors highlight that the assistance paradigm is more advantageous (in terms of possible behaviors that it can result in) than the reward learning paradigm.\n\n=======================================================\n#### Reasons for score\n\nStrengths\n  + The topic of \u2018agent learning from human feedback\u2019 is topical and of interest to the ICLR community.\n  + The proposed taxonomy (i.e., assistance games) can serve as a useful common ground for discussing the different paradigms, problems, and solutions of \u2018agent learning from human feedback.\u2019\n  + The paper is well written and organized.\n\nWeaknesses\n- While the submission does discuss related work (Section 2), the discussion omits several related research threads. Some of these threads have strong overlap with the setting proposed in the submission.\n- Similarly, the qualitative behaviors that emerge from assistance games (Section 4) have been demonstrated in prior research (including on larger problems and with human users), thereby making it difficult to assess the novelty of the formalism.\n- The key contribution of the submission is unclear (e.g., whether it is a survey, a model, a taxonomy, or all?).\n \nI am truly on the fence regarding this submission. A novel taxonomy is certainly needed to relate and compare the diverse and growing body of research in the area of \u2018agent learning from human feedback.\u2019 However, to arrive at this taxonomy a more complete consideration of existing formalisms and algorithms is necessary. Please see suggestions listed below on prior research that relates to and, in certain cases, extends the formalism of assistance / collaboration.\n\n=======================================================\n#### Key Comments\n\n1.\t(Introduction and Proposition 1) The insight of having a single control policy for both reward learning and control modules has been previously explored. This insight is identical to that of planning / control formulations in human-robot interaction literature that model the human\u2019s preferences (which in turn influence the reward) as latent states. These planning methods use POMDPs to represent the interaction / collaboration problem, and solve the exploration-exploitation tradeoff associated with reward learning (exploration) and control (exploitation) using POMDP solvers or MPC. Please discuss the novelty of the proposed formalism in relation to these methods. For instance, \n    - (considers continuous states and actions spaces; assistance paradigm) Sadigh, Dorsa, et al. \"Information gathering actions over human internal state.\" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2016.\n    - (does not require one agent, R or H, to act before the other; assistance paradigm) Chen, Min, et al. \"Planning with trust for human-robot collaboration.\" Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction. 2018.\n    - Gopalan, Nakul, and Stefanie Tellex. \"Modeling and solving human-robot collaborative tasks using pomdps.\" RSS Workshop on Model Learning for Human-Robot Communication. 2015.\n    - Nikolaidis, Stefanos, et al. \"Game-theoretic modeling of human adaptation in human-robot collaboration.\" Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction. 2017.\n\n2. (Section 2.3) Assistance games, as defined, assume parametric specification of the hypothesis space of the reward / preference of humans. However, nonparametric extensions (both for assistance and reward learning) have been proposed. Please consider relating the proposed formalism with these prior works. For instance, \n  - Michini, Bernard, and Jonathan P. How. \"Bayesian nonparametric inverse reinforcement learning.\" Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012.\n  - Panella, Alessandro, and Piotr Gmytrasiewicz. \"Bayesian learning of other agents' finite controllers for interactive POMDPs.\" Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. 2016.\n\n3. (Section 2.3) In assistance games, as defined, the reward (or human preferences over reward) does not change during the task. However, extensions exist which model the latent state corresponding to reward as being locally active and / or time varying. Please consider relating the proposed formalism with these related works. For example, \n  - (locally active reward; reward learning paradigm) Michini, Bernard, and Jonathan P. How. \"Bayesian nonparametric inverse reinforcement learning.\" Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012.\n  - (locally active reward; reward learning paradigm) Park, Daehyung, et al. \"Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning.\" Conference on Robot Learning. PMLR, 2020.\n  - (considers time varying preferences with learned dynamics; assistance paradigm) Nikolaidis, Stefanos, David Hsu, and Siddhartha Srinivasa. \"Human-robot mutual adaptation in collaborative tasks: Models and experiments.\" The International Journal of Robotics Research 36.5-7 (2017): 618-634.\n  - (considers time varying preferences with learned dynamics; assistance paradigm) Unhelkar, Vaibhav V., Shen Li, and Julie A. Shah. \"Semi-Supervised Learning of Decision-Making Models for Human-Robot Collaboration.\" Conference on Robot Learning. 2020.\n\n4.  The behaviors arising from solving assistance games (Section 4) have been previously formalized by multiple human-AI collaboration approaches and demonstrated with human users. For instance,\n  -\t(exhibits behaviors outlined in Section 4.2) Kamar, Ece, Ya\u2019akov Gal, and Barbara J. Grosz. \"Incorporating helpful behavior into collaborative planning.\" Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS). Springer Verlag, 2009.\n  -\t(reasons about communicative actions) Whitney, David, et al. \"Reducing errors in object-fetching interactions through social feedback.\" 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017.\n  -\t(reasons about both physical actions and communications) Nikolaidis, Stefanos, et al. \"Planning with verbal communication for human-robot collaboration.\" ACM Transactions on Human-Robot Interaction (THRI) 7.3 (2018): 1-21.\n  -\t(reasons about both physical actions and communications) Unhelkar, Vaibhav V., Shen Li, and Julie A. Shah. \"Decision-Making for Bidirectional Communication in Sequential Human-Robot Collaborative Tasks.\" Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction. 2020.\n  -\t(communicative actions) Liang, Claire, et al. \"Implicit communication of actionable information in human-ai teams.\" Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 2019.\n\nPlease discuss the connection of the proposed formalism with these approaches.\n\n5.\t(Section 2.2) Please also consider discussing the following related works on active reward learning,\n  - Lopes, Manuel, Francisco Melo, and Luis Montesano. \"Active learning for reward estimation in inverse reinforcement learning.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Berlin, Heidelberg, 2009.\n  - Brown, Daniel S., Yuchen Cui, and Scott Niekum. \"Risk-aware active inverse reinforcement learning.\" Conference on Robot Learning. 2018.\n  - Tschiatschek, Sebastian, et al. \"Learner-aware teaching: Inverse reinforcement learning with preferences and constraints.\" Advances in Neural Information Processing Systems. 2019.\n  - Cui, Yuchen, and Scott Niekum. \"Active reward learning from critiques.\" 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.\n\n=======================================================\n#### Minor Comment\n\n- (Section 2.1) Please consider including a note explaining the asterisk notation (used to define the domain for POMDP policy).\n\n=======================================================", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105154, "tmdate": 1606915768652, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2050/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review"}}}, {"id": "xABc0UhVmV", "original": null, "number": 3, "cdate": 1603911030726, "ddate": null, "tcdate": 1603911030726, "tmdate": 1606793581738, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review", "content": {"title": "Official Review #1", "review": "Summary:\nThis work proposes learning a single control policy for human-in-the-loop learning rather than having a reward learning component and a control component. The key difference is that the action selection can use information from the reward learning module. The authors formulate an assistance game in this setting and show that it can reduce to an equivalent POMDP. The work then describes a communicative assistance problem and shows the equivalence of reward learning to assistance and visa versa. Results show qualitative improvements on variants of the kitchen domain.\n\nPros:\n- The paper was overall well-written (although some key differences with prior work were unclear, as described below).\n- The high-level area of human-in-the-learning is a useful and important space.\n\nCons:\n- There are many works in human-in-the-loop learning, and it wasn\u2019t clear whether the ideas in this paper were novel enough at a high-level. Some relevant works are included below.\n- The paper was hard to follow with respect to the distinction between reward learning and assistance. Adding more examples or describing the difference from other angles could be useful.\n- The experimental domains were simple and there were no computational results shown in the main paper. This is important to show in order to support the claims of the paper.\n\nComments:\n- The qualitative results are useful for showing specific cases in which the proposed approach may be beneficial but this restricts how applicable the approach seems to be. It would be nice to see computational results over a variety of domains and comparisons with baseline approaches to see the benefit of the approach more generally. \n- Section 4.2 focuses on asking the right questions at the right time. What is the main novelty here with respect to more general active learning approaches?\n- A few minor notational confusions: for example, it\u2019s confusing to have R be the robot as it\u2019s often the reward function. And in Section 4.1, C is used for choices and for cherry.\n- Other work that might be relevant to the problem proposed here:\n\nGame-theoretic modeling of human adaptation in human-robot collaboration\nS Nikolaidis, S Nath, AD Procaccia, S Srinivasa\n\nMaximizing BCI Human Feedback using Active Learning\nZ Wang, J Shi, I Akinola, P Allen\n\nModeling humans as observation providers using pomdps\nS Rosenthal, M Veloso\n\nActive Learning for Risk-Sensitive Inverse Reinforcement Learning\nR Chen, W Wang, Z Zhao, D Zhao\n\nContact: Deciding to communicate during time-critical collaborative tasks in unknown, deterministic domains\nVV Unhelkar, JA Shah\n\nEfficient model learning from joint-action demonstrations for human-robot collaborative tasks\nS Nikolaidis, R Ramakrishnan, K Gu, J Shah\n\nRecommendation:\nOverall, I thought the work at a high-level was limited in its novelty compared with prior work in human-in-the-loop learning. The distinction between reward learning and assistance, a key part of the paper, was hard to fully understand so clarifying this description through examples or more clear text would be valuable. The evaluation was also simple and not very convincing with respect to supporting the claims. Adding computational experiments and appropriate baselines would be important to show the general applicability of the approach.\n\n--------------------\nResponse after rebuttal:\nThank you to the authors for their response. I appreciate the detailed answers to each of the prior works. I still do think the paper needs to be more clear in the problem and differentiation with prior work in the writing itself, which will require a non-trivial update to the paper.\n\nOn the computational results + baselines side, while the authors have run the experiment and have described these qualitative behaviors, this isn't a substitute for quantitative results, especially because this is important to show when comparing with baselines. The authors say \"We have updated Section 4 to be clearer about what baseline approaches would do in the environments we have tested\". It's important to show evidence that this is what the baselines actually did.\n\nBased on these points, I don't think the paper is quite ready for publication yet.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105154, "tmdate": 1606915768652, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2050/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review"}}}, {"id": "0hIITL-hCcP", "original": null, "number": 1, "cdate": 1602776167966, "ddate": null, "tcdate": 1602776167966, "tmdate": 1606755765816, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review", "content": {"title": "It seems to me that the authors disregard a rich related literature", "review": "------------------------------------------------------------\nPost-rebuttal\n-----------------------------------------------------------\nGiven the effort of the authors of improving their manuscript, I am improving my original score. However, my evaluation is still \"weak reject\" for the reasons below:\n\n(1) I still fail to see clear differences between \"assistance\", as defined by the authors, and the other reinforcement learning-like approaches that assume that the reward function is unknown. I can see that they perhaps provide a more organized and methodological description of how that \"assistance\" can happen when compared to the previous works. However, the paper lacks practical advice, exactly how should I build an agent to leverage such \"assistance\"? I don't think their ideas are so novel that other methods couldn't be at least adapted to work in their scenario (to include some empirical evaluation in the manuscript).\n\n(2) The paper seems a little displaced to me in this conference. The paper neither provides practical and direct guidance on how to build algorithms to leverage \"assistance\", nor is a survey that focuses on organizing the area and discussing differences between works. Perhaps the paper would be better placed in a \"Blue Sky\" track. \n\n-----------------------------------------------------------\n\nThe authors propose two learning paradigms where the learning agent doesn't have access to a reward function but instead has to learn directly from the \"assistance\" from a trainer agent.\n\nWhile looking for ways to facilitate task specification and human integration in the learning process is a relevant and promising research goal, the authors don't explain the difference between their newly-proposed paradigms and the very rich literature on Inverse Reinforcement Learning and Learning from Demonstrations.\n\nLearning from human \"assistance\" instead of a reward function is not a new thing, and the surveys below (not cited by the authors) summarize a rich literature that does precisely that:\n\nSilva, Felipe Leno, and Anna Helena Reali Costa. \"A survey on transfer learning for multiagent reinforcement learning systems.\" Journal of Artificial Intelligence Research 64 (2019): 645-703. -> Surveys many categories of works where one agent provides guidance to others, including humans providing \"assistance\" to learning agents.\n\nArgall, Brenna D., et al. \"A survey of robot learning from demonstration.\" Robotics and autonomous systems 57.5 (2009): 469-483. -> Surveys learning from demonstration, where a human provides policy demonstrations to a learning agent, which usually doesn't have access to a reward function\n\nGao, Yang, et al. \"A survey of inverse reinforcement learning techniques.\" International Journal of Intelligent Computing and Cybernetics (2012). -> Inverse reinforcement learning, where the learning agent doesn't have access to a reward function and has to infer a policy from human assistance\n\nWithout a comprehensive discussion about the differences between the authors' proposal and those paradigms, I can't judge the paper contribution. To my eyes, all the descriptions gave in the paper look just like the same as one of the problems surveyed in the papers above.\n\nFor example, \"Non-active reward learning\" seems to me equivalent to inverse reinforcement learning.\n\nAlso, \"active reward learning\" seems to me a special case of the action advising problem surveyed in the first paper above.\n\nIn the case the authors deal with a different problem in this paper, I suggest that the manuscript is rewritten to thoughtfully explain the difference between all those scenarios. In case they indeed are correspondent scenarios, I suggest that the authors use the same notation as the previous works, and include comparisons with the state of the art methods in the experimental evaluation.\n\n-----------------------------------------------------\nOther suggestions\n\n- I don't get how the policy decision function will compute the expected reward if the reward function is unknown.\n\n- You assume access to a \"human decision function\", what exactly does that mean? do you need the probabilities for taking each action? If that's the case it is very unrealistic to expect that a human is able to provide probabilities for each action. Asking s/he to simply pick one action when requested is more realistic.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105154, "tmdate": 1606915768652, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2050/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review"}}}, {"id": "BF2Wh9JwPo8", "original": null, "number": 4, "cdate": 1603928768920, "ddate": null, "tcdate": 1603928768920, "tmdate": 1606351332891, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review", "content": {"title": "[UPDATED] [Official Review]: Well written, but a bit too one-sided", "review": "UPDATE:\nAfter extensive discussion with the authors, I'm raising my score to a 7.  \nI believe their revision will adequately address the concerns I've raised.\nI think this paper clearly identifies and illustrates the qualitative advantages of assistance, and that this is a novel and significant contribution.\n\nIn particular, I do *not* believe, as other reviewers seem to, that any of the following are sufficient reasons for rejecting this work:\n* The wealth of prior work on variants of reward learning and assistance \n* The lack of a comprehensive survey or categorization of such work in this submission\n* The lack of further results\n\nAfter reading the other reviews and responses, I am more confident that this paper makes a valuable contribution, although I stand ready to be challenged by other reviewers.  This is because the authors have argued that the qualitative benefits they describe in sections 4.1/2/3 have not been available to any of the many previous works reviewers mentioned, and no reviewers disputed this.  Furthermore, I did not find the reasons provided for rejection to be very relevant to the goals of this work.  So overall, I do not believe that other reviewers have made a strong case for rejecting this work.\n\nIn my mind, the best argument would seem to be simply that the contribution is insufficient.  I think this is a common criticism of papers that do not adhere to a conventional format or \"type\", but in this case, it seems unfair.  I believe the intellectual contribution of this paper is rather modest, but nonetheless novel and significant.  And I found this motivation for the work quite compelling (emphasis mine):\n> This existing literature is exactly why we wrote this paper: almost all of the existing literature on learning without reward functions can be captured by the reward learning paradigm as we formalize it. But (as we show) the assistance paradigm can enable significantly better behavior from the agents we train! **We are hoping to influence researchers to put more effort into algorithms for the assistance domain, in order to realize these qualitative benefits, instead of continuing to work in the reward learning paradigm as they have done so far.**\n\nI would encourage the authors to explain this goal in their revision, and make sure their claims about the superiority of assistance are appropriately modest.  Overall, I think the qualitative benefits of assistance presented provide a compelling argument for more work in the assistance paradigm, *given the paucity of such work*.  But I think the overall message of the paper should be: \"Given these advantages, and the lack of work on assistance, there should be more work on assistance, since it seems promising and neglected\", and not \"Assistance is better, so why would you do reward learning?\"  And my first impression of the paper was closer to the latter.  \n\nEND UPDATE\n\n-------------------------\nEvaluation:\n\nThis paper is well written and makes a nice point regarding qualitative advantages of assistance over reward learning.\u00a0 Specifically, the authors show how assistance naturally leads an agent to \"agent to (1) choose questions based on their relevance, (2) create plans whose success depends on future feedback, and (3) learn from physical human actions in addition to communicative feedback.\"  However, the framing is\u00a0quite one-sided, and the authors make no mention of potential advantages of reward learning.\u00a0 And it is possible to achieve the same qualitative advantages by slightly modifying the (rather restrictive) formulation of reward learning that the authors use.\u00a0 I think the work should either include such a discussion, or offer more convincing evidence regarding when assistance is in fact preferable\u00a0in practice (e.g. experiments with some features that might advantage either approach).\n\nSome potential advantages of reward learning are:\n- reduced complexity of the learning problem\n- the human retains more control\n- it seems to require less modeling of human\u00a0psychology\n- less opportunities to corrupt the reward signal\n\nRegarding the qualitative advantages mentioned in Sections 4.1/2, we can achieve the same benefits in many instances by incorporating regular feedback sessions where H and R can communicate.\u00a0 I was confused by the claimed advantage in 4.3, since IRL already can learn from physical actions.\n\nOverall, I think this paper sets up a bit of a false dichotomy between (non-interactive) reward learning, and the full assistance game formulation.\u00a0 We can instead view these methods as varying wrt 1) when and how we ask R to interpret H's actions as communicative, and 2) how interactive the learning process is.\nThis paper still makes important (corresponding) points: 1) there is a benefit to considering H's behavior as communicative whenever we understand its semantics, and 2) interaction is important.\n\u00a0 \nThese are not very surprising, although very nicely and clearly argued for and demonstrated. However, regarding point (1): if we don't understand the semantics of H's behavior, it makes sense to restrict the communication to a more well-defined channel, as in reward learning; furthermore, even if we *do* know the semantics, H may not wish all of her behavior to be viewed as communicative, and allowing her to directly control when and how her behavior will be viewed as communicative grants H more agency over the behavior of R.\n\nAccepting points (1) and (2), it still seems like most effective methods are likely to lie somewhere between the two extremes described in this work.\u00a0 For example, the authors state: \"Thus, for good performance we need to model \u03c0^H. This will require insights from a broad range of fields that study human behavior.\"\u00a0 However, accurately modelling\u00a0\u03c0^H \u00a0seems significantly less urgent when communication from H is more limited and literal (i.e. when we move closer towards current reward learning practice).  As another piece of loosely supporting evidence,\u00a0in\u00a0\"Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning\", Milli and Dragan show that more sophisticated assumptions about the semantics of human behaviors may also be more brittle.\n\nAnother contribution of the paper is to explicitly frame reward learning as a special case of assistance.\u00a0 This seems like a straightforward, minor, contribution.\n\nI was also disappointed that the paper didn't discuss reward corruption.\u00a0The need to more fully understand H in the assistance paradigm creates more opportunities for R to misinterpret H's behavior.  In practice, the tighter feedback loops favored by the assistance paradigm might also\u00a0create more opportunities for R to (e.g. irreversibly) \"corrupt\" H's reward function and/or policy, since an initial misunderstanding could be self-reinforcing.\u00a0 For instance,\u00a0H might fail to tell R that its behavior was intimidating if doing so previously had led R to become more (instead of less) intimidating.\u00a0 This could lead R to become confident that this behavior was *not* intimidating to H.\u00a0 Such a scenario seems less likely when H provides feedback to R in an \"offline\" or \"purely communicative\" context.\n\n-------------------------\nDetailed Suggestions:\n- Figure 1 caption could be clearer (what is depicted on the right?)\n- \"These behaviors cannot be expressed by a reward learning agent.\" <-- not literally true; suggest a rephrase.\n- \"Since c0:k-1 only serves to provide information to R\" <-- It doesn't have to... e.g. people do IRL on data that doesn't fit this description.\n- Provide more of an informal introduction in section 2.3 before jumping into definitions\n- \"However, since we want to compare to\u00a0reward learning, we follow its assumptions and so assume that the human policy \u03c0H is available.\"\u00a0While I was eventually able to understand this reasoning, it sounds false out of context, and should be clarified.\n- a^H_noop is not defined\n- I consider the first paragraph of Section 4 to be the \"meat\" of the paper, and I think some of this content should be front-loaded more.\u00a0 It's a shame to wait until page 6.\n- I would start each of 4.1/2/3 with a paragraph giving a non-mathematical, qualitative description of the result.\n- I like the paragraph at top of page 7, which gives an explanation of how reward learning would act/fail.\u00a0 Can you include such a paragraph in 4.1/3 as well?\u00a0 I think the more you mirror structure in these sections, the better.\u00a0", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DFIoGDZejIB", "replyto": "DFIoGDZejIB", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105154, "tmdate": 1606915768652, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2050/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Review"}}}, {"id": "5Y2WxFLJXiD", "original": null, "number": 23, "cdate": 1606278508605, "ddate": null, "tcdate": 1606278508605, "tmdate": 1606278508605, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "aGhj1JcB6fQ", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Actually, this problem is *solved* by assistance", "comment": "Ah, we didn't realize you meant this sort of corruption, where we posit that the agent has an incentive to corrupt the reward learning process itself. Actually, this corruption might be a worry with interactive reward learning, but it is solved in assistance!\n\nConsider the cake-or-death problem. Quoting the linked LessWrong post:\n\n> p(C(u)|w) (probability of the correctness of u in world w) was replaced with p(C(u)|e,a) (probability of the correctness of u given the evidence e and the action a).\n\nIn other words: the naive cake-or-death problem arises when the true reward is modeled as a function of the agent's observations and actions, rather than as a function of the underlying world state. But the assistance formulation explicitly models the reward as part of the state -- when we reduce the assistance game to a POMDP, the unknown reward parameters $\\theta$ become part of the hidden state in the POMDP.\n\n> In summary: the sophisticated cake-or-death problem emerges for a value learning agent when it expects its utility to change predictably in certain directions dependent on its own behaviour.\n\nAn optimal assistance agent can never have this. We have seen that assistance problems can be cast as POMDPs with $\\theta$ as the hidden state variable. Optimal solutions to POMDPs maintain a Bayesian belief over the hidden state (in this case $\\theta$). For a Bayesian belief, the expected impact of new information must be zero (conservation of expected evidence: https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence ). This means that it is impossible for an optimal assistance agent to \"expect [$\\theta$] to change predictably in certain directions dependent on its own behaviour\".\n\nThis sort of problem is more possible with interactive reward learning, because you can think of these as two separate \"modules\" -- one that is responsible for reward learning, and one that is responsible for control. If the module that is responsible for control is \"smarter\" than the module responsible for reward learning, then it may be able to find behaviors that cause the reward learning module to update in a predictable way. But this crucially relies on the _separation_ between reward learning and control, which is exactly what we avoid with assistance.\n\n----\n\nLet's turn to the pitfalls paper. It allows for arbitrary \"reward learning processes\", which can \"update\" about the reward function in arbitrary ways. However, assistance only allows for reward learning processes that are properly Bayesian (for example, it should not be possible to predict in advance how the agent will update in the future, as mentioned above).\n\nThe pitfalls paper proposes a notion of uninfluencability:\n\n> Definition 2 (Uninfluenceable). The reward-function learning process $\\rho$ is uninfluenceable given the prior $\\xi$ on $M$ if there exists $\\eta$, a probability distribution on $R$ conditional on environments, such that for all $R \\in \\mathcal{R}$ and $h_n \\in \\mathcal{H}_n$:\n\n> $P(R | h_n, \\eta, \\xi) = P(R | h_n, \\rho)$\n\n(Here $M$ is the set of environments.) Intuitively, this criterion asks, \"is there a prior $\\eta$ over reward functions (given the environment), such that the future behavior of the learning process is equivalent to a Bayesian updating process starting with the prior $\\eta$?\"\n\nThis is automatically true for the optimal policy for any assistance problem: we identify $R$ with $\\theta$, $\\mathcal{R}$ with $\\Theta$, $M$ with $\\Theta$, $\\xi$ with $P_{\\theta}$ and then set $\\eta(\\theta_{\\eta} \\mid \\theta_{\\xi}) = 1[\\theta_{\\eta} = \\theta_{\\xi}]$.\n\nIn English, since the set of environments is the set of possible reward functions, if we are given a particular environment, the corresponding prior over rewards is just the reward function for that environment.\n\nThe equation then holds because the optimal policy for an assistance problem can be viewed as maintaining a Bayesian belief over $\\theta$, which is exactly what the criterion checks for.\n\nWe are not completely confident that this math works out -- we wrote this up in the span of an hour -- but we are confident in the broader point that assistance is not subject to these sorts of corruptions.\n\n----\n\nOverall, these \"corruption\" worries stem from a simple argument: if the \"action-selection part\" of the agent can predict how the \"reward learning part\" of the agent will update, then the action-selection mechanism might be incentivized to cause the reward to update in a specific way. However, assistance explicitly merges these two parts into a single unified whole, and as a result this argument no longer applies.\n\nWhen we were initially writing this paper, we were considering including this argument, but didn't because we expected that most readers would not care and because it seemed hard to formalize. Given that you were interested in it, maybe other readers will be as well, so we may add an appendix discussing this (if we come up with a good example environment)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "ID70IcgsrK5", "original": null, "number": 22, "cdate": 1606264248052, "ddate": null, "tcdate": 1606264248052, "tmdate": 1606264248052, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "V3xSYZiKD_B", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "I like the planned change in sections 4.1/4.2", "comment": "that's all."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "aGhj1JcB6fQ", "original": null, "number": 21, "cdate": 1606264169636, "ddate": null, "tcdate": 1606264169636, "tmdate": 1606264169636, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "2DKWdwzOiW", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Revisiting the \"reward corruption\" concern", "comment": "I think this clarification helps explain my concerns regarding assistance and reward corruption.\nThe core of this concern comes from this property of assistance:\n> 2. Decision-making for \"control\" can take into account the fact that the agent can do \"reward learning\" to get more information about $\\theta$ (this enables plans conditional on future info, as in Section 4.1).\n\nNaively, taking the possibility of reward learning into account when doing control seems like an invitation for reward corruption, since the agent might expect that certain actions might update its reward function in a way that would increase expected reward (directly, e.g. for any trajectory or policy).  \n\nSuch issues are discussed in these works:\n* Pitfalls of learning a reward function online (Armstrong et al. 2020)\n* Motivated Value Selection for Artificial Agents (Armstrong 2015).\n(It looks like the 2nd of these is not available online right now, but I believe it includes the \"cake or death\" example, which is also discussed in this blog post, and others on LessWrong: https://www.lesswrong.com/posts/6bdb4F6Lif5AanRAd/cake-or-death)\n\nCan you please elaborate on whether and to what extent this is a concern for assistance and reward learning?\nI also think this still deserves more discussion in the paper!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "j_i3ygSW-_s", "original": null, "number": 20, "cdate": 1606263633519, "ddate": null, "tcdate": 1606263633519, "tmdate": 1606263633519, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "kThpyTpuNAz", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Thanks", "comment": "> I maintain that this claim is literally false, and should be removed or qualified. See \"Scalable agent alignment via reward modeling: a research direction\" Leike et al. (2018), bottom of page 6 for an (informal) proof that any behavior can be expressed via a reward function.\n\nWe have changed this to \"Vanilla reward learning agents do not show these behaviors\", because we can see how the original sentence would be confusing.\n\nThat being said, we do still stand by our intended meaning. A more formal version of the claim in Leike et al is:\n\nEvery (potentially non-Markovian) deterministic policy is the uniquely optimal policy for some (potentially non-Markovian) reward function.\n\n(We say \"potentially non-Markovian\" because Leike et al works with histories rather than states.)\n\nIn contrast, our claim is about the _learning process_, not about the optimal policy for a reward function.\n\n> By the way, you should probably cite this work\n\nAgreed, and done.\n\nThanks for the suggestions on Figure 1 -- we will think about how to incorporate these suggestions and make it better."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "V3xSYZiKD_B", "original": null, "number": 19, "cdate": 1606262981635, "ddate": null, "tcdate": 1606262981635, "tmdate": 1606262981635, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "uiSiTzCZh2A", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Revisions to the introduction and probably Section 4", "comment": "We actually just updated the paper to rewrite the introduction (see our response in the other thread), to put more of the emphasis here. There are still more edits to make, such as changing the key insight -- while it does talk about the integration of reward learning and control, it doesn't do so very clearly. Really, we would like to put the paragraph we wrote above directly into the introduction, but we are running up against the 9-page limit. Plausibly we could rewrite the entire introduction, compressing the first three paragraphs into 1-2 sentences, as it isn't that relevant to our main point.\n\nAnother potential change would be to explicitly identify what information needs to be communicated \"from\" reward learning \"to\" control or vice versa in Sections 4.1 and 4.2. This would make it clearer that this is the key feature that we are talking about."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "uiSiTzCZh2A", "original": null, "number": 18, "cdate": 1606262344566, "ddate": null, "tcdate": 1606262344566, "tmdate": 1606262344566, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "2DKWdwzOiW", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "That was a very helpful clarification; how will you update the paper to make this clear?", "comment": "Thanks for this clarification!\nCan you sketch out (e.g. at a high level) how you might revise your submission to make this clear to the reader?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "CRjS32NeivL", "original": null, "number": 17, "cdate": 1606261826457, "ddate": null, "tcdate": 1606261826457, "tmdate": 1606261826457, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "0Xrlurh789H", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Agreed", "comment": ">  I think your line of argument shows that assistance has these advantages in principle, but this doesn't say very much about how important that is in practice. [...] I don't think that's a knock-down argument for assistance over reward learning in practice.\n\nYes, we agree with this, and we did not mean to imply otherwise. (Your review makes more sense now, and we can see how this was a very reasonable interpretation of what we literally wrote -- we did mention that in the future work section that we wanted to modify existing reward learning algorithms to get these benefits, even in the original submission, but that was just 1-2 sentences of the entire paper.)\n\nWe have rewritten the introduction to be clearer about this. Among other edits, we have added the following paragraph:\n\n> We do not mean to suggest that all work on reward learning should cease and only research on assistive agents should be pursued. Amongst other limitations, assistive agents are very computationally complex. Our goal is simply to clarify what qualitative benefits an assistive formulation could theoretically provide. Further research is needed to develop efficient algorithms that can capture these benefits. Such algorithms may look like algorithms designed to solve assistance problems as we have formalized them here, but they may also look like modified variants of reward learning, where the modifications are designed to provide the qualitative benefits we identify.\n\nWe believe that it is still an important and useful contribution to identify these qualitative behaviors, show that they can be achieved with an assistance formulation, and show that reward learning algorithms would need to be modified in order to capture the same benefits.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "2DKWdwzOiW", "original": null, "number": 16, "cdate": 1606259226264, "ddate": null, "tcdate": 1606259226264, "tmdate": 1606259226264, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "W0YBiptg5Ko", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Communicativity and interactivity aren't the important axes of variation", "comment": "We definitely want to get the framing right -- we've clearly had some trouble communicating exactly what we mean. Thanks for the help with that.\n\n> we can instead view these methods as varying wrt 1) when and how we ask R to interpret H's actions as communicative, and 2) how interactive the learning process is\n\nWe agree that these are axes that methods can vary on, and that we've presented assistance and reward learning in a way that makes them look like extremes on this spectrum, but they aren't the salient axes to us. The important axis is the extent to which reward learning and control are integrated into a single decision-making process.\n\nTo go into more detail, we can imagine two types of cognition -- first, \"reward learning\", in which the agent learns something about the unknown reward $\\theta$, and second, \"control\", in which the agent takes actions in the environment in pursuit of reward. We call a method part of \"assistance\" if:\n1. Decision-making for \"reward learning\" can take into account how that learning will be used for \"control\" (this enables relevance-aware questioning, as in Section 4.2), and \n2. Decision-making for \"control\" can take into account the fact that the agent can do \"reward learning\" to get more information about $\\theta$ (this enables plans conditional on future info, as in Section 4.1).\n\nThe distinction between reward learning and assistance is primarily about whether these two types of reasoning are integrated or not. (The notion of \"two phase\" is trying to get at the situation where they are not integrated.) We could imagine assistance-style agents that rarely interact or communicate with humans, and we could imagine reward learning agents that have a significant amount of interaction and communication with humans.\n\nUnfortunately, our qualitative examples do tend to imply increased interactivity and communication. In some sense we would like to disavow these \"extras\" -- we agree that it is not clear that we always want to increase interactivity and communication. We're not really sure how to mitigate this."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "x5UQWpCuEKo", "original": null, "number": 15, "cdate": 1606257965584, "ddate": null, "tcdate": 1606257965584, "tmdate": 1606257965584, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "1KBK_bezeQD", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Contribution is the identification of qualitative behaviors and the restrictions under which assistance is equivalent to reward learning", "comment": "The contributions are:\n\n1. Precisely identifying the differences between assistance and reward learning, by showing that the communicative and two phase restrictions together make assistance equivalent to reward learning.\n2. Showing that these differences matter: by removing the restrictions, we get several new qualitative behaviors, described in Section 4.\n\n> For a non-survey paper, I would expect to extract a specific empirical method from the paper (which would require comparisons against state-of-the-art similar methods), or new discoveries enabled through theoretical analysis. I can't clearly see any of those.\n\nWe argue that the qualitative behaviors in assistance (Section 4) are a \"new discovery\" and thus are an important contribution even though they do not take the form of theorems. (Arguably, they were enabled by theoretical analysis; we generated them by asking ourselves what useful behaviors could not be expressed by a two phase algorithm; the concept of \"two phase\" came about because of theoretical analysis.)"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "C6OkjcHi7Sk", "original": null, "number": 14, "cdate": 1606257589037, "ddate": null, "tcdate": 1606257589037, "tmdate": 1606257589037, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "Uf7gewHICRD", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "We maintain that the behaviors in Section 4.1 and 4.2 are novel", "comment": "> there have been several assistance or assistance-style formalisms in prior work which exhibit the behaviors described in Section 4.\n\nWe continue to disagree with this, in the case of Sections 4.1 and 4.2 -- it seems to us that:\n- Prior work has shown the qualitative behavior of Section 4.2 using probabilistic recipe trees\n- Prior work has created assistance-based algorithms, and demonstrated them on environments that don't lead to the qualitative behaviors of Sections 4.1 and 4.2\n- No prior work has connected the behaviors of Sections 4.1 and 4.2 to the assistance formalism.\n\nIn your review, you mention the following papers:\n\n> (exhibits behaviors outlined in Section 4.2) Kamar, Ece, Ya\u2019akov Gal, and Barbara J. Grosz. \"Incorporating helpful behavior into collaborative planning.\" Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS). Springer Verlag, 2009.\n\nThis is the one that uses probabilistic recipe trees.\n\n> (reasons about communicative actions) Whitney, David, et al. \"Reducing errors in object-fetching interactions through social feedback.\" 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017.\n> (reasons about both physical actions and communications) Nikolaidis, Stefanos, et al. \"Planning with verbal communication for human-robot collaboration.\" ACM Transactions on Human-Robot Interaction (THRI) 7.3 (2018): 1-21.\n> (reasons about both physical actions and communications) Unhelkar, Vaibhav V., Shen Li, and Julie A. Shah. \"Decision-Making for Bidirectional Communication in Sequential Human-Robot Collaborative Tasks.\" Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction. 2020.\n> (communicative actions) Liang, Claire, et al. \"Implicit communication of actionable information in human-ai teams.\" Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 2019.\n\nThese are all about treating physical actions as communicative, and so are examples of the qualitative behavior of Section 4.3, but not Sections 4.1 and 4.2.\n\n> In particular, there are several settings where reward learning is possible and assistance is not\n\nTaken literally this cannot be true, since reward learning is a strict subset of assistance. We assume you are claiming that a \"full assistance approach\" where we have complete interactivity and mixing of learning and acting phases is impossible. This may be the case in some settings, but there are also settings where assistance is feasible (as with, e.g., future household robots), and if we can do so it would be worth getting the qualitative benefits of assistance in these situations."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "Uf7gewHICRD", "original": null, "number": 13, "cdate": 1606255046455, "ddate": null, "tcdate": 1606255046455, "tmdate": 1606255046455, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "tQoL9m2B7O", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Thank you for the detailed response.", "comment": "Thanks for clarifying that the paper isn't of a \"standard type.\" Papers certainly don't need to be. However, my concern about the core contributions still remain.\u00a0\n\nAs highlighted in the reviews and response, there have been several assistance or assistance-style formalisms in prior work which exhibit the behaviors described in Section 4. Given this, I take the key messages of the paper as:\n1. reward learning is a special case of assistance games (as defined), and\n2. given 1, assistance games can exhibit additional behaviors that reward learning cannot, and\n3. given 2, assistance games should be used to generate agent behavior.\n \nHowever, to achieve these benefits, the assistance paradigm also imposes additional requirements on the problem setting (e.g., it requires presence of both human and robot in the same episode). In particular, there are several settings where reward learning is possible and assistance is not, making the third message moot for these settings."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "1KBK_bezeQD", "original": null, "number": 12, "cdate": 1606254607649, "ddate": null, "tcdate": 1606254607649, "tmdate": 1606254607649, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "OpL8Sd0AfoF", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Contribution", "comment": "It is still unclear to me the difference between the \"reward learning paradigm\" and the \"assistance paradigm\". \n\nAs you say \"almost all of the existing literature on learning without reward functions can be captured by the reward learning paradigm as we formalize it\", I understand that there isn't any difference and that all the approaches in which the agent learns without explicitly knowing the reward function are contained in the most \"general class\" of methods named by you as \"assistance paradigm\".\n\nIn this case, what is the contribution of the paper? Defining a unified framework seems to me a good contribution to a survey, which the own authors state that this is not the case for this specific paper.\n\nFor a non-survey paper, I would expect to extract a specific empirical method from the paper (which would require comparisons against state-of-the-art similar methods), or new discoveries enabled through theoretical analysis. I can't clearly see any of those."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "W0YBiptg5Ko", "original": null, "number": 11, "cdate": 1606253574812, "ddate": null, "tcdate": 1606253574812, "tmdate": 1606253574812, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "izEv6Ymi86t", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Regarding the framing", "comment": "Overall, I'm still not entirely satisfied with the framing.  Assistance is a strict generalization of reward learning, as you've defined it.  But as I mentioned, \"we can instead view these methods as varying wrt 1) when and how we ask R to interpret H's actions as communicative, and 2) how interactive the learning process is,\" with reward learning and assistance at opposite extremes.  In this case, it seems like we both agree that we might often want something in the middle, but you want to say that everything in the middle is assistance and not reward learning (which is true, as you've defined things).  But this seems to sort of ignore previous work that considers extensions of reward learning (that you would call assistance, not reward learning) to be variants of reward learning.  \n\nUltimately, I don't think this objection is about the naming, as much as the framing though.  I would rather the framing describe methods as being on a continuum as I've described, and discuss the pros and cons of moving in either direction along either axis of that continuum.  In this framing, my criticism of this work as \"one-sided\" could be rephrased as \"you talk about the benefits, but not the costs, of moving towards more expressive communication channels and more interaction\".  I think the benefits are important, and I expect we'll want to move increasingly in the \"assistance\" directions over time for the reasons you provide, but I think this paper should highlight the pros *and cons* of doing that (which it seems like you also agree are *not* merely computational).  I appreciate that you've made some updates in that direction, and I'll look at them more closely before deciding whether to maintain my score.  But I think I'd still push for a more fundamental reframing.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "0Xrlurh789H", "original": null, "number": 10, "cdate": 1606251467894, "ddate": null, "tcdate": 1606251467894, "tmdate": 1606251467894, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "D7s4_3fCYj", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "The middle ground between reward learning and assistance", "comment": "Thanks, this clarified your perspective somewhat.  I wouldn't say I'm fully convinced, however.  I think your line of argument shows that assistance has these advantages *in principle*, but this doesn't say very much about how important that is in *practice*.  We have a lot of examples in AI (e.g. neural networks) where less principled, more heuristic approaches seem to work better in practice.\n\nAn example of something that would be in between these two paradigms would be where the agent has a distribution over possible reward functions, behaves in a risk-averse way, and queries the human when it is uncertain how to act.  \n\nThis could look something like a combination of these two papers (for example):\n* Active Reinforcement Learning: Observing Rewards at a Cost (Krueger et al. 2016)\n* Trial without Error: Towards Safe Reinforcement Learning via Human Intervention (Saunders et al. 2017)\n\nI agree with your point that you will never capture *all* of the benefits of assistance in *every* situation using something more heuristic, but I don't think that's a knock-down argument for assistance over reward learning in practice.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "kThpyTpuNAz", "original": null, "number": 9, "cdate": 1606250663048, "ddate": null, "tcdate": 1606250663048, "tmdate": 1606250663048, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "iRW15Sn_MS5", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "response", "comment": "> \"These behaviors cannot be expressed by a reward learning agent.\"\n\nI maintain that this claim is literally false, and should be removed or qualified.  \nSee \"Scalable agent alignment via reward modeling: a research direction\" Leike et al. (2018), bottom of page 6 for an (informal) proof that *any* behavior can be expressed via a reward function.  Manually specifying the correct reward function is a degenerate form of reward modelling (with a prior that's a delta function on the specified reward function), but the general point is that a (e.g. learned) reward function can, in principle, be appropriately responsive to arbitrary kinds of feedback/interaction with a human.  \nBy the way, you should probably cite this work; the only reason not to in my mind is that it's only a tech report.\n\n> Figure 1 is our attempt to front-load some of the content of Section 4; we\u2019d be excited to hear any other suggestions you have for front-loading more of the content.\n\nI didn't have specific things in mind.  But I think you could expand and improve Figure 1.  For instance, the first sentence of the caption could say: \"An illustration of the qualitative behaviors that assistance, but not reward learning, encourages\" (or something like that).  And the figure should also have a title above the 4 robots, I think, (e.g. \"benefits of assistance\").  I would recommend spending some significant effort on improving this figure, and try to make it easily stand on its own.  Show it to people who haven't read the paper, etc. \n\nThe paragraph that begins \"Consider for example the kitchen environment illustrated in Figure 1\" could also begin instead by saying something like: \"In our paper, we show that assistance leads to qualitatively different (advantageous) behaviors, as illustrated in Figure 1.  And then you could also make this paragraph a list.  Overall, it wasn't blindingly obvious from this paragraph that these were the meat of the paper, so I'd work on that.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "OpL8Sd0AfoF", "original": null, "number": 8, "cdate": 1606211321391, "ddate": null, "tcdate": 1606211321391, "tmdate": 1606211321391, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "0hIITL-hCcP", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Yes, reward learning includes the papers you cite", "comment": "Thanks for the review. It sounds like your primary concern is that there is existing literature on inverse reinforcement learning and learning from demonstrations, which is very similar to our reward learning paradigm (whether active or non-active).\n\nYou are correct that the reward learning paradigm is very similar to these existing areas. We explain this in Section 2.2, where we cited Ng et al 2000, Ziebart et al 2010, and Fu et al 2017, which are all canonical inverse reinforcement learning papers. In fact, it also includes a large variety of other work, such as learning from preferences, for which we cited Zhang et al 2017 Wirth et al 2017, Christiano et al 2017, and Sadigh et al 2017. There are of course many other papers on all of these topics, and we could not cite them all; this is not intended to be a survey paper. We have added citations to the surveys you mention, as we expect those will be useful to readers.\n\nThis existing literature is exactly why we wrote this paper: almost all of the existing literature on learning without reward functions can be captured by the reward learning paradigm as we formalize it. But (as we show) the assistance paradigm can enable significantly better behavior from the agents we train! We are hoping to influence researchers to put more effort into algorithms for the assistance domain, in order to realize these qualitative benefits, instead of continuing to work in the reward learning paradigm as they have done so far.\n\nWe would appreciate suggestions on how we can make this clearer in the paper, as it is crucial to our main point.\n\n----\n\nResponses to other points:\n\n> I don't get how the policy decision function will compute the expected reward if the reward function is unknown.\n\nWe\u2019re not totally sure what you\u2019re asking, so we\u2019ll try two different explanations:\n\nExplanation 1: The policy decision function can take an expectation over the form of the reward, and then take an expectation over the trajectory that the policy will take, in order to get the expected reward of a given policy.\n\nExplanation 2: Consider Atari games. From the perspective of an AI agent, there is only a set of pixels; there\u2019s no obvious or trivial way of computing the reward from these pixels. However, the _environment_ does not just consist of pixels -- it also includes the memory in the Atari emulator. Given this, the reward is easy to calculate -- we just extract the number in the \u201cscore\u201d variable in the memory. Similarly, our _environments_ know the reward function, but the _agent_ does not.\n\n> You assume access to a \"human decision function\", what exactly does that mean? do you need the probabilities for taking each action? If that's the case it is very unrealistic to expect that a human is able to provide probabilities for each action. Asking s/he to simply pick one action when requested is more realistic.\n\nIf we are training with a human-in-the-loop, the human is only queried when stepping the environment forward. As a result, in this setting we only need the human to choose a particular action, rather than give probabilities over each action.\n\nHowever, we expect it will be more typical to train with a simulated human, which need not be perfectly accurate, especially if we use deep reinforcement learning. This simulated human can take the same form as the human models used in inverse reinforcement learning. For example, a common choice of human model is the Boltzmann-rational model, in which we have\n\n$$\\pi^H(a \\mid s, \\theta) \\propto \\exp(Q(s, a; \\theta))$$\n\nThat is, the human is modeled as being more likely to choose actions that are higher-value. Notice that given a specific $\\theta$, the probability of a particular action can be computed fully automatically, without asking a real human for any input."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "-zNiCVp94ti", "original": null, "number": 7, "cdate": 1606211263903, "ddate": null, "tcdate": 1606211263903, "tmdate": 1606211263903, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "xABc0UhVmV", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "The cited papers do not show the qualitative behaviors we identify", "comment": "Thanks for the review, and the links to related work! We\u2019re glad that you found the topic important and the paper well-written. We hope to convince you that the qualitative behaviors we illustrate are indeed novel.\n\n> There are many works in human-in-the-loop learning, and it wasn\u2019t clear whether the ideas in this paper were novel enough at a high-level.\n\nAll of these works either:\n1. Have a known reward function, or\n2. Fall within the reward learning paradigm, or\n3. Are related to the assistance paradigm but don\u2019t illustrate the qualitative behaviors we do.\n\nWe provide more details on each of the papers below:\n\n> Game-theoretic modeling of human adaptation in human-robot collaboration\n\nThis paper assumes that the robot knows the reward function and focuses on the robot communicating to the human, whereas we focus on cases where the robot does not know what must be done and the robot must learn from the human.\n\n> Maximizing BCI Human Feedback using Active Learning\n\nThis paper is a central example of the active reward learning paradigm, for which we have several other citations.\n\n> Modeling humans as observation providers using pomdps\n\nThis paper also assumes a known reward function. That being said, there is an equivalence between unknown rewards and unknown observations, so this paper is related -- in particular, their HOP-POMDPs can be used to encode communicative assistance problems, and so can encode the environments of Sections 4.1 and 4.2 (though not Section 4.3). However, the paper does not describe any of the qualitative behaviors of Section 4, nor does it identify the differences with reward learning, and so it is mostly orthogonal to our paper. We have mentioned this in Section 3 when introducing communicative assistance problems.\n\n> Active Learning for Risk-Sensitive Inverse Reinforcement Learning\n\nThis is another central example of the active reward learning paradigm.\n\n> Contact: Deciding to communicate during time-critical collaborative tasks in unknown, deterministic domains\n\nThis seems to be a paper about multiagent communication? As far as we can tell it cannot be applied to human-in-the-loop learning, because it assumes that we can control the policies of all of the agents, but we cannot control the human\u2019s policy. (Perhaps we are misunderstanding the application of the paper to human-AI interaction though.)\n\n> Efficient model learning from joint-action demonstrations for human-robot collaborative tasks\n\nThis is probably the most related paper -- it does have a robot that assists a human, and it uses POMDP-style reasoning to make inferences about what the human prefers. It is best thought of as an approach to solve an assistance problem when there are joint human-robot demonstrations on which the robot can be trained. We have added a sentence about this paper in Section 2.3.\n\nWe emphasize that our contribution is *not* in the definition of the assistance formalism, which has been done in prior work: our contribution is in the comparison to reward learning and the illustration of the qualitative benefits of assistance over reward learning.\n\n> Section 4.2 focuses on asking the right questions at the right time. What is the main novelty here with respect to more general active learning approaches?\n\nCan you provide citations for an active learning approach that would show the behavior we illustrate in Section 4.2? All the approaches we know of would either ask about wormy apples at the beginning (rather than after picking up an apple with worms), or would never ask about wormy apples.\n\n> The experimental domains were simple and there were no computational results shown in the main paper. This is important to show in order to support the claims of the paper.\n\nWe have implemented the environments of Section 4, and run experiments that show the qualitative behaviors that we describe. Does that address your comment? If not, what additional results would you want, and why do you think they are necessary to support our claims?\n\n> It would be nice to see computational results over a variety of domains and comparisons with baseline approaches to see the benefit of the approach more generally.\n\nWe have updated Section 4 to be clearer about what baseline approaches would do in the environments we have tested, and added an interactive reward learning baseline. Unfortunately, we don\u2019t know of environment suites that have been designed with assistance in mind, and so we cannot test the approach in multiple domains without building these environments ourselves, which would require a huge effort.\n\n> A few minor notational confusions: for example, it\u2019s confusing to have R be the robot as it\u2019s often the reward function. And in Section 4.1, C is used for choices and for cherry.\n\nGood point, we will think about how to change these. We did think about R and didn\u2019t find any alternatives we liked, but we hadn\u2019t noticed the conflict with C."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "izEv6Ymi86t", "original": null, "number": 6, "cdate": 1606211207470, "ddate": null, "tcdate": 1606211207470, "tmdate": 1606211207470, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "BF2Wh9JwPo8", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Human modeling is orthogonal to reward learning vs. assistance", "comment": "Thanks for the review and especially the detailed suggestions on how to improve the paper! We\u2019re glad that you found that the paper makes a nice point about the qualitative advantages of assistance.\n\nAs we understand it, your main objection is that there are several advantages to the reward learning paradigm that we did not mention in the paper, leading to the paper being \u201cone-sided\u201d. We believe that the only major advantage of the reward learning paradigm is that it is less computationally complex (which we meant to imply in the beginning of Section 5, but weren\u2019t sufficiently clear about; we have changed this). The other advantages that you mention seem to us to be orthogonal axes to the reward learning / assistance distinction, which we\u2019ll discuss in more detail below.\n\n----\n\nIt seems as though you are assuming that in the assistance paradigm, it is necessary to model the human with a lot of fidelity in order to interpret all of their behavior as evidence about the reward. This is not necessary: if there is some particular domain in which we either don\u2019t know how to interpret human behavior, or don\u2019t want the robot to make inferences using that behavior, then we can choose an appropriately conservative human model.\n\nFor example, Sections 4.1 and 4.2 use an extremely simple model, that H answers questions accurately. Even if H was capable of taking physical actions that affect the state, if we didn\u2019t want to make any inferences based on physical actions and only wanted to rely on H\u2019s answers to questions, we could model H as choosing physical actions uniformly at random. In this case, R can only make inferences about the reward based on H\u2019s answers to questions. This seems to alleviate many of the concerns you raise.\n\n> it seems to require less modeling of human psychology [...] if we don't understand the semantics of H's behavior, it makes sense to restrict the communication to a more well-defined channel, as in reward learning\n\nYou can choose how much modeling you want to do -- in Sections 4.1 and 4.2, if we imagine that H also has physical movement, we can choose to model question answering, but not H\u2019s physical movement. In both paradigms, the more modeling you do, the better the results (as long as the modeling is accurate).\n\nWe suspect that all three of our examples would still work if we assumed a Boltzmann-rational human model, the model used most often in reward learning.\n\n> it still seems like most effective methods are likely to lie somewhere between the two extremes described in this work.\n\nYes, we agree that we do not want perfect modeling of $\\pi^H$, especially in the near term (whether using reward learning or assistance). Our comment about better human modeling was thinking about the future, once we hit the limits of our current (very naive) modeling of $\\pi^H$.\n\n> the human retains more control [...] H may not wish all of her behavior to be viewed as communicative, and allowing her to directly control when and how her behavior will be viewed as communicative grants H more agency over the behavior of R.\n\nHere also, the choice of $\\pi^H$ determines which of H\u2019s behavior is viewed as communicative. Reward learning corresponds to an assumption of some specific $\\pi^H$ in a particular narrow domain, and ignorance of $\\pi^H$ elsewhere; similar assumptions can be encoded with assistance.\n\nIf anything, we would say that H can have more control with assistance, because H has more control over R while it is acting -- if H starts screaming \u201cNo, stop doing that!\u201d, R will learn that its estimate of the reward is quite bad and so will stop doing whatever it is doing (see also [1]).\n\n> less opportunities to corrupt the reward signal [...] The need to more fully understand H in the assistance paradigm creates more opportunities for R to misinterpret H's behavior.\n\nAgain, if you were particularly worried about this, you could design $\\pi^H$ to only allow R to interpret H\u2019s behavior in a very narrow domain.\n\nNonetheless, we agree that in practice assistance-based solutions will rely more on extracting information from H, and so it is worse if this model is misspecified. We don\u2019t see this as a major limitation -- it seems like this argument could be levied against nearly any method that increases the amount of information that R has about the reward, but that should simply make us more cautious about preventing misspecification, not prevent us from using the method altogether. Nonetheless, we have added a paragraph on this to the limitations section.\n\n[1] Dylan Hadfield-Menell et al. \"The off-switch game.\" arXiv preprint arXiv:1611.08219 (2016)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "D7s4_3fCYj", "original": null, "number": 5, "cdate": 1606211141623, "ddate": null, "tcdate": 1606211141623, "tmdate": 1606211141623, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "BF2Wh9JwPo8", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "It is not just about interactivity", "comment": "One main claim you make is that we could get the same benefits of assistance by alternating feedback and action phases for reward learning:\n\n> Overall, I think this paper sets up a bit of a false dichotomy between (non-interactive) reward learning, and the full assistance game formulation. [...] And it is possible to achieve the same qualitative advantages by slightly modifying the (rather restrictive) formulation of reward learning that the authors use. [...] Regarding the qualitative advantages mentioned in Sections 4.1/2, we can achieve the same benefits in many instances by incorporating regular feedback sessions where H and R can communicate.\n\nHere, we argue that this does not work -- even when alternating phases in this way, you don\u2019t get all of the qualitative benefits we have outlined. Specifically, the examples in Sections 4.1 and 4.2 would not work, and the example in Section 4.3 would work but not as well as with assistance. Intuitively the issue is that it\u2019s not clear in this setup how R would choose when to switch between various phases, and to use beliefs about future phases to determine what to do in the current phase.\n\nLet\u2019s consider the environment in Section 4.1 (where H is not present and will return later, and R must make a pie for her), and let\u2019s imagine that R first optimizes expected reward under its current belief over $\\theta$ for 5 timesteps (but assuming a horizon of 10 timesteps) (phase 1), then H and R can communicate (phase 2), and then R optimizes expected reward under its new belief over $\\theta$ for another 5 timesteps (phase 3).\n\nIn this case, R will immediately make an apple pie in phase 1, because that is the best way to get reward _under its initial belief over_ $\\theta$. The key issue is that R doesn\u2019t \u201cknow\u201d that it should stop acting after making the dough -- that is only a good decision if it takes into account the fact that in the future it will have better information about which filling is desired. It is not clear within the reward learning paradigm how one can provide this information to R during phase 1. The assistance paradigm is the natural resolution to this issue.\n\nNow consider the environment in Section 4.2. In this case, since during phase 1, R doesn\u2019t \u201cknow\u201d that it can learn in the future, it will immediately pick up an apple, and if it has worms, dispose of it in an arbitrary trash can (since it never expects to know where to dispose of the apple).\n\nIf you aren\u2019t convinced, we\u2019d be interested in a more concrete proposal for how \u201cincorporating regular feedback sessions\u201d would lead to the desired behavior in the examples of Section 4.1 and 4.2. We could imagine some hacks that would work for each example in particular but wouldn\u2019t work in other situations, but we don\u2019t want to try and argue against every possible hack that could work in this environment, and we might be missing a simple resolution that you know of.\n\n> I was confused by the claimed advantage in 4.3, since IRL already can learn from physical actions.\n\nTo clarify, the advantage is that the method can learn from physical actions and can then act on its learning _within the same episode_. IRL can learn from physical actions, but it learns from actions that happened in previous episodes. We have clarified this in the paper.\n\nIt is certainly possible to learn from physical actions within an episode by creating a form of interactive reward learning, for example by saying that the first 5 timesteps are for learning the reward using IRL (or something like it) and then the remaining timesteps are for acting in the environment. This would lead to similar behavior, but would require a hardcoded heuristic for choosing when to switch from the learning phase to the acting phase."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "iRW15Sn_MS5", "original": null, "number": 4, "cdate": 1606211087624, "ddate": null, "tcdate": 1606211087624, "tmdate": 1606211087624, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "BF2Wh9JwPo8", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "Responses to detailed suggestions (can be ignored)", "comment": "> Detailed Suggestions\n\nThanks for these! We\u2019ve updated the paper accordingly, and expect these updates will significantly increase the clarity of the paper. Some notes on specific suggestions below:\n\n> \"These behaviors cannot be expressed by a reward learning agent.\" <-- not literally true; suggest a rephrase.\n\nWe\u2019ve added a parenthetical \u201c(as formalized in this paper)\u201d, but we believe it is literally true as written. The behaviors in Sections 4.1 and 4.2 require an agent that can act, then learn, and then act again; whereas reward learning (as we formalize it) requires only two phases. The behavior in Section 4.3 requires the agent to learn from physical actions and then act _within a single episode_; this cannot even be expressed in the reward learning paradigm (as we formalize it).\n\n> \"Since c0:k-1 only serves to provide information to R\" <-- It doesn't have to... e.g. people do IRL on data that doesn't fit this description.\n\nThe intended meaning was that _within the reward learning problem_ (what R has to do) the choices can only provide information to R, they don\u2019t somehow help R to gain expected reward (because they don\u2019t affect the environment that R acts in). We did not mean to imply that $c_{0:k-1}$ weren\u2019t used for some other purpose previously; just that they don\u2019t affect what R has to do now. We have rephrased to \u201cSince $H$'s choices $c_{0:k-1}$ do not affect the state of the environment that $R$ is acting in\u201d.\n\n> a^H_noop is not defined\n\nThe first mention is in the equation:\n\n$\\exists a^H_{noop} \\in A^H, \\forall \\tau \\in \\text{Traj}(\\pi^{R*}), \\; \\left[ \\forall t < t_{act} : a^R_t\\text{ is communicative }\\wedge \\forall t \\geq t_{act} : \\; a^H_t = a^H_{noop} \\right]$\n\n$a^H_{noop}$ is simply whichever human action makes this equation hold (it is bound by the initial $\\exists$ quantifier.) Perhaps the name is too evocative though -- do you have suggestions for alternative notation here?\n\n> I consider the first paragraph of Section 4 to be the \"meat\" of the paper, and I think some of this content should be front-loaded more.  It's a shame to wait until page 6.\n\nIndeed, we would also have liked to do this. The main issue is that the qualitative examples in Section 4 do depend on having adequate formalizations (Section 2) as well as the notions of \u201ccommunicative\u201d and \u201ctwo phase\u201d (Section 3), and so it seemed quite challenging to bring Section 4 any earlier in the paper. If there was no space limitation, we could consider having an extra section between Sections 1 and 2 that provide the qualitative examples, say what behavior we want from them, and say why reward learning wouldn\u2019t show those behaviors, but then we would still need to have the current Section 4 later to explain how exactly we do get those behaviors from the assistance paradigm, which means this would lengthen the paper too much.\n\nFigure 1 is our attempt to front-load some of the content of Section 4; we\u2019d be excited to hear any other suggestions you have for front-loading more of the content.\n\n> I would start each of 4.1/2/3 with a paragraph giving a non-mathematical, qualitative description of the result.\n\nWe thought we already had some of this (though not in 4.1), but we\u2019ve added some more details. We\u2019re curious if this addresses what you were thinking about, and if not what you would suggest instead.\n\n> I like the paragraph at top of page 7, which gives an explanation of how reward learning would act/fail.  Can you include such a paragraph in 4.1/3 as well?\n\nYeah, in hindsight this is a clear improvement. We\u2019ve done this for both reward learning and an interactive variant of reward learning. Thanks!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "tQoL9m2B7O", "original": null, "number": 3, "cdate": 1606211028465, "ddate": null, "tcdate": 1606211028465, "tmdate": 1606211028465, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "DcvDLwyij8r", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "We have incorporated the works you mention", "comment": "Thanks for the review, and especially for the very detailed elaboration of related works!\n\n> The key contribution of the submission is unclear (e.g., whether it is a survey, a model, a taxonomy, or all?)\n\nUnfortunately this isn\u2019t a standard \u201ctype\u201d of paper. Our paper compares and contrasts the assistance paradigm to the (very common) reward learning paradigm, making it neither a model, nor a survey, nor a taxonomy.\n\n\n> the qualitative behaviors that emerge from assistance games (Section 4) have been demonstrated in prior research [...], thereby making it difficult to assess the novelty of the formalism.\n\nAs far as we can tell, while some qualitative behaviors have been shown by prior work, they use very different formalisms. For example, amongst your citations, the results of Section 4.2 have only been shown using probabilistic recipe trees (PRTs), which are _extremely_ different from the assistance paradigm.\n\nWe do agree that the behavior in Section 4.3 has been demonstrated previously (we cited shared autonomy). We included it anyway because it is an important contrast to reward learning. We were considering whether to add a Section 4.4 in which R can use its physical actions to \u201cquery\u201d H, but decided against it, precisely because this had been shown in much prior work.\n\n> While the submission does discuss related work (Section 2), the discussion omits several related research threads.\n\nWe are unsure whether you are viewing this as a survey paper -- many of the research threads you list are certainly related, but delving into all of the related threads would be a paper in and of itself.\n\n> (Introduction and Proposition 1) The insight of having a single control policy for both reward learning and control modules has been previously explored.\n\nYes, we agree this has been explored before -- we cited some of these works in Section 2.3.1. On the specific papers you suggested:\n\n> Information gathering actions over human internal state\n\nWhile this is not an exact fit for the active reward learning formalism, since the robot\u2019s \u201cqueries\u201d do affect the state of the environment, it is much closer to that than to full assistance. The robot chooses its actions to maximize information gain, not value of information. This means that we don\u2019t get the benefits of Section 4. In particular, in Section 4.1, the method would function like interactive reward learning -- it would wait until H arrived, then ask about their preferences, and only then start to make the pie. In Section 4.2, the method would either immediately ask where to dispose of wormy apples (if the information gain is worth the time cost) or would never ask (if the information gain is too small to be worth it).\n\n> Planning with trust for human-robot collaboration\n\nThanks, this is a good example of the use of an assistance-style formalism. We have added a sentence to Section 2.3, alongside Macindoe et al (2012) and Woodward et al (2019). (Note though that this paper does not discuss or demonstrate any of the qualitative behaviors we identify, nor does it compare against reward learning.)\n\n> Modeling and solving human-robot collaborative tasks using pomdps\n\nThis is another good example.\n\n> Game-theoretic modeling of human adaptation in human-robot collaboration\n\nThis paper seems to be about how the robot can teach the human, rather than about how the human can teach the robot as in our setting. A quote: \u201cwe assume that in the beginning of the game, the robot has perfect information about the reward matrix\u201d.\n\n> Assistance games, as defined, assume parametric specification of the hypothesis space of the reward / preference of humans. However, nonparametric extensions (both for assistance and reward learning) have been proposed.\n\nThe parametric definition is simply for exposition; our results also apply to the non-parametric case. We value exposition over exhaustiveness, since our focus is on conceptual clarity\n\n> (Section 2.3) In assistance games, as defined, the reward (or human preferences over reward) does not change during the task. However, extensions exist which model the latent state corresponding to reward as being locally active and / or time varying. Please consider relating the proposed formalism with these related works.\n\nA detailed discussion is out of scope, as it isn\u2019t relevant to the meat of the paper. We have added a very brief discussion to Section 5.1.\n\n> The behaviors arising from solving assistance games (Section 4) have been previously formalized by multiple human-AI collaboration approaches and demonstrated with human users.\n\nThanks for the citations -- we have added details to Sections 4.2 and 4.3. (None of these seem related to Section 4.1.)\n\n> Please also consider discussing the following related works on active reward learning,\n\nThe listed papers are central examples of active reward learning, which we discussed (with citations) in Section 2.2. It is beyond our scope to discuss each specific paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}, {"id": "DyKva9YED3", "original": null, "number": 2, "cdate": 1606210923156, "ddate": null, "tcdate": 1606210923156, "tmdate": 1606210923156, "tddate": null, "forum": "DFIoGDZejIB", "replyto": "IsJJ_J7rcBM", "invitation": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment", "content": {"title": "The contribution is in identifying the qualitative behaviors and restrictions", "comment": "Thanks for the review! We\u2019re glad that you found the discussion interesting and the paper well-written.\n\n> I am not certain about the contribution of the paper.\n\nTo recap, the primary contributions of our paper are:\n1. Identifying reward learning problems as special cases of assistance problems; in particular corresponding to assistance problems that are _communicative_ and _two phase_.\n2. Illustrating the qualitative behaviors that can arise when we remove these restrictions on assistance problems.\n\n> The derivations, although they seem technically sound, are not particularly surprising\n\nWe think there is value in precisely defining the two characteristics of reward learning (communicative and two phase) that distinguish it from assistance, and showing that these are exhaustive (i.e. there are no other differences that we have failed to uncover). We do not think the theorems were particularly technically challenging or surprising once the two characteristics were identified.\n\n> assistance learning, being a more general problem than reward learning (as follows from the discussion of the paper), will lead to more diverse behaviors.\n\nIt is certainly not surprising that by generalizing the problem setup we are able to get new behaviors, and we do not claim that it is. Our novel contribution is to illustrate what exactly these new behaviors are, and why they might be particularly useful. As an analogy, multiagent environments are a generalization of single agent environments, but it is still useful to study what these new behaviors are (cooperation, competition, commitments, bargaining, etc).\n\nIt also seems to us that the other reviewers did not find the qualitative behaviors particularly obvious.\n\n> The particular behaviors observed are also a consequence of the POMDP formulation, as the agent will act to strike an adequate tradeoff between information gathering and goal optimization.\n\nAnalogously, all theorems are consequences of their axioms. But surely after stating a set of axioms, there is still useful work to be done in proving theorems?\n\nWe assume you meant that the key feature of POMDPs is that they allow for trading off information gathering and goal optimization (which was known before our work), and all of the behaviors we outline are easily-understood consequences of that fact. We do expect researchers who have spent a lot of time working on human-robot interaction in POMDPs to intuitively understand most of these qualitative behaviors, but we would not expect a typical AI researcher to identify these qualitative behaviors given just the formalisms for them."}, "signatures": ["ICLR.cc/2021/Conference/Paper2050/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benefits of Assistance over Reward Learning", "authorids": ["~Rohin_Shah1", "~Pedro_Freire1", "~Neel_Alex1", "~Rachel_Freedman1", "~Dmitrii_Krasheninnikov1", "~Lawrence_Chan2", "~Michael_D_Dennis1", "~Pieter_Abbeel2", "~Anca_Dragan1", "~Stuart_Russell1"], "authors": ["Rohin Shah", "Pedro Freire", "Neel Alex", "Rachel Freedman", "Dmitrii Krasheninnikov", "Lawrence Chan", "Michael D Dennis", "Pieter Abbeel", "Anca Dragan", "Stuart Russell"], "keywords": ["assistance", "reward learning", "preference learning", "active learning"], "abstract": "Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward model through human feedback that is provided externally from the environment. The second is assistance, in which the human is modeled as a part of the environment, and the true reward function is modeled as a latent variable in the environment that the agent may make inferences about. The key difference between the two paradigms is that in the reward learning paradigm, by construction there is a separation between reward learning and control using the learned reward. In contrast, in assistance these functions are performed as needed by a single policy. By merging reward learning and control, assistive agents can reason about the impact of control actions on reward learning, leading to several advantages over agents based on reward learning. We illustrate these advantages in simple environments by showing desirable qualitative behaviors of assistive agents that cannot be found by agents based on reward learning.", "one-sentence_summary": "Illustrate qualitative advantages of assistive agents over agents that use reward learning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shah|benefits_of_assistance_over_reward_learning", "pdf": "/pdf/96d0eb5e1ece0b892cf21c8328fe2c1ddbfb6942.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JYBbbLC_UZ", "_bibtex": "@misc{\nshah2021benefits,\ntitle={Benefits of Assistance over Reward Learning},\nauthor={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},\nyear={2021},\nurl={https://openreview.net/forum?id=DFIoGDZejIB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DFIoGDZejIB", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2050/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2050/Authors|ICLR.cc/2021/Conference/Paper2050/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2050/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852833, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2050/-/Official_Comment"}}}], "count": 29}