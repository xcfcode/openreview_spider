{"notes": [{"id": "iEcqwosBEgx", "original": "dFPkH7lIqlD", "number": 1214, "cdate": 1601308135944, "ddate": null, "tcdate": 1601308135944, "tmdate": 1614985736862, "tddate": null, "forum": "iEcqwosBEgx", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5WEbQYRbQ6r", "original": null, "number": 1, "cdate": 1610040401427, "ddate": null, "tcdate": 1610040401427, "tmdate": 1610473997443, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper investigate the interesting problem of policy seeking in reinforcement learning via constrained optimization. Conditioned on reviewers' judgements, this is a good submission but hasn't reached the bar of ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040401414, "tmdate": 1610473997427, "id": "ICLR.cc/2021/Conference/Paper1214/-/Decision"}}}, {"id": "cdwvByElFz7", "original": null, "number": 7, "cdate": 1605287491095, "ddate": null, "tcdate": 1605287491095, "tmdate": 1606235412127, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "Rebuttal Revision Version Updated", "comment": "Dear Reviewers, AC, PC, and Readers,\n\nIn our revised version of the work, we \n1. include a more complex environment, the Ant-v3 (in Appendix F), which has 111-dim state space and 8-dim action space, to demonstrate our claims in the paper: the constrained optimization perspective of novelty-seeking problems can help to generate well-performing policies with diverse behaviors.\n2. include a paragraph of problem setting to address the concern of Review #1.\n3. include more qualitative analysis in Appendix G.\n4. provide experiment results in Ant-v3 (in Appendix F) to support our claim on the superiority of Wasserstein distance over KL-divergence as a distance metric in quantifying the differences between policies.\n\nThanks for your attention,    \nPaper 1214 Authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "HjIn2-9ym7", "original": null, "number": 11, "cdate": 1606235205182, "ddate": null, "tcdate": 1606235205182, "tmdate": 1606235205182, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "7JXOyOUWwX9", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "Adding Comparison Between IPD and Hong et al. 2018", "comment": "We have updated a revision version of our paper and include a comparison between IPD, WSR with Wasserstein distance, and the method of Hong et al. 2018 using KL-divergence in Figure 4 (left). The method of Hong et al. can be regarded as the WSR method with KL-divergence as a distance metric. It can be seen that the diversity of agents trained with Hong et al. have a relatively large standard deviation than other methods, showing the instability of applying such a *distance* metric(the KL-divergence is not a distance metric, rigorously.). In 3M training timesteps, the method of Hong et al. can not converge to a satisfying performance compared to its Wasserstein correspondence, WSR. Thus the superiority of Wasserstein distance over KL-divergence can be seen clearly.\n\nWe want to highlight that applying KL-divergence may also lead to numerical instability due to the log operation over proportions of two policy distributions, and normally a clip function[1] is used to tackle such an issue. On the other hand, using the Wasserstein distance will not suffer from such instability as the computation of the Wasserstein distance between two Gaussian distributions is much simpler.\n\nTo sum up, we believe there are adequate reasons to use the Wasserstein metric rather than the KL-divergence:\n1. Rigorous definition of a distance metric: as we have shown in our work, using Wasserstein can help to build up the distance metric between policies.\n2. Numerical stability: the computation of Wasserstein distance between Gaussian policies is much simpler than applying the KL-divergence. And avoiding the division as well as the clip operation can improve the stability and accuracy, regarding the clipping as an approximation.\n3. Our additional experiment results in Ant-v3 support our claim that the Wasserstein distance is a better choice.\n\nThank you so much for your insightful comments, reply, and advice!\n\n[1] Dhariwal, Prafulla, et al. \"Openai baselines.\" (2017)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "7JXOyOUWwX9", "original": null, "number": 10, "cdate": 1606198785953, "ddate": null, "tcdate": 1606198785953, "tmdate": 1606198785953, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "BRlZP6Znwb", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "On the Distance Metric", "comment": "The main concern we do not apply KL-divergence as the distance metric is due to its un-symmetry.\n\nAs pointed by reviewer#2, in the work of Hong et al. 2018, the KL-divergence is used for distance calculation.  However, all the previous work including PPO/ TRPO etc. concentrates on the temporal diversity or exploration diversity of a single policy, while our work mainly focus on diversity between policies. \n\nWe will soon provide a comparison between IPD with the Wasserstein metric and using KL-divergence as Hong et al. 2018 did in their work."}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "BRlZP6Znwb", "original": null, "number": 9, "cdate": 1606196224793, "ddate": null, "tcdate": 1606196224793, "tmdate": 1606196224793, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "porKHMxzpi", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "The benefits of the proposed metric.", "comment": "Thanks for your clear response! I can get that KL-Divergence may suffer from the support disjoint problem, which may lead to numerical instability.  I am curious about how existing works, which adopt KL-divergence as distance measure, address such numerical problem. Also, is this problem crucial in practice? \n\nI would like to echo Reviewer2's comments on the metric. A comparative study on the proposed metric and KL-Divergence can make your claim much more stronger. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "cwCeV9zAJX", "original": null, "number": 8, "cdate": 1606112830628, "ddate": null, "tcdate": 1606112830628, "tmdate": 1606112830628, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "nRoGXy35z5U", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "Additional Experiments and More on Q5", "comment": "We run additional experiments on the Ant-v3 environment to show the performance of our algorithm on the more complicated continuous control task (with 111-dim state space and 8-dim action space). Results are shown in Figure 4. Our method of IPD achieves on-par performance with PPO but improves the novelty between policies by $20$ percent.\n\nNotably, both the performance and novelty of CTNB in Ant-v3 is better than its multi-objective optimization counterpart, the TNB, we attribute the reason to the limited training timesteps in TNB and CTNB: in limited training timesteps (3M timesteps in Ant-v3), the policies trained with CTNB and TNB can not converge to well-performing policies, and therefore the behavior difference between those policies are limited (even less than PPO). On the contrary, the method of IPD does not fuse the gradient of primal task reward and the novelty reward, thus similar learning efficiency can be achieved and result in well-performing policies. \n\nThis experiment demonstrates our claim on the superiority of constrained optimization perspectives of novelty-seeking again: too much pursuance of the novelty will hinder the primal task performance as well as hinder the generating of both diverse and well-performing policies."}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "GYnehVcho7", "original": null, "number": 3, "cdate": 1605116142258, "ddate": null, "tcdate": 1605116142258, "tmdate": 1605417681106, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "NLAfW2ioxlH", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "Author Responses to Reviewer 2", "comment": "Thanks for your insightful comments. Please see our responses below.\n\nQ1: The combination of CTNB and IPD\n\nA combination of the two proposed methods is not included in our work as we propose CTNB mainly to demonstrate the benefit of constrained optimization perspective of novelty-seeking over the previous multi-objective optimization methods. In all experiments, the CTNB outperforms TNB in the primal task reward while diminishing some diversity. However, in experiments, we also find that TNB can not always find more diverse policies, partially because gradient fusion in TNB and CTNB is not stable [1, 2]. On the contrary, the method of IPD does not need gradient fusions and performs more robust in different environments.\n\n\nQ2: On the proposed novelty metric.\n\nIn previous work, the distance of policies has been explored. e.g., In TRPO and PPO, the distance between new and old policies is used to constrain the policy updates during policy learning. \nHowever, in these work the distances are always modeled by the KL-divergence, as in their problem settings there is no need for a rigorous distance metric, which should have the property of symmetry. In our work, the absolute values of such a distance are important and with our definition, the distance between $\\theta_i$ and $\\theta_j$ is the same as between $\\theta_j$ and $\\theta_i$. This makes our definition necessary and distinguished from the previously defined distance.\n\nIn another previous work of TNB, the authors considered using Auto-Encoders as a black-box distance measure, and the evaluation of policy distances is conducted on the trajectory level. With our proposed metric, there is no need for building another neural network for novelty measurement and more importantly, the distance can be calculated at every timestep, and such immediate novelty reward (instead of episodic novelty reward) enables our implementation of IPD.\n\n\nQ3: More environments:\n\nWe will run more higher-dimensional experiments and provide the results. (However, the process may take some time as the problem setting needs to train novel policies sequentially)\n\nFor the maze environment, we provide a simple maze (without obstacles) environment in our appendix to visualize different methods. For more complex maze environments, considering diverse policies may not be helpful as the optimal solution can easily be defined according to the reward function. Exceptions include in some tasks, a vanilla algorithm like PPO may always converge to a bad local minimum [3]. It becomes another task of better exploration with diversity-seeking. In such cases, the criteria should be finding one policy that can solve the task instead of a set of diverse policies that are able to solve the same task in different ways, which is not the main topic of our work.\n\n\nQ4: On the threshold $r_0$:\n\nIn principle, fixing the threshold $r_0$ might be harmful to the later policies as they need to be optimized under more constraints. In our experiments, we choose to fix the $r_0$ for conciseness and we believe a search on the decay mechanism of such $r_0$ can be helpful for improving the task performance. \n\nQ5: Visualization: \nWe provide visualization results of the locomotion tasks in our revised paper.\n\n[1] Sener, Ozan, and Vladlen Koltun. \"Multi-task learning as multi-objective optimization.\" Advances in Neural Information Processing Systems. 2018.    \n[2] Yu, Tianhe, et al. \"Gradient surgery for multi-task learning.\" arXiv preprint arXiv:2001.06782. 2020.     \n[3] Zhang, Yunbo, Wenhao Yu, and Greg Turk. \"Learning novel policies for tasks.\" arXiv preprint arXiv:1905.05252 (2019)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "a3880670_zY", "original": null, "number": 2, "cdate": 1605116030878, "ddate": null, "tcdate": 1605116030878, "tmdate": 1605116470821, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "jOp2T9iGhp", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "Author Responses to Reviewer 1", "comment": "Thanks for your insightful comments. Please see our responses below.\n\n[Problem Setting] What\u2019s the exact problem setting? The authors should clearly describe the problem setting before presenting the methods, even one paragraph would be helpful.\n\nThanks for pointing it out. Our problem setting is: given a task, we hope to find a set of diverse policies that can make a good trade-off between diversity (novelty) and the primal task performance. We regard the diversity of 10 independently trained PPO policies as baseline, and during training new policies sequentially, we aim at generating policies that behave differently from all previous policies, but try to keep the primal task performance. We will provide a more detailed description in the revision.\n\nQ1 and Q4: Why use $q = \\bar{\\rho}$? Why just using the on policy samples to estimate the distance? \n\nIn principle, q can be selected as any distribution on the state space. It must remain unchanged in considering any given policies $\\theta_i, \\theta_j$ in calculating their distance according to the symmetry property of a metric by definition.\n\nHowever in practice, it will be difficult to sample from a given distribution other than the state visitation distribution of the current policy [1, 2]. We therefore rely on the assumption that policies generated by the same algorithm like PPO are similar in state visitation frequency and use approximation $\\bar{\\rho} \\approx \\bar{\\rho_{\\theta}}$ in distribution, $\\forall \\theta \\in \\Theta_{PPO}$.\nWe provide some illustration on such a choice below the Equation (4) in our paper.\n\nThe choice of using on-policy samples in estimating the distance is by the consideration of computational efficiency. Otherwise we may need to maintain a buffer of states that has been visited by previous policies and do much more inference in calculating the difference. Notice that the above approximation will be bad only when the visitation frequency of the current policy is clearly different from previous policies, in which case we have found a novel policy.\n\n\nQ2: what is $\\rho_{\\theta_i}$ in (4)? Is it the current policy, or a reference policy?\n\n$\\theta_i$ is the current policy and $\\theta_j$ is the reference policy.\nWe have a description of this in our paper (the paragraph above Proposition 2): \u2018However, during training, \u03b8i is a policy to be optimized and \u03b8j \u2208 \u0398ref is a fixed reference policy.\u2019\n\nQ3 and Q5: Computation issue, and implementation?\n\nWe believe this question can partially be answered by the response for Q1 and Q4. The importance correction is assumed to be close to $1$ as an approximation considering the computational efficiency. \n\n\nQ6: How the novel policy seeking problem is related to the exploration problem.\n\nIndeed we believe the topic of novel policy seeking is closely related to the exploration problems in RL. We have provided discussion on the relation as well as the differences between the objective and previous exploration papers in the related work.\nGenerally speaking, our work provides an approach of generating a set of well-performing diverse policies, which can be potentially used for downstream applications like policy ensemble [3-5], or maybe replacing the random initialization-based bootstrapping value estimations in [6, 7]. We leave these applications of learning novel policies to future work and focus on how to get a set of well-performing diverse policies in this work.\n\n[1] Sutton, Richard S., et al. \"Policy gradient methods for reinforcement learning with function approximation.\" Advances in neural information processing systems. 2000.    \n[2] Silver, David, et al. \"Deterministic policy gradient algorithms.\" 2014.  \n[3] Anschel, Oron, Nir Baram, and Nahum Shimkin. \"Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2017.  \n[4] Wiering, Marco A., and Hado Van Hasselt. \"Ensemble algorithms in reinforcement learning.\" IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 38.4 (2008): 930-936.  \n[5] Fau\u00dfer, Stefan, and Friedhelm Schwenker. \"Neural network ensembles in reinforcement learning.\" Neural Processing Letters 41.1 (2015): 55-69.  \n[6] Osband, Ian, John Aslanides, and Albin Cassirer. \"Randomized prior functions for deep reinforcement learning.\" Advances in Neural Information Processing Systems. 2018.  \n[7] Osband, Ian, et al. \"Deep exploration via bootstrapped DQN.\" Advances in neural information processing systems. 2016.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "porKHMxzpi", "original": null, "number": 6, "cdate": 1605116359713, "ddate": null, "tcdate": 1605116359713, "tmdate": 1605116443843, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "DHGXlNfDicD", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "Author Responses to Reviewer 4", "comment": "Thanks for your insightful comments and your precise summary of our work! Please see our responses below.\n\nQ1: The benefits of using our proposed metric.\n\n1. Different from previous approaches using KL-Divergence in modeling policy differences [1-3], the distance measure we proposed is a distance metric with the property of identity of indiscernible, symmetry, and subadditivity.\n2. The proposed metric can be easily calculated in practice with Gaussian policy parameterization. And do not suffer from the support disjoint problems[4] when using KL-Divergence, which is extremely important for numerical stability during training.\n3. Another way of modeling the difference between policies is to use additional neural networks[5,6]. Compared to our proposed metric, this approach is computationally expensive and can not result in immediate (per-timestep) reward which is crucial for the learning of IPD.\n\n\nQ2: How to choose $r_0$ for different environments? And soft constraint. (Also in Review#3 Q6 and Review#2 Q4)\n\nIn our experiments we use a very simple method to determine the thresholds in different tasks. We regard the average distance between a group of PPO policies as baseline, and choose a threshold around that baseline in Figure 3. \nThe intuition is: the novel policies we have should be more diverse than the naive PPO policies with different random seeds.\n\nYes, using a soft constraint may help to relax the constraints in training later policies and fixing the threshold $r_0$ might be harmful for the later policies as they need to be optimized under more constraints. In our experiments we choose to fix the $r_0$ for conciseness and we believe a search on the decay mechanism of such $r_0$ can be helpful for improving the task performance. And it is also possible to determine the $r_0$\u2019s according to the variety of other policies at each state, i.e., consider the constraints as \u201cgetting more novelty at every timestep, relatively\u201d. We will try this in our future research.\n\n\n\n[1] Hong, Zhang-Wei, et al. \"Diversity-driven exploration strategy for deep reinforcement learning.\" Advances in Neural Information Processing Systems. 2018.     \n[2] Schulman, John, et al. \"Trust region policy optimization.\" International conference on machine learning. 2015.  \n[3] Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).  \n[4] Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein gan.\" arXiv preprint arXiv:1701.07875 (2017).  \n[5] Burda, Yuri, et al. \"Exploration by random network distillation.\" arXiv preprint arXiv:1810.12894 (2018).  \n[6] Zhang, Yunbo, Wenhao Yu, and Greg Turk. \"Learning novel policies for tasks.\" arXiv preprint arXiv:1905.05252 (2019).  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "nRoGXy35z5U", "original": null, "number": 5, "cdate": 1605116311724, "ddate": null, "tcdate": 1605116311724, "tmdate": 1605116422034, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "yglCJ3Obhmx", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "References", "comment": "[1] Sener, Ozan, and Vladlen Koltun. \"Multi-task learning as multi-objective optimization.\" Advances in Neural Information Processing Systems. 2018.     \n[2] Yu, Tianhe, et al. \"Gradient surgery for multi-task learning.\" arXiv preprint arXiv:2001.06782. 2020.    \n[3] Fujimoto, Scott, Herke Van Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" arXiv preprint arXiv:1802.09477 (2018).     \n[4] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).         \n[5] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).     \n[6] Ciosek, Kamil, et al. \"Better exploration with optimistic actor critic.\" Advances in Neural Information Processing Systems. 2019.           \n[7] Tessler, Chen, Guy Tennenholtz, and Shie Mannor. \"Distributional policy optimization: An alternative approach for continuous control.\" Advances in Neural Information Processing Systems. 2019.     \n[8] Hafner, Danijar, et al. \"Dream to control: Learning behaviors by latent imagination.\" arXiv preprint arXiv:1912.01603 (2019).        \n[9] Badia, Adri\u00e0 Puigdom\u00e8nech, et al. \"Agent57: Outperforming the atari human benchmark.\" arXiv preprint arXiv:2003.13350 (2020)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "yglCJ3Obhmx", "original": null, "number": 4, "cdate": 1605116290765, "ddate": null, "tcdate": 1605116290765, "tmdate": 1605116290765, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "tMLMjipJE9", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment", "content": {"title": "Author Responses to Reviewer 3", "comment": "Thanks for your insightful comments. Please see our responses below.\n\nQ1: Prior knowledge:\n\nNo, in our work we do not assume any prior knowledge. For the problem setting, we must have a set of policies to make comparison, or we can not define the concept of novelty.\nDuring learning, every policy is trained from scratch.\n\nQ2: Variance of the estimation:\n\nWe do not consider the variance reduction issue because in practice we find the method performs well with the single trajectory estimation, which is the most concise and convenient approach based on the proposed novelty metric.\n\nQ3: $r_{int}$, and moving average:\n\nWe assume your question is on $r_{int, t}$ and $g_{int}$ because we do not use $r_{int}$ in (5) or (6).\n\nAs we defined in the paper, $g_{int} = \\sum_{t=0}^{t = T} r_{int, t}$ is the intrinsic reward (i.e., novelty in our problem setting) in an episode, while $r_{int, t}$ is the intrinsic reward at timestep $t$. At each timestep, the policy receives an immediate reward, and we denote different values of such rewards with subscript $t$.\n\nThe moving average means we permit a new policy to perform as previous policies in some certain step. On the contrary, if we do not include such a moving average operation, a new policy should be different from previous policies in EVERY timestep, which we believe is too strong to be a constraint. For example, in locomotion tasks there might be some crucial states that only limited choice of actions can result in a healthy state in the future, and forcing a policy to perform differently in such states will strongly hinder the performance.\n\n\nQ4: Ts in Equation (4) and Equation (6):\n\n$T$\u2019s have the same meaning of denoting the maximal environment steps. In Mujoco tasks, the default setting is 1000 and we use the default settings in our experiments.\n\nQ5: CTNB is not novel enough:\n\nWhile we propose two algorithms in the work, the main purpose of CTNB is to demonstrate the priority of constrained optimization perspective for the task of novelty seeking over the previous multi-objective optimization methods. In all experiments, the CTNB outperforms TNB in the primal task reward while diminishing some diversity, which supports our claim that the constrained optimization perspective can help to improve the primal task performance.\n\nAs pointed, in experiments we also find that TNB can not always find more diverse policies, partially because gradient fusion in TNB and CTNB are not stable [1, 2]. On the contrary, the method of IPD does not need gradient fusions and performs more robust in different environments. According to the stronger empirical performance, IPD is the recommended solution for novelty seeking tasks.\n\nQ6: Selection of $r_0$:\n\nIn our experiments we use a very simple method to determine the thresholds in different tasks. We regard the average distance between a group of PPO policies as baseline, and choose a threshold around that baseline in Figure 3. \nThe intuition is: the novel policies we have should be more diverse than the naive PPO policies with different random seeds.\n\nQ7: Number of random seeds:\n\nWhile it is reasonable to use more random seeds to test an algorithm, in most cases, the conclusions can be drawn clearly with around five to ten seeds[3-9]. (Those references include both on-policy[7] methods and off-policy[3-6,9] methods, both continuous control[3-8] and discrete domains[9], and both model-based[8] and model-free[3-7,9] approaches in RL).\n\nMoreover, in our experimental setting, 5 random seeds can generate 5 * 10 policies for each environment in total. And it can be seen that in our main results Figure 2, conclusions can be made clearly that IPD generates more diverse policies, while at least able to keep the primal task performance.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iEcqwosBEgx", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1214/Authors|ICLR.cc/2021/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862318, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Comment"}}}, {"id": "DHGXlNfDicD", "original": null, "number": 1, "cdate": 1603782184165, "ddate": null, "tcdate": 1603782184165, "tmdate": 1605024501061, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "This paper proposes a novel constrained optimization based method, to optimize the expected return as well as encourage novelty of a new policy in contrast to existing policies. By modeling the problem as a constrained optimization problem, they can avoid excessive novelty seeking effectively, which is common in existing methods which model the problem with multi-objective optimization. \n\nTo be specific, they first propose a novel metric to measure the novelty of a new policy. To estimate such metric on sampled state with dense online reward, they propose an importance-based estimator for the proposed metric. With the estimation of the novelty metric, they propose to formulate the problem as a constrained optimization problem. The novelty is constrained to be larger than certain threshold r_0. In this way, the algorithm will only encourage larger novelty when the novelty is less than r_0, therefore avoiding excessive novelty seeking which may hurt the performance. They improve TNB proposed in (Zhang et al., 2019) with CTNB, where the \u2207\u03b8g term exists only when the constraint is violated. They also propose another method based on Interior Point Method. Since IPM is computationally expensive and numerically unstable, they made an adaptation to RL setting, by bounding the collected transitions in the feasible region. \n\nOverall, the method is intuitive and reasonable. I have the following questions:\n\n1. The first contribution of this paper, is proposing a novel metric to measure the novelty of current policy in contrast to existing policies. Why propose a novel metric? Is existing metric for measuring the novelty not good? If so, can you verify your claim in experiments? \n\n2. The hyper-parameter r_0.  From Figure 3, we can see the different performance under different novelty thresholds r_0. The algorithm seems to be sensitive to r_0, which is of course reasonable. How did you choose r_0 for different environments? Did you consider a soft r_0 rather than a hard constraint(that is, maybe the constraint has different weight for different r_0)?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123952, "tmdate": 1606915773017, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1214/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review"}}}, {"id": "tMLMjipJE9", "original": null, "number": 2, "cdate": 1603855433232, "ddate": null, "tcdate": 1603855433232, "tmdate": 1605024500996, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review", "content": {"title": "Seeking novel policy only after finding a good one ", "review": "This paper aims at novel policy seeking which incorporates curiosity-driven exploration for better reinforcement learning. This paper first propose to use  a Wasserstein-based metric to calculate the difference between policies, and use it to define  the policy novelty. With these, the authors modeled the novel policy seeking as a constrained markov decision process(CMDP) and solved it using CTNB and IPD.\n\n1. This paper allows to consider the novelity issue dynamically.  However, when training policy according to the proposed CTNB or IPD, there should be some pretrained policies as perconditions, in other words, the proposed method needs some prior knowledge rather than learning policy from scratch. This may be a limitation for its application.\n\n2. About the proposition 2, the single trajectory estimation is unbiased, however, the variance seems to be large, the influence about the estimation variance should be considered.\n\n3.  in fomula (5) and (6), is $r_{int, t}$ equal to $r_{int}$ ? If is,  why use t? and what does moving average mean since there are several kinds of moving averages?  \n\n4. in fomula (4) and (6), Are Ts the same?\n\n5. Fig.2  shows that in Waklker2d and HalfCheetah, the proposed CTNB has less novel than PPO, which doesn't match the purpose of CTNB.\n\n6. It seems not easy to tune the novelty threshold for different task, as it performs different on different tasks. Can the author provide some insight on how to tune this.\n\n7. Five random seeds is not sufficient for experiments. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123952, "tmdate": 1606915773017, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1214/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review"}}}, {"id": "NLAfW2ioxlH", "original": null, "number": 3, "cdate": 1603859502764, "ddate": null, "tcdate": 1603859502764, "tmdate": 1605024500930, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review", "content": {"title": "Interesting method but can be more convincing", "review": "Summary: This paper proposed a method to leverage the constrained optimization for policy training to learn diverse policies given some references. Based on a diversity metric defined on policy divergences, the paper employs two constrained optimization techniques for this problem with some modifications. Experiments on mujoco environments suggest that the proposed algorithms can beat existing diversity-driven policy optimization methods to learn both better and novel policies. Generally, the paper is well-written and easy to follow. Some concerns/comments:\n\n* The state distributions of proposed CTNB and IPD are different: In the CTNB method, the trajectories will keep rollout until they reach some termination conditions such as time limit or failure behavior. However, in the IPD method, if the cumulative novel reward is below some thresholds, then the trajectories will be truncated. It will be helpful to compare the CTNB with that extra termination condition. \n\n* Using the divergence of policies to quantify the difference between policies seems not a very innovative metric. Some related work could be:\n\nHong, Z. W., Shann, T. Y., Su, S. Y., Chang, Y. H., Fu, T. J., & Lee, C. Y. (2018). Diversity-driven exploration strategy for deep reinforcement learning.\n\nIt will be great if the authors can compare and explain the relationship between the proposed metric and some related ones.\n\n* The experiments can be more convincing if more locomotion environments are included, especially some higher-dimensional environments such as Humanoid and HumanoidStandup. Also, some other environments with a long-term/sparse reward setting can be more illustrative such as some mazes or Atari games. For some of those games, since it is stage-based, the IPD might terminate some rollouts if all reasonable policies are similar at the beginning of the trajectory. For a maze example, all good policies should choose to open the door at the beginning and then behave diversely.  \n\nOther/Minor Comments:\n\n* The choice of r_0 can affect the performance: When sequentially training the policy, should r_0 be adjusted when training each new policy?\n\n* It can be more interesting if some visualization of hopper policy diversity is included.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123952, "tmdate": 1606915773017, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1214/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review"}}}, {"id": "jOp2T9iGhp", "original": null, "number": 4, "cdate": 1604036668988, "ddate": null, "tcdate": 1604036668988, "tmdate": 1605024500860, "tddate": null, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "invitation": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review", "content": {"title": "Novel Policy Seeking with Constrained Optimization", "review": "Summary:\n\nThis paper attempts to solve the problem of seeking novel policies in reinforcement learning from a constrained optimization perspective. This new perspective motives two new algorithms to solve the optimization problem, which are based on feasible direction and the interior point methods. The authors provide empirical results on several Mujoco benchmarks. \n\nDetails:\n\nThe idea of formulating the problem from a constrained optimization perspective is interesting. This new perspective motivates new and better algorithms to solve the optimization problem. \n\nHowever, I feel like the presentation is poor and the writing should be improved.  \n\nWhat\u2019s the exact problem setting? The authors should clearly describe the problem setting before presenting the methods, even one paragraph would be helpful.  \n\nA lot of algorithm details are missing: \n\nQ1. $\\bar{D}^q_W (\\theta_i, \\theta_j)$ is a metric for any state distribution $q$. What\u2019s the motivation of using $q = \\bar{\\rho}$? \n\nQ2. When computing the policy distance, what is $\\rho_{\\theta_i}$ in (4)? Is it the current policy, or a reference policy?\n\nQ3. I assume $\\theta_i$ is the current policy. According to (4), the algorithm uses $\\theta_i$ to get samples, and compute an importance correction ratio $q/\\rho_{\\theta_i}$ to approximate the distance. How is the $q(s)=\\bar{\\rho}(s)$ computed? The authors propose to approximate $\\rho_{\\theta}$ using monte-carlo methods. Does it mean the algorithm need to approximate $\\bar{\\rho}(s)$ using the reference policies for each $s\\sim \\rho_{\\theta_i}$? Is there a computation issue?\n\nQ4. This goes back to Q1. Why just using the on policy samples to estimate the distance? Is there any potential advantage to use $q = \\bar{\\rho}$? \n\nQ5. Learning the stationary distribution is a hard research problem itself. See recent work for example: \n\nZhang, R., Dai, B., Li, L. and Schuurmans, D., 2019, September. GenDICE: Generalized Offline Estimation of Stationary Values. In International Conference on Learning Representations.\n\nI agree the stationary distribution can be approximated using MC methods, but it might need a lot of samples as the variance is very high. This makes me wonder how is the algorithm implemented in practice, and how does the stationary distribution estimation subroutine affect the algorithm\u2019s performance. \n\nOther suggestions:\n\nIf I understand correctly, this paper tries to solve the problem of finding a set of novel polices that solve a given task while exhibiting different behaviors. This seems also related to the exploration problem, as some works try to make the current policy different with previous policies to encourage exploration. See for example:\n\nHazan, E., Kakade, S., Singh, K. and Van Soest, A., 2019, May. Provably efficient maximum entropy exploration. In International Conference on Machine Learning (pp. 2681-2691). \n\nIt might be worth to discuss how the novel policy seeking problem is related to the exploration problem. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1214/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1214/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel Policy Seeking with Constrained Optimization", "authorids": ["~Hao_Sun3", "~Zhenghao_Peng1", "~Bo_Dai2", "guoj@pcl.ac.cn", "~Dahua_Lin1", "~Bolei_Zhou5"], "authors": ["Hao Sun", "Zhenghao Peng", "Bo Dai", "Jian Guo", "Dahua Lin", "Bolei Zhou"], "keywords": ["Novel Policy Seeking", "Reinforcement Learning", "Constrained Optimization"], "abstract": "We address the problem of seeking novel policies in reinforcement learning tasks. Instead of following the multi-objective framework commonly used in existing methods, we propose to rethink the problem under a novel perspective of constrained optimization. We at first introduce a new metric to evaluate the difference between policies, and then design two practical novel policy seeking methods following the new perspective, namely the Constrained Task Novel Bisector (CTNB), and the Interior Policy Differentiation (IPD), corresponding to the feasible direction method and the interior point method commonly known in the constrained optimization literature. Experimental comparisons on the MuJuCo control suite show our methods can achieve substantial improvements over previous novelty-seeking methods in terms of both the novelty of policies and their performances in the primal task.", "one-sentence_summary": "We address the problem of seeking novel policies in reinforcement learning tasks with constrained optimization to generate well-performed diverse policies.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sun|novel_policy_seeking_with_constrained_optimization", "supplementary_material": "/attachment/f9650cb093f0fe97e9fa4f6df613071d25d51776.zip", "pdf": "/pdf/4005ceb2ae6b55f6d6b67ea5027929352e3b024f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=aO2MWpJaWo", "_bibtex": "@misc{\nsun2021novel,\ntitle={Novel Policy Seeking with Constrained Optimization},\nauthor={Hao Sun and Zhenghao Peng and Bo Dai and Jian Guo and Dahua Lin and Bolei Zhou},\nyear={2021},\nurl={https://openreview.net/forum?id=iEcqwosBEgx}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iEcqwosBEgx", "replyto": "iEcqwosBEgx", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123952, "tmdate": 1606915773017, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1214/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1214/-/Official_Review"}}}], "count": 16}