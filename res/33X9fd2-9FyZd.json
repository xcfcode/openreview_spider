{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392691860000, "tcdate": 1392691860000, "number": 5, "id": "vn-PvD2UEmnyw", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "33X9fd2-9FyZd", "replyto": "33X9fd2-9FyZd", "signatures": ["Durk Kingma"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Reviewers, thank you for your overall very positive and constructive reviews. Your points of critique are highly appreciated and will be incorporated into a revised version.\r\n\r\nBelow we listed your key points, followed by our responses.\r\n\r\n==> Reviewer bd7f: \u201cCouldn\u2019t an online Monte Carlo EM approach be followed?\u201d\r\n\r\nWe tried an online version of Monte Carlo EM algorithm, but found that the online version didn\u2019t compare favourably to the batch Monte Carlo EM algorithm used in experiments. The main reason is that the computational cost per E-step grows with the size of the dataset. With larger datasets, each minibatch gets processed more rarely, and the posterior distribution changes to a higher degree between epochs, such that more MCMC samples are required to get an approximately unbiased sample from the new posterior given a sample from the old posterior computed at the previous epoch.\r\n\r\n==> Reviewer 79a6: \u201cThe experiments compare favourably [\u2026] but don\u2019t give the reader a good sense of the trade-offs. A simple way to rectify this would be a discussion of the training: how sensitive the model is to various hyperparameter choices, how robust the model is to overfitting, etc.\u201d\r\n\r\nWe\u2019ll include a discussion of the trade-offs. Some of our responses below also relevant to this point.\r\n\r\n==> Reviewer 79a6: \u201cDifferent noise distributions are discussed, but they are not compared. What are the implications of choosing different classes of perturbation?\u201d\r\n\r\nInteresting question. One important aim is to chose an approximate posterior such that KL(q||p), the divergence of the approximate posterior q(z|x) from the true posterior p(z|x), can be minimized well. We chose a Gaussian since modes of posteriors are often locally approximately quadratic, in which case a Gaussian q(z|x) and leads to a low KL(q||p). We did perform some experiments with student-T (heavy-tailed) noise distributions, which did not improve the reported performance. Performance of this experiment were not included, since the main aim of the experiments is to compare to other approaches, rather than finding the optimal hyper-parameter setting.\r\n\r\n==> Reviewer 79a6: \u201cHow powerful is the regularization effect of the variational bound? At what point does overfitting occur, or do things simply plateau when more latent variables are added?\u201d\r\n\r\nTrain and test performances indeed plateau when increasing the dimensionality of latent space beyond necessity. For the models of MNIST, only about ~20 dimensions were used by the model; the others were 'switched off' automatically: after training their incoming and outgoing weights were approximately zero, and their posterior distributions were always (practically) equal to their prior distributions. Everything else being equal, a model with a latent dimensionality of 200 performed almost identical to a model with latent dimensionality of 20 (see figure 2).\r\n\r\n==> Reviewer 79a6: \u201cAn important baseline would be to measure the effectiveness of this approach on held-out data. For example, rather than training this variational bound, one could perhaps use a regular auto-encoder with a similar architecture and train for reconstruction error. How would the authors expect the test-set reconstruction error of their approach to compare to this baseline? Is the only difference between the proposed approach and a standard auto-encoder the variational regularizer (first term of equation 10)?\u201d\r\n\r\nBesides the regularizer, an important distinctive feature of the variational auto-encoder is the noisy activation of the central hidden layer, distributed as q(z|x). The noise does not contributing to the reconstruction error, and can be tuned down by changing the encoder weights. The effect of removing the regularization term would be that the best solution is to ignore the noise (i.e. the variance of q is set to approximately zero). This would lead to a high KL divergence between the true and approximate posteriors, consequently a low marginal likelihood of the model, and most probably a high reconstruction error on held-out data.\r\n\r\n==> Reviewer 79a6: \u201cIntuitively, what is the regularizer provided by the variational bound doing to the encoding parameters? What would the regularization provided by the full variational approach do to the decoding parameters?\u201d\r\n\r\nThe regularizer makes sure that q(z|x) does not diverge too much from the prior p(z). In practice this means that excess latent dimensions are \u201cswitched off\u201d; for the fully connected models in the paper this means that all the incoming and outgoing connection weights corresponding to excess dimensions are set to zero. Therefore, the dimensionality of latent space does not require much tuning, and should simply be set to a high enough number.\r\n\r\nImportant to note is that overfitting can still occur simply by choosing too many units for the hidden layers of the encoder and/or decoder (i.e. the layers sandwiched between \u2018x\u2019 and \u2018z\u2019). In our comparative experiments we did not tune the amount of hidden units, and simply chose a regime educated by settings that work well for other auto-encoder architectures; we did not encounter overfitting problems in these experiments.\r\n\r\n==> Reviewer 79a6: \u201cIn equation (6), should z^{(l)} be z^{(i,l)}?\u201d\r\n\r\nIndeed."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Variational Bayes", "decision": "submitted, no decision", "abstract": "Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised on-line learning method that efficiently optimizes the variational lower bound on the marginal likelihood and that, under some mild conditions, even works in the intractable case. The method optimizes a probabilistic encoder (also called a recognition network) to approximate the intractable posterior distribution of the latent variables. The crucial element is a reparameterization of the variational bound with an independent noise variable, yielding a stochastic objective function which can be jointly optimized w.r.t. variational and generative parameters using standard gradient-based stochastic optimization methods. Theoretical advantages are reflected in experimental results.", "pdf": "https://arxiv.org/abs/1312.6114", "paperhash": "kingma|autoencoding_variational_bayes", "keywords": [], "conflicts": [], "authors": ["Diederik P. Kingma", "Max Welling"], "authorids": ["dpkingma@gmail.com", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391850300000, "tcdate": 1391850300000, "number": 4, "id": "8jCZt9B-uYtDE", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "33X9fd2-9FyZd", "replyto": "33X9fd2-9FyZd", "signatures": ["anonymous reviewer 79a6"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Auto-Encoding Variational Bayes", "review": "This paper proposes a variational approach for training directed graphical models with continuous latent variables and intractable posteriors/marginals. The idea is to reparameterize the latent variables so that they can be written as a deterministic mapping followed by a stochastic perturbation. This allows Monte Carlo estimators of the variational lower bound to be differentiated with respect to the variational parameters.\r\n\r\nThe proposed method is novel and interesting, and seemingly more practical than previous approaches. The experiments favour comparably and show that this method is quite promising, however they still feel preliminary and don\u2019t give the reader a good sense of the trade-offs and difficulties one might encounter using this approach. A simple way to rectify this would be a discussion of the training: how sensitive the model is to various hyperparameter choices, how robust the model is to overfitting, etc. Another way would be to compare against a tractable model such as PPCA, and show how the approach compares to EM and exact inference. The results here would give a sense of how the method might perform on intractable cases.\r\n\r\nOn a high level, the auto-encoder presented here reminds me of the back-constrained GP-LVM [1]. The goals are different, but the models seem similar.\r\n\r\n[1] Local Distance Preservation in the GP-LVM through Back Constraints, Neil D. Lawrence and Joaquin Quinonero-Candela, ICML 2006\r\n\r\nQuestions:\r\n-Different noise distributions are discussed, but they are not compared. What are the implications of choosing different classes of perturbation?\r\n\r\n-How powerful is the regularization effect of the variational bound? At what point does overfitting occur, or do things simply plateau when more latent variables are added?\r\n\r\n-An important baseline would be to measure the effectiveness of this approach on held-out data. For example, rather than training this variational bound, one could perhaps use a regular auto-encoder with a similar architecture and train for reconstruction error. How would the authors expect the test-set reconstruction error of their approach to compare to this baseline? Is the only difference between the proposed approach and a standard auto-encoder the variational regularizer (first term of equation 10)?\r\n\r\n-Intuitively, what is the regularizer provided by the variational bound doing to the encoding parameters? What would the regularization provided by the full variational approach do to the decoding parameters?\r\n\r\nTypos and grammar:\r\n-The first sentence could perhaps be reworded slightly.\r\n\r\n-Please remove \u201cis\u201d from \u201cis even works\u201d in the sentence \u201dConversely, we are here interested in a general algorithm that is even works efficiently in the case of\u2026\u201d\r\n\r\n-In equation (6), should z^{(l)} be z^{(i,l)}?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Variational Bayes", "decision": "submitted, no decision", "abstract": "Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised on-line learning method that efficiently optimizes the variational lower bound on the marginal likelihood and that, under some mild conditions, even works in the intractable case. The method optimizes a probabilistic encoder (also called a recognition network) to approximate the intractable posterior distribution of the latent variables. The crucial element is a reparameterization of the variational bound with an independent noise variable, yielding a stochastic objective function which can be jointly optimized w.r.t. variational and generative parameters using standard gradient-based stochastic optimization methods. Theoretical advantages are reflected in experimental results.", "pdf": "https://arxiv.org/abs/1312.6114", "paperhash": "kingma|autoencoding_variational_bayes", "keywords": [], "conflicts": [], "authors": ["Diederik P. Kingma", "Max Welling"], "authorids": ["dpkingma@gmail.com", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391491680000, "tcdate": 1391491680000, "number": 3, "id": "iiWhiIg_AYM1I", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "33X9fd2-9FyZd", "replyto": "33X9fd2-9FyZd", "signatures": ["anonymous reviewer 62c4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Auto-Encoding Variational Bayes", "review": "This paper adresses the problem of learning the parameters of directed probabilistic models with continuous latent variables when the latent posterior is intractable.\r\n\r\nThe proposed method (Auto-Encoding Variational Bayes or AEVB for short) can be summarized as:\r\n 1 - Maximization of the variational lower bound as a proxy for the marginal log-likelihood.\r\n 2 - Estimation of this lower-bound using a Monte-Carlo estimation.\r\n 3 - Using a re-parametrization trick for the parametric variational inference approximation q_{\theta}(z|x), i.e. expressing it as a deterministic function f(x,e) of x and e, where e is a random variable.\r\n\r\nThe AEVB method is in theory applicable to a wide range of choices for p(x|z), p(z) and q(z|x). In their study, the authors propose a specific application where the prior p(z) is a simple gaussian centered on 0 and where p(x|z) and q(z|x) are both Gaussian distributions with their means and covariance matrices being determined by the output of a neural network. For instance q(z|x) is defined as\r\n\tq(z|x) = N(z;mu,sigma^2*I) with mu and sigma being determined from x using a neural network, i.e. mu = f_1(x) and sigma = f_2(x), with f_1 and f_2 deterministic.\r\n\r\nThis work is closely related to several recent contributions (Generative Stochastic Networks,Denoising Auto-Encoder theory) which are properly discussed.\r\n\r\nSeveral experiments are run on both the Frey-faces dataset and the Mnist dataset:\r\n - quantitatively comparing the lower bound with AEVB vs with the wake sleep algorithm.\r\n - quantitatively comparing the marginal likelihood (with small latent space as is to be expected) with AEVB vs with the wake sleep algorithm.\r\n - showing visualizations of 2D manifolds learned with AEVB\r\n - showing random samples obtained by sampling from the learned generative model\r\n \r\nThe proposed approach (AEVB) offers a new and exciting solution to the well known problem of estimating the parameters of a graphical model. Both the variational inference and generative distribution can be quite complex as they are both represented using neural networks with hidden layers, however the training criterion is tractable and includes hyper-parameter-free regularization terms which can prevent overfitting, as supported by the experiments.\r\n\r\npros:\r\n - Very good summary of previous work / very good discussion of related work.\r\n - The approach is tractable even with intractable latent posteriors.\r\n - The training criterion contains hyper-parameter free regularization.\r\n - The experiments fully support the practical applicability of the approach.\r\ncons:\r\n - Experiments on more complex datasets would have made for an even more convincing argument."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Variational Bayes", "decision": "submitted, no decision", "abstract": "Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised on-line learning method that efficiently optimizes the variational lower bound on the marginal likelihood and that, under some mild conditions, even works in the intractable case. The method optimizes a probabilistic encoder (also called a recognition network) to approximate the intractable posterior distribution of the latent variables. The crucial element is a reparameterization of the variational bound with an independent noise variable, yielding a stochastic objective function which can be jointly optimized w.r.t. variational and generative parameters using standard gradient-based stochastic optimization methods. Theoretical advantages are reflected in experimental results.", "pdf": "https://arxiv.org/abs/1312.6114", "paperhash": "kingma|autoencoding_variational_bayes", "keywords": [], "conflicts": [], "authors": ["Diederik P. Kingma", "Max Welling"], "authorids": ["dpkingma@gmail.com", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390782300000, "tcdate": 1390782300000, "number": 2, "id": "NAoK5VPWK8Nm3", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "33X9fd2-9FyZd", "replyto": "33X9fd2-9FyZd", "signatures": ["anonymous reviewer bd7f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Auto-Encoding Variational Bayes", "review": "This paper presents a variational learning algorithm  for probabilistic graphical model embedded with a latent representation that is continuous-valued. The proposed algorithm is stochastic in nature. It is able to scale to large datasets and deal with models for which tractable EM is not an option. It is based on gradient descent on a Monte Carlo approximation of the standard variational lower-bound on the marginal likelihood. Crucially, in order to obtain an unbiased gradient estimate, the Monte Carlo approximation is taken such that the sampling of the MC points is independent of the parameters of the model, thanks to an 'alternative parametrization trick' of the variational posterior. An interesting connection is also made with autoencoders. Experiments show that the algorithm can improve over the wake-sleep algorithm.  \r\n\r\nI really like this paper. The idea is simple and non-trivial. The alternative parametrization trick is a nice tool to have in one's toolbox. The connection with autoencoders is also really nice.\r\n\r\nThe only 'cons' I see is that the experiments are somewhat limited, in the sense that they are performed only on MNIST and Frey Faces, and only log-likelihood or visualization results are reported (as opposed to experiments in the context of a real application). But I still think this is good first step in a potentially promising direction for training complex non-linear continuous representation models. So I'd argue that this work deserves to get in.\r\n\r\nMinor comments:\r\n- Couldn't an online Monte Carlo EM approach be followed, where each update of the M step would be performed on a minibatch?\r\n- This is super minor, but NADE isn't just applicable to binary observations and actually has a real-valued extension (RNADE: The real-valued neural autoregressive density-estimator, by Uria, Murray and Larochelle, NIPS 2013). Otherwise however, as the authors mention, it's not directly related to the approach in this paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Variational Bayes", "decision": "submitted, no decision", "abstract": "Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised on-line learning method that efficiently optimizes the variational lower bound on the marginal likelihood and that, under some mild conditions, even works in the intractable case. The method optimizes a probabilistic encoder (also called a recognition network) to approximate the intractable posterior distribution of the latent variables. The crucial element is a reparameterization of the variational bound with an independent noise variable, yielding a stochastic objective function which can be jointly optimized w.r.t. variational and generative parameters using standard gradient-based stochastic optimization methods. Theoretical advantages are reflected in experimental results.", "pdf": "https://arxiv.org/abs/1312.6114", "paperhash": "kingma|autoencoding_variational_bayes", "keywords": [], "conflicts": [], "authors": ["Diederik P. Kingma", "Max Welling"], "authorids": ["dpkingma@gmail.com", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389363660000, "tcdate": 1389363660000, "number": 1, "id": "h_5g8aM13EhoG", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "33X9fd2-9FyZd", "replyto": "33X9fd2-9FyZd", "signatures": ["Durk Kingma"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have just submitted a new version to arXiv.\r\n\r\nIn previous versions, we treated both the MNIST and Frey Face datasets as binary, where the pixel values (normalized to the interval (0,1)) were treated as probabilities, using the negative binary cross-entropy function in place of log p(x|z). This use of the binary cross-entropy function is common practice, and is equivalent (in expectation of the objective and gradients) to log p(x|z) with binary samples drawn from the data according to the probabilities indicated by the pixel values. While this treatment of pixel values makes sense for MNIST (the dataset is almost binary anyway), it makes less sense for Frey Face.\r\nWe therefore ran new experiments where the Frey Face data was treated appropriately as continuous data. We chose log p(x|z) to be an isotropic Gaussian, with a conditional distribution identical to log q(z|x), with a minor difference that the predicted means were constrained to the interval (0,1) by appending a sigmoid activation function to the MLP output units that correspond to the predicted mean of log p(x|z). \r\n\r\nAs shown in the new version of the paper, the wake-sleep algorithm had much more difficulty with learning this (more appropriate) model of the Frey Face data.\r\n\r\nAnother difference is that models were allowed to train for a longer period of time, and results for higher dimensional latent space were included. Most interestingly, superfluous dimensionality of latent space did not result in increased overfitting, which is explained by the regularizing effect of the prior and entropy terms of the objective function."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Auto-Encoding Variational Bayes", "decision": "submitted, no decision", "abstract": "Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised on-line learning method that efficiently optimizes the variational lower bound on the marginal likelihood and that, under some mild conditions, even works in the intractable case. The method optimizes a probabilistic encoder (also called a recognition network) to approximate the intractable posterior distribution of the latent variables. The crucial element is a reparameterization of the variational bound with an independent noise variable, yielding a stochastic objective function which can be jointly optimized w.r.t. variational and generative parameters using standard gradient-based stochastic optimization methods. Theoretical advantages are reflected in experimental results.", "pdf": "https://arxiv.org/abs/1312.6114", "paperhash": "kingma|autoencoding_variational_bayes", "keywords": [], "conflicts": [], "authors": ["Diederik P. Kingma", "Max Welling"], "authorids": ["dpkingma@gmail.com", "jingf@cs.ubc.ca"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387823040000, "tcdate": 1387823040000, "number": 37, "id": "33X9fd2-9FyZd", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "33X9fd2-9FyZd", "signatures": ["dpkingma@gmail.com"], "readers": ["everyone"], "content": {"title": "Auto-Encoding Variational Bayes", "decision": "submitted, no decision", "abstract": "Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised on-line learning method that efficiently optimizes the variational lower bound on the marginal likelihood and that, under some mild conditions, even works in the intractable case. The method optimizes a probabilistic encoder (also called a recognition network) to approximate the intractable posterior distribution of the latent variables. The crucial element is a reparameterization of the variational bound with an independent noise variable, yielding a stochastic objective function which can be jointly optimized w.r.t. variational and generative parameters using standard gradient-based stochastic optimization methods. Theoretical advantages are reflected in experimental results.", "pdf": "https://arxiv.org/abs/1312.6114", "paperhash": "kingma|autoencoding_variational_bayes", "keywords": [], "conflicts": [], "authors": ["Diederik P. Kingma", "Max Welling"], "authorids": ["dpkingma@gmail.com", "jingf@cs.ubc.ca"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 6}