{"notes": [{"id": "Ldau9eHU-qO", "original": "1Oc9VcEmNr", "number": 1279, "cdate": 1601308143031, "ddate": null, "tcdate": 1601308143031, "tmdate": 1615921278447, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DqCGUbTRMp0", "original": null, "number": 1, "cdate": 1610040397219, "ddate": null, "tcdate": 1610040397219, "tmdate": 1610473992601, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "invitation": "ICLR.cc/2021/Conference/Paper1279/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper considers the problem of learning interpretable, low-dimensional representations from high-dimensional multimodal input via weak supervision in a learning from demonstration (LfD) context. To mitigate the disparity between the abstractions that humans reason over and the robot's low-level action and observation spaces, the paper argues for learning a low-dimensional embedding that captures the underlying concepts. The primary contribution of the paper is the ability to learn disentangled low-dimensional representations that are interpretable from weak supervision using conditional latent variable models.\n\nThe paper was reviewed by three knowledgeable referees, who read the author response and discussed the paper. The paper considers a challenging problem in learning from demonstration, namely dealing with the disparity that exists between the ways in which humans and robots model and observe the world, a problem that is exacerbated when reasoning over high-dimensional multimodal observations. As the reviewers note, the use of variational inference to learn low-dimensional interpretable representations from weak supervision is compelling. The primary concerns are that the contributions need to be more clearly scoped and that the experimental evaluation is a bit narrow. The authors make an effort to resolve some of these issues, in part through the inclusion of an additional experiment that considers pouring tasks. However, the extent to which this second task mitigates concerns about the narrow evaluation is not fully clear. The paper would be strengthened by the inclusion of experiments in a less contrived setting (and one for which the concepts are not necessarily disjoint) as well as a clearer discussion of the primary contributions."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040397206, "tmdate": 1610473992580, "id": "ICLR.cc/2021/Conference/Paper1279/-/Decision"}}}, {"id": "y8hDgJnCJQZ", "original": null, "number": 4, "cdate": 1606238816921, "ddate": null, "tcdate": 1606238816921, "tmdate": 1606239273921, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": "BkAZ4Ywf4p", "invitation": "ICLR.cc/2021/Conference/Paper1279/-/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "We thank the reviewer for their comments and feedback.\n\n**Re. comments on results in Tables 2 and 3:** The latent spaces of all trained models can be used to condition on a single user label. It is only the semantically-aligned ones that can be used to compose multiple labels\u2014see Eqs. (7) and (8) and Alg. 1 and 2 in Appendix B. The small difference in numerical results shows that all models have the capacity to represent the full set of demonstrated phenomena. However, unless otherwise optimised, they will be unintuitively entangled and not interpretable. So, even where quantitative results seem close, we show the benefit of using weak labels in Figure 5\u2014we can only perform label composition effectively with a disentangled latent space.\n\n**Re. comments on additional experiments:** We\u2019ve devised an additional experiment based around the task of pouring and different manners of executing it. This physical experiment is still ongoing, but the setup and preliminary description is in Section 7 and Appendix E of the revised manuscript. Videos of provided demonstrations can be seen on the supplementary website. If accepted, the camera ready version will include full results from this experiment. \n\nThe experiment still follows the problem description in Section 2 -- namely some factors of variation describe where to pour (red or blue cups present in the image) and others how to pour (e.g. behind the target cup or on its side). The preliminary results analyse the usefulness of weak supervision through discrete labels for incorporating information from all input modalities. Complete analysis across all different baselines and demonstration concepts, as in section 6, will be in the final version of the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ldau9eHU-qO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1279/Authors|ICLR.cc/2021/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861550, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1279/-/Official_Comment"}}}, {"id": "aCVxfvEYqxr", "original": null, "number": 5, "cdate": 1606238906616, "ddate": null, "tcdate": 1606238906616, "tmdate": 1606239241886, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": "lVIfTjz82a", "invitation": "ICLR.cc/2021/Conference/Paper1279/-/Official_Comment", "content": {"title": "Response to reviewer 4", "comment": "We thank the reviewer for their comments and feedback.\n\n**Re. different styles of supervision:** We agree that studying the effect of different styles, quality and amounts of supervision on the eventual model performance are interesting questions. A potential avenue for future work is analysing how much correlation can be present in the captured data without hurting the generalisation capabilities of the model when composing the disentangled factors of variation. I.e. if the demonstrations contain only 3 out of 4 possible underlying behaviour combinations - e.g. \u201csoft slow dab\u201d, \u201csoft quick dab\u201d and \u201chard slow dab\u201d - can the model still generate the 4th, never-seen one? What about having demonstrated only 2 out of 4 possible behaviour combinations. However, we\u2019ve found that doing full justice to these questions requires a separate, in-depth study which is the focus of our current and future work.\n\n**Re. claims on generalisation:** Adaptation and generalisation are meant to refer to fact that perturbations in the high-level concept space (equivalent to changing the task specification - e.g. where to dab next) can be easily mapped to the corresponding low-level robot behaviour. The claims and motivation in the introduction will be revised to better-reflect the findings of the performed experiments.\n\n**Re. narrow experimental evaluation:** We\u2019ve devised an additional experiment based around the task of pouring and different manners of executing it. This physical experiment is still ongoing, but the setup and preliminary description is in Section 7 and Appendix E of the revised manuscript. Videos of provided demonstrations can be seen on the supplementary website. If accepted, the camera ready version will include full results from this experiment. \n\nThe experiment still follows the problem description in Section 2 -- namely some factors of variation describe where to pour (red or blue cups present in the image) and others how to pour (e.g. behind the target cup or on its side). The preliminary results analyse the usefulness of weak supervision through discrete labels for incorporating information from all input modalities. Complete analysis across all different baselines and demonstration concepts, as in section 6, will be in the final version of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ldau9eHU-qO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1279/Authors|ICLR.cc/2021/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861550, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1279/-/Official_Comment"}}}, {"id": "1aR4P-aPTYr", "original": null, "number": 6, "cdate": 1606239204226, "ddate": null, "tcdate": 1606239204226, "tmdate": 1606239204226, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": "8YW2gLw8Jc", "invitation": "ICLR.cc/2021/Conference/Paper1279/-/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We thank the reviewer for their comments and feedback.\n\n**Re. coverage of related & past work:** Within the constraints of tight page limits, and because we have added a new Section 7, we are restricted in how much we can survey. We are happy to provide further pointers to existing literature, in a potential camera-ready submission, given the permitted extra page. We would be grateful for specific pointers to references that are essential.\n\n**Re. Figure 2:** Figure 2 depicts the generative process for generating x and y. Some of the latent variables c are meant to encode continuous versions of the discrete, spatially-relative phenomena, described by y. Thus, knowing only the values of c is sufficient to infer y, regardless of the scene image, since both c_0 and y_0 are relative in nature. E.g. we could learn that c_0 being normally-distributed around -2 corresponds to y_0 = dab left and being normally-distributed around 2 corresponds to y_0 = dab right. On the contrary, sampling x requires the additional information the image encoding provides which helps resolve the relative nature of the learned spatial concepts - sample a joint angle trajectory which results in a left dab with respect to *what is present in i*.\n\n**Re. DMP baseline:** While a dynamic movement primitive seems like a good potential baseline, we are not aware of DMPs being compatible with multiple different demonstrations of the same task---ProMPs might be a good solution to that problem. However, mapping perturbations of DMP parameters to semantically-consistent perturbations (with respect to given user specifications) of robot joint trajectories is a non-trivial process, making any comparisons challenging.\n\n**Re. real robot experiments:**  To clarify, the output of the model are 7 DoF joint position (angles) and joint effort trajectories. The prediction performance of each model is not reported over a held-out set of trajectories but rather over a set of newly-sampled trajectories which are evaluated through a set of heuristics. Moreover, we demonstrate the suitability of the generated trajectories to be executed on the robot by using the robot kinematic model in order to translate the joint position part of each sample to corresponding end-effector movements and project those to the image plane of the robot\u2019s camera sensor (see Appendices D and E). We choose to evaluate in this way to separate this paper\u2019s contributions from issues of hybrid impedance control on the PR2 platform, and different ways of utilising both joint position and joint effort information within low-level control.\n\nWe\u2019ve additionally devised an experiment based around the task of pouring and different manners of executing it. This physical experiment is still ongoing, but the setup and preliminary description is in Section 7 and Appendix E of the revised manuscript. Videos of provided demonstrations can be seen on the supplementary website. If accepted, the camera ready version will include full results from this experiment. \n\nThe experiment still follows the problem description in Section 2 -- namely some factors of variation describe where to pour (red or blue cups present in the image) and others how to pour (e.g. behind the target cup or on its side). The preliminary results analyse the usefulness of weak supervision through discrete labels for incorporating information from all input modalities. Complete analysis across all different baselines and demonstration concepts, as in section 6, will be in the final version of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1279/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ldau9eHU-qO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1279/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1279/Authors|ICLR.cc/2021/Conference/Paper1279/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861550, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1279/-/Official_Comment"}}}, {"id": "8YW2gLw8Jc", "original": null, "number": 1, "cdate": 1603925537504, "ddate": null, "tcdate": 1603925537504, "tmdate": 1605024484355, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "invitation": "ICLR.cc/2021/Conference/Paper1279/-/Official_Review", "content": {"title": "The paper needs real robot experiments", "review": "== SUMMARY ==\n\nThis is a well-written paper that discusses how to learn disentangled representations for the learning from demonstrations (LfD) task in robotics. It is shown that using weak-supervision on top of unsupervised learning frameworks (that use the variational autoencoder for instance) can work well in this case. These disentangled factors of variation in the data are shown to correspond well to the 'abstract concepts' of the human demonstrations. This is shown in the example of the PR2 robot dabbing demonstrations, including visual data as well robot trajectories. \n\n== QUALITY & CLARITY ==\n\nThe paper is written very well, in fact most papers contain a lot of spelling mistakes but this paper was a joy to read in this regard :) The concepts are also explained clearly. However I would have expected better and more detailed coverage of related & past work.\n\n== ORIGINALITY & SIGNIFICANCE ==\n\nUnfortunately, I think the paper suffers from lack of originality, at least with respect to ML. From a robotics point of view, I would have accepted it as a very good application paper, if the authors had also presented real-robot results that show the learned model in action (generating actual trajectories for the robot). The authors however show only the prediction performance for (held-out) test demonstrations.\n\n== VERDICT ==\n\nI would like to thank the authors for a very well written paper. Overall, I think the paper needs some improvements such that it can be accepted in a revised version or most likely, in another conference. The paper needs to present real robot results and cover related work in more detail. Comparing the method to related work (DMP, or ProMP-variants, or any other competitive method) in the real-robot experiments would also be crucial. \n\n=== NOTES & SOME MINOR COMMENTS ===\n\n* No need to mention the link in the abstract\n\n* \"For example, the concept of pressing softly against a surface\nmanifests itselfin a data stream associated with the 7 DoF real-valued\nspace of joint efforts, spread across multipletime steps.\" -> For the PR2 robot? Either remove 7 DoF or add for which robot.\n\n* \"However, the essence of what differentiates one type of soft press from another nearby concept can be summarised conceptually using a lower dimensional abstract space\" -> Nearby concept sounds vague, please be more explicit or give\nexamples.\n\n* In Figure 2, the image encoding 'i' does not affect y. Why?\n\n* Related Work: ProMPs are not represented as dynamical systems\n\n* \"To fully close the loop, the trajectories which we sample from the\nmodel could further be executed on the physical robot through a hybrid\nposition/force controller (Reibert,1981). However, such evaluation is\nbeyond the scope of the paper.\" -> I don't think so. The real proof of concept is the actual robot\nexperiments!  Without real robot experiments to show how the generated\nconditioned trajectories actually perform, in my opinion the paper is\nan application paper without any significant ML contributions. The\npaper as of now I fear only confirms the fact already acknowledged in\nprevious ML papers, that disentanglement can be achieved through\n(semi)supervised learning [see Locatello et al. and papers citing\nthis work)\n\n* equivallent -> equivalent\n\n* force-relate -> force-related\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1279/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122344, "tmdate": 1606915771802, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1279/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1279/-/Official_Review"}}}, {"id": "lVIfTjz82a", "original": null, "number": 2, "cdate": 1603954081434, "ddate": null, "tcdate": 1603954081434, "tmdate": 1605024484295, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "invitation": "ICLR.cc/2021/Conference/Paper1279/-/Official_Review", "content": {"title": "Experiments need to better establish the claims", "review": "*Summary*\u00a0\nUnder the context of learning\u00a0from demonstrations, the paper studies the problem of leaning interpretable low dimensional representations\u00a0from high dimensional multimodal inputs using\u00a0weak supervision. Paper argues that since robots and humans have different levels of abstractions and mechanisms, observation+action spaces between them are greatly misaligned which\u00a0complicates learning by directly observing humans. However, the underlying concepts essential for tasks\u00a0lie in a much lower-dimensional manifold. Learning this\u00a0manifold\u00a0effectively and in an interpretable way, especially using weak supervision, can significantly change how robots\u00a0can acquire skills from demonstrations and generalize them to new unseen scenarios. Towards this end, the paper proposes to learn probabilistic\u00a0generative models capturing high-level notions from demonstrations using variational inference. The strength of the paper is in demonstrating that conditional latent variable models can learn disentangled\u00a0low dimensional represented using weak supervision;\u00a0which authors effectively demonstrated\u00a0using real-world\u00a0experiments. My main reservations are in terms\u00a0of the technical novelty of the paper and the narrow scope of experimental evaluation.\n\n*Suggestions*\n1. Use of variational inference for organizing high dimensional multi-modal inputs to low dimensional useful representations is already a well-established idea. However, the idea of learning \"interpretable representations\" using \"weak supervision\" is interesting and probably the most significant bit in this work. This however warrants\u00a0appropriate experimental section probing styles/ strengths/ quality/ modality of supervision used. The current experimental section starts strong on the former (generating behaviors using concepts) but overlooks the latter, arguably the more interesting questions.\u00a0\n2. Authors motivate the paper using the idea that learning interpretable low dimensional concepts called \"common sense\" will enable generalization and adaptation. Experimental section fails to\u00a0investigate and establish these claims.\n3. Lastly, the experimental setup and the problem formulation around dabbing is very cleverly defined to communicate the ideas. This however comes across as too narrow. Additionally, the label groups for this problem cleanly separates\u00a0into interpretable disjoint concepts. It's\u00a0unclear if such interpretability and clear separation (hence the possibility of weak supervision) is true in general for common real-world problems. Paper makes no attempt to establish\u00a0the generality of the approach.\u00a0", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1279/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122344, "tmdate": 1606915771802, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1279/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1279/-/Official_Review"}}}, {"id": "BkAZ4Ywf4p", "original": null, "number": 3, "cdate": 1604180526829, "ddate": null, "tcdate": 1604180526829, "tmdate": 1605024484232, "tddate": null, "forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "invitation": "ICLR.cc/2021/Conference/Paper1279/-/Official_Review", "content": {"title": "Novel idea", "review": "This paper presents a way to learn from demonstrations with weak or no labels. The premise behind this paper is that even when humans provide labels during a demonstration, those labels often do not fully describe the data (e.g., the human may say \"soft\" when \"fast\" would also apply). This paper presents a technique that uses latent variables to model the uncertainty over a group of class labels that could describe the task (e.g., slow, soft, left-of-object). The variables are modeled such that the observation is conditionally independent of the human provided labels given the latent variables. This allows the human provided labels to be decoupled (or disentangled as the paper calls it) from the observations. By doing so, it is possible to have only partial labels (weak labels). This model was applied to a task where a human would teleoperate a robot arm and apply a dabbing motion in relation to an object in the scene. The operator would provide only one of several possible applicable labels for each demonstration. The results show that the models using the weak labeling out-performed models with no labeling.\n\nThis paper proposes and interesting and novel way to handle weak labels from human demonstrators. By separating them, not only can they handle weak labels, but also multiple non-conflicting labels, or even no labels. This is an interesting contribution. The paper does a good job of comparing the methodology with 3 different off-the-shelf models for implementing the inference networks (GS, AAE, and VAE), as well as comparing each with and without the weak labels (baseline). There are several instances in the results where the weak label models far outperform the baseline models without weak labels (e.g., Table 3 VAE for \"soft\").\n\nHowever, there are many instances where the baseline models outperform the weak-label models, although not as dramatically. Nonetheless, the results in Tables 2 and 3 seem inconsistent. While the weak-label models are usually better, there are several cases where the baseline outperforms. Furthermore the extremeness of some of the results are a bit concerning. It would be nice to see some further exploration of this. In particular, the choice of task and label groups could be significantly correlated with the performance of the models. Further experiments should be done with other, more standard tasks and potentially user-provided labels to better determine the performance of the weak-label models as compared to the baselines.\n\nOverall, though, this paper does present a novel solution to the problem of user-provided labels. They're often not fully descriptive, which can leave some models confused (e.g., a demonstration can be \"soft\" but be labelled \"fast\"). The proposed solution attempts to solve this while also being probabilistically complete. While this certainly necessitates further experimental evidence to prove the usefulness of the model, the idea itself and preliminary evidence provided here are sufficient for publication.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1279/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1279/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstration with Weakly Supervised Disentanglement", "authorids": ["~Yordan_Hristov1", "~Subramanian_Ramamoorthy1"], "authors": ["Yordan Hristov", "Subramanian Ramamoorthy"], "keywords": ["representation learning for robotics", "physical symbol grounding", "semi-supervised learning"], "abstract": "Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot \u2013 that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.", "one-sentence_summary": "We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hristov|learning_from_demonstration_with_weakly_supervised_disentanglement", "supplementary_material": "/attachment/39868fdd1ca17a6a10f6c0e6729b6f8b277cc279.zip", "pdf": "/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhristov2021learning,\ntitle={Learning from Demonstration with Weakly Supervised Disentanglement},\nauthor={Yordan Hristov and Subramanian Ramamoorthy},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Ldau9eHU-qO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ldau9eHU-qO", "replyto": "Ldau9eHU-qO", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1279/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122344, "tmdate": 1606915771802, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1279/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1279/-/Official_Review"}}}], "count": 8}