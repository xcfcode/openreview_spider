{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488582506676, "tcdate": 1478297989644, "number": 512, "id": "rJTKKKqeg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJTKKKqeg", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "content": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396647165, "tcdate": 1486396647165, "number": 1, "id": "Hk1xafIdx", "invitation": "ICLR.cc/2017/conference/-/paper512/acceptance", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Reviewers found this work to be a \"well-motivated\", \"good contribution\", and \"clever\". The idea was clearly conveyed and reviewers were convinced that the approach was simpler than others like NTMs. Experiments are sufficient, and the work will likely be used in the future. \n \n Pros:\n - Well-explained and expected to be widely implemented\n - Experimental results convincing on the tasks.\n \n Cons:\n - Several questions about \"generalization to some unseen entities\". \n - Reliance on synthetic tasks unclear if \"scalable to complex real tasks\"\n - Training process seems quite complicated (although again simpler that NTMs)", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396648210, "id": "ICLR.cc/2017/conference/-/paper512/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396648210}}}, {"tddate": null, "tmdate": 1485859910053, "tcdate": 1485859910053, "number": 11, "id": "SJ0rnJ0Pl", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["~Raza_Habib1"], "readers": ["everyone"], "writers": ["~Raza_Habib1"], "content": {"title": "Question on Training bAbI", "comment": "1) When you first train on the bAbi tasks, do you train on all the tasks simultaneously and test simultaneously or do you train and test each task individually?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1484068886579, "tcdate": 1484068886579, "number": 10, "id": "SJymuqMUl", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "HkLyMxLVx", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "writers": ["~Mikael_Henaff1"], "content": {"title": "Re: Reviewer 3", "comment": "Thank you for the review! To answer your questions:\n\n1. The f_i vectors are fixed to be the same size as the embedding dimension, which is d=20 for the world model experiments and d=100 for the others. The number of f_i vectors is equal to the maximum sentence/window length. For the bAbI tasks this was typically between 5 and 10 (depending on the task), for the CBT experiments it was 5 and for the world model experiments it was 4. So, the words in a sentence or window are embedded, each word embedding is then multiplied pointwise with the corresponding f_i vector, and the resulting vectors are summed to produce a single vector representing the entire sentence or window.  \n\n2. Yes, we will release the source code in the near future, so that others can reproduce the results. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1484067089417, "tcdate": 1484067089417, "number": 9, "id": "SJtfb5zIx", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "H1K9R08Eg", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "writers": ["~Mikael_Henaff1"], "content": {"title": "Re: Single-pass models", "comment": "Thank you for the comment. For both the LSTM and the EntNet, the set of possible answers is restricted to the candidate set at test time. In the case of the EntNet, since there is no interaction between the different memory slots, even if we made more memories for words that were not candidates (for example, one memory slot per word in the vocabulary), this would not affect the final result since the set of possible answers is restricted to the candidate set at test time. Restricting the set of memories to the candidates does not change performance and is simply a computational speedup. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1483927540998, "tcdate": 1483927540998, "number": 2, "id": "rkTglOg8l", "invitation": "ICLR.cc/2017/conference/-/paper512/official/comment", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/conference/paper512/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper512/areachair1"], "content": {"title": "Authors: Any rebuttal comments?", "comment": "Hi authors, \n\nHave you had a chance to read over the reviews and the anonymous comments posted? If you have any rebuttals this would be a great chance to add them to the discussion.\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544768, "id": "ICLR.cc/2017/conference/-/paper512/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper512/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper512/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544768}}}, {"tddate": null, "tmdate": 1482251932158, "tcdate": 1482251920977, "number": 8, "id": "H1K9R08Eg", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Single-pass models", "comment": "I am not entirely sure that the comparison between LSTM and EntityNet that you make is fair. You call both LSTM and EntityNet \"single-pass\" models, as opposed to \"multi-pass\" models with attention. However, thanks to the weight-tying that you employ, EntityNet knows what the candidate answers are when it reads the context. I think that this alone gives it a huge advantage over LSTM. Can you please comment on that?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1482201291261, "tcdate": 1482201291261, "number": 3, "id": "rk7ROfU4e", "invitation": "ICLR.cc/2017/conference/-/paper512/official/review", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/conference/paper512/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper512/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The paper proposed a multi-memory mechanism that memorizes different information into different components/entities. It could be considered as a mixture model in RNN. This is a very interesting model and result is convincing.\n\nA limitation is that we do not know how to generalize to some unseen entities and how to visualize what entities the model learned.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512558077, "id": "ICLR.cc/2017/conference/-/paper512/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper512/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper512/AnonReviewer2", "ICLR.cc/2017/conference/paper512/AnonReviewer3", "ICLR.cc/2017/conference/paper512/AnonReviewer1"], "reply": {"forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512558077}}}, {"tddate": null, "tmdate": 1482191326335, "tcdate": 1482191326335, "number": 2, "id": "HkLyMxLVx", "invitation": "ICLR.cc/2017/conference/-/paper512/official/review", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/conference/paper512/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper512/AnonReviewer3"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper proposes a new memory augmented neural network (MANN) model called recurrent entity network (EntNet). EntNet can be considered as a bank of RNNs with gating mechanism to update the hidden states of the RNNs and the hidden states act like the memory slots. The model is very much relevant to NTM style architectures. It is known that training the controller in NTM to read/write from memory slots is challenging. EntNet cleverly pushes the complexity of the controller to individual memory slots. It is as if each slot has a controller and they all act in a distributed manner.\n\nAuthors report strong results in bAbI tasks where the model achieves state of the art performance. Synthetic world experiments justify that the model learns to capture the world dynamics. However it is not clear if this will be scalable to complex real tasks. EntNet also achieves reasonable performance with one-shot reading of the passage in CBT task.\n\nI see EntNet as a generalization of RNNs and has some advantage over NTMs when it comes to training complexity. This is definitely a good contribution to the conference. I see that EntNet can have several other applications in future.\n\nAuthors have provided convincing answers to my pre-review questions.\n\nFew more questions:\n1. Do you fix the size of the f vector set in equation (1)? If so, to what size in all the experiments?\n2. There are so many training details in the paper which makes it difficult to reproduce the results. Can the authors release the source code to reproduce all the results in the paper? I am willing to increase my rating if authors can release the code.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512558077, "id": "ICLR.cc/2017/conference/-/paper512/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper512/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper512/AnonReviewer2", "ICLR.cc/2017/conference/paper512/AnonReviewer3", "ICLR.cc/2017/conference/paper512/AnonReviewer1"], "reply": {"forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512558077}}}, {"tddate": null, "tmdate": 1482182210501, "tcdate": 1482182210501, "number": 1, "id": "Sy9rRTHEg", "invitation": "ICLR.cc/2017/conference/-/paper512/official/review", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/conference/paper512/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper512/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The work proposes a variant of a recurrent neural network that can selectively update a fixed number of multiple memory slots to update entity states.\n\nThe architecture is well motivated, especially with the motivating example, and the operation is shown to validate the intuition as shown in visualizations.\n\nExperimental results, datasets and the baselines used are sufficient to quantitatively show the strength of the proposed architecture.\n\nA limitation is failing to (explicitly) generalize to unseen entities, however this is not a trivial problem on its own and the authors have addressed to this issue and proposed several ideas as workarounds.\n\nI consider the work as a good conference contribution.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512558077, "id": "ICLR.cc/2017/conference/-/paper512/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper512/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper512/AnonReviewer2", "ICLR.cc/2017/conference/paper512/AnonReviewer3", "ICLR.cc/2017/conference/paper512/AnonReviewer1"], "reply": {"forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512558077}}}, {"tddate": null, "tmdate": 1481584409125, "tcdate": 1481584409121, "number": 7, "id": "ryWXk3nQl", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "writers": ["~Mikael_Henaff1"], "content": {"title": "Paper update", "comment": "Dear all, \n\nWe have updated the paper with the following changes:\n\n- Specified error ranges for tables in Section 5.1.\n- Added NTM results to main table in Section 5.2.\n- Separated world model visualization table in Section 5.2 into two separate tables. \n- Added CBT results for general EntNet to table in Section 5.3\n- Added note in Appendix C specifying the experiments are designed to better understand effect of architecture and weight tying. \n- Added note to Section 3 about how weight tying can be used to handle unseen entities.\n\nPlease let us know if you have any other questions or comments. \n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1481015635424, "tcdate": 1481015596083, "number": 6, "id": "Sk4VWZVQe", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "SJJAOvyQe", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "writers": ["~Mikael_Henaff1"], "content": {"title": "Re: Question about entity", "comment": "1. We did not experiment with using a POS tagger ourselves. However, in the CBT experiments we tied the key vectors to the embeddings of the words occurring in the candidate set, which were originally chosen using a POS tagger when the dataset was constructed.\n\n2. Yes, it is possible that there could be a different number of entities in the training and testing sets. Entities not occurring in the testing data could have their memory slots removed, which might help performance. One strategy for dealing with entities occurring in the test set but not the training set would be to add memory slots for these entities and initialize the word embeddings/key vectors with embeddings trained on a larger corpus.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1480960472639, "tcdate": 1480960472635, "number": 5, "id": "rybk5X7Xg", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "SJzcDl0fx", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "writers": ["~Mikael_Henaff1"], "content": {"title": "Re: Interpretability", "comment": "Good question! We checked in the case using BoW encoding and untied keys, and found that the embedding for a given entity was often close to more than one key vector (in terms of cosine distance). We also plotted gate activations and found that more than 2 or 3 gates can open at a time (for the examples we looked at, up to 7). This suggests that the model is storing information distributed across several memory slots. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1480960138997, "tcdate": 1480960138990, "number": 4, "id": "S1mc_XXmg", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "S1MO6B17g", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "writers": ["~Mikael_Henaff1"], "content": {"title": "Re: Few questions", "comment": "Thank you for your helpful feedback. To answer your questions:\n\n1. Yes, in Tables 1 and 2 the error ranges from 0 to 1. We will make this clear.\n\n2. Thank you for pointing this out, we will add the NTM results to the table.\n\n3. We will make the change to the figure.\n\n4. We will run this experiment and add the results to the paper soon.\n\n5. We included the results comparing EntNet to MemN2N using the exact same BoW encoding to help understand the effects of the model architecture while keeping the encoder fixed. Since none of the other published models shown in Table 3 use simple BoW (which is generally less effective than other encoding schemes), we felt it would be confusing to include the results with BoW encoding in Table 3. We view Table 3 as the \u201cbest results\u201d for each model architecture, and Table 6 as an ablation study to understand the effects of architecture and weight tying. We will clarify this distinction in the paper.\n\n6. With our current implementation, the EntNet takes about 3x longer to train than a MemN2N. We very much agree that trying different initializations is a suboptimal approach, and we are currently investigating the variance problem. However, this is a common issue with models evaluated on the bAbI tasks and not specific to our model (all models reported in Table 3 except D-NTM select the best out of 10 or more runs). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1480714438695, "tcdate": 1480714438690, "number": 3, "id": "SJJAOvyQe", "invitation": "ICLR.cc/2017/conference/-/paper512/pre-review/question", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/conference/paper512/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper512/AnonReviewer1"], "content": {"title": "Question about entity", "question": "1. The paper mentioned it could use POS tagging to define entities for finding entities. Have you tried it for some experiments/cases? \n2. If the entities are extracted from data, would there be different number of entities for a testing data? Then the length of your hidden state h_j would be varied based on the data. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959239530, "id": "ICLR.cc/2017/conference/-/paper512/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper512/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper512/AnonReviewer2", "ICLR.cc/2017/conference/paper512/AnonReviewer3", "ICLR.cc/2017/conference/paper512/AnonReviewer1"], "reply": {"forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959239530}}}, {"tddate": null, "tmdate": 1480707433955, "tcdate": 1480707433951, "number": 2, "id": "S1MO6B17g", "invitation": "ICLR.cc/2017/conference/-/paper512/pre-review/question", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/conference/paper512/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper512/AnonReviewer3"], "content": {"title": "Few questions", "question": "1. Error values in tables 1 and 2 range from 0 to 1 or 0 to 100? I can guess that it is from 0 to 1, but good to make it explicit in the caption.\n2. You say that you report NTM results in Table 3, but it is missing.\n3. Can you please modify table 4 and make the story part and key/NN part two separate figures or tables? My initial impression from the table was that for each line in the story you have a key/NN. It is quite confusing.\n4. For CBT, authors say that setting U=V=0 and W=I helps. What is the performance of the general EntNet without these changes? Is it too worse? I think it is good to report those numbers as well.\n5. I think EntNet+BoW results should go to Table 3 (and not to appendix). In table 3, all the models are not exactly comparable since different models use different encoder representations. So authors should report the performance of EntNet with proposed encoder representation and EntNet with BoW encoder representation in the main table itself.\n6. How much time does it take to train EntNet on babi tasks? 10 runs with different initialization for all tasks seems to be too much for me. It is worth investigating and solving the variance problem than simply increasing the computation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959239530, "id": "ICLR.cc/2017/conference/-/paper512/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper512/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper512/AnonReviewer2", "ICLR.cc/2017/conference/paper512/AnonReviewer3", "ICLR.cc/2017/conference/paper512/AnonReviewer1"], "reply": {"forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959239530}}}, {"tddate": null, "tmdate": 1480619914127, "tcdate": 1480619914122, "number": 1, "id": "SJzcDl0fx", "invitation": "ICLR.cc/2017/conference/-/paper512/pre-review/question", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["ICLR.cc/2017/conference/paper512/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper512/AnonReviewer2"], "content": {"title": "Interpretability", "question": "I really enjoyed the visualization in Table 4. Have you checked key vectors in the case where you do not explicitly tie them to see if there is anything interpretable still, e.g. by looking at which s_t activates which w_j? Or is it mostly distributed and does not focus to a single concept/topic/entity?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959239530, "id": "ICLR.cc/2017/conference/-/paper512/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper512/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper512/AnonReviewer2", "ICLR.cc/2017/conference/paper512/AnonReviewer3", "ICLR.cc/2017/conference/paper512/AnonReviewer1"], "reply": {"forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper512/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959239530}}}, {"tddate": null, "tmdate": 1479944087385, "tcdate": 1479944087380, "number": 2, "id": "rykiPj7Me", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "BJFtxpdbl", "signatures": ["~Mikael_Henaff1"], "readers": ["everyone"], "writers": ["~Mikael_Henaff1"], "content": {"title": "Re: Nature of keys", "comment": "Thank you for your comment. The untied model assumes that the training and test sets are drawn from the same distribution. However, the tied model can be applied to cases with entities unseen during training, if the type of entity is known beforehand to be important for the task. For example, one could use a part-of-speech tagger to return a list of nouns in the story at test time, and tie memory keys to their embeddings. Word embeddings could be pre-trained on a larger corpus to cover words not occurring in the training set, as is usually done. \n\n\nNote that in the CBT experiments, there are entities which occur in the test set and not the training set. In that task, candidates are given which allow us to know beforehand which words to record information about. These could also be produced with a part-of-speech tagger (which is in fact used to construct the dataset in the first place) or another method."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}, {"tddate": null, "tmdate": 1479229569053, "tcdate": 1479229569049, "number": 1, "id": "BJFtxpdbl", "invitation": "ICLR.cc/2017/conference/-/paper512/public/comment", "forum": "rJTKKKqeg", "replyto": "rJTKKKqeg", "signatures": ["~Rajhans_Samdani1"], "readers": ["everyone"], "writers": ["~Rajhans_Samdani1"], "content": {"title": "Nature of keys", "comment": "It's a nice and clean paper that is easy to understand. I am however still not clear about the notion of having separate learned keys for each entity cell. It seems clear that keys, w_j, are important in storing the entity information; but how do these keys generalize to new entities? E.g. if the model is only trained on data with entities John, Mary, and Jim, and during testing we have a new entity Michael, how would it match any of the keys? The paper mentions that the key parameters are obtained during learning and fixed during inference (except for the \"tied\" case but that is not used in any comparative experiments), so the explanation on section 3 paragraph 2 (One choice the model could make is to associate...) also does not seem sufficient. What am I missing?\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tracking the World State with Recurrent Entity Networks", "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped\nwith a dynamic long-term memory which allows it to maintain and update a rep-\nresentation of the state of the world as it receives new data. For language under-\nstanding tasks, it can reason on-the-fly as it reads text, not just when it is required\nto answer a question or respond as is the case for a Memory Network (Sukhbaatar\net al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer\n(Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to\nperform location and content-based read and write operations. However, unlike\nthose models it has a simple parallel architecture in which several memory loca-\ntions can be updated simultaneously. The EntNet sets a new state-of-the-art on\nthe bAbI tasks, and is the first method to solve all the tasks in the 10k training\nexamples setting. We also demonstrate that it can solve a reasoning task which\nrequires a large number of supporting facts, which other methods are not able to\nsolve, and can generalize past its training horizon. It can also be practically used\non large scale datasets such as Children\u2019s Book Test, where it obtains competitive\nperformance, reading the story in a single pass.", "pdf": "/pdf/2492f18cfd03efe076f230140b0523b03e033c86.pdf", "TL;DR": "A new memory-augmented model which learns to track the world state, obtaining SOTA on the bAbI tasks amongst other results.", "paperhash": "henaff|tracking_the_world_state_with_recurrent_entity_networks", "conflicts": ["nyu.edu", "fb.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun"], "authorids": ["mbh305@nyu.edu", "jase@fb.com", "azslam@fb.com", "abordes@fb.com", "yann@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544897, "id": "ICLR.cc/2017/conference/-/paper512/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJTKKKqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper512/reviewers", "ICLR.cc/2017/conference/paper512/areachairs"], "cdate": 1485287544897}}}], "count": 19}