{"notes": [{"id": "Hke0K1HKwr", "original": "HyeCkuROvr", "number": 1859, "cdate": 1569439621532, "ddate": null, "tcdate": 1569439621532, "tmdate": 1583912038443, "tddate": null, "forum": "Hke0K1HKwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "hbKo5rF0Fw", "original": null, "number": 1, "cdate": 1576798734288, "ddate": null, "tcdate": 1576798734288, "tmdate": 1576800902122, "tddate": null, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "invitation": "ICLR.cc/2020/Conference/Paper1859/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper proposes a sequential latent variable model for the knowledge selection task for knowledge grounded dialogues. Experimental results demonstrate improvements over the previous SOTA in the WoW, knowledge grounded dialogue dataset, through both automated and human evaluation. All reviewers scored the paper highly, but they also made several suggestions for improving the presentation. Authors responded positively to all these suggestions and provided updated results and other stats. The paper will be a good contribution to ICLR.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706122, "tmdate": 1576800254069, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1859/-/Decision"}}}, {"id": "ryxezvS0tB", "original": null, "number": 2, "cdate": 1571866375848, "ddate": null, "tcdate": 1571866375848, "tmdate": 1573879200451, "tddate": null, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "invitation": "ICLR.cc/2020/Conference/Paper1859/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Post author response edit: The authors did a good job of addressing many of the concerns of reviewers. I believe with these new results (esp to reviewer 4), they will have a stronger version for the camera ready. I'm bumping up my recommendation for this reason.\n\n\n\nThe authors propose a novel architecture for selecting knowledge in knowledge-grounded multi-turn dialogue. Their knowledge selection module uses a sequential latent variable scheme, and is claimed to be able to both handle diversity of knowledge selection in conversation as well as leverage the information from the response. The proposed model yields state of the art on two relevant benchmark datasets in terms of perplexity and F1, and scores higher in human evaluations as well.\n\nThe paper is relatively well-written, and the authors offer extensive insight into their approach, providing relevant equations and diagrams where necessary. The approach is well-motivated, and the experiments indicate that the model indeed helps on all evaluation fronts. A variety of baselines are considered and are shown to be inferior, in nearly every metric. I did not spend a lot of effort to try to understand their factorization, but the intuition makes sense, and their use of gumbel softmax provides a clear avenue to fix some of the hard-backprop issues apparent in the original Dinan et al. paper. I also appreciate the addition of the knowledge loss to the PostKS baseline: it\u2019s a good effort to make the baseline as good as possible.\n\nA few things bother me with the paper. The primary one is it concerns me a bit that the BERT pretraining does not improve significantly over the E2E transformer memnet (with just bert vocabulary). Unless I\u2019m missing something, that model contained NO pretraining, so I would expect massive improvements. A sanity check there would be checking ppl with gold knowledge: if that doesn\u2019t significantly improve, then I suspect the authors have something really weird about the pretraining or fine tuning. However, It also appears to me that replacing the GRU with a transformer in PostKS might be unfair: Transformers are way more data hungry than RNNs, and so both variants should be tried (though I would be okay with the loser being relegated to a footnote or appendix).\n\nThe human evaluations are not as convincing as the authors propose them to be, especially the difference in the \u201cTest Seen\u201d case. It is unclear to me why the authors believe that their \u201cmodel\u2019s merit would be more salient\u201d in a multi-turn setting, and I think such an experiment would be good to show - or, at the very least, an indication that such an experiment was tried but results were not considered due to reasons X,Y, Z, etc. Overall, the rough improvement that is being provided in the first-stage of the two stage setting seems rather minor (23% -> 26% accuracy; 2.21 -> 2.35 human eval), and that the task remains extremely difficult\n\nQuestions\n* I know the Dinan et al. models, at human evaluation time, hardcoded to not pick the same knowledge twice. Do you have a similar restriction? If not, maybe you can at least say that you manage to get rid of the need for that!\n* As mentioned earlier, I would be curious to see multi-turn human evaluations. I understand this is expensive and a large ask.\n* How are the examples in figure 3 chosen? Are they generally indicative of what is seen throughout the human evaluation?\n* It would be useful to see a qualitative example of the model\u2019s knowledge selection process when comparing to other models, rather than just the utterance generation (which is not the novel contribution of the paper).\n\nNits\n* Small grammatical errors dealing with subject-verb agreement (plurals mostly).\n* Using \u201c-\u201d instead of n/a in tables would make it mildly easier to see digest and see where metrics don\u2019t make sense.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1859/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1859/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575705625586, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1859/Reviewers"], "noninvitees": [], "tcdate": 1570237731271, "tmdate": 1575705625605, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1859/-/Official_Review"}}}, {"id": "rJxe9OPnsr", "original": null, "number": 3, "cdate": 1573841031578, "ddate": null, "tcdate": 1573841031578, "tmdate": 1573841031578, "tddate": null, "forum": "Hke0K1HKwr", "replyto": "B1ehFnYiYr", "invitation": "ICLR.cc/2020/Conference/Paper1859/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank Reviewer 2 for positive and constructive reviews. Below, we respond to each comment in detail. Please see the blue fonts in the newly uploaded draft to check how our paper is updated.\n\n\n1. Could you describe the updates from the previous sequential latent variable models more clearly?\n\nOur main contribution is to model the knowledge selection in dialogue as sequential latent variable models for the first time, and validate that it leads to the new state-of-the-art performances on two benchmark datasets. The use of sequential latent models can correctly deal with diversity nature in knowledge selection in a semi-supervised way and improves the interpretability of the flow of selected knowledge over other models. Methodologically, our model is similar to [1], although it uses the latent variable to represent the underlying attention in seq2seq models for machine translation (unlike ours for knowledge-grounded chit-chat problem).\n\n[1] S. Shankar and S. Sarawagi. Posterior Attention Models for Sequence to Sequence Learning. ICLR 2019.\n\n\n2. Ablation studies for the three advantages of the proposed method: (i) weakly-supervised inference with no labels, (ii) reduce the scope of knowledge candidates, and (iii) better utilization of response information.\n\nWe here answer the reviewer\u2019s second and fourth questions together.\n\nWe add the experiments of our model with partial knowledge labels (including an experiment without knowledge loss) on the Wizard of Wikipedia in Table 6 in Appendix D. Results show that the better performance is attained with more labeled knowledge data for training as expected. Furthermore, our model achieves competitive performance with less label. For instance, our model using only 1/4 labeled training data is comparable to E2E Transformer MemNet and is even better in Test Unseen. As a result, our sequential latent knowledge selection model can be utilized in a semi-supervised method without severe drop in its performance.\n\nDue to many new experiments during the limited time of rebuttal, we cannot finish an ablation study for the reduced scope and utilization of response information, which will be presented in the final draft. \n\n\n3. It would be also interesting to see more detailed aspects of knowledge-selection itself in both quantitative and qualitative manners. \n\nWe add more quantitative and qualitative results of knowledge selection in Appendix C and G. In Appendix C, we measure the knowledge selection accuracy over turns. Our model consistently outperforms other models for all turns in knowledge selection accuracy. Notably, in all models, the accuracy significantly drops after the first turn (which is often easily predictable topic definition sentence), which shows the diversity nature in knowledge selection. In Appendix G, we show selected examples of utterance prediction along with selected knowledge. We will update more qualitative results (e.g. attention distribution) to our final version."}, "signatures": ["ICLR.cc/2020/Conference/Paper1859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0K1HKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference/Paper1859/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1859/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1859/Reviewers", "ICLR.cc/2020/Conference/Paper1859/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1859/Authors|ICLR.cc/2020/Conference/Paper1859/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149853, "tmdate": 1576860539060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference/Paper1859/Reviewers", "ICLR.cc/2020/Conference/Paper1859/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1859/-/Official_Comment"}}}, {"id": "rJediVDhoS", "original": null, "number": 2, "cdate": 1573840031951, "ddate": null, "tcdate": 1573840031951, "tmdate": 1573840031951, "tddate": null, "forum": "Hke0K1HKwr", "replyto": "ryxezvS0tB", "invitation": "ICLR.cc/2020/Conference/Paper1859/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank Reviewer 1 for positive and constructive reviews. Below, we respond to each comment in detail. Please see blue fonts in the newly uploaded draft to check how our paper is updated.\n\n\n1. Sanity check for BERT implementation & Reason for the low performance of BERT\n\nFollowing your suggestion, we measure our model\u2019s performance with gold knowledge. The table below shows that providing gold knowledge significantly improves our model\u2019s performance. It can be a good sanity check for our implementation.\n\n\t\t\t\tTest Seen\t\t\t\tTest Unseen\nMethod\t\t\tPPL\t\tR-1\t\tR-2\t\tPPL\t\tR-1\t\tR-2\nOurs\t\t\t52.0\t\t19.3\t\t6.8\t\t81.4\t\t16.1\t\t4.2\nOurs (w/ gold)\t23.1\t\t34.2\t\t18.4\t\t27.8\t\t32.6\t\t16.6\n\nThe reason for the low performance of BERT may be the diversity nature of knowledge selection in knowledge-grounded dialogue. As discussed in Section 2, there can be one-to-many relations between the dialogue context and the knowledge to be selected. One can choose any diverse knowledge to carry on the conversation. Table 1 confirms this conjecture. In the Wizard of Wikipedia dataset, knowledge selection is extremely challenging even for human (17.1) and BERT is marginally better than Transformer (23.4 of BERT vs 22.5 of Transformer). On the other hand, once we change the task to have one-to-one relations by providing a GT response, BERT significantly boosts performance over Transformer (78.2 in BERT vs 70.4 in Transformer).\n\n\n2. Quantitative results of PostKS with GRU\n\nWe add quantitative results of PostKS+Transformer and PostKS+GRU on the Wizard of Wikipedia and Holl-E in Table 7 in Appendix E. Results show that PostKS+GRU consistently outperforms PostKS+Transformer without the knowledge loss term. The lower performance of PostKS+Transformer may be due to the data starvation problem as the review anticipated. However, PostKS+Transformer performs better than PostKS+GRU with the knowledge loss. It seems that the knowledge loss term reduces the overfitting and thus increases the data efficiency.\n\n\n3. Multi-turn human evaluation results\n\nWe add human evaluation results in a multi-turn setting using the evaluation toolkit from Wizard of Wikipedia. Following their setting, humans are paired with one of the models and chat about a specific topic (given a choice of 2-3 topics) for 3-5 dialogue turns. After conversation, they rate their dialogue partner on a scale of 1-5, with the rating indicating how much they \u201cliked\u201d the conversation. We collect the votes for 110 randomly sampled conversations from 10 different turkers.\n\nModels\t\t\t\t\t\tTest Seen\tTest Unseen\nE2E Transformer MemNet\t\t2.36 (1.38)\t2.10 (0.96)\nOurs\t\t\t\t\t\t2.39 (0.99)\t2.38 (1.01)\n\nAs shown in the table (avg and stddev), human annotators prefer our results to those of baselines with a larger gap in Test Unseen.\n\n\n4. Overall, the rough improvement that is being provided in the first-stage of the two stage setting seems rather minor (23% -> 26% accuracy; 2.21 -> 2.35 human eval), and that the task remains extremely difficult.\n\nConsidering the difficulty of the task, our improvement in knowledge selection (23.2% -> 26.8%) is not minor. Dinan et al. (2019) recorded 25.5% accuracy in knowledge selection on WoW with additional 700 million Reddit conversations [1] and knowledge selection data of [2], while ours achieves better performance even without them. Agreeing that the task remains challenging, we strongly believe that our work brings important contributions for knowledge-ground conversation: (i) focusing the diversity issue of knowledge selection for the first time, (ii) correctly modeling it as a sequential latent model and (iii) achieving new state-of-the-art performance with nontrivial margins. \n\n[1] P. Mazare, S. Humeau, M. Raison, and A. Bordes. Training Millions of Personalized Dialogue Agents. EMNLP, 2018.\n[2] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. EMNLP, 2016.\n\n\n5. I know the Dinan et al. models, at human evaluation time, hardcoded to not pick the same knowledge twice. Do you have a similar restriction? If not, maybe you can at least say that you manage to get rid of the need for that!\n\nThank you for your suggestion. We did not use their hardcoding. We will add that statement to our final version.\n\n\n6. How are the examples in figure 3 chosen? Are they generally indicative of what is seen throughout the human evaluation?\n\nWe manually select one example for Figure 3. For human evaluation, we randomly sample test examples without knowing which examples are chosen at all. We will add more examples of knowledge selection and utterance prediction in Appendix G.\n\n\n7. Qualitative examples with selected knowledge\n\nThank you for your suggestion. We add qualitative examples of selected knowledge in Appendix G.\n\n\n8. Grammatical errors\n\nWe update our paper per your suggestion."}, "signatures": ["ICLR.cc/2020/Conference/Paper1859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0K1HKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference/Paper1859/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1859/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1859/Reviewers", "ICLR.cc/2020/Conference/Paper1859/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1859/Authors|ICLR.cc/2020/Conference/Paper1859/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149853, "tmdate": 1576860539060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference/Paper1859/Reviewers", "ICLR.cc/2020/Conference/Paper1859/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1859/-/Official_Comment"}}}, {"id": "rkxOuxPhor", "original": null, "number": 1, "cdate": 1573838959973, "ddate": null, "tcdate": 1573838959973, "tmdate": 1573838959973, "tddate": null, "forum": "Hke0K1HKwr", "replyto": "Bkxtl24c5r", "invitation": "ICLR.cc/2020/Conference/Paper1859/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank Reviewer 4 for positive and constructive reviews. Below, we respond to each comment in detail. Please see the blue fonts in the newly uploaded draft to check how our paper is updated.\n\n\n1. In Figure 3, please provide the knowledge sentence that was selected.\n\nThanks for the suggestion. We add some examples of selected knowledge and predicted utterances in Appendix G.\n\n\n2. Please provide the inter-annotator agreement for human evaluation.\n\nWe measured the agreement among the annotators using Fleiss\u2019 kappa [1]. All kappa values exceeded or were close to 0.2, indicating the slight agreement among annotators. There were some diversity among annotators\u2019 responses, because of utilizing the 4 point scale in order to avoid having a \u201ccatch-all\u201d category (i.e. no middle response scale) in the answer choice [2]. To mitigate such annotator bias and inter-annotator variability, we adjusted human evaluation results via Bayesian calibration [3]. Table 4 shows raw and calibrated results of human evaluation, which consistently validates that annotators prefer our results to those of the baselines.\n\n\t\t\t\t\tTest Seen\t\t\t\t\t\tTest Unseen\nMethod\t\tEngagingness\tKnowledgeability\tEngagingness\tKnowledgeability\nPostKS\t\t0.12\t\t\t\t0.17\t\t\t\t0.12\t\t\t\t0.09\nTMN\t\t0.22\t\t\t\t0.19\t\t\t\t0.16\t\t\t\t0.17\nOurs\t\t0.20\t\t\t\t0.20\t\t\t\t0.21\t\t\t\t0.17\nHuman\t\t0.22\t\t\t\t0.22\t\t\t\t0.23\t\t\t\t0.31\n\n[1] J. L Fleiss. Measuring Nominal Scale Agreement among Many Raters. Psychol. Bull. 1971.\n[2] D. K. Dalal, N. T. Carter, and C. J. Lake. Middle Response Scale Options are Inappropriate for Ideal Point Scales. J. Bus. Psychol. 2014.\n[3] I. Kulikov, A. H. Miller, K. Cho, and J. Weston. Importance of Search and Evaluation Strategies in Neural Dialogue Modeling. INLG 2019.\n\n\n3. I think it would be interesting to see what is the copy mechanism actually adding in terms of integration of knowledge vs the WoW MemNet approach.\n\nIn newly updated draft, we add quantitative results of \u201cE2E Transformer MemNet + BERT + PostKS + Copy\u201d to Table 2 and 3. To make the results more reliable, we run the model three times with different random seeds and report its mean. We also update our model\u2019s results in the same manner. The \u201cE2E Transformer MemNet + BERT + PostKS + Copy\u201d performs the best among baselines, but is not as good as ours, which confirms that sequential latent modeling is critical for improving the accuracy of knowledge selection and subsequently utterance generation. Adding the copy mechanism to the baseline substantially improves the accuracy of utterance generation, but barely improves the knowledge selection accuracy, which also justifies the effectiveness of the sequential latent variable. Additionally, the performance gaps between ours and baselines are larger in Test Unseen. It can be understood that the sequential latent variable can generalize better.\n\n\n4. For Related Work, also cite Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations.\n\nThank you. We update our paper as your suggestion.\n\n\n5. How is the performance of the model impacted with longer dialog context vs shorter?\n\nTable 5 in Appendix C compares the knowledge selection accuracy of different methods for each turn on the Wizard of Wikipedia. Thanks to the sequential latent variable, our model consistently outperforms other models for all turns in knowledge selection accuracy. Notably, in all models, the accuracy significantly drops after the first turn (which is often easily predictable topic definition sentence), which shows the diversity nature in knowledge selection, as discussed in Section 2.\n\n\n6. The Holl-E dataset was transformed from spans of knowledge to a single knowledge sentence. It would be interesting to see what happens when the knowledge selected is over multiple sentences.\n\nWe select the sentence that includes the span as the ground-truth (GT) knowledge sentence. If the span is given over multiple sentences, we select the minimum number of consecutive sentences containing the span and use them as GT. If all of the candidate sentences have zero F1 scores to the span and the response, we tag \u2018no_passages_used\u2019 as the GT, which amounts to 5% of GT labels. All of the details are updated in Section 4.1.\n\n\n7. The knowledge pool currently consists of 67.57 sentences on average. How will this method scale as the amount of knowledge sentences grows?\n\nDue to the use of BERT (or Transformer) as the sentence encoder, our memory complexity is O(nm^2) where n is the number of candidate sentences in knowledge pool and m is the length of the longest sentence. For example, when training with 1 dialogue batch on NVIDIA TITAN RTX GPU, our model scales up to n=95 and m = 32. But at test time, it is highly scalable up to n>=500 because of no need for backpropagation.\n\n\n8. Grammatical errors\n\nThank you. We update our paper as your suggestion."}, "signatures": ["ICLR.cc/2020/Conference/Paper1859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hke0K1HKwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference/Paper1859/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1859/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1859/Reviewers", "ICLR.cc/2020/Conference/Paper1859/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1859/Authors|ICLR.cc/2020/Conference/Paper1859/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149853, "tmdate": 1576860539060, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1859/Authors", "ICLR.cc/2020/Conference/Paper1859/Reviewers", "ICLR.cc/2020/Conference/Paper1859/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1859/-/Official_Comment"}}}, {"id": "B1ehFnYiYr", "original": null, "number": 1, "cdate": 1571687556261, "ddate": null, "tcdate": 1571687556261, "tmdate": 1572972414429, "tddate": null, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "invitation": "ICLR.cc/2020/Conference/Paper1859/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a sequential latent variable model for knowledge selection in dialogue generation. More specifically, the authors extended the posterior attention model (Shankar and Sarawagi, 2019) to the latent knowledge selection problem. The proposed model achieved higher performances than previous state-of-the-art knowledge-grounded dialogue models on Wizard of Wikipedia and Holl-E datasets.\n\nThis work presents a reasonable ideas with new state-of-the-art results in both quantitative and qualitative evaluations.\nAnd overall the paper reads well.\n\nBut I think it could be further improved with the following points:\n- Could you describe the updates from the previous sequential latent variable models more clearly? It would help to further highlight the contribution of this work. Now it might not be very clear enough for those who are not familiar with the previous work.\n- In the introduction, the authors claim the following three advantages of the proposed method: reduced scope of knowledge candidates, better utilization of response information, and weakly-supervised inference with no labels.\nBut I'm not very convinced whether the experimental results indicate the aspects clearly enough. More detailed analysis should be added to support the contributions.\n- The current experiments mainly focus on end-to-end dialogue generation performances. But it would be also interesting to see more detailed aspects of knowledge-selection itself in both quantitative and qualitative manners. I guess this analysis can be done based on the sampled or selected knowledge from the attention distribution.\n- Could you possibly add some ablation studies to show the effectiveness of each component? Especially, I'm curious about the results of the proposed model without knowledge loss."}, "signatures": ["ICLR.cc/2020/Conference/Paper1859/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1859/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575705625586, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1859/Reviewers"], "noninvitees": [], "tcdate": 1570237731271, "tmdate": 1575705625605, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1859/-/Official_Review"}}}, {"id": "Bkxtl24c5r", "original": null, "number": 3, "cdate": 1572649968883, "ddate": null, "tcdate": 1572649968883, "tmdate": 1572972414347, "tddate": null, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "invitation": "ICLR.cc/2020/Conference/Paper1859/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper looks at the problem of knowledge selection for open-domain dialogue. The motivation is that selecting relevant knowledge is critical for downstream response generation.\nThe paper highlights the one-to-many relations when selecting knowledge which makes the problem even more challenging. It tries to address this by taking into account the history of knowledge selected at previous turns.\nThe paper proposes a Sequential Latent Model which represents the knowledge history as some latent representation. From this methodology they select a piece of knowledge at the current turn and use it to decode an utterance. The model is trained in a joint fashion to learn which knowledge to select and on generating the response. As the two are strongly correlated. Additionally there is an auxiliary loss to help correctly identify if the knowledge was correctly selected. Additionally a copy mechanism is introduced to try to copy words from the knowledge during decoding.\nThe experiments are run on the Wizard of Wikipedia dataset where there are annotations for which knowledge sentence is selected and on Holl-E, where they transform the dataset to have a single sentence tied to a response.\nFor automatic metrics there is significant improvement over baselines for correctly selecting a piece of knowledge and generating a response. Additionally there is human evaluation that also shows significant improvement.  Their model also seems to generalize well to domains that were not seen during training time over baselines models.\n\nThe contribution of the paper is the novel approach to selecting knowledge for open-domain dialogue. This work is significant in that by improving knowledge selection we see a subsequent improvement in response generation quality which is the overall downstream task within this problem space.\nI believe this paper should be accepted because of the significant and novel approach of modeling previous knowledge sentences selected. The linking of this knowledge selection model to topic tracking as stated in the paper is of clear importance, as ensuring topical depth and topical transition are two key aspects for open-domain dialog.\n \nFeedback on the paper\nIn Figure 3, please provide the knowledge sentence that was selected.\nPlease provide the inter-annotator agreement for human evaluation.\nI think it would be interesting to see what is the copy mechanism actually adding in terms of integration of knowledge vs the WoW MemNet approach. Are those two truely comparable because one does not have copy?\nFor Related Work, also cite Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations\n\nSmall grammatical errors\n\"Recently, Dinan et al. (2019) propose to tackle\" -> \"Recently, Dinan et al. (2019) proposed to tackle\"\n\"which subsequently improves the knowledge-grounded chit-chat.\" -> \"which subsequently improves knowledge-grounded chit-chat.\"\n\n\nSome questions for the authors in terms of future direction\nHow is the performance of the model impacted with longer dialog context vs shorter?\n\nThe Holl-E dataset was transformed from spans of knowledge to a single knowledge sentence. It would be interesting to see what happens when the knowledge selected is over multiple sentences.\n\nThe knowledge pool currently consists of 67.57 sentences on average. How will this method scale as the amount of knowledge sentences grows?\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1859/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1859/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["byeongchang.kim@vision.snu.ac.kr", "jaewoo.ahn@vision.snu.ac.kr", "gunhee@snu.ac.kr"], "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue", "authors": ["Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim"], "pdf": "/pdf/1a7e5e43b339d1db95cd03b0dc33382c6af1e444.pdf", "TL;DR": "Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark.", "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "keywords": ["dialogue", "knowledge", "language", "conversation"], "paperhash": "kim|sequential_latent_knowledge_selection_for_knowledgegrounded_dialogue", "code": "https://github.com/bckim92/sequential-knowledge-transformer", "_bibtex": "@inproceedings{\nKim2020Sequential,\ntitle={Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\nauthor={Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hke0K1HKwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/63006ade3338aa6dcc914324e4a9f120baf0ff75.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hke0K1HKwr", "replyto": "Hke0K1HKwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575705625586, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1859/Reviewers"], "noninvitees": [], "tcdate": 1570237731271, "tmdate": 1575705625605, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1859/-/Official_Review"}}}], "count": 8}