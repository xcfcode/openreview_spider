{"notes": [{"tddate": null, "tmdate": 1487002453861, "tcdate": 1487002453861, "number": 9, "id": "HkRLoL1Fg", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "Final version uploaded", "comment": "We have uploaded the final version. The text is unchanged, but we have modified the title to emphasise the aspects of the paper which have been of most interest to readers (particularly the inverted softmax).\n\nWe'd like to thank the PC for accepting our manuscript,\nSam"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487002231089, "tcdate": 1478283717692, "number": 289, "id": "r1Aab85gg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1Aab85gg", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "content": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396483072, "tcdate": 1486396483072, "number": 1, "id": "BJjr2GI_g", "invitation": "ICLR.cc/2017/conference/-/paper289/acceptance", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This is a nice contribution and that present some novel and interesting ideas. At the same time, the empirical evaluation is somewhat thin and could be improved. Nevertheless, the PCs believe this will make a good contribution to the Conference Track.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396483601, "id": "ICLR.cc/2017/conference/-/paper289/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1Aab85gg", "replyto": "r1Aab85gg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396483601}}}, {"tddate": null, "tmdate": 1483718493845, "tcdate": 1483718493845, "number": 8, "id": "r1LvJBprl", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "ByTk9Yt4e", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "Thank you", "comment": "Thank you for your interest in our work. \n\n1.\tWe were not aware of the work by Artetxe et al., which was published shortly after we submitted our manuscript to ICLR. We have now cited this work appropriately, and drawn it to the reviewers\u2019 attention.\n2.\tWe have realised that our method, while extremely similar, is not identical to CCA. We apologise for this mistake, which is now corrected in the text. The method we propose is very similar to the method independently proposed by Artetxe et al..\n3.\tWe acknowledge that the use of the term \u201ccognates\u201d was confusing and we have removed it from the manuscript.\n4.\tAlthough the theoretical analysis of bilingual word vectors presented here is similar to the analysis presented by Artetxe et al., we present a number of novel contributions not present in their work, including\n\na)\tHow to perform dimensionality reduction following the SVD\nb)\tThe inverted softmax\nc)\tThe identical-string pseudo dictionary\nd)\tOffline vector alignment using a phrase dictionary\ne)\tSentence retrieval between languages using bilingual vectors\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1483718349751, "tcdate": 1483718349751, "number": 7, "id": "SkICA4pSx", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "BJxR_kU4x", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "Response to review", "comment": "We would like to thank you for your positive assessment of our work, and of the inverted softmax in particular.\n\nWe have realised that our procedure, while very similar to CCA, is not identical. We apologise for this mistake, which we have corrected in the new version. We believe this realisation strengthens the manuscript. We have included additional experiments, and a discussion of the very close relationship between the methods. The two methods have very similar performance, but our approach is numerically cheaper.\n\n In response to your specific comments,\n\n1.\tWe have now cited this work, and briefly discuss it in the text.\n2.\tSimilarly, this paper is now cited. We still consider Chandar et al. the first to obtain bilingual vectors from monolingual corpora and paired sentences, since Hermann and Blunsom cite an early version of Chandar\u2019s work in their manuscript.\n3.\tWe hope to explore additional language pairs in future. However, we believe that the range of tasks we consider in this manuscript, including a number of new tasks not considered in previous work, does provide a rigorous experimental evaluation. These new tasks include the pseudo dictionary of identical strings, the phrase dictionary, and sentence retrieval between languages. One of the goals of this work was to show that offline bilingual vectors can be used in a number of ways not previously considered in the literature.\n4.\tThis is a very interesting point. The Mahalanobis distance is an alternative to the cosine similarity. Although we do not discuss it in the manuscript, one could devise an alternative alignment procedure, which minimises the Mahalanobis distance of the dictionary rather than maximising the cosine similarity. I suspect that this relates to the slight difference between our procedure and CCA; but we have been unable to find a reference which discusses this.\n5.\tWe do recognise your concern, but I\u2019m afraid we couldn\u2019t come up with an appropriate alternative to \u201ctranslation\u201d.\n6.\tWe fixed this.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1483718251420, "tcdate": 1483718251420, "number": 6, "id": "SkmuCVaSg", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "HkP2OYBEg", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "Response to review", "comment": "We would like to thank you for your positive assessment of our work.\nIn response to your remaining comments:\n\n1.\tWe have replaced \u201cWord frequency\u201d with \u201cWord ranking by frequency\u201d\n2.\tYes, these results were removed due to the space constraints, but we have now added them in an appendix.\n3.\tYes, the only difference here is NN vs. inverted softmax. Both columns use exactly the same orthogonal transformation. We have changed the text to make this clearer.\n4.\tWe did try naively combining the expert dictionary with the identical-strings pseudo-dictionary, but this performed worse than the expert dictionary alone. In hindsight this is not surprising, since the pseudo-dictionary is significantly larger but lower quality. It might be possible to get improved performance by introducing a weighting factor between the two dictionaries, but we have not tried this.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1483718141302, "tcdate": 1483718141302, "number": 5, "id": "r1B-RVarl", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "rkM8HK44l", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "Response to review", "comment": "Thank you for your review.\n\nWe\u2019d also like to thank you for correcting our use of the term \u201ccognates\u201d, which we have removed from the new version of the text. The pseudo-dictionary is formed from every character string (e.g. \u201cCiao\u201d) in the English vocabulary which also appears in the Italian vocabulary. This dictionary is obtained by searching for perfect character string matches between the two vocabularies. \n\nAlongside theoretical clarification of the existing literature, our work makes a number of novel contributions, including:\n\n1.\tThe inverted softmax\n2.\tThe identical strings pseudo-dictionary\n3.\tOffline bilingual vectors from a phrase dictionary\n4.\tSentence translation retrieval using bilingual vectors\n\nPreviously offline bilingual word vectors have only been obtained using expert word dictionaries, and they have not been used to retrieve sentence translations.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1483717890479, "tcdate": 1483717890479, "number": 4, "id": "Skq-p4TSg", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "Changes to new version", "comment": "Dear reviewers and readers,\n\nWe\u2019d like to thank you all for your positive comments about our manuscript. We were particularly pleased that all three reviewers recommended our work be accepted, and by the interest reviewers expressed in the \u201cinverted softmax\u201d. We have uploaded an updated version. There are three main changes we would like to draw readers\u2019 attention to:\n\nOur use of the term \u201ccognates\u201d was misleading and we have removed it from the new version. To be completely clear, we extract the pseudo-dictionary by finding the identical character strings like \u201cDNA\u201d and \u201cCiao\u201d which appear in both the English and Italian vocabularies. These identical strings can be found trivially without any expert knowledge. \n\nWe also realised that our procedure, while very similar to CCA, is not identical. We apologise for this mistake, which we have corrected in the new version. We believe this realisation strengthens the manuscript. We provide additional experiments, and a discussion of the very close relationship between the methods. The two methods have very similar performance, but our approach is numerically cheaper.\n\nShortly after our manuscript was submitted to ICLR, another paper was published [1], which presents a similar theoretical analysis of offline bilingual word vectors. We would like to thank the anonymous reader for bringing this work to our attention, now properly cited. This paper also discusses the need for an orthogonal transformation, and proposes the same novel SVD procedure we propose here to obtain this transformation. However, our work contains a number of contributions not present in their work, including:\n\n1.\tThe use of dimensionality reduction after the SVD\n2.\tThe inverted softmax\n3.\tThe identical strings pseudo-dictionary\n4.\tOffline vector alignment using a phrase dictionary\n5.\tSentence translation retrieval using bilingual vectors\n\nWe will respond to the specific comments of each reviewer underneath their reviews.\nBest wishes,\nSam\n\n[1] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1482426853355, "tcdate": 1482426853355, "number": 3, "id": "ByTk9Yt4e", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Theory on \"offline\" bilingual word vectors and cognates", "comment": "Thank you for the interesting paper.\n1. Could you elaborate on how your method provides additional theoretical insight into the importance of orthogonality beyond existing work [1] and [2]? We would additionally encourage you to cite [2].\n2. In accordance with the review of AnonReviewer1, could you elaborate how your method is different from the existing use of CCA in [3]?\n3. As AnonReviewer1 pointed out, cognates are words with the same etymological origin but are usually spelled differently (see [4] for more examples). You should replace this term to make your manuscript more accurate.\n4. Given the above points, the main contributions of your paper are a) the inverted softmax and b) the \"cognate\" dictionary. Is that correct?\n\n[1] Xing, C., Liu, C., Wang, D., & Lin, Y. (2015). Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation. NAACL-2015, 1005\u20131010. \n[2] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294. \n[3] Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using Multilingual Correlation. Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, 462 \u2013 471.\n[4] https://en.wikipedia.org/wiki/Cognate"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1482189000301, "tcdate": 1482189000301, "number": 3, "id": "BJxR_kU4x", "invitation": "ICLR.cc/2017/conference/-/paper289/official/review", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["ICLR.cc/2017/conference/paper289/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper289/AnonReviewer2"], "content": {"title": "Inverted Softmax is nice", "rating": "7: Good paper, accept", "review": "This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.\n\nThe inverted Softmax idea is very nice.\n\nA few minor issues that ought to be addressed in a published version of this paper:\n\n1) There is no mention of Haghighi et al (2008) \"Learning Bilingual Lexicons from Monolingual Corpora.\", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed.\n2) Likewise, Hermann & Blunsom (2013) \"Multilingual distributed representations without word alignment.\" is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.\n3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages\n4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion\n5) I don't have a better suggestion, but is there an alternative to using the term \"translation (performance/etc.)\" when discussing word alignment across languages? Translation implies something more complex than this in my mind.\n6) The Mikolov citation in the abstract is messed up", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512635310, "id": "ICLR.cc/2017/conference/-/paper289/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper289/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper289/AnonReviewer1", "ICLR.cc/2017/conference/paper289/AnonReviewer3", "ICLR.cc/2017/conference/paper289/AnonReviewer2"], "reply": {"forum": "r1Aab85gg", "replyto": "r1Aab85gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512635310}}}, {"tddate": null, "tmdate": 1482164398683, "tcdate": 1482164398683, "number": 2, "id": "HkP2OYBEg", "invitation": "ICLR.cc/2017/conference/-/paper289/official/review", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["ICLR.cc/2017/conference/paper289/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper289/AnonReviewer3"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper focuses on bilingual word representation learning with the following setting:\n\n1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.\n2. There is no direct word to word alignments available between the source and target language.\n\nThis is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:\n\n1. Theory for offline bilingual representation learning.\n2. Inverted softmax.\n3. Using cognate words for languages that share similar scripts.\n4. Showing that this method also works at sentence level (to some extent).\n\nAuthors have addressed all my pre-review questions and I am ok with their response. I have few more comments:\n\n1. Header for table 3 which says \u201cword frequency\u201d is misleading. \u201cword frequency\u201d could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.\n2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.\n3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.\n4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512635310, "id": "ICLR.cc/2017/conference/-/paper289/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper289/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper289/AnonReviewer1", "ICLR.cc/2017/conference/paper289/AnonReviewer3", "ICLR.cc/2017/conference/paper289/AnonReviewer2"], "reply": {"forum": "r1Aab85gg", "replyto": "r1Aab85gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512635310}}}, {"tddate": null, "tmdate": 1482097994404, "tcdate": 1482097994404, "number": 1, "id": "rkM8HK44l", "invitation": "ICLR.cc/2017/conference/-/paper289/official/review", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["ICLR.cc/2017/conference/paper289/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper289/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the authors propose two changes: \"CCA\" and \"inverted softmax\".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.\n\nOverall, I wonder which aspect of this paper is really new. You mention:\n - Faruqui & Dyer 2014 already used CCA and dimensionality reduction\n - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal\n\nCould you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?\n\nUsing cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)\n\nAlso, it seems to me that in linguistics the term \"cognate\" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512635310, "id": "ICLR.cc/2017/conference/-/paper289/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper289/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper289/AnonReviewer1", "ICLR.cc/2017/conference/paper289/AnonReviewer3", "ICLR.cc/2017/conference/paper289/AnonReviewer2"], "reply": {"forum": "r1Aab85gg", "replyto": "r1Aab85gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512635310}}}, {"tddate": null, "tmdate": 1481048882552, "tcdate": 1481048882544, "number": 2, "id": "BJcEmKNQe", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "BkeeRzQQx", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "(text now updated)", "comment": "we've uploaded a new version, incorporating the comments above."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1480957416503, "tcdate": 1480957416496, "number": 1, "id": "BkeeRzQQx", "invitation": "ICLR.cc/2017/conference/-/paper289/public/comment", "forum": "r1Aab85gg", "replyto": "ByM-SP1Xl", "signatures": ["~Samuel_L_Smith1"], "readers": ["everyone"], "writers": ["~Samuel_L_Smith1"], "content": {"title": "Thank you", "comment": "Thank you for your helpful comments! \nWe will upload a new version soon to incorporate your suggestions and the extra experiments you request, but to answer your specific questions:\n\n1. We apologise for this and will ensure the work you mention is cited properly.\n\n2. We have run the additional experiments you requested, and will include them in the updated version. CCA + inverted softmax outperforms Mikolov and Dinu\u2019s methods in all three cases. The advantage is particularly pronounced when using the pseudo-dictionary, for which the precision @1 when translating from English to Italian was 0.01 for Mikolov\u2019s method and 0.06 for Dinu\u2019s method, compared to 0.40 when using CCA with the inverted softmax. We believe that this difference arises because orthogonal transformations are more robust to the high level of noise present in the pseudo-dictionary. We think these results strengthen the paper, and we are grateful to the referee for requesting them.\n\n3. We certainly do not claim to be the first authors to obtain bilingual word vectors using CCA. Our goal is to show the close connection between the works of Mikolov, Dinu, Faruqui and Xing, and enhance the translation performance using the inverted softmax. Finally, we demonstrate that high-quality offline word vectors can be obtained without dictionaries from either cognate words or aligned sentences, and that bilingual word vectors can be used to retrieve sentence translations. We will ensure this is clearer in the new version.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287638521, "id": "ICLR.cc/2017/conference/-/paper289/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Aab85gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper289/reviewers", "ICLR.cc/2017/conference/paper289/areachairs"], "cdate": 1485287638521}}}, {"tddate": null, "tmdate": 1480713465704, "tcdate": 1480713465700, "number": 1, "id": "ByM-SP1Xl", "invitation": "ICLR.cc/2017/conference/-/paper289/pre-review/question", "forum": "r1Aab85gg", "replyto": "r1Aab85gg", "signatures": ["ICLR.cc/2017/conference/paper289/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper289/AnonReviewer3"], "content": {"title": "Few questions and comments", "question": "1. Gouws et al., 2015 were not the first to show that aligned sentences can be used alongside monolingual source to learn online bilingual vectors. It was Chandar et al., 2014 [1] who first showed that you can use both monolingual data and sentence aligned bilingual data to learn bilingual representations. Please correct this.\n\n2. Table 4,5,6 are incomplete without comparison with Dinu and Mikolov. Note that you can also use pseudo-dictionary and phrase dictionary in Dinu or Milovo's models. I would like to see those numbers before accepting the message from these tables.\n\n3. Faruqui and Dyer 2014 already did offiline bilingual word vectors using CCA. I agree that their motivation was different. I also agree that authors have some good theory to back it up. However it is extremely important to highlight the fact that this is not a new model and has been already proposed before. Paper would benefit if authors can list all the major contributions of this paper in the beginning: theory, pseudo-dictionary, inverted softmax, ..\n\nReferences:\n\n[1] Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. An autoencoder approach to learning bilingual word representations. In Advances in\nNeural Information Processing Systems, pp. 1853\u20131861, 2014"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "pdf": "/pdf/3a5fccf6658985bbe4ae4c22b0e1e0ddb65a5247.pdf", "TL;DR": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "paperhash": "smith|offline_bilingual_word_vectors_orthogonal_transformations_and_the_inverted_softmax", "keywords": ["Natural language processing", "Transfer Learning", "Applications"], "conflicts": ["babylonhealth.com", "cam.ac.uk"], "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959359597, "id": "ICLR.cc/2017/conference/-/paper289/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper289/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper289/AnonReviewer3"], "reply": {"forum": "r1Aab85gg", "replyto": "r1Aab85gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper289/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959359597}}}], "count": 15}