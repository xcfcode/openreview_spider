{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486660398935, "tcdate": 1478322433291, "number": 549, "id": "SkYbF1slg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkYbF1slg", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "content": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396671253, "tcdate": 1486396671253, "number": 1, "id": "ByvWaM8_x", "invitation": "ICLR.cc/2017/conference/-/paper549/acceptance", "forum": "SkYbF1slg", "replyto": "SkYbF1slg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers were not completely happy with the presentation, but it seems the theory is solid and interesting enough. I think ICLR needs more papers like this, which have convincing mathematical theory instead of merely relying on empirical results.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396671752, "id": "ICLR.cc/2017/conference/-/paper549/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkYbF1slg", "replyto": "SkYbF1slg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396671752}}}, {"tddate": null, "tmdate": 1484927697727, "tcdate": 1484927697727, "number": 9, "id": "B1qRG3kwe", "invitation": "ICLR.cc/2017/conference/-/paper549/public/comment", "forum": "SkYbF1slg", "replyto": "SkYbF1slg", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "writers": ["~Wentao_Huang1"], "content": {"title": "A new update", "comment": "We have clarified some of the questions on paragraph 2 on page 4, on paragraph 2 on page 6 and on paragraph 2 on page 15, respectively. Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526564, "id": "ICLR.cc/2017/conference/-/paper549/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287526564}}}, {"tddate": null, "tmdate": 1484330553869, "tcdate": 1484330553869, "number": 6, "id": "B1zBUq8Ul", "invitation": "ICLR.cc/2017/conference/-/paper549/public/comment", "forum": "SkYbF1slg", "replyto": "SkYbF1slg", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "writers": ["~Wentao_Huang1"], "content": {"title": "Paper updated", "comment": "We would like to thank the reviewers for their conscientious review and comments and for providing us with valuable feedback. We already updated the paper based on the reviewer comments.\n\nThanks!\n\nWentao"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526564, "id": "ICLR.cc/2017/conference/-/paper549/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287526564}}}, {"tddate": null, "tmdate": 1482266051732, "tcdate": 1482266051732, "number": 5, "id": "HJhTrGw4l", "invitation": "ICLR.cc/2017/conference/-/paper549/public/comment", "forum": "SkYbF1slg", "replyto": "H1mSqlPVl", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "writers": ["~Wentao_Huang1"], "content": {"title": "Responding", "comment": "Sure. The following is the response to your questions:\n\n(1) This question is similar to AnonReviewer3's question. From the Proposition 2.1 we can know that maximizing I(X;R) is equivalent to maximizing I(Y;R).  However, there may be many linear transformation from X to Y to meet maximizing I(Y;R) (or maximizing I(X;R)) (see, for example, Section 2.2.1), and we only need Y to satisfy certain conditions (see, for example, Eq. 2.60 in Section 2.2.2). A reasonable choice for the linear transformation is  to meet maximizing I(X;Y) (or maximizing I(X;\\breve{Y})), which also means that the minimum information loss in the first step transformation. Ideally, this choice would satisfy maximizing I(X;Y) and maximizing I(Y;R) (or I(X;R)) at the same time. On the other hand, if we do not choose the transformation to meet maximizing I(X;Y), obviously, this transformation will probably affect the maximum value of I(Y;R). \n\n(2) Dropout can result in a sparse weights (low rank matrix) update. This means that the update is only concerned with more important parts, which is similar to the denoising process by the SVD (singular value decomposition) low rank approximation.\n\n(3) Similar to question (3), if we want to maximize I(X;R), then we require that matrix C satisfies certain condition to maximize I(Y;R) (see Section 2.2 and Section 2.3). However, actually we completely abandon the orthogonality constraint (Eq. 2.80) after the initial iteration t_0 epoches (see Section 2.3.1 and 2.3.2). The orthogonality constraint (Eq. 2.80) used by the initial iteration can accelerate the convergence rate. \n\nThanks!\n\nWentao Huang"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526564, "id": "ICLR.cc/2017/conference/-/paper549/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287526564}}}, {"tddate": null, "tmdate": 1482259002683, "tcdate": 1482259002683, "number": 1, "id": "H1mSqlPVl", "invitation": "ICLR.cc/2017/conference/-/paper549/official/comment", "forum": "SkYbF1slg", "replyto": "rkpo8kv4x", "signatures": ["ICLR.cc/2017/conference/paper549/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper549/AnonReviewer2"], "content": {"title": "Responding here", "comment": "Although ultimately the questions should be taken care of in the final revision, could you provide some response here to those questions as well?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287525831, "id": "ICLR.cc/2017/conference/-/paper549/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper549/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper549/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287525831}}}, {"tddate": null, "tmdate": 1482253988717, "tcdate": 1482253988717, "number": 4, "id": "rkpo8kv4x", "invitation": "ICLR.cc/2017/conference/-/paper549/public/comment", "forum": "SkYbF1slg", "replyto": "r1hN6IU4l", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "writers": ["~Wentao_Huang1"], "content": {"title": "Responding to AnonReviewer2", "comment": "Thank you very much for review and suggestion! Your suggestion is very good and we will reorganize the article based on your suggestion and clarify your questions in the revised version. \n\nThanks again!\n\nWentao Huang"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526564, "id": "ICLR.cc/2017/conference/-/paper549/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287526564}}}, {"tddate": null, "tmdate": 1482218881141, "tcdate": 1482218803697, "number": 3, "id": "r1hN6IU4l", "invitation": "ICLR.cc/2017/conference/-/paper549/official/review", "forum": "SkYbF1slg", "replyto": "SkYbF1slg", "signatures": ["ICLR.cc/2017/conference/paper549/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper549/AnonReviewer2"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents an information theoretic framework for unsupervised learning. The framework relies on infomax principle, whose goal is to maximize the mutual information between input and output. The authors propose a two-step algorithm for learning in this setting. First, by leveraging an asymptotic approximation to the mutual information, the global objective is decoupled into two subgoals whose solutions can be expressed in closed form. Next, these serve as the initial guess for the global solution, and are refined by the gradient descent algorithm.\n\nWhile the story of the paper and the derivations seem sound, the clarity and presentation of the material could improve. For example, instead of listing step by step derivation of each equation, it would be nice to first give a high-level presentation of the result and maybe explain briefly the derivation strategy. The very detailed aspects of derivations, which could obscure the underlying message of the result could perhaps be postponed to later sections or even moved to an appendix.\n\nA few questions that the authors may want to clarify:\n1. Page 4, last paragraph: \"from above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)\". While I see the former holds due to equality in 2.20, the latter is related via a bound in 2.21. Due to the possible gap between I(X;R) and I(X,Y^U), can your claim that maximizing of the former indeed maximizes the latter be true?\n2. Paragraph above section 2.2.2: it is stated that, dropout used to prevent overfitting may in fact be regarded as an attempt to reduce the rank of the weight matrix. No further tip is provided why this should be the case. Could you elaborate on that?\n3. At the end of page 9: \"we will discuss how to get optimal solution of C for two specific cases\". If I understand correctly, you actually are not guaranteed to get the optimal solution of C in either case, and the best you can guarantee is reaching a local optimum. This is due to the nonconvexity of the constraint 2.80 (quadratic equality). If optimality cannot be guaranteed, please correct the wording accordingly.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512545738, "id": "ICLR.cc/2017/conference/-/paper549/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper549/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper549/AnonReviewer1", "ICLR.cc/2017/conference/paper549/AnonReviewer3", "ICLR.cc/2017/conference/paper549/AnonReviewer2"], "reply": {"forum": "SkYbF1slg", "replyto": "SkYbF1slg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper549/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper549/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512545738}}}, {"tddate": null, "tmdate": 1482212888943, "tcdate": 1482212888943, "number": 3, "id": "S1-m8B8Ve", "invitation": "ICLR.cc/2017/conference/-/paper549/public/comment", "forum": "SkYbF1slg", "replyto": "BkMfqzUNx", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "writers": ["~Wentao_Huang1"], "content": {"title": "Responding to AnonReviewer3", "comment": "Thanks so much for your review! \n\n(1) Since there is no similar work in the previous papers, so our paper is longer and there are relatively more formulas and derivations. We are now trying to condense this paper.\n\n(2) Here notice that when noise variance \\sigma^{2}\\rightarrow 0, from Proposition 2.1. we can get I(X;Y)\\simeq I(X;\\bar{Y})=I(X;\\breve{Y}), and I(X;R)=I(Y;R)\\simeq I(\\bar{Y};R). From (2.20) we know that maximizing I(X;R) is equivalent to maximizing I(Y;R) and I(Y;R)<=I(X;\\breve{Y}) \\simeq I(X;Y). We can see that as I(X;R) or I(Y;R) increases I(X;\\breve{Y}) also increases. While notice that there may be many transformation from X to Y (or \\breve{Y}) to meet maximizing I(Y;R)(or maximizing I(X;R)) (see, for example, Section 2.2.1). A reasonable choice is this optimal transformation to meet maximizing I(X;Y) (or maximizing I(X;\\breve{Y})), which also means that the minimum information loss in the first step transformation. We will explain these in detail in the revised version.\n\nMinor comments:\n(1) You are right. \u201c\\approx\u201d should be used. \n\n(2) Notice that N is very large, N >> K, and usually it can be clustered into K_1 classes, e.g. K_1 << N.\n\n(3) In Eq.(2.12), I_G is equal to 0.5*<...> + H(X). But here H(X) is a constant and does not affect the final optimization solution. \n\n(4) It is a good suggestion. We will divide Section 3 into subsections in the revised version.\n\nThanks again!\n\nWentao Huang"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526564, "id": "ICLR.cc/2017/conference/-/paper549/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287526564}}}, {"tddate": null, "tmdate": 1482201610690, "tcdate": 1482201610690, "number": 2, "id": "BkMfqzUNx", "invitation": "ICLR.cc/2017/conference/-/paper549/official/review", "forum": "SkYbF1slg", "replyto": "SkYbF1slg", "signatures": ["ICLR.cc/2017/conference/paper549/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper549/AnonReviewer3"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a hierarchical infomax method. My comments are as follows: \n\n(1) First of all, this paper is 21 pages without appendix, and too long as a conference proceeding. Therefore, it is not easy for readers to follow the paper. The authors should make this paper as compact as possible while maintaining the important message. \n\n(2) One of the main contribution in this paper is to find a good initialization point by maximizing I(X;R). However, it is unclear why maximizing I(X;\\breve{Y}) is good for maximizing I(X;R) because Proposition 2.1 shows that I(X;\\breve{Y}) is an \u201cupper\u201d bound of I(X;R) (When it is difficult to directly maximize a function, people often maximize some tractable \u201clower\u201d bound of it).\n\nMinor comments:\n(1) If (2.11) is approximation of (2.8), \u201c\\approx\u201d should be used. \n\n(2) Why K_1 instead of N in Eq.(2.11)?\n\n(3) In Eq.(2.12), H(X) should disappear?\n\n(4) Can you divide Section 3 into subsections?\n\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512545738, "id": "ICLR.cc/2017/conference/-/paper549/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper549/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper549/AnonReviewer1", "ICLR.cc/2017/conference/paper549/AnonReviewer3", "ICLR.cc/2017/conference/paper549/AnonReviewer2"], "reply": {"forum": "SkYbF1slg", "replyto": "SkYbF1slg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper549/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper549/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512545738}}}, {"tddate": null, "tmdate": 1481930070104, "tcdate": 1481930070104, "number": 2, "id": "SyR8BlzNg", "invitation": "ICLR.cc/2017/conference/-/paper549/public/comment", "forum": "SkYbF1slg", "replyto": "HypYOCbEe", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "writers": ["~Wentao_Huang1"], "content": {"title": "Responding to AnonReviewer1", "comment": "Thanks so much for your suggestions! In fact, the method of hierarchical infomax can be extended to a deep net hierarchy which is explained on page 5 of this paper. We have also used this method to train the deep nets. The work of Karklin & Simoncelli 2011 is really highly related, but they just solved a linear problem. I will try to condense this paper. \nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526564, "id": "ICLR.cc/2017/conference/-/paper549/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287526564}}}, {"tddate": null, "tmdate": 1481922692815, "tcdate": 1481922692815, "number": 1, "id": "HypYOCbEe", "invitation": "ICLR.cc/2017/conference/-/paper549/official/review", "forum": "SkYbF1slg", "replyto": "SkYbF1slg", "signatures": ["ICLR.cc/2017/conference/paper549/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper549/AnonReviewer1"], "content": {"title": "Review of \"information theoretic framework\"", "rating": "7: Good paper, accept", "review": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512545738, "id": "ICLR.cc/2017/conference/-/paper549/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper549/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper549/AnonReviewer1", "ICLR.cc/2017/conference/paper549/AnonReviewer3", "ICLR.cc/2017/conference/paper549/AnonReviewer2"], "reply": {"forum": "SkYbF1slg", "replyto": "SkYbF1slg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper549/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper549/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512545738}}}, {"tddate": null, "tmdate": 1478584480008, "tcdate": 1478584479360, "number": 1, "id": "SJvsdkyZe", "invitation": "ICLR.cc/2017/conference/-/paper549/public/comment", "forum": "SkYbF1slg", "replyto": "SkYbF1slg", "signatures": ["~Wentao_Huang1"], "readers": ["everyone"], "writers": ["~Wentao_Huang1"], "content": {"title": "Some background on this paper", "comment": "My main research interests are in computational neuroscience, information theory, machine learning and deep learning. For those of us who engage in AI related research, we all want to learn from the human brain how to process information to achieve intelligence. For example, deep learning is now said to be brain-like, but also some researchers say that nothing to do with the neuroscience. But one thing is for sure, and that is deep nets learning the brain processing information with hierarchical structure. However, the current advances in neuroscience is of limited role on the inspiration for AI research. \n\nHow do we learn from the brain, learn what? We think we should learn the basic design principles for the information processing in our nervous system. What are the fundamental principles to guide the brain to design these complex structures and neural coding. \nIn fact, the principle on energy and information provides a fundamental constraint for the actual nervous systems. The efficient coding hypothesis proposed by Horace Barlow provides a good description of this principle. But how to model this hypothesis, we will encounter great challenges. Because a direct calculation of Shannon's mutual information (MI) is generally a very difficult thing in many cases. \n\nWe first solve the problem of effective approximation for evaluating MI in the context of neural population coding, especially for high-dimensional inputs. Then an fast and robust unsupervised learning algorithm is developed in this paper. With our methods, we have explained some interesting phenomena in neuroscience, which were previously unsolvable in other ways. \nIn machine learning and deep learning, this paper is a more fundamental part  that provides the basis for our other works. We have also got a lot of interesting results from other related works we have done or are doing. For example, we use our method to train CNN, RNN and generative model, etc. We can unify supervised learning and unsupervised learning and can completely without the backpropagation algorithm for supervised learning of deep nets. We can also prove that the cost function of SVM (Hinge loss) is a special case of our neural population infomax. For the current deep nets, we need the big data for training, and in our framework, the small data we also can achieve good results.\n\nThe above is a brief introduction to the background of this paper. Since there is no similar work on this paper before, so there may be relatively more formulas and derivations and not very easy to follow. \n\nThanks!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax", "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "pdf": "/pdf/94058ba880bcc05cf75a967aec77aa1434b7a76d.pdf", "TL;DR": "We present a novel information-theoretic framework for fast and robust unsupervised Learning via information maximization for neural population coding.", "paperhash": "huang|an_informationtheoretic_framework_for_fast_and_robust_unsupervised_learning_via_neural_population_infomax", "conflicts": ["jhmi.edu"], "authors": ["Wentao Huang", "Kechen Zhang"], "keywords": ["Unsupervised Learning", "Theory", "Deep learning"], "authorids": ["whuang21@jhmi.edu", "kzhang4@jhmi.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526564, "id": "ICLR.cc/2017/conference/-/paper549/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkYbF1slg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper549/reviewers", "ICLR.cc/2017/conference/paper549/areachairs"], "cdate": 1485287526564}}}], "count": 13}