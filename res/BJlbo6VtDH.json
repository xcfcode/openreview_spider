{"notes": [{"id": "BJlbo6VtDH", "original": "rylnd3RvDS", "number": 734, "cdate": 1569439129437, "ddate": null, "tcdate": 1569439129437, "tmdate": 1577168253015, "tddate": null, "forum": "BJlbo6VtDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XxEAzVCsdu", "original": null, "number": 1, "cdate": 1576798704541, "ddate": null, "tcdate": 1576798704541, "tmdate": 1576800931520, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a generalized way to generate sequences from undirected sequence models.\n\nOverall, I believe a framework like this could definitely be a valuable contribution, but as Reviewer 1 and Reviewer 3 noted, the paper is a bit lacking both in theoretical analysis and strong empirical results. I don't think that this is a bad paper at all, but it feels like the paper needs a little bit of an extra push to tighten up the argumentation and/or results before warranting publication at a premier venue such as ICLR. I'd suggest the authors continue to improve the paper and aim to re-submit at revised version at a future conference. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722411, "tmdate": 1576800273705, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper734/-/Decision"}}}, {"id": "ByeiMsb2sH", "original": null, "number": 5, "cdate": 1573817106678, "ddate": null, "tcdate": 1573817106678, "tmdate": 1573817106678, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "r1g_yDrjiB", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment", "content": {"title": "Thanks for following up", "comment": "Thanks for following up on the review! \n\nWhat is your definition of general purpose decoder ? We are hoping to clarify it in order to understand better how decoding time constraints fit in your definition of general purpose decoder.\n\nWith a learnable position selection mechanisms, we would imagine that under different time constraints (linear-time vs constant-time decoding) the model would learn different position selection strategies and time budget is important to include in the error signal. "}, "signatures": ["ICLR.cc/2020/Conference/Paper734/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlbo6VtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper734/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper734/Authors|ICLR.cc/2020/Conference/Paper734/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167031, "tmdate": 1576860546145, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment"}}}, {"id": "r1g_yDrjiB", "original": null, "number": 4, "cdate": 1573766880143, "ddate": null, "tcdate": 1573766880143, "tmdate": 1573766880143, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "rJlX9i3Ssr", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "I thank the authors for the response to the review. The authors point out that under different time-constraints, they observe performance difference between different strategies. But for a general purpose decoder, I still feel that results regarding the best ordering strategy are inconclusive.\n\nMoreover, while I understand the importance of working on non left-to-right decoding strategies, my comment was about working on decoding in isolation instead of thinking about both training and decoding since the training/inference compatibility plays a huge role performance-wise.\n\nAlso, in light of the mixed results, my mind is unchanged on inclusion of learnable position selection mechanisms."}, "signatures": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlbo6VtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper734/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper734/Authors|ICLR.cc/2020/Conference/Paper734/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167031, "tmdate": 1576860546145, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment"}}}, {"id": "SkxZ8anSjH", "original": null, "number": 3, "cdate": 1573403976786, "ddate": null, "tcdate": 1573403976786, "tmdate": 1573403976786, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "BklHgEkAFB", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment", "content": {"title": "Thanks for the review!", "comment": "We believe that our framework is a probabilistically sound way of generating from masked language models, and language models generally. Our methodology does require some approximations to tractably generate sentences, but the overall generative story we offer is still valid. Moreover, even generating in traditional left-to-right autoregressive models requires approximations (e.g. greedy decoding or beam search) due to the exponential hypothesis space. \n\nWe believe our approximations are theoretically sensible, but they are difficult to verify how good they are due to the fact that what we\u2019re trying to approximate in the first place is highly intractable to compute. One piece of evidence we\u2019ve looked at and included in the paper (Fig 3 in appendix in revision) is the probability of the intermediate sequences over the generation process. The energy (in the sense of energy-based models, see https://arxiv.org/abs/1902.04094) of the target sequence decreases (equivalently, the probability increases) throughout the generation process. The figure also shows that more sophisticated decoding schemes such as left2right, easy-first, and least2most converge to lower energy (higher probability) target sequences faster compared to uniformly picking positions. Overall, we believe that this is evidence that even with the approximations involved, our framework is able to find target sequences in a probabilistically sound manner and points to the value of further research in developing better coordinate selection mechanisms. If the reviewer has any thoughts on further experiments or evidence to explore, we\u2019d be happy to hear it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper734/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlbo6VtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper734/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper734/Authors|ICLR.cc/2020/Conference/Paper734/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167031, "tmdate": 1576860546145, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment"}}}, {"id": "HklJJ62SoB", "original": null, "number": 2, "cdate": 1573403863398, "ddate": null, "tcdate": 1573403863398, "tmdate": 1573403863398, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "ryeBboGP9B", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment", "content": {"title": "Thanks for the review!", "comment": "We appreciate that you found our work interesting!\n\n> \u201cIt should be analyzed more why different coordinate selection approaches perform differently in linear-time decoding vs. constant-time decoding. Even in constant-time decoding, the conclusion varies in different decoding setting, easy-first is the worst for the L->1 case, but the best for the L/T case, why is that?\u201d\n\nIn the L->1 case, we refine tokens multiple times, so it makes sense to generate the hard tokens first and refine them several times, rather than generate the hard tokens at the end and get no opportunity to refine them. In the L->T setting, there is no refinement, so generating the easy tokens first makes sense as it gives additional context for the hard tokens.\n\n> \u201cWhat is the motivation for \"hard-first\"?\u201d\n\nAs stated above, our intuition is that in settings where we allow the model to refine its predictions, it makes sense for the model to first generate the hard-to-predict tokens and get multiple attempts to fix them, rather than predicting the hard tokens at the end and not getting a chance to refine them.\n\n> \u201cThe setting of \"least2most\" with L->1 is similar to Ghazvininejad et al. 2019. But Table 4 in the appendix shows the result in this paper is still worse (21.98 vs. 24.61, when both systems use 10 iterations without AR). Also, the gap from the AR baseline is larger than that in Ghazvininejad et al. 2019. Given the two systems are considered similar, it should be explained in the paper the possible reasons for these discrepancies in results.\u201d \n\nThere are differences in both model and training hyperparameters between our work and work by Ghazvininejad et al. 2019. We use smaller Transformer model with 1024 hidden units vs 2048 units in Ghazvininejad et al. 2019. We also train the model with more than twice smaller batch size since we use 8 GPUs on DGX-1 machine and Ghazvininejad et al. 2019 use 16 GPUs on two DGX-1 machine with float16 precision. Finally we don\u2019t average best 5 checkpoints and don\u2019t use label smoothing for our model.\n\n> \u201cIn the introduction, it mentions the baseline AR is (Vaswani et al. 2017), while in the experimental settings, it mentions the baseline AR is (Bahdanau et al. 2015). Please clarify which one is used.\u201d\n\nWe used Transformer AR model (Vaswani et al 2017) initialized with the same pretrained model weights used for our model. Bahdanau et al 2015 used a reference to general autoregressive machine translation approach with attention. We removed Bahdanau et al 2015 citation from that paragraph to remove ambiguity in the revision.\n\n> \u201cIn Table 1, how does T = 2L work for the \"Uniform\" case while the target sequence length is only T, since it is mentioned the positions are sampled without replacement. Similarly, how does T = 2L work for the \"Left2Right\" case? Is it just always choosing the last position when L < t <= 2L? In these two cases, it seems T > L is not needed.\u201d\n\nIn uniform case, position selection is done by sampling uniformly without replacement with population of the set of all possible positions for a defined length. During generation once number of generation steps T becomes the same as length L we reset the population and do uniform sampling without replacement again. For left2right case when T = 2L the generation process goes left-to-right twice, resetting at the leftmost token after L steps.\n\n> \u201cIn Table 3, the header for the 2nd column should be o_t, as defined in Section 4 - \"Decoding scenarios\". What is the actual value of K and K'' for the constant-time machine translation experiments in the paper?\u201d\n\nThanks for pointing out that the header typo! It should be updated in the revision. We don\u2019t do beam search for constant-time machine translation so K=K\u2019\u2019=1.\n\n> \u201c\"Rescoring adds minimal overhead as it is run in parallel\" - it still needs to run left-to-right in sequence since it is auto-regressive. Please clarify what it means by \"in parallel\" here.\u201d\n\nFor rescoring we only need to obtain log probabilities given the full sequence available to us. Compared to generation where tokens are generated sequentially one at a time in left-to-right order, rescoring is done in parallel where left-to-right constraint is enforced by masking out future tokens (i.e. tokens to the right of current token) \n\n> \u201cWhat is the range and average for the target sentence length? How is T = 20 for constant-time decoding compared to linear-time decoding in terms of speed?\u201d\n\nFor the WMT\u201914 En-De dataset we used, the average target sentence length for both English and German sides is about 25 and sentence lengths range from 3 up to 120.   "}, "signatures": ["ICLR.cc/2020/Conference/Paper734/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlbo6VtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper734/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper734/Authors|ICLR.cc/2020/Conference/Paper734/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167031, "tmdate": 1576860546145, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment"}}}, {"id": "rJlX9i3Ssr", "original": null, "number": 1, "cdate": 1573403531212, "ddate": null, "tcdate": 1573403531212, "tmdate": 1573403531212, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "B1gITGseqB", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment", "content": {"title": "Thanks for the review!", "comment": "We appreciate that you found our work exciting! We disagree that there is no clear pattern on which decoding strategy to use for linear/constant time decoding scenarios. For linear time case (Table 1), left2right and easy-first decoding strategies outperform both least2most and uniform decodings while left2right having slight performance improvement over easy-first decoding strategy. These results hold across various linear-time decoding hyperparameter settings (beam size, decoding budget (T) and whether using reranking with an autoregressive model). For constant-time decoding results (Table 3) least2most decoding strategy works best when annealing number of generated symbols at each step (L -> 1) while easy-first strategy works best when generating constant L/T number of tokens. In practice if one would to use our method, left2right is recommended for linear-time translation and least2most is recommended for constant-time translation with annealing number of tokens L->1.\n\n> \u201cThe central question is: if the training prefers left-to-right generation then how valuable is it to device more reasonable but incompatible decoding procedures?\u201d\n\nWe don\u2019t think it\u2019s obvious that left2right decoding is the best decoding strategy for all possible decoding settings. For example, from our experiments in the constant time decoding, left2right decoding performs considerably worse than least2most decoding. Furthermore, given the proliferation of and advantages of non-left-to-right pretraining objectives, we argue that it is worthwhile to investigate non-left-to-right decoding strategies. Such strategies have additional potential benefits, such as being easier to parallelize.\n\n> \u201cI think the paper would be strengthened with results with a more sophisticated learned index selection procedure in addition to the heuristics investigated in this paper.\u201d\n\nWe would agree that learning position selecting mechanism would be interesting to investigate, but we think this would be a significant undertaking on its own, as we would need to develop and test ways of providing generation order error signal and methods to learn against that signal."}, "signatures": ["ICLR.cc/2020/Conference/Paper734/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlbo6VtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper734/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper734/Authors|ICLR.cc/2020/Conference/Paper734/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167031, "tmdate": 1576860546145, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper734/Authors", "ICLR.cc/2020/Conference/Paper734/Reviewers", "ICLR.cc/2020/Conference/Paper734/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Comment"}}}, {"id": "BklHgEkAFB", "original": null, "number": 1, "cdate": 1571841005135, "ddate": null, "tcdate": 1571841005135, "tmdate": 1572972559014, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a general framework for sentence generation using a BERT-like model. The authors decompose the problem of sentence generation into two problems. One is selecting the positions at which changes should be made, and the other is actually replacing the current word with a new word. This framework enables them to represent many decoding strategies including that of Ghazvininejas et al. (2019) in a unified manner, and they propose a new decoding strategy that considers the prediction confidence of the current and the new word. The paper also presents a heuristic algorithm for beam search decoding to find the most likely generation path. Their experimental results on the WMT14 English-German dataset suggest that the proposed approach could achieve translation quality comparable to that of the standard autoregressive approach under a constant-time translation setting.\n\nIt is nice to see existing decoding strategies represented in a generalized framework, but I was a bit disappointed that the authors do not seem to address the most critical problem in using a BERT-like model for sentence generation, namely, how to find the most likely sentence in a probabilistically sound way. It seems to me that the authors rely on at least two approximations. One is using pseudo-likelihood and the other is using the most likely generation path instead of performing marginalization. It is fine that the authors focus on empirical results of translation quality but then I would like to see more strong and extensive evidence that supports the use of such approximation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper734/Reviewers"], "noninvitees": [], "tcdate": 1570237747866, "tmdate": 1574723087736, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Review"}}}, {"id": "B1gITGseqB", "original": null, "number": 2, "cdate": 1572020925904, "ddate": null, "tcdate": 1572020925904, "tmdate": 1572972558969, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper focuses on decoding/generation in neural sequence models (specifically machine translation) in a non-autoregressive manner that instead of generating in a left-to-right manner, focuses on generating sequences by picking a length first ,and then indices to replace in a deterministic or random scheme and, finally using a context sensitive distribution over vocabulary (using BERT-like masked LM scheme)  to pick the word to replace. In practice, this procedure of picking indices and words to replace is repeated T number of times and hence the final sequence is obtained by this iterative refinement procedure. This is an interesting and important research direction because not only would it result in better and context sensitive greedy/approximate-MAP decoded outputs, but also opens up opportunities for parallelization of the decoding procedure which is difficult to achieve with left-to-right decoders.\nThat said, the results are fairly inconclusive and the practical implementation does leave things desired for a practical decoder. As observed by the authors, different deterministic strategies for choosing T results in very different performances among the variants of the proposed approach. Besides among the variants, one clear pattern is that uniformly random picking of indices is worse than other schemes (left-to-right, least-to-most, easy-first) which is not unexpected but no conclusive empirical evidence can be found for relative differences between the performances of other 3 schemes. Moreover, the proposed decoding variants generally perform worse than or at best similarly to standard autoregressive baselines. As authors note, this is due to the mismatch between the method in which the model was trained and the decoding procedure which is not surprising, but at the same time this does not give insight into the effectiveness of the proposed decoding objective. The central question is: if the training prefers left-to-right generation then how valuable is it to device  more reasonable but incompatible decoding procedures?\n\nAlso, authors also note that index picking schemes investigated in the paper are heuristic based and a more interesting decoder could be learned if index selection procedure itself was learned with features depending on the previous index selection states, decoded states Y, and other relevant quantities. They attribute poor performance of the proposed decoder to the nature of index selection approaches investigated in the paper. I think the paper would be strengthened with results with a more sophisticated learned index selection procedure in addition to the heuristics investigated in this paper.\n\nOverall, while the idea and motivation behind this work is exciting, the inconclusive results and the approaches for practical implementation leave open significant room for improvement. "}, "signatures": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper734/Reviewers"], "noninvitees": [], "tcdate": 1570237747866, "tmdate": 1574723087736, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Review"}}}, {"id": "ryeBboGP9B", "original": null, "number": 3, "cdate": 1572444924694, "ddate": null, "tcdate": 1572444924694, "tmdate": 1572972558926, "tddate": null, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "invitation": "ICLR.cc/2020/Conference/Paper734/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a generalized framework for sequence generation that can be applied to both directed and undirected sequence models. The framework generates the final label sequence through generating a sequence of steps, where each step generates a coordinate sequence and an intermediate label sequence. This procedure is probabilistically modeled by length prediction, coordinate selection and symbol replacement. For inference, instead of the intractable naive approach based on Gibbs sampling to marginalize out all generation paths, the paper proposes a heuristic approach using length-conditioned beam search to generate the most likely final sequence. With the proposed framework, the paper shows that masked language models like BERT, even though they are undirected sequence models, can be used for sequence generation, which obtains close performance to the traditional left-to-right autoregressive models on the task of machine translation.\n\nOverall the paper has significant contributions in the following aspects:\n1. It enables undirected sequence models, like BERT, to perform decoding or sequence generation directly, instead of just serving as model pre-training.\n2. The proposed framework unifies directed and undirected sequence models decoding, and it can represent a few existing sequence model decoding as special cases.\n3. The coordinate sequence selection function in the framework can be dependent on the intermediate label sequence. A few simple selection approaches proposed in the paper are shown to be effective. It could be further extended. \n4. The analysis of the coordinate selection order is interesting and helpful for understanding the algorithm.\n5. The experiment results for decoding masked language models on machine translation are promising. It also provides the comparison to recent related work on non-autoregressive approaches.\n\nThe presentation of the paper is also clear. I am leaning towards accepting the paper.\n\nHowever, there are some weaknesses:\n\n1. It should be analyzed more why different coordinate selection approaches perform differently in linear-time decoding vs. constant-time decoding. Even in constant-time decoding, the conclusion varies in different decoding setting, easy-first is the worst for the L->1 case, but the best for the L/T case, why is that?\n\n2. What is the motivation for \"hard-first\"?\n\n3. The setting of \"least2most\" with L->1 is similar to Ghazvininejad et al. 2019. But Table 4 in the appendix shows the result in this paper is still worse (21.98 vs. 24.61, when both systems use 10 iterations without AR). Also, the gap from the AR baseline is larger than that in Ghazvininejad et al. 2019. Given the two systems are considered similar, it should be explained in the paper the possible reasons for these discrepancies in results.\n\nAdditional minor comments for improving the paper:\n\n1. In the introduction, it mentions the baseline AR is (Vaswani et al. 2017), while in the experimental settings, it mentions the baseline AR is (Bahdanau et al. 2015). Please clarify which one is used.\n\n2. In Table 1, how does T = 2L work for the \"Uniform\" case while the target sequence length is only T, since it is mentioned the positions are sampled without replacement. Similarly, how does T = 2L work for the \"Left2Right\" case? Is it just always choosing the last position when L < t <= 2L? In these two cases, it seems T > L is not needed.\n\n3. In Table 3, the header for the 2nd column should be o_t, as defined in Section 4 - \"Decoding scenarios\". What is the actual value of K and K'' for the constant-time machine translation experiments in the paper?\n\n4. \"Rescoring adds minimal overhead as it is run in parallel\" - it still needs to run left-to-right in sequence since it is auto-regressive. Please clarify what it means by \"in parallel\" here.\n\n5. What is the range and average for the target sentence length? How is T = 20 for constant-time decoding compared to linear-time decoding in terms of speed?"}, "signatures": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper734/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models", "authors": ["Elman Mansimov", "Alex Wang", "Kyunghyun Cho"], "authorids": ["elman.mansimov@gmail.com", "wangalexc@gmail.com", "kyunghyun.cho@nyu.edu"], "keywords": ["nlp", "sequence modeling", "natural language generation", "machine translation", "BERT", "Sesame Street"], "TL;DR": "We unify several language generation paradigms (monotonic autoregressive, non-autoregressive, etc.) in a single framework, and use the framework to do machine translation with undirected sequence models.", "abstract": "Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. \nThe problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT'14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "pdf": "/pdf/64b304375caa3247dafbd0c1221508d50a42f996.pdf", "paperhash": "mansimov|a_generalized_framework_of_sequence_generation_with_application_to_undirected_sequence_models", "original_pdf": "/attachment/293c360b1a912bc973994cf83677769e661b3fdb.pdf", "_bibtex": "@misc{\nmansimov2020a,\ntitle={A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models},\nauthor={Elman Mansimov and Alex Wang and Kyunghyun Cho},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlbo6VtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlbo6VtDH", "replyto": "BJlbo6VtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper734/Reviewers"], "noninvitees": [], "tcdate": 1570237747866, "tmdate": 1574723087736, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper734/-/Official_Review"}}}], "count": 10}