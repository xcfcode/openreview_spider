{"notes": [{"id": "SJgCEpVtvr", "original": "SkeiXdVwwB", "number": 505, "cdate": 1569439029801, "ddate": null, "tcdate": 1569439029801, "tmdate": 1577168241605, "tddate": null, "forum": "SJgCEpVtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XqVtarN0Wn", "original": null, "number": 1, "cdate": 1576798698366, "ddate": null, "tcdate": 1576798698366, "tmdate": 1576800937434, "tddate": null, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "invitation": "ICLR.cc/2020/Conference/Paper505/-/Decision", "content": {"decision": "Reject", "comment": "The paper is develops a self-training framework for graph convolutional networks where we have partially labeled graphs with a limited amount of labeled nodes. The reviewers found the paper interesting. One reviewer notes the ability to better exploit available information and raised questions of computational costs. Another reviewer felt the difference from previous work was limited, but that the good results speak for themselves. The final reviewer raised concerns on novelty and limited improvement in results. The authors provided detailed responses to these queries, providing additional results.\n\nThe paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706857, "tmdate": 1576800255001, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper505/-/Decision"}}}, {"id": "rJe6somhiH", "original": null, "number": 1, "cdate": 1573825445029, "ddate": null, "tcdate": 1573825445029, "tmdate": 1573827449195, "tddate": null, "forum": "SJgCEpVtvr", "replyto": "H1eZGztV9B", "invitation": "ICLR.cc/2020/Conference/Paper505/-/Official_Comment", "content": {"title": "Response to AnonReviewer #2", "comment": "We thank the reviewer for the constructive comments.\n\n1. \"As the self-training is going on, are there different computational costs or are they about the same?\"\nThe computational costs is only slightly increased. The reason is that the computational cost of the original GCN model is dominated by previous layers, where the entire graph is included. So even if all nodes become pseudo labels, the size of the entire network is increased by at most a factor of 2, and the number of parameters remains the same. Therefore, the computational costs will increase by at most a small constant in theory. We have also verified this empirically. We record the training time of GCN and GAT before and after applying our framework. In the experiments, the training size is 20 per class, the number of epochs is 200, and the time is the average time (in seconds) of 25 runs.\n---------------------------------------------------------------------------------------------------\nCora       \n           GCN        2.6\n         DSGCN     6.3\n           \n           GAT        4.7\n         DSGAT     8.7\n---------------------------------------------------------------------------------------------------\n\n---------------------------------------------------------------------------------------------------\n    Citeseer\n          GCN         3.0\n         DSGCN     9.7\n\n         GAT           5.2\n        DSGAT      11.9\n---------------------------------------------------------------------------------------------------\n\n2. \"For CiteSeer 20 and 50, why does \\beta = 0.45 switch from the other experiments?\"\nWe didn\u2019t use \\beta = 0.45 for CiteSeer 20 and 50. As explicitly explained in the paper, we use a threshold of 0.6 when the number of labels per class is below 3 and set the threshold to 0.75 for label rate above 3 but below 10. Otherwise, the threshold is 0.9 by default. In Figure 1, \\beta=0.45 here is used for comparison with other thresholds on various label rate. We are sorry for any ambiguity in the paper. \n\n\n3. \"Will such self-training be useful for general NN self-training procedures\"\nWe believe our self-training method will be useful for general NN training, although we think it is most effective for GNN models. We have tested our self-training methods on other GNNs, e.g., GraphSage, GAT, SGC, and some preliminary results are as follows. \n---------------------------------------------------------------------------------------------------\n                                                  Cora\n---------------------------------------------------------------------------------------------------\n                              5            10              20             50         \n---------------------------------------------------------------------------------------------------\nGraphSage       69.3         75.3          79.2          82.5\nDSGraphSage  72.5         78.4          81.0          84.0\n---------------------------------------------------------------------------------------------------\nGAT                    71.1         76.0          79.6          83.4\nDSGAT               71.9         77.1          81.0          83.6\n---------------------------------------------------------------------------------------------------\nSGC                    63.5         72.5         75.9          78.9\nDSSGC               65.0         73.4         76.2          78.9    \n---------------------------------------------------------------------------------------------------\n\n---------------------------------------------------------------------------------------------------\n                                             Citeseer\n---------------------------------------------------------------------------------------------------\n                               5             10           20             50         \n---------------------------------------------------------------------------------------------------\nGraphSage        59.7        65.4         68.8          72.1\nDSGraphSage   60.6        66.3         69.5          72.6\n---------------------------------------------------------------------------------------------------\nGAT                     54.9        60.8         68.2          71.5\nDSGAT                58.3        67.0         70.8          73.5\n---------------------------------------------------------------------------------------------------\nSGC                     55.5        63.7         69.0           72.6\nDSSGC                59.6        65.0         69.7           73.4        \n---------------------------------------------------------------------------------------------------\n\n\n4. \"If we had soft-labelling or uncertainty on which label each node has, how would the dynamic self-training be changed?\"\nIn general, we can treat all original labels as \u201cpseudo labels\u201d as well. We just need a mechanism to determine the initial confidence of these labels and then all labels in the training set can be treated equally.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgCEpVtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference/Paper505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper505/Reviewers", "ICLR.cc/2020/Conference/Paper505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper505/Authors|ICLR.cc/2020/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170430, "tmdate": 1576860550555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference/Paper505/Reviewers", "ICLR.cc/2020/Conference/Paper505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper505/-/Official_Comment"}}}, {"id": "SyxZv7N2oB", "original": null, "number": 3, "cdate": 1573827416547, "ddate": null, "tcdate": 1573827416547, "tmdate": 1573827416547, "tddate": null, "forum": "SJgCEpVtvr", "replyto": "r1lIXkR6YH", "invitation": "ICLR.cc/2020/Conference/Paper505/-/Official_Comment", "content": {"title": "Response to AnonReviewer #1", "comment": "We thank the reviewer for the constructive comments.\n\nWe have revised our paper. As suggested by the reviewers, additional experiments are conducted. We have tested the computation overhead of applying our dynamic self-training framework. The results show that the training time increases only slightly. Moreover, we test the effect of our framework being applied on other GNN models, including GraphSage, GAT, and SGC. We observe notable improvements on all these models. Please see our responses to other reviewers for more details on the experimental results. "}, "signatures": ["ICLR.cc/2020/Conference/Paper505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgCEpVtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference/Paper505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper505/Reviewers", "ICLR.cc/2020/Conference/Paper505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper505/Authors|ICLR.cc/2020/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170430, "tmdate": 1576860550555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference/Paper505/Reviewers", "ICLR.cc/2020/Conference/Paper505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper505/-/Official_Comment"}}}, {"id": "rke6baXnsH", "original": null, "number": 2, "cdate": 1573825797069, "ddate": null, "tcdate": 1573825797069, "tmdate": 1573825883893, "tddate": null, "forum": "SJgCEpVtvr", "replyto": "ByxRt-tpFr", "invitation": "ICLR.cc/2020/Conference/Paper505/-/Official_Comment", "content": {"title": "Response to AnonReviewer #3", "comment": "We thank the reviewer for the constructive comments.\n\n1. \"Only GCN instantiation are provided, it is suggested to evaluate the effectiveness on the other GNN variants, such as GraphSage, GAT and MoNet.\"\n\nThanks for the suggestion! We have implemented and tested our self-training methods on other GNNs as well, e.g., GraphSage, GAT, SGC, and some preliminary results are as follows. \n---------------------------------------------------------------------------------------------------\n                                                  Cora\n---------------------------------------------------------------------------------------------------\n                              5            10              20             50         \n---------------------------------------------------------------------------------------------------\nGraphSage       69.3         75.3          79.2          82.5\nDSGraphSage  72.5         78.4          81.0          84.0\n---------------------------------------------------------------------------------------------------\nGAT                    71.1         76.0          79.6          83.4\nDSGAT               71.9         77.1          81.0          83.6\n---------------------------------------------------------------------------------------------------\nSGC                    63.5         72.5         75.9          78.9\nDSSGC               65.0         73.4         76.2          78.9    \n---------------------------------------------------------------------------------------------------\n\n---------------------------------------------------------------------------------------------------\n                                             Citeseer\n---------------------------------------------------------------------------------------------------\n                               5             10           20             50         \n---------------------------------------------------------------------------------------------------\nGraphSage        59.7        65.4         68.8          72.1\nDSGraphSage   60.6        66.3         69.5          72.6\n---------------------------------------------------------------------------------------------------\nGAT                     54.9        60.8         68.2          71.5\nDSGAT                58.3        67.0         70.8          73.5\n---------------------------------------------------------------------------------------------------\nSGC                     55.5        63.7         69.0           72.6\nDSSGC                59.6        65.0         69.7           73.4        \n---------------------------------------------------------------------------------------------------\nClearly, our dynamic self-training framework achieves similar improvements on all the three base models. For these models, we observe notable improvements even when the label rate is high. We emphasize that our framework is very flexible to use: to apply to a different a base model, the implementation and tuning is almost effortless. All the additional experimental results will be added in the final version.\n\n2. \"The proposed framework makes modification on the existing work, which is a good extension, but the novelty is limited.\"\n\nWe don\u2019t think our framework is a just modification of [1]. \n\nWe propose three techniques to address additional complexity introduced when applying a standard self-training framework. Our techniques are very effective in this aspect: only roughly 15 lines of code are needed to wrap any base GNN model, only one additional hyperparameter, and significant accuracy boosting even compare with standard self-training. \n\nMoreover, in our framework, we update the training set after each epoch instead of after a full training of the base model, and we also propose a simple gradient computation strategy, so that the training process is end-to-end and there are no explicit training stages as in standard multistage self-training. This makes the model easier to implement, train, and extend. \n\nOur framework's apparent simplicity might make it seem like a simple modification, but we don\u2019t think simple means limited novelty. Simplicity, flexibility, and better performance of our framework makes it more suitable to use in practice.\n\n3. \"The gap of the experiment results between the proposed method and the baseline methods are quite small.\"\nThis is not true. 1. The improvement achieved by dynamic self-train over the base model GCN could be more than 15%. 2. Compared with the four self-training methods in [1], the gap could be also as large as 14%. \n\n[1] Li et al. Deeper insights into graph convolutional networks for semi-supervised learning.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper505/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgCEpVtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference/Paper505/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper505/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper505/Reviewers", "ICLR.cc/2020/Conference/Paper505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper505/Authors|ICLR.cc/2020/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170430, "tmdate": 1576860550555, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper505/Authors", "ICLR.cc/2020/Conference/Paper505/Reviewers", "ICLR.cc/2020/Conference/Paper505/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper505/-/Official_Comment"}}}, {"id": "ByxRt-tpFr", "original": null, "number": 1, "cdate": 1571815813942, "ddate": null, "tcdate": 1571815813942, "tmdate": 1572972587069, "tddate": null, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "invitation": "ICLR.cc/2020/Conference/Paper505/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper propose to modify the existing work [1] of self-training framework for graph convolutional networks. It tracks three limitations of [1] and propose three\u00a0 use a threshold-based rule to insert new pseudo-labels and dynamic change the pseudo-label set. Moreover personalized weight are assigned to each activate\u00a0pseudo-label proportional to its current classification margin.\u00a0Evaluation of the proposed framework is performed on four networks for semi-supervised node classification task with varying label rates.\nPros:\n1. This work tracks and addresses the limitations of existing work.\n2. Authors conduct experiments on multiple dataset with varying 2-hop coverage ratio.\u00a0\n3.\u00a0The overall paper is well written, except some typos, e.g. in page 6, section 5.1 \"Each of three dataset is ......\". Should \"three\" be \"four\".\u00a0\u00a0\nCons:\n1. The proposed framework makes modification on the existing work, which is a good extension but the novelty is limited.\n2. The gap of the experiment results between the proposed method and the baseline methods are quite small.\n3. Only GCN\u00a0instantiation\u00a0are provided, it is suggested to evaluate the effectiveness on the other GNN variants, such as GraphSage, GAT and MoNet.\n[1]\u00a0Li et al. Deeper insights into graph convolutional networks for semi-supervised learning."}, "signatures": ["ICLR.cc/2020/Conference/Paper505/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper505/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576709643842, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper505/Reviewers"], "noninvitees": [], "tcdate": 1570237751163, "tmdate": 1576709643856, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper505/-/Official_Review"}}}, {"id": "r1lIXkR6YH", "original": null, "number": 2, "cdate": 1571835677759, "ddate": null, "tcdate": 1571835677759, "tmdate": 1572972587026, "tddate": null, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "invitation": "ICLR.cc/2020/Conference/Paper505/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes an approach for learning graph convolutional networks for inferring labels on the nodes of a partially labeled graph  when only limited amount of labeled nodes are available.\n\nThe proposal is inspired from Graph convolution Networks with the idea of overcoming the major drawback of these models that lies of their behavior in case of limited coverage of the labeled nodes, which implies using deeper versions of the model leading at the price of what the authors call the over-smoothing problem.  \n\nThe main idea here consists in relying on self training to get a better coverage of labeled nodes enabling learning with less deep models, this translates to a simple and intuitive algorithm. Using self training is not new in GCN but the way it is used here, computing adaptively a threshold for incorporating pseudo labels and using weights according to the confidence off predictions is new.\n\nExperimental results are reported on citation datasets and compared with many baselines show similar results as baselines when the coverage increases up to 50 labeled nodes /class, but the method brings significant improvements when the coverage is low (e.g. only few, <20, labels /class). \n\nAlthough the difference with previous approaches do not look like a huge step, the method seems to be quite justified empirically and achieve real good results wrt state of the art."}, "signatures": ["ICLR.cc/2020/Conference/Paper505/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper505/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576709643842, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper505/Reviewers"], "noninvitees": [], "tcdate": 1570237751163, "tmdate": 1576709643856, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper505/-/Official_Review"}}}, {"id": "H1eZGztV9B", "original": null, "number": 3, "cdate": 1572274696685, "ddate": null, "tcdate": 1572274696685, "tmdate": 1572972586983, "tddate": null, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "invitation": "ICLR.cc/2020/Conference/Paper505/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "#Summary\n\nThis paper proposes a generalised self-training framework to build a Graph Neural Network to label graphs.  Of importance is the dynamic nature of the self-training. The authors do not change the GCN but extend the self-training portion as per the prior GCN paper by introducing Dynamic Self-Training that keeps a confidence score of labels predicted for unlabelled nodes.\n\n# Comments\n\nThis is a very interesting paper in terms of looking at the effects of changing the self-training framework to better utilise the underlying structure. As such we can exploit information from other nodes that are yet to be labelled.\n\n1. As the self-training is going on, are there different computational costs or are they about the same?\n2. For CiteSeer 20 and 50, why does \\beta = 0.45 switch from the other experiments?\n3. Will such self-training be useful for general NN self-training procedures\n4. If we had soft-labelling or uncertainty on which label each node has, how would the dynamic self-training be changed?\n\n#Other notes\nPlease remove the \n\nAn appendix\nYou may include other additional sections here"}, "signatures": ["ICLR.cc/2020/Conference/Paper505/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper505/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DYNAMIC SELF-TRAINING FRAMEWORK  FOR GRAPH CONVOLUTIONAL NETWORKS", "authors": ["Ziang Zhou", "Shenzhong Zhang", "Zengfeng Huang"], "authorids": ["15300180085@fudan.edu.cn", "17210980007@fudan.edu.cn", "huangzf@fudan.edu.cn"], "keywords": ["self-training", "semi-supervised learning", "graph convolutional networks"], "TL;DR": "Propose a novel self-training framework which performs well in few-label cases combined with GCN.", "abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.", "pdf": "/pdf/15d64d7eae0a0667de0c3bfa5c4821f40f2cf63b.pdf", "code": "https://anonymous.4open.science/r/f7efb5cb-adfc-4f47-908a-edc4025c18d8/", "paperhash": "zhou|dynamic_selftraining_framework_for_graph_convolutional_networks", "original_pdf": "/attachment/8bd70931d12b04751ab7f1d5b6fed5203a4b1fea.pdf", "_bibtex": "@misc{\nzhou2020dynamic,\ntitle={{\\{}DYNAMIC{\\}} {\\{}SELF{\\}}-{\\{}TRAINING{\\}} {\\{}FRAMEWORK{\\}}  {\\{}FOR{\\}} {\\{}GRAPH{\\}} {\\{}CONVOLUTIONAL{\\}} {\\{}NETWORKS{\\}}},\nauthor={Ziang Zhou and Shenzhong Zhang and Zengfeng Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgCEpVtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgCEpVtvr", "replyto": "SJgCEpVtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576709643842, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper505/Reviewers"], "noninvitees": [], "tcdate": 1570237751163, "tmdate": 1576709643856, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper505/-/Official_Review"}}}], "count": 8}