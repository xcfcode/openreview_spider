{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396380406, "tcdate": 1486396380406, "number": 1, "id": "B1Ny3zU_g", "invitation": "ICLR.cc/2017/conference/-/paper149/acceptance", "forum": "rJsiFTYex", "replyto": "rJsiFTYex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper attempts to perform an interesting exploration (how to combine different tricks for LSTM training) but does not take it far enough. \n \n Pros:\n - interesting attempt at studying different techniques to improve LSTM training results\n Cons:\n - not very strong baselines\n - limited set of domains were explored\n - low in novelty (which wouldn't be a problem if the comparison was more thorough -- see above 2 points)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396380948, "id": "ICLR.cc/2017/conference/-/paper149/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJsiFTYex", "replyto": "rJsiFTYex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396380948}}}, {"tddate": null, "tmdate": 1482162589838, "tcdate": 1482162589838, "number": 3, "id": "rJLoZtr4g", "invitation": "ICLR.cc/2017/conference/-/paper149/official/review", "forum": "rJsiFTYex", "replyto": "rJsiFTYex", "signatures": ["ICLR.cc/2017/conference/paper149/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper149/AnonReviewer3"], "content": {"title": "official review", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes and analyses three methods applied to traditional LSTMs: Monte Carlo test-time model averaging, average pooling, and residual connections. It shows that those methods help to enhance traditional LSTMs on sentiment analysis. \n\nAlthough the paper is well written, the experiment section is definitely its dead point. Firstly, although it shows some improvements over traditional LSTMs, those results are not on par with the state of the art. Secondly, if the purpose is to take those extensions as strong baselines for further research, the experiments are not adequate: the both two datasets which were used are quite similar (though they have different statistics). I thus suggest to carry out more experiments on more diverse tasks, like those in \"LSTM: A Search Space Odyssey\"). \n\nBesides, those extensions are not really novel.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512683162, "id": "ICLR.cc/2017/conference/-/paper149/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper149/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper149/AnonReviewer2", "ICLR.cc/2017/conference/paper149/AnonReviewer1", "ICLR.cc/2017/conference/paper149/AnonReviewer3"], "reply": {"forum": "rJsiFTYex", "replyto": "rJsiFTYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512683162}}}, {"tddate": null, "tmdate": 1481965332742, "tcdate": 1481965332742, "number": 2, "id": "r1pM1KGVe", "invitation": "ICLR.cc/2017/conference/-/paper149/official/review", "forum": "rJsiFTYex", "replyto": "rJsiFTYex", "signatures": ["ICLR.cc/2017/conference/paper149/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper149/AnonReviewer1"], "content": {"title": "review", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents three improvements to the standard LSTM architecture used in many neural NLP models: Monte Carlo averaging, embed average pooling, and residual connections. Each of the modifications is trivial to implement, so the paper is definitely of interest to any NLP researchers experimenting with deep learning. \n\nWith that said, I am concerned about the experiments and their results. The residual connections do not seem to consistently help performance; on SST the vertical residuals help but the lateral residuals hurt, and on IMDB it is the opposite. More fundamentally, there need to be more tasks than just sentiment analysis here. I'm not quite sure why the paper's focus is on text classification, as any NLP task using an LSTM encoder could conceivably benefit from these modifications. It would be great to see a huge variety of tasks like QA, MT, etc., which would really make the paper much stronger. \n\nAt this point, while the experiments that are included in the paper are very thorough and the analysis is interesting, there need to be more tasks to convince me that the modifications generalize, so I don't think the paper is ready for publication.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512683162, "id": "ICLR.cc/2017/conference/-/paper149/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper149/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper149/AnonReviewer2", "ICLR.cc/2017/conference/paper149/AnonReviewer1", "ICLR.cc/2017/conference/paper149/AnonReviewer3"], "reply": {"forum": "rJsiFTYex", "replyto": "rJsiFTYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512683162}}}, {"tddate": null, "tmdate": 1481939873432, "tcdate": 1481939873432, "number": 1, "id": "S1FjsGfNe", "invitation": "ICLR.cc/2017/conference/-/paper149/official/review", "forum": "rJsiFTYex", "replyto": "rJsiFTYex", "signatures": ["ICLR.cc/2017/conference/paper149/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper149/AnonReviewer2"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks. \n\nThe paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.\n\nI like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.  ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512683162, "id": "ICLR.cc/2017/conference/-/paper149/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper149/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper149/AnonReviewer2", "ICLR.cc/2017/conference/paper149/AnonReviewer1", "ICLR.cc/2017/conference/paper149/AnonReviewer3"], "reply": {"forum": "rJsiFTYex", "replyto": "rJsiFTYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512683162}}}, {"tddate": null, "tmdate": 1481918803684, "tcdate": 1481918803684, "number": 2, "id": "HJhLKpWEx", "invitation": "ICLR.cc/2017/conference/-/paper149/public/comment", "forum": "rJsiFTYex", "replyto": "ry_bEmsXx", "signatures": ["~Shayne_Longpre1"], "readers": ["everyone"], "writers": ["~Shayne_Longpre1"], "content": {"title": "Choice of Datasets/Tasks", "comment": "This is a great point, and one we plan to explore in the near future, probably starting with language modeling. Our focus in this paper was simply to establish a number of techniques which showed statistically relevant and compounding improvements on a simple, yet highly competitive (read: benchmarked) task. These \u201cenhancements\u201d are also intended for \u2018baseline\u2019, deep models, or to be used in conjunction (since we show their ability to effectively compound improvements) with whatever other techniques one is experimenting with for classification. We did however intentionally choose two quite distinct sentiment classification datasets, in their makeup, to verify that our best techniques were effective across quite different forms of text data. SST and IMDB\u2019s examples differ quite markedly from each other on their length, the detail of the descriptions, grammar and degree to which they\u2019ve been sanitized. Of the SST training examples (including the sub-phrases) their mean and standard deviation for length are 19.14 and 9.31 respectively, whereas for IMDB they are 279.93 and 207.87 respectively. SST is also fine-grain while IMDB is coarse-grain. Lastly, SST examples are usually single sentences, cleanly written and grammatically correct, whereas IMDB examples are littered with \u201c<br />\u201d, email addresses, mid-sentence punctuation (usually for emphasis) and other such colloquial features. \n\nWe absolutely intend to further our research with these techniques into other tasks, as next steps, though felt that the datasets provided were ample to show their efficacy for text classification as a whole. This more narrow focus also gave us the capacity to delve deeper into all the alternative implementations of these techniques (a great deal of failed experimentation not shown) and determine what actually are the most effective off-the-shelf enhancements for baseline deep classification models."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287711145, "id": "ICLR.cc/2017/conference/-/paper149/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJsiFTYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper149/reviewers", "ICLR.cc/2017/conference/paper149/areachairs"], "cdate": 1485287711145}}}, {"tddate": null, "tmdate": 1481916193060, "tcdate": 1481916193060, "number": 1, "id": "HkFXypZ4l", "invitation": "ICLR.cc/2017/conference/-/paper149/public/comment", "forum": "rJsiFTYex", "replyto": "ryQKQ7iml", "signatures": ["~Sabeek_Mani_Pradhan1"], "readers": ["everyone"], "writers": ["~Sabeek_Mani_Pradhan1"], "content": {"title": "Clarification and modification re terms used in the paper", "comment": "Point taken. Sorry if some of the terms we used were confusing or ambiguous. We acknowledge that deep vector averaging and Monte Carlo model averaging aren't modifications to the LSTM cell itself. Originally, when we talked about \"architectural modifications\", we didn't intend it to mean architectural modifications just to the LSTM cell itself but rather to the entire model (\"LSTM network\"), which includes the LSTM cell, the procedure for stacking layers, and the inference method. We've changed the wording to clear up this ambiguity. \n\nAnd you're right that in principle you could use an MLP of any size for our deep vector averaging; after some experimentation, we ultimately used an MLP with a single layer and a hidden dimension of size 300. We initially chose to use the term \"deep\" to indicate that we were using not the vector average itself but rather the output of a network using that average as input. However, we have renamed it to \"embed average pooling,\" which we hope is better at conveying what the method does."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287711145, "id": "ICLR.cc/2017/conference/-/paper149/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJsiFTYex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper149/reviewers", "ICLR.cc/2017/conference/paper149/areachairs"], "cdate": 1485287711145}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481916044568, "tcdate": 1478248867223, "number": 149, "id": "rJsiFTYex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJsiFTYex", "signatures": ["~Sabeek_Mani_Pradhan1"], "readers": ["everyone"], "content": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481483264217, "tcdate": 1481483264211, "number": 1, "id": "ry_bEmsXx", "invitation": "ICLR.cc/2017/conference/-/paper149/official/comment", "forum": "rJsiFTYex", "replyto": "rJsiFTYex", "signatures": ["ICLR.cc/2017/conference/paper149/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper149/AnonReviewer3"], "content": {"title": "Datasets", "comment": "I find the experiments not convincing because the two datasets are quite similar (sentiment analysis). I was wondering if you have tried your proposed models/methods on much more different tasks (e.g. machine translation, question answering, etc.)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287711007, "id": "ICLR.cc/2017/conference/-/paper149/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJsiFTYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper149/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper149/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper149/reviewers", "ICLR.cc/2017/conference/paper149/areachairs"], "cdate": 1485287711007}}}, {"tddate": null, "tmdate": 1481483130702, "tcdate": 1481483130696, "number": 1, "id": "ryQKQ7iml", "invitation": "ICLR.cc/2017/conference/-/paper149/pre-review/question", "forum": "rJsiFTYex", "replyto": "rJsiFTYex", "signatures": ["ICLR.cc/2017/conference/paper149/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper149/AnonReviewer3"], "content": {"title": "terms used in the paper", "question": "I find some \"key\" terms not proper. It would be great if you can explain more. \n\n1. You said that you proposed a series of architectural modifications for LSTM networks. But in fact many of them are not \"architectural modifications\". In the Monte Carlo model averaging, you didn't change the model at all. What you did is to use a different method for inference. In Deep vector averaging, what you did is to simply combine an LSTM network with a MLP at the (pre)-output layer, you thus didn't modify the LSTM architecture. \n\n2. \"Deep vector averaging\": I was wondering if you are abusing the term \"deep\". Basically any MLPs can be used in spite of their depth. Can you also report the size of the MLPs you used? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "pdf": "/pdf/4cb2b5080ff414bc0ab0b750be4943baaad466ee.pdf", "TL;DR": "Relatively simple augmentations to the LSTM, such as Monte Carlo test time averaging, deep vector averaging, and residual connections, can yield massive accuracy improvements on text classification datasets.", "paperhash": "longpre|a_way_out_of_the_odyssey_analyzing_and_combining_recent_insights_for_lstms", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"], "conflicts": ["salesforce.com", "zoox.com", "stanford.edu", "cs.stanford.edu"], "authors": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "authorids": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481483131310, "id": "ICLR.cc/2017/conference/-/paper149/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper149/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper149/AnonReviewer3"], "reply": {"forum": "rJsiFTYex", "replyto": "rJsiFTYex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481483131310}}}], "count": 9}