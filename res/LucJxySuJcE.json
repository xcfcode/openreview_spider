{"notes": [{"id": "LucJxySuJcE", "original": "h9md5uETeT", "number": 215, "cdate": 1601308032544, "ddate": null, "tcdate": 1601308032544, "tmdate": 1615075800599, "tddate": null, "forum": "LucJxySuJcE", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "16M6aR1OiRD", "original": null, "number": 1, "cdate": 1610040488330, "ddate": null, "tcdate": 1610040488330, "tmdate": 1610474093990, "tddate": null, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposed an ensemble of diverse models as a mechanism to protect models from theft. \nThe idea is quite novel. There are some concerns regarding the robustness of the hashing function (that I share), however not every paper has to be perfect, especially when it introduces a novel setup. \n\nAC"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040488317, "tmdate": 1610474093972, "id": "ICLR.cc/2021/Conference/Paper215/-/Decision"}}}, {"id": "U4DPhdE0Gor", "original": null, "number": 1, "cdate": 1603818123422, "ddate": null, "tcdate": 1603818123422, "tmdate": 1606817044694, "tddate": null, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Review", "content": {"title": "Blind Review", "review": "## Summary\n- The paper proposes a defense against recent flavours of model stealing attacks by exploiting the insight that the recent effective attack query out of distribution examples to the victim model.\n- The approach introduces discontinuities in the input-prediction space by (i) training an ensemble of models such that for a given OOD input, the predictions are randomized (ii) at inference time hashing an input to a particular input in the diverse ensemble.\n- Evaluation on simple datasets (MNIST, Fashion, CIFAR) show that the approach is reasonably effective at defending and moreover without a significant degradation of the utility.\n\n## Strengths\n\n**1. Well-motivated problem**\n- The defenses for model stealing are not as well-investigated as attacks. I appreciate the authors take a step towards addressing defenses; the initial results look promising.\n\n**2. Insight**\n- I like the insight used by the authors in the defense i.e., to generate discontinuities in the input-prediction space. This is well illustrated in Fig. 1.\n\n**3. Writing**\n- The paper is written well and is easy to follow.\n\n## Concerns\n\n### Major Concerns\n\n**1. $D_{in}$ vs. $D_{out}$**\n- It is a nice insight that adversarial users indeed rely on querying data from a different distribution. However, my first concern is that is also naturally true for benign users. After all, no one except the victim/defender has access to the training data distribution and all queried data (whether by adversary or benign users) is out of distribution.\n- As a result, I wonder how applicable the defense is -- would the defense intentionally mispredict in non-IID query setups? For instance, when queried with a particular dog breed unseen during training? Or querying slightly translated digit images to standard MNIST classifier?\n- More generally, the proposed approach requires drawing a strict line between in and out distribution which I find is not clearly analyzed (follow-up remarks in point 4).\n\n**2. Diversity Objective**\n- If I understand the training objective correctly (Fig. 2b, Eq. 3), each model $f_i$ in the ensemble is encouraged to generate a random prediction when queried with out-of-distribution (OOD) inputs. Moreover, in expectation, results in a uniform distribution over predictions on OOD data. For in-distribution data, the predictions ideally remain unchanged.\n- As a result, my concern is that this appears to be simple OOD detection followed by misprediction/calibration in disguise. Consequently, I wonder if one can simply leverage existing advances in OOD detection to achieve the objective.\n- Moreover the OOD implicitly performed here requires access to a corresponding dataset $D_{out}$ to model unknown OOD samples. This contrasts some existing OOD detection works (e.g., ODIN, Liang et al., ICLR '18) which shows success without explicitly modelling OOD data.\n\n**3. Hashing**\n- I am also concerned that the hashing strategy (Sec. 4.2) employed by the authors appears to be a weak link in the defense.\n- Specifically, it appears that a robust defense requires the hashing function to consistently select the same model in the ensemble in spite of small changes in the input. However, going by Fig. 6 it does not necessarily seem to be the case.\n- As a result, I wonder if an attacker can exploit the hashing function to recover whether the output is clean/poisoned -- such as by aggregating predictions over a set of transformed inputs.\n\n**4. Evaluation**\n- While the results look promising, I have two concerns here related to the evaluation.\n- First, I would have preferred if the results in Table 3 were presented as curves (with defense vs. attacker accuracy). This would have provided some insights on how the defense performs w.r.t the important $\\lambda$ hyperparameter in their objective. Additionally, it would also make the comparison with AM baseline fair, especially given that AM was evaluated on a curve and it is unclear which specific instance was used for comparison.\n- Second, which also connecting to point 1, is that the paper's experimental setting assumes a significant discrepancy between the distributions e.g., Victim's $D_{in}$ = MNIST, $D_{out}$ = KMNIST, Attacker's data = FashionMNIST. Here, the data and semantic classes are completely disjoint. I wonder how the defense performs with other choices of attacker's data e.g., EMNIST which is a bit more similar to MNIST.\n\n\n### Minor Concerns\n\n**5. Experimental settings**\n- I find missing some important experimental parameters that is not mentioned in the paper: how many images were queried by the attacker for results in Table 1? What was the value of $\\lambda$ used?\n\n**6. Subverting schemes**\n- I was also disappointed that the authors do not demonstrate that the defense is robust to some simple schemes used by an attacker to bypass the proposed defense. For instance, connecting to point (3), by aggregating predictions over a set of transformed inputs.\n\n\n### Nitpicks\n\n**7. Some nitpicks**\n- $\\lambda$ is overloaded: in Eq. 3 and also for the JBDA attacks\n- \"AM trades off benign accuracy for improved security\" -- would the proposed approach also trade this off by increasing the value of $\\lambda$?\n\n### Post-rebuttal updates\n- Thanks for the detailed response and additional evaluation.\n- Q1. Fair point -- it appears that defenses do assume attackers query from a reasonably different distribution. This defense does make this assumption explicit by including it in the training objective ($D_{out}$ in Eq. 3). But then again, it seems typical for OOD-based defenses (e.g., AM).\n- Q3. This concern also generally connects to Q6. It would be nonetheless interesting to analyze scenarios where the attacker attempts to use some auxiliary knowledge to break the defense. This is also shared by some other reviewers.\n- Q4. Thanks for presenting the curves. Assuming strictly non-overlapping OOD data does seem like a strong assumption though.\n- Nitpick editorial comment: Please serif-based fonts for text in equations e.g. $argmax(\\cdot) \\rightarrow \\text{argmax}(\\cdot), index \\rightarrow \\text{index}$\n- I am slightly increasing my rating.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper215/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147958, "tmdate": 1606915797034, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper215/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Review"}}}, {"id": "AO6nTNFioQg", "original": null, "number": 5, "cdate": 1605668809307, "ddate": null, "tcdate": 1605668809307, "tmdate": 1605884377538, "tddate": null, "forum": "LucJxySuJcE", "replyto": "U4DPhdE0Gor", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewers for their valuable and positive feedback. We are happy that the reviewers found our problem to be \u201cwell-motivated\u201d and our idea to be \u201cnovel\u201d. We are also glad that the reviewers found our paper to be \u201cwell-written\u201d and our experimentation to be \u201cextensive\u201d. We hope to address the concerns raised by the reviewers below:\n\n**Q1. Din vs. Dout -- Would the defense intentionally mispredict in non-IID query setups?**\n\nEDM does not draw a strict line between in and out of distribution data (i.e. there is no step-function change in the misprediction/discontinuity of EDM beyond a threshold). Instead, the discontinuity/misprediction-rate produced by EDM increases gradually with the amount of \u201cdataset shift\u201d in the test data. \n\nThe amount of dataset shift is assumed to be large in case of the adversary\u2019s queries compared to the queries of a benign user (distribution of Translated-MNIST is much closer to MNIST than say FashionMNIST). Thus, we expect the misprediction rate to be high for the adversary\u2019s queries compared to benign queries with a small dataset shift. This is a similar assumption that is made in other defenses (e.g., Prediction poisoning [1], Adaptive Misinformation [2]) as well.\n\n**Q2. Diversity Objective: My concern is that this appears to be simple OOD detection followed by misprediction/calibration in disguise. Consequently, I wonder if one can simply leverage existing advances in OOD detection to achieve the objective.**\n\nWhile our goal with EDM was to design a classifier that outputs discontinuous predictions on OOD data, our proposal also ends up generating random predictions on OOD data as a side effect. In fact, there is an existing proposal --Adaptive Misinformation (AM) [2] that is explicitly designed to output random predictions for OOD data, as suggested by the reviewer. AM uses an OOD detector to detect OOD queries and uses a misinformation model to output random predictions for such OOD queries. Our results comparing our defense with AM (Section 5.2 \u201ccomparison with prior work\u201d) shows that our proposal offers better security compared to AM owing to the discontinuous predictions produced by our defense. We\u2019ve also added more extensive evaluations comparing EDM with the trade-off curve of AM in Appendix A.7.\n\n**Q3. Hashing:  I wonder if an attacker can exploit the hashing function to recover whether the output is clean/poisoned -- such as by aggregating predictions over a set of transformed inputs.**\n\nSince the DNN used in the perceptual hashing function is not known to the adversary, the adversary needs a way to reliably ensure that the transformed input maps to a different model, which can be challenging.  Even if the adversary is able to figure out which inputs were poisoned, our defense would continue to outperform the baseline simply because a large fraction of the adversary\u2019s data, which were deemed to have been poisoned, would have to be thrown out. Furthermore, the adversary would require a higher query budget to perform aggregation over multiple transformed inputs. Lastly, querying the target model with a set of transformed inputs opens up the attack to detection using methods such as PRADA (Juuti et al. 2019 [3]), which can limit the applicability of such adaptive attacks.\n\n**Q4. Evaluation: I would have preferred if the results in Table 3 were presented as curves (with defense vs. attacker accuracy)...Additionally, it would also make the comparison with AM baseline fair, .. I wonder how the defense performs with other choices of attacker's data e.g., EMNIST which is a bit more similar to MNIST**\n\nWe\u2019ve added more extensive evaluations comparing EDM with the trade-off curve of AM in Appendix A.7. The reason why we didn\u2019t opt for defense vs attacker accuracy curves for EDM is that for most of the datasets, even with a very large value of \u03bb, we saw negligible degradation in the defender accuracy of the defense. This is because our regularization term in Eqn 3 (DivLoss) gets minimized to its lowest value of 0 (0 is the lowest value of cosine similarity with positive valued vectors) with a large enough value of \u03bb. Increasing \u03bb further has no impact on clean accuracy. \n\nWe did not evaluate with EMNIST as there is significant overlap between the classes of EMNIST and MNIST (0 and o, 1 and i/l, 9 and g/q, 8 and B, s and 5, z and 2). For the other datasets in our evaluations, we tried to pick surrogate datasets that are very similar to the target (CIFAR100 to attack CIFAR10, CIFAR10 to attack CIFAR100)\n\n\n[1] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Utility-constrained defenses against model stealing attacks. \n\n[2] Sanjay Kariyappa and Moinuddin K Qureshi. Defending against model stealing attacks with adaptive misinformation.\n\n[3] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper215/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Paper215/Reviewers", "ICLR.cc/2021/Conference/Paper215/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LucJxySuJcE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper215/Authors|ICLR.cc/2021/Conference/Paper215/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873417, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment"}}}, {"id": "s6R91OkixB", "original": null, "number": 4, "cdate": 1605668551509, "ddate": null, "tcdate": 1605668551509, "tmdate": 1605884365002, "tddate": null, "forum": "LucJxySuJcE", "replyto": "39Y2ORadZUr", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment", "content": {"title": "Response", "comment": "\nWe thank the reviewers for their valuable and positive feedback. We are happy that the reviewers found our problem to be \u201cwell-motivated\u201d and our idea to be \u201cnovel\u201d. We are also glad that the reviewers found our paper to be \u201cwell-written\u201d and our experimentation to be \u201cextensive\u201d. We hope to address the concerns raised by the reviewers below:\n\n**Q1. It seems one could try to launch an adversarial attack on the hash, why should it fail?**\n\nAs the reviewer noted, the DNN used in the hash function is kept secret from the adversary. Furthermore, the adversary cannot access the output of the hash function which makes an adversarial attack on the hash function hard to carry out. One possible mode of attack is to make large perturbations to the input and hope that this changes the output of the hash function. We evaluate such an adaptive attack in Appendix A.1 and show that EDM continues to offer better security compared to our baseline. Additionally, querying the target model with a set of transformed inputs opens up the attack to detection using methods such as PRADA (Juuti et al. 2019 [1]), which can limit the applicability of such adaptive attacks.\n\n\n\n**Q2. How far from the in-distribution images would one have to move to start hitting the effect of the diversity loss? As a user of an API, I have some knowledge about which images are in-distribution, why is using this information impractical?**\n\nWhile there is no fixed boundary separating the in and out of distribution data, beyond which the effects of the diversity loss kick in, the amount of discontinuity offered by EDM increases as we move farther away from the in-distribution data. Similar to prior works (Adaptive Misinformation, Prediction Poisoning, Deceptive Perturbations ), our defense is intended for a data-limited adversary, who does not have access to a large collection of in-distribution data. The assumption here is that the number of in-distribution examples available to the adversary in and of itself is not sufficient to train a high-accuracy clone model. Hence the adversary either uses surrogate datasets (KnockoffNets attack) or perturbed versions of in-distribution data (JBDA, JBDA-TR) to carry out the attack. \n\n**Q3. At the end of Sec 4.1, results on generalization to unseen out-of-distribution datasets are referred to, supposedly to be found in Sec 5.3, but I cannot find them there, where are they?**\n\nFig. 5 shows that a diverse ensemble produces lower coherence values on the adversary\u2019s unseen OOD data compared to a regular ensemble. This shows that our diverse ensemble, which is trained to produce low coherence value on an auxiliary OOD dataset, can generalize to unseen OOD data used by the adversary. We have updated Section 5.3 to state this more explicitly.\n\n**Q4. The hash function should map to as many members of the ensemble as uniform as possible on both the in-distribution and out-of-distribution data, how is this achieved?**\n\nTo ensure uniformity in the mapping, the output space of the M-class DNN classifier (M=10) used in the hash function was increased by subdividing it into (m=2) sub-classes yielding a total of M*m classes. We empirically found this to have enough dispersion for both in and out of distribution data to offer good security. \n\n[1] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 512\u2013527. IEEE, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper215/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Paper215/Reviewers", "ICLR.cc/2021/Conference/Paper215/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LucJxySuJcE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper215/Authors|ICLR.cc/2021/Conference/Paper215/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873417, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment"}}}, {"id": "sDozuRDAae1", "original": null, "number": 3, "cdate": 1605668227300, "ddate": null, "tcdate": 1605668227300, "tmdate": 1605884354627, "tddate": null, "forum": "LucJxySuJcE", "replyto": "40VEF1c43jT", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewers for their valuable and positive feedback. We are happy that the reviewers found our problem to be \u201cwell-motivated\u201d and our idea to be \u201cnovel\u201d. We are also glad that the reviewers found our paper to be \u201cwell-written\u201d and our experimentation to be \u201cextensive\u201d. We hope to address the concerns raised by the reviewers below:\n\n**Q1. If the adversaries have access to the auxiliary OOD datasets or use other complex OOD datasets, will the defense still work?**\n\nAn adversary who uses the auxiliary OOD dataset to query EDM would likely obtain a worse clone accuracy compared to using other OOD datasets. For example, for the EDM CIFAR-10 model trained with the Tiny-Images auxiliary OOD dataset, using Tiny-Images to perform model stealing yields a clone accuracy of 34.95 (0.37x), which is lower than the clone accuracy of 51.34 (0.52x) obtained by using CIFAR-100. This is because the models in EDM are trained to produce maximally dissimilar (i.e. discontinuous) predictions on the auxiliary OOD dataset. Thus, an adversary who uses this same OOD dataset to query the target model obtains highly discontinuous predictions, resulting in poor clone accuracy. \n\n**Q2. As mentioned in the paper, the hashing function requires transformation invariance, so that the adversaries cannot get the average prediction. However, can the DNN-based perceptual hashing function be generalized and applied to other types of transformation? For example, to break the transformation invariance, the adversary can use different algorithms to craft noises into the input data.**\n\nIf the adversary had white-box/black-box access to the predictions of the DNN used in our perceptual hashing algorithm, the adversary could easily pick a perturbation algorithm with a high likelihood of changing the output of the hash. However, since the DNN model used in the hash function is kept secret and the adversary does not have access to the output of the hash function, it is difficult for the adversary to  find a perturbation algorithm that can break the transformation invariance. Additionally, querying the target model with a set of transformed inputs opens up the attack to detection using methods such as PRADA (Juuti et al. 2019), which can limit the applicability of such adaptive attacks.\n\n**Q3. From Table 3, it seems that the performance of EDM on JBDA and JDBA-TR is not very well. My understanding is that since JBDA and JDBA-TR do not rely on a fixed OOD dataset, EDM trained on auxiliary OOD dataset may be well generalized to these attacks\u2019 OOD data.**\n\nFor most of the attacks and datasets, we found that the auxiliary OOD dataset does generalize well to the attacker\u2019s OOD dataset. However, in some cases (FashionMNIST with JBDA) the benefits of EDM are less pronounced possibly due to the observation made by the reviewer. We acknowledge this in section 5.3--\u201dWith the JBDA and JBDA-TR attacks, the benefits of EDM are less pronounced as these attacks use perturbed versions of in-distribution examples to query the target, which may not produce large discontinuities in the predictions.\u201d\n\n**Q4. A larger number of queries are used in the previous attacks. For example, in Juuti et al. 2019, 102,400 queries are used to steal models trained on the MNIST dataset. Does the small query budget used in the paper limit the performance of attacks?**\n\nWe\u2019ve added evaluations with 2x the query budget (100K instead of 50K) in Appendix A.6. For the JBDA and JBDA-TR, the clone accuracies plateaus before the query budget we use in our paper, therefore more trials do not provide significant benefits (this result is consistent with the observations made by Juuti et al. 2019). For KnockoffNets, the attacker is limited not only by the query budget but also by the amount of surrogate data to query the target model with. For the surrogate datasets where more data is available, we show that the benefits of our defense persist even if the query budget is doubled. \n\n**Q5 a.Minor comments: a. What is CS in equation (1) and (2)? b. \u201cA perceptual hashing algorithm is used to prevent adaptive attacks\u201d c. \u201cOur empirical evaluations shows that\u201d**\n\nCS is the Cosine Similarity. Thank you for these comments. We have updated the paper to fix these issues.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper215/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Paper215/Reviewers", "ICLR.cc/2021/Conference/Paper215/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LucJxySuJcE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper215/Authors|ICLR.cc/2021/Conference/Paper215/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873417, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment"}}}, {"id": "LYpkQTWja9V", "original": null, "number": 2, "cdate": 1605668112038, "ddate": null, "tcdate": 1605668112038, "tmdate": 1605884339794, "tddate": null, "forum": "LucJxySuJcE", "replyto": "wvJwBdRdmA", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment", "content": {"title": "Response", "comment": "We thank the reviewers for their valuable and positive feedback. We are happy that the reviewers found our problem to be \u201cwell-motivated\u201d and our idea to be \u201cnovel\u201d. We are also glad that the reviewers found our paper to be \u201cwell-written\u201d and our experimentation to be \u201cextensive\u201d. We hope to address the concerns raised by the reviewers below:\n\n**Q1. What happens if the surrogate model is more complex?**\n\nWe\u2019ve updated our paper with results from using a more complex surrogate model architecture in Appendix A.5. We obtain similar clone accuracies as before with a more complex model.\n\n**Q2. It seems inconsistent results are observed on the Flowers17 dataset, any explanation?**\n\nWe\u2019ve fixed a typo in our results (Flowers-17, hard label, JBDA-TR, EDM). Please let us know if you have a specific concern with the results aside from this.\n\n**Q3.  What will happen if more trials or unlimited trials are allowed?**\n\nWe\u2019ve added evaluations with double the query budget (100K instead of 50K) in Appendix A.6. For the JBDA and JBDA-TR, the clone accuracies plateau before the query budget we use in our paper, therefore more trials do not provide significant benefit (this result is consistent with the observations made by Juuti et al. 2019 [1]). For KnockoffNets, the attacker is limited not only by the query budget but also by the amount of surrogate data to query the target model with -- more queries than the amount of surrogate data do not help. For the surrogate datasets where more data is available, we show that the benefits of our defense persist even if the query budget is doubled. \n\n**Q4. Have you tried your method on larger tasks such as ImageNet or other fields, for example, machine translation and speech recognition?**\n\nWe haven\u2019t evaluated our defense on ImageNet. To the best of our knowledge, model stealing attacks have not been demonstrated to work on the Imagenet task in the data-limited setting, so it may be premature to evaluate defenses. However, we do evaluate our defense on 5 different datasets with resolutions ranging from 28x28 (FashionMNIST) to 256x256 ( Flowers17) and number of classes ranging from 10 (FashionMNIST) to 100 (CIFAR-100). Most of the MS attacks focus on stealing models in the computer vision domain. While we haven\u2019t tried our method on other fields like machine translation and speech recognition, it would be interesting to explore these attacks in other domains as a future direction.\n\n[1] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 512\u2013527. IEEE, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper215/Authors"], "readers": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Paper215/Reviewers", "ICLR.cc/2021/Conference/Paper215/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LucJxySuJcE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper215/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper215/Authors|ICLR.cc/2021/Conference/Paper215/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873417, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Comment"}}}, {"id": "39Y2ORadZUr", "original": null, "number": 2, "cdate": 1603838292923, "ddate": null, "tcdate": 1603838292923, "tmdate": 1605024738631, "tddate": null, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Review", "content": {"title": "Good paper", "review": "Summary: The paper proposes a method to protect deep neural networks against model stealing. The propose defense trains an ensemble of classifiers using two losses one targeting accuracy and the other diversity of the ensemble. In particular, the trained classifiers are consistent on in-distribution data, but contradict each other on out-of-distribution data. The proposed defense does not affect the test accuracy of the victim model while it strongly limits the test accuracy of the clone model.\n\nDespite the concerns below, I vote for accepting the paper. The addressed issue is relevant, the proposed method is interesting. The performance in the presented experiments is convincing and the writing is overall reasonably clear.\n\nConcerns:\n1. the method hinges on keeping the hash private (\"security through obscurity\")\n2. it also hinges on the attacker not being able to query the API with in-distribution images\n3. the authors argue that the inference runtime is not impacted by the method, however, the GPU memory requirements scale linearly with the size of the ensemble, which might be costly\n\nQuestions:\n- it seems one could try to launch an adversarial attack on the hash, why should it fail?\n- how far from the in-distribution images would one have to move to start hitting the effect of the diversity loss? As a user of an API, I have some knowledge about which images are in-distribution, why is using this information impractical?\n- at the end of Sec 4.1, results on generalization to unseen out-of-distribution datasets are referred to, supposedly to be found in Sec 5.3, but I cannot find them there, where are they?\n- the hash function should map to as many members of the ensemble as uniform as possible on both the in-distribution and out-of-distribution data, how is this achieved?\n\nTypos:\n- Table 3, the number for hard-label - Flowers-17 - JBDA-TR - EDM should most probably be 16.54 not 6.54, please check\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper215/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147958, "tmdate": 1606915797034, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper215/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Review"}}}, {"id": "40VEF1c43jT", "original": null, "number": 3, "cdate": 1603863291363, "ddate": null, "tcdate": 1603863291363, "tmdate": 1605024738570, "tddate": null, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Review", "content": {"title": "EDM's access to the adversary's OOD dataset", "review": "This paper tackles a timely problem of protecting deep neural networks from mode stealing attacks. This paper proposed an Ensemble of Diverse Model to provide diversity prediction for the adversary\u2019s OOD query. The main contribution of this paper is the introduction of the diversity loss function on OOD data and the discontinuous prediction provided by ensembled models. The results show that EDM defense reduces the accuracy of stolen models.\n\nPros:\n1.\tThe idea of using ensemble of diverse models to create discontinuous predictions on OOD datasets is interesting.\n2.\tThe experimental evaluation is comprehensive. Five datasets and three attacks are investigated in the evaluation. Adaptive attacks using average predictions are considered in the evaluation as well.\n3.\tThis paper is well-written. Figures make the paper easy to understand.\n\nCons:\n1.\tMy major concern is the auxiliary OOD dataset used by EDM. Does the defender have access to the adversary\u2019s OOD dataset? In the experiments, although the EDM used auxiliary OOD datasets, these auxiliary OOD datasets are more complex than the adversary\u2019s OOD datasets. If the adversaries have access to the auxiliary OOD datasets or use other complex OOD datasets, will the defense still work?\n2.\tAs mentioned in the paper, the hashing function requires transformation invariance, so that the adversaries cannot get the average prediction. However, can the DNN-based perceptual hashing function be generalized and applied to other types of transformation? For example, to break the transformation invariance, the adversary can use different algorithms to craft noises into the input data.\n3.\tFrom Table 3, it seems that the performance of EDM on JBDA and JDBA-TR is not very well. My understanding is that since JBDA and JDBA-TR do not reply on a fixed OOD dataset, EDM trained on auxiliary OOD dataset may be well generalized to these attacks\u2019 OOD data. \n4.\tThe query budget of the adversary is set as 50,000. A larger number of queries are used in the previous attacks. For example, in Juuti et al. 2019, 102,400 queries are used to steal models trained on the MNIST dataset. Does the small query budget used in the paper limit the performance of attacks?\n5.\tMinor comments:\na.\tWhat is CS in equation (1) and (2)? \nb.\t\u201cA perceptual hashing algorithm is used prevent adaptive attacks\u201d\nc.\t\u201cOur empirical evaluations shows that\u201d\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper215/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147958, "tmdate": 1606915797034, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper215/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Review"}}}, {"id": "wvJwBdRdmA", "original": null, "number": 4, "cdate": 1603870268160, "ddate": null, "tcdate": 1603870268160, "tmdate": 1605024738512, "tddate": null, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "invitation": "ICLR.cc/2021/Conference/Paper215/-/Official_Review", "content": {"title": "Effective Model Defense Technique, Need More Experimental Evaluation", "review": "This paper proposes a simple but effective way to avoid model stealing. To the best of my knowledge, the idea is novel. The writing of this paper is quite clear and experimental results also show the effectiveness of proposed method. By introducing diversity loss, the model ensemble tends to produces discontinuous outputs which is hard to be distilled. I still have some questions:\n\n1. In your experiments, there is an assumption that the thief knows what exactly the model archs. Given this prior knowledge, what will happen if the thief use a more complex/bigger model to distill ? \n\n2. It seems  inconsistent results are observed on Flowers17 dataset, any explanation?\n\n3. There is another assumption that the attackers are constrained by attach budget (50000 trials), what will happen if more trials or unlimited trials are allowed? I think this question is important because this is often true scenario if the attacked models are of high business value.\n\n4. Have you tried your method on larger tasks such as ImageNet or other fields, for example machine translation and speech recognition?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper215/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper215/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Protecting DNNs from Theft using an Ensemble of Diverse Models", "authorids": ["~Sanjay_Kariyappa1", "~Atul_Prakash1", "~Moinuddin_K_Qureshi2"], "authors": ["Sanjay Kariyappa", "Atul Prakash", "Moinuddin K Qureshi"], "keywords": ["Model stealing", "machine learning security"], "abstract": "Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kariyappa|protecting_dnns_from_theft_using_an_ensemble_of_diverse_models", "one-sentence_summary": "Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.", "supplementary_material": "/attachment/3d185c122b2fc5e08fd7d1dc17f89aa7aafe5986.zip", "pdf": "/pdf/fb157da741cd6b15681066ac173712bdecd45316.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkariyappa2021protecting,\ntitle={Protecting {\\{}DNN{\\}}s from Theft using an Ensemble of Diverse Models},\nauthor={Sanjay Kariyappa and Atul Prakash and Moinuddin K Qureshi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LucJxySuJcE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LucJxySuJcE", "replyto": "LucJxySuJcE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper215/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147958, "tmdate": 1606915797034, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper215/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper215/-/Official_Review"}}}], "count": 10}