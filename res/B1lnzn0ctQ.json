{"notes": [{"id": "B1lnzn0ctQ", "original": "B1xa-XacFQ", "number": 1304, "cdate": 1538087956274, "ddate": null, "tcdate": 1538087956274, "tmdate": 1550646780331, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gphYh4xV", "original": null, "number": 1, "cdate": 1545025973281, "ddate": null, "tcdate": 1545025973281, "tmdate": 1545354472926, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Meta_Review", "content": {"metareview": "This is a well executed paper that makes clear contributions to the understanding of unrolled iterative optimization and soft thresholding for sparse signal recovery with neural networks.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Solid contribution to unrolled iterative optimization and soft thresholding"}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1304/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352888148, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352888148}}}, {"id": "HJen7j87JN", "original": null, "number": 9, "cdate": 1543887651611, "ddate": null, "tcdate": 1543887651611, "tmdate": 1543893000421, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "HklQ7zppRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "content": {"title": "Re: Rating has been upgraded ", "comment": "[Opening is okay]\n\nPoints 1 and 2: There is no word \"tree\" or \"graph\", no \"beta\" or \"$\\beta$\", in our paper. We are confused and think they may refer to another paper. Could you kindly clarify?\n\n3: This is great suggestion. The matrix W is the solution of a convex quadratic program subject to linear constraints and, thereby, a linear system. Solving this system costs a negligible amount compared to training the remaining parameters. For example, when W is 250-by-500, computing W takes a few seconds but the remaining of ALISTA takes 1.5 hours. As you suggested, we will add this explanation and the complexity of computing W to the camera-ready version."}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616711, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1304/Authors|ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616711}}}, {"id": "HklQ7zppRQ", "original": null, "number": 8, "cdate": 1543520795258, "ddate": null, "tcdate": 1543520795258, "tmdate": 1543887819487, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "rkxXkC3DTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "content": {"title": "Rating has been upgraded", "comment": "Just some minor comments on the responses from the authors:\n[Removed comments that were incorrectly included in this response]\n* The complexity of the algorithm should also include that of the optimization that finds the matrix W, equation (16) in Stage 1."}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1304/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616711, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1304/Authors|ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616711}}}, {"id": "B1xca8C4nX", "original": null, "number": 2, "cdate": 1540839106209, "ddate": null, "tcdate": 1540839106209, "tmdate": 1543108357888, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Review", "content": {"title": "ALISTA - Review", "review": "The paper describes ALISTA, a version of LISTA that uses the dictionary only for one of its roles (synthesis) in ISTA and learns a matrix to play the other role (analysis), as seen in equations (3) and (6). The number of matrices to learn is reduced by tying the different layers of LISTA together.\n\nThe motivation for this paper is a little confusing. ISTA, FISTA, etc. are algorithms for sparse recovery that do not require training. LISTA modified ISTA to allow for training of the \"dictionary matrix\" used in each iteration of ISTA, assuming that it is unknown, and offering a deep-learning-based alternative to dictionary learning. ALISTA shows that the dictionary does not need to change, and fewer parameters are used than in LISTA, but it still requires learning matrices of the same dimensionality as LISTA (i.e., the reduction is in the constant, not the order). If the argument that fewer parameters are needed is impactful, then the paper should discuss the computational complexity (and computing times) for training ALISTA vs. the competing approaches.\n\nThere are approaches to sparse modeling that assume separate analysis and synthesis dictionaries (e.g., Rubinstein and Elad, \"Dictionary Learning for Analysis-Synthesis Thresholding\"). A discussion of these would be relevant in this paper.\n\n* The intuition and feasibility of identifying \"good\" matrices (Defs. 1 and 2) should be detailed. For example, how do we know that an arbitrary starting W belongs in the set (12) so that (14) applies? \n* Can you comment on the difference between the maximum entry \"norm\" used in Def. 1 and the Frobenius norm used in (17)?\n* Definition 3: No dependence on theta(k) appears in (13), thus it is not clear how \"as long as theta(k) is large enough\" is obtained. \n* How is gamma learned (Section 2.3)?\n* The notation in Section 3 is a bit confusing - lowercase letters b, d, x refer to matrices instead of vectors. In (20), Dconv,m(.) is undefined; later Wconv is undefined.\n* For the convolutional formulation of Section 3, it is not clear why some transposes from (6) disappear in (21).\n* In Section 3.1, \"an efficient approximated way\" is an incomplete sentence - perhaps you mean \"an efficient approximation\"?. Before (25), Dconv should be Dcir? The dependence on d should be more explicitly stated.\n* Page 8 typo \"Figure 1 (a) (a)\".\n* Figure 2(a): the legend is better used as the label for the y axis.\n* I do not think Figure 2(b) verifies Theorem 1; rather, it verifies that your learning scheme gives parameter values that allow for Theorem 1 to apply (which is true by design).\n* Figure 3: isn't it easier to use metrics from support detection (false alarm/missed detection proportions given by the ALISTA output)?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Review", "cdate": 1542234259375, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335917551, "tmdate": 1552335917551, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxXkC3DTQ", "original": null, "number": 3, "cdate": 1542077914684, "ddate": null, "tcdate": 1542077914684, "tmdate": 1542826468159, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "B1xca8C4nX", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "content": {"title": "Response to Reviewer 2 (Continued)", "comment": "Answers to individual comments:\n\n- Q1 (Intuition and feasibility of identifying \"good\" matrices; Definition 1): \nDefinition 1 describes a property of good matrices: small coherence with respect to D. This is inspired by Donoho & Elad, 2003; Elad, 2007; Lu et al, 2018. Our Theorem 1 validates this point: a small mutual coherence leads to a large c and faster convergence. Feasibility is proved in (Chen et al., 2018). We have added these clarifications in our update.\n \n- Q1 (Clarification of Definition 2): \nBecause W and D are both \u201cfat\u201d matrices, the product W\u2019D, and such products of their submatrices consisting of two or more their corresponding columns, generally cannot be very close to the identity matrix. For a given D, Definition 2 let sigma_min represent the minimal \u201cdistance\u201d and define the set of corresponding W matrices. A larger sigma_min implies slower convergence in Theorem 2.  We have added numerical validations of (11) to the appendix in the update. (The original definition (12) is (11) in the updated version.)\n\n- Q2 (Difference between the maximum entry \"norm\" and the Frobenius norm): \nWe use a Frobenius norm in (16) instead of a sup-norm in Def. 1 (8) for computational efficiency. Directly minimizing the sup norm leads to a large-scale linear program. The sizes of the matrices W and D that we used in our numerical experiments are 250 by 500. We implemented an LP solver for the sup-norm minimization (8) based on Gurobi, which requires more than 8GB of memory and may be intractable on a typical PC. However, solving (16) in MATLAB needs only around 10MB of memory and a few seconds. Besides the Frobenius norm, we also tried to minimize the L_{1,1} norm but found no advantages. (The original formula (17) is (16) in the update.)\n\n- Q3 (Definition 3): \nBy (6), x^k depends on thresholding parameters theta^0, theta^1, ..., theta^{k-1}. When these theta parameters are large enough, x^k can be sufficiently sparse. Theorem 1 implies we can ensure \u201csupport(x^k) belongs to S\u201d for all k by properly choosing the theta^k sequence.\n\n- Q4 (How is gamma learned): \nThe step sizes gamma^k and thresholds theta^k (for all k) are updated to minimize the empirical recovery loss in (5), using the standard training method based on backpropagation and the Adam method. For ALISTA, the big Theta in (5), which is the set of parameters subject to learning, consists of only gammas and thetas. The matrix W is pre-computed by analytic optimization and, therefore, is fixed during training.\n\n- Q5 (The notation in Section 3): \nThe lowercase letters are always vectors. The matrices D_{conv,m} are defined so that (18), which is precise but complicated, is equivalent to (19), which is simple and compact. The full definition of D_{conv,m} is given in Appendix C.2. The matrices W_{conv,m} are defined for a similar purpose before (21). We have added these clarifications in the updated version. (The original formula (20) is (19) in the current version.)\n\n- Q6 (Transpose in convolution): \nTransposing a circulant matrix is equivalent to applying the convolution with rotated filters (Equation (6) and Footnote 2 in Chalasani et al., 2013). We have made clarifications in the update. \n\n- Q7 & Q8 & Q9 (Typos and figure suggestions): \nThanks for finding the typos and making suggestions for figures. We have fixed the typos and will carefully proofread our paper. \n\n- Q10 (\u201cI do not think Figure 2(b) verifies Theorem 1\u201d): \nWe agree that we incorrectly used the words \"verify\" and \"validation.\" Rather, the numerical observations in Figure 2(b) justify our choices of parameters in Theorem 1. We have made this correction.\n\n- Q11 (Figure 3): \nWe agree that the number and proportion of false alarms are a more straightforward performance metric. However, they are sensitive to the threshold. We found that, although using a smaller threshold leads to more false alarms, the final recovery quality is better and those false alarms have small magnitudes and are easy to remove by thresholding during post-processing. That's why we chose to show their magnitudes, implying that we get easy-to-remove false alarms. We have added this reasoning to the final version."}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616711, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1304/Authors|ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616711}}}, {"id": "r1xH4AhDpX", "original": null, "number": 4, "cdate": 1542077996560, "ddate": null, "tcdate": 1542077996560, "tmdate": 1542825941813, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "B1xca8C4nX", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "content": {"title": "ALISTA pre-computes the weight matrix and only learns a series of threshold and step size parameters", "comment": "Thanks for your careful review and the comments! We have revised our paper and we believe our responses and revisions address your concerns. We would be very grateful if you would look over our paper again, and reconsider your opinion.\n\nLet us first provide a general response, followed by responses to your specific comments.\n\nThe goal of work is to significantly speed up sparse recovery. The basis of this line of work is ISTA (iterative soft-thresholding algorithm), a classic iterative method for recovering a sparse vector x from it linear measurements Dx, which are further contaminated by additive noise. Like most iterative methods, ISTA repeats the same operation (matrix-vector multiplications by D and D\u2019 and a soft thresholding) at each iteration. Therefore, it can be written as a simple for-loop. However, depending on the problem condition, it can take hundreds of iterations or tens of thousands of iterations. Gregor & LeCun, 2010, instead of using the original matrices D and D\u2019 and soft-thresholding scalars in ISTA, select a series of new matrices and scalars by training using a set of synthetic sparse signals and their linear measurements. The resulting method, called LISTA (or learned ISTA), has a small fixed number of iterations, roughly 20, and is not only much faster but recovers more accurate sparse vectors than ISTA even if ISTA runs order-of-magnitude more iterations. On the other hand, training LISTA takes a long time, typically ten hours or longer, much like training a neural network with lots of parameters. Also, one must train new matrices and scalars for each encoding matrix D. These shortcomings are addressed by a line of work that follows LISTA. \n\nThis paper introduces ALISTA, which significantly simplifies LISTA by using only one free matrix (besides the encoding matrix D) for all iterations, and pre-computing that matrix by analytic optimization, as opposed to data-driven training. Therefore, when it comes to training ALISTA, there remain only a series of scalars for thresholding and step sizes to be learned from synthetic data. Despite this huge simplification, the performance of ALISTA is no worse than LISTA and other work along the line, supported by our theoretical results and numerical verification. \n\nYour question on computational complexity is great. Let us compute how much saving in flops ALISTA has over LISTA or its variants. Assume there are K layers (i.e., iterations) in total, and the encoding matrix has N rows and M columns with N < M, possibly N << M. In its typical implementation, vanilla LISTA learns O(KM^2+K+MN) parameters. That is one matrix and one scalar per layer and another matrix shared between all layers. LISTA in Chen et al., 2018 (also (6) in this paper) learns O(KNM + K) parameters as they learn only one N-by-M matrix and one thresholding parameter per layer. Tied LISTA ((15) in this paper) learns only O(NM + K) parameters by using only one matrix for all the K layers plus a step size and a thresholding parameter per layer. ALISTA ((16) in this paper) learns only O(K) parameters because it determines the only matrix by analytic optimization and fixes it during training. All these methods achieve similar recover quality. We have added this comparison to the revised paper.\n\nThe model in the paper that you has mentioned, \u201cDictionary Learning for Analysis-Synthesis Thresholding\u201d, is related to our paper as a special LISTA model with only one layer. We have cited this and related papers (listed below) in Section 1 of our updated version and discussed their contributions. \n\nYang et al., 2016. \u201cAnalysis-Synthesis Dictionary Learning for Universality-Particularity Representation Based Classification.\u201d"}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616711, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1304/Authors|ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616711}}}, {"id": "HyltGkJFiQ", "original": null, "number": 1, "cdate": 1540054800720, "ddate": null, "tcdate": 1540054800720, "tmdate": 1542539067871, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Review", "content": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "review": "The papers studies neural network-based sparse signal recovery, and derives many new theoretical insights into the classical LISTA model. The authors proposed Analytic LISTA (ALISTA), where the weight matrix in LISTA is pre-computed with a data-free coherence minimization, followed by a separate data-driven learning step for merely (a very small number of) step-size and threshold parameters. Their theory is extensible to convolutional cases. The two-stage decomposed pipeline was shown to keep the optimal linear convergence proved in (Chen et al., 2018). Experiments observe that ALISTA has almost no performance loss compared to the much heavier parameterized LISTA, in contrast to the common wisdom that (brutal-force) \u201cend-to-end\u201d always outperforms stage-wise training. Their contributions thus manifest in both novel theory results, and the practical impacts of simplifying/accelerating LISTA training.  Besides, they also proposed an interesting new strategy called Robust ALISTA to overcome the small perturbations on the encoding basis, which also benefits from this decomposed problems structure. \n\nThe proofs and conclusions are mathematically correct to my best knowledge. I personally worked on similar sparse unfolding problems before so this work looks particularly novel and interesting to me. My intuition then was that, it should not be really necessary to use heavily parameterized networks to approximate a simple linear sparse coding form (LISTA idea). Similar accelerations could have been achieved with line search for something similar to steepest descent (also computational expensive, but need learn step-sizes only, and agnostic to input distribution). Correspondingly, there should exist a more elegant network solution with very light learnable weights. This work perfectly coincides with the intuition, providing very solid guidance on how a LISTA model could be built right. Given in recent three years, many application works rely on unfold-truncating techniques (compressive sensing, reconstruction, super resolution, image restoration, clustering\u2026), I envision this paper to generate important impacts for practitioners pursuing those ideas. \n\nAdditionally, I like Theorem 3 in Section 3.1, on the provable efficient approximation of general convolution using circular convolution. It could be useful for many other problems such as filter response matching. \n\nI therefore hold a very positive attitude towards this paper and support for its acceptance. Some questions I would like the authors to clarify & improve in revision:\n\n1.\tEqn (7) assumes noise-free case. The author stated \u201cThe zero-noise assumption is for simplicity of the proofs.\u201d Could the authors elaborate which part of current theory/proof will fail in noisy case? If so, can it be overcome (even by less \u201csimpler\u201d way)? How about convolutional case, the same? Could the authors at least provide some empirical results for ALISTA\u2019s performance under noise?\n\n2.\tSection 5.3. It is unclear to me why Robust ALISTA has to work better than the data augmented ALISTA. Is it potentially because that in the data augmentation baseline, the training data volume is much amplified, and one ALISTA model might become underfitting? It would be interesting to create a larger-capacity ALISTA model (e.g., by increasing unfolded layer numbers), train it on the augmented data, and see if it can compare more favorably against Robust ALISTA?\n\n3.\tThe writeup is overall very good, mature, and easy to follow. But still, typos occur from time to time, showing a bit rush. For example, Section 5.1, \u201cthe x-axes denotes is the indices of layers\u201d should remove \u201cis\u201d. Please make sure more proofreading will be done.\n\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Review", "cdate": 1542234259375, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335917551, "tmdate": 1552335917551, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryxQcC8jp7", "original": null, "number": 6, "cdate": 1542315659105, "ddate": null, "tcdate": 1542315659105, "tmdate": 1542315659105, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "HyltGkJFiQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "content": {"title": "Continued Response to Reviewer 3", "comment": "As you kindly suggested, we added two experiments to train the data-augmented version of ALISTA with 20 and 24 layers, to compare with the robust ALISTA model (the concatenation of a feed-forward encoder network that learns to solve the coherence minimization and a ALISTA network with step size and thresholds parameters). \n\nFor training ALISTA with data-augmentation, in each step, we first generate a batch of perturbed dictionaries \\tilde{D}s around an original dictionary D. Then these perturbed dictionaries are used to generate observations, by multiplying sparse vector samples from the same distribution. The data-augmented version of ALISTA is then trained with those dictionary-perturbed samples. It still follows the standard ALISTA to use a fixed weight matrix W that is analytically pre-solved from the original dictionary D.\n\nThe robust ALISTA model instead uses the encoder network to adaptively produce weight matrices to be used in ALISTA. Apart from the encoder network, the robust ALISTA needs to learn a set of step size and thresholds parameters just like the baseline ALISTA. We fix using a 16-layer ALISTA network and a 4-layer encoder in the robust ALISTA model.\n\nIn this experiment, we  compare both models\u2019 robustness to dictionary perturbations, by plotting recovery normalized MSEs (in dB) in testing, w.r.t. the standard deviation of perturbation noise, and also w.r.t. the layers used for data-augmented ALISTA. We set the maximal standard deviation of generated perturbations to 0.02 and followed the same settings described in Appendix E in the paper: \n\nSigma (standard deviation)   |   0.0001    |    0.001    |     0.01      |    0.015    |    0.02     |   0.025\nAugmented ALISTA T=16       |   -26.58     |    -25.87   |   -15.49    |   -11.71   |   -8.84    |   -6.74\nAugmented ALISTA T=20       |   -24.43     |   -24.46    |   -15.39    |   -11.77   |   -8.94    |   -6.82\nAugmented ALISTA T=24       |   -24.12     |   -24.00    |   -15.45    |   -11.68   |   -8.81    |   -6.70\nRobust ALISTA T=16                |   -62.47     |   -62.41    |   -62.02    |   -61.50   |   -60.67  |   -45.00\n\n\n- Observation: as we may see in the above results, more layers didn\u2019t bring obvious empirical benefits to the recoverability of ALISTA. We could even observe that ALISTA of 24 layers had slightly worse NMSE that ALISTA of 16 and 20 layers.\n\n- Analysis: we agree with your insight that the limited parameter volume of augmented ALISTA  might limited its capacity and robustness to recover from dictionary-perturbed measurements, compared to robust ALISTA which has another encoder network that adaptively and efficiently encodes the perturbed dictionary \\tilde{D} into new (dynamic) weight matrix \\tilde{W}. ALISTA only has two scalars to be learned in each layer (one scalar as step size and the other as threshold), therefore adding more layers do not enlarge the parameter volume significantly. \n\n- Remark: from the comparison, we could conclude that it takes more than adjusting step sizes and thresholds to gain robustness to dictionary perturbations in LISTA/ALISTA. Therefore, robust ALISTA makes the meaningful progress in creating an efficient encoder network, that can dynamically address the dictionary variations \\tilde{D} by always adjusting \\tilde{W}. Without incurring much higher complexity, robust ALISTA witness remarkable improvements over ALISTA, making it a worthy effort in advancing LISTA-type network research into the practical domain. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616711, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1304/Authors|ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616711}}}, {"id": "rJey90hw6m", "original": null, "number": 5, "cdate": 1542078087105, "ddate": null, "tcdate": 1542078087105, "tmdate": 1542078139329, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "HyltGkJFiQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your careful reading and comments!\n\n- Q1: In our proofs, we take b as b=Ax*. If we add noise to the measurements, almost all the inequalities in the proof need to be modified. We will end up getting \u201cconvergence\u201d to a neighbor of x* with a size depending on the noise level. Such modifications also apply to the analysis for convolutional dictionaries. Numerically, figures 1(b), 1(c) and 1(d) depict the results of ALISTA under SNRs = 40dB, 30dB and 20dB, respectively. \n\n- Q2: We basically agree with your comment on why data augmented TiLISTA and ALISTA are not performing as well as robust ALISTA. We are conducting the experiments that you have suggested and will update the results in comments once they become available, and also add them to the paper\u2019s next update.\n\n- Q3: Thanks for kindly pointing out our writing issues. We will carefully fix typos and use more proofreading."}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616711, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1304/Authors|ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616711}}}, {"id": "r1lP83nDpX", "original": null, "number": 1, "cdate": 1542077518816, "ddate": null, "tcdate": 1542077518816, "tmdate": 1542077518816, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "SyxX7z5uhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your careful reading and kindly identifying the typos in our paper! We will fix these typos and meticulously proofread our article.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616711, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lnzn0ctQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1304/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1304/Authors|ICLR.cc/2019/Conference/Paper1304/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers", "ICLR.cc/2019/Conference/Paper1304/Authors", "ICLR.cc/2019/Conference/Paper1304/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616711}}}, {"id": "SyxX7z5uhQ", "original": null, "number": 3, "cdate": 1541083675292, "ddate": null, "tcdate": 1541083675292, "tmdate": 1541533251254, "tddate": null, "forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1304/Official_Review", "content": {"title": "important theoretical contribution to unrolling literature", "review": "The paper raises many important questions about unrolled iterative optimization algorithms, and answers many questions for the case of iterative soft thresholding algorithm (ISTA, and learned variant LISTA). The authors demonstrate that a major simplification is available for the learned network: instead of learning a matrix for each layer, or even a single (potentially large) matrix, one may obtain the matrix analytically and learn only a series of scalars. These simplifications are not only practically useful but allow for theoretical analysis in the context of optimization theory. On top of this seminal contribution, the results are extended to the convolutional-LISTA setting. Finally, yet another fascinating result is presented, namely that the analytic weights can  be determined from a Gaussian-perturbed version of the dictionary. Experimental validation of all results is presented.\n\nMy only constructive criticism of this paper are a few grammatical typos, but specifically the 2nd to  last sentence before Sec 2.1 states the wrong thing \"In this way, the LISTA model could be further significantly simplified, without little performance loss\"\n...\nit should be \"with little\".\n", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1304/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "abstract": "Deep neural networks based on unfolding an iterative algorithm, for example, LISTA (learned iterative shrinkage thresholding algorithm), have been an empirical success for sparse signal recovery. The weights of these neural networks are currently determined by data-driven \u201cblack-box\u201d training. In this work, we propose Analytic LISTA (ALISTA), where the weight matrix in LISTA is computed as the solution to a data-free optimization problem, leaving only the stepsize and threshold parameters to data-driven learning. This signi\ufb01cantly simpli\ufb01es the training. Speci\ufb01cally, the data-free optimization problem is based on coherence minimization. We show our ALISTA retains the optimal linear convergence proved in (Chen et al., 2018) and has a performance comparable to LISTA. Furthermore, we extend ALISTA to convolutional linear operators, again determined in a data-free manner. We also propose a feed-forward framework that combines the data-free optimization and ALISTA networks from end to end, one that can be jointly trained to gain robustness to small perturbations in the encoding model.", "keywords": ["sparse recovery", "neural networks"], "authorids": ["liujl11@math.ucla.edu", "chernxh@tamu.edu", "atlaswang@tamu.edu", "wotaoyin@math.ucla.edu"], "authors": ["Jialin Liu", "Xiaohan Chen", "Zhangyang Wang", "Wotao Yin"], "pdf": "/pdf/e664f80e199d7c901289a4b7319d1f7558b83040.pdf", "paperhash": "liu|alista_analytic_weights_are_as_good_as_learned_weights_in_lista", "_bibtex": "@inproceedings{\nliu2018alista,\ntitle={{ALISTA}: Analytic Weights Are As Good As Learned Weights in {LISTA}},\nauthor={Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lnzn0ctQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1304/Official_Review", "cdate": 1542234259375, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lnzn0ctQ", "replyto": "B1lnzn0ctQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1304/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335917551, "tmdate": 1552335917551, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1304/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}