{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1531503440325, "tcdate": 1518463382297, "number": 225, "cdate": 1518463382297, "id": "S10qYwywf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S10qYwywf", "signatures": ["~Soorya_Gopalakrishnan1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Combating Adversarial Attacks Using Sparse Representations", "abstract": "It is by now well-known that small adversarial perturbations can induce classification errors in deep neural networks (DNNs). In this paper, we make the case that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against l\u221e-bounded attacks, reducing output distortion due to the attack by a factor of roughly K/N where N is the data dimension and K is the sparsity level. We then extend this concept to DNNs, showing that a \u201clocally linear\u201d model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.", "paperhash": "gopalakrishnan|combating_adversarial_attacks_using_sparse_representations", "keywords": ["Adversarial examples", "sparse representations", "robust machine learning"], "_bibtex": "@misc{\n  gopalakrishnan2018combating,\n  title={Combating Adversarial Attacks Using Sparse Representations},\n  author={Soorya Gopalakrishnan and Zhinus Marzi and Upamanyu Madhow and Ramtin Pedarsani},\n  year={2018},\n  url={https://openreview.net/forum?id=S10qYwywf}\n}", "authorids": ["soorya@ucsb.edu", "zhinus_marzi@ucsb.edu", "madhow@ucsb.edu", "ramtin@ucsb.edu"], "authors": ["Soorya Gopalakrishnan", "Zhinus Marzi", "Upamanyu Madhow", "Ramtin Pedarsani"], "TL;DR": "We show via a theoretically grounded framework that sparsity in natural data can be exploited to combat adversarial attacks.", "pdf": "/pdf/d9dfac913db549bbbc7642926cc30090ea7db5e7.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"ddate": null, "original": null, "tddate": 1520570270504, "tmdate": 1521582855837, "tcdate": 1520569633821, "number": 1, "cdate": 1520569633821, "id": "rJ97TtJFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper225/Official_Review", "forum": "S10qYwywf", "replyto": "S10qYwywf", "signatures": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer2"], "content": {"title": "Sparse Representations in NNs to Defend Against Adversarial Attacks", "rating": "8: Top 50% of accepted papers, clear accept", "review": "In this paper, the authors discuss sparse representations as a tool to combat adversarial attacks on neural networks. The idea is that if the input dimension is projected to a smaller subspace, the effects of small perturbations in the input space is lessened. They use a theoretical framework to prove guarantees about their approach, and apply their methods to MNIST to show that they are better equipped to deal with adversarial attacks than non-sparse models. \n\nThis was a well-written paper with an interesting idea, and the authors provide a lot of intuition. My main comment is that it skips a lot of background -- I would suggest adding more background about adversarial attacks and the terminology used. ", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combating Adversarial Attacks Using Sparse Representations", "abstract": "It is by now well-known that small adversarial perturbations can induce classification errors in deep neural networks (DNNs). In this paper, we make the case that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against l\u221e-bounded attacks, reducing output distortion due to the attack by a factor of roughly K/N where N is the data dimension and K is the sparsity level. We then extend this concept to DNNs, showing that a \u201clocally linear\u201d model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.", "paperhash": "gopalakrishnan|combating_adversarial_attacks_using_sparse_representations", "keywords": ["Adversarial examples", "sparse representations", "robust machine learning"], "_bibtex": "@misc{\n  gopalakrishnan2018combating,\n  title={Combating Adversarial Attacks Using Sparse Representations},\n  author={Soorya Gopalakrishnan and Zhinus Marzi and Upamanyu Madhow and Ramtin Pedarsani},\n  year={2018},\n  url={https://openreview.net/forum?id=S10qYwywf}\n}", "authorids": ["soorya@ucsb.edu", "zhinus_marzi@ucsb.edu", "madhow@ucsb.edu", "ramtin@ucsb.edu"], "authors": ["Soorya Gopalakrishnan", "Zhinus Marzi", "Upamanyu Madhow", "Ramtin Pedarsani"], "TL;DR": "We show via a theoretically grounded framework that sparsity in natural data can be exploited to combat adversarial attacks.", "pdf": "/pdf/d9dfac913db549bbbc7642926cc30090ea7db5e7.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582855645, "id": "ICLR.cc/2018/Workshop/-/Paper225/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper225/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper225/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper225/AnonReviewer1"], "reply": {"forum": "S10qYwywf", "replyto": "S10qYwywf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper225/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper225/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582855645}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582778453, "tcdate": 1520634500056, "number": 2, "cdate": 1520634500056, "id": "rynKqFltG", "invitation": "ICLR.cc/2018/Workshop/-/Paper225/Official_Review", "forum": "S10qYwywf", "replyto": "S10qYwywf", "signatures": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer3"], "content": {"title": "interesting idea; a little terse", "rating": "6: Marginally above acceptance threshold", "review": "the paper presents an interesting extension from sparsifying front end for linear classifiers to deep neural networks. this is potentially an interesting idea. I believe the paper could benefit from expanding sec2: the proof of sparsifying front end effective against l_\\infty-bounded attacks; the current terseness has hindered my understanding of the precise argument. (the intuition makes sense though.)", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combating Adversarial Attacks Using Sparse Representations", "abstract": "It is by now well-known that small adversarial perturbations can induce classification errors in deep neural networks (DNNs). In this paper, we make the case that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against l\u221e-bounded attacks, reducing output distortion due to the attack by a factor of roughly K/N where N is the data dimension and K is the sparsity level. We then extend this concept to DNNs, showing that a \u201clocally linear\u201d model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.", "paperhash": "gopalakrishnan|combating_adversarial_attacks_using_sparse_representations", "keywords": ["Adversarial examples", "sparse representations", "robust machine learning"], "_bibtex": "@misc{\n  gopalakrishnan2018combating,\n  title={Combating Adversarial Attacks Using Sparse Representations},\n  author={Soorya Gopalakrishnan and Zhinus Marzi and Upamanyu Madhow and Ramtin Pedarsani},\n  year={2018},\n  url={https://openreview.net/forum?id=S10qYwywf}\n}", "authorids": ["soorya@ucsb.edu", "zhinus_marzi@ucsb.edu", "madhow@ucsb.edu", "ramtin@ucsb.edu"], "authors": ["Soorya Gopalakrishnan", "Zhinus Marzi", "Upamanyu Madhow", "Ramtin Pedarsani"], "TL;DR": "We show via a theoretically grounded framework that sparsity in natural data can be exploited to combat adversarial attacks.", "pdf": "/pdf/d9dfac913db549bbbc7642926cc30090ea7db5e7.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582855645, "id": "ICLR.cc/2018/Workshop/-/Paper225/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper225/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper225/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper225/AnonReviewer1"], "reply": {"forum": "S10qYwywf", "replyto": "S10qYwywf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper225/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper225/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582855645}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582773154, "tcdate": 1520636191038, "number": 3, "cdate": 1520636191038, "id": "HJPm-qgKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper225/Official_Review", "forum": "S10qYwywf", "replyto": "S10qYwywf", "signatures": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer1"], "content": {"title": "Interesting work on use of a sparsifying front end to linear/neural network models to tackle adversarial attacks with promising results.", "rating": "6: Marginally above acceptance threshold", "review": "The submitted work, aims to present theoretical bases to demonstrate the efficacy of sparsifying input data to linear/neural network models in order to tackle adversarial attacks. \nThe aim of the work is to propose a \"systematic, theoretically grounded, framework for design of both attacks and defenses\". A short workshop paper like this one does not provide all elements to judge such statement but the approach is clear.\nThe application of the approach to a linear model and then expanding the evaluation to a neural network via a \"locally\" linear model is well laid.\nApplication of the approach to varying dimensionality spaces and determination of the corresponding best sparsity values to handle most adversarial attacks would be interesting to analyze.\nThe results on the MNIST handwritten digit dataset, both for SVM and a four layer CNN look promising.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combating Adversarial Attacks Using Sparse Representations", "abstract": "It is by now well-known that small adversarial perturbations can induce classification errors in deep neural networks (DNNs). In this paper, we make the case that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against l\u221e-bounded attacks, reducing output distortion due to the attack by a factor of roughly K/N where N is the data dimension and K is the sparsity level. We then extend this concept to DNNs, showing that a \u201clocally linear\u201d model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.", "paperhash": "gopalakrishnan|combating_adversarial_attacks_using_sparse_representations", "keywords": ["Adversarial examples", "sparse representations", "robust machine learning"], "_bibtex": "@misc{\n  gopalakrishnan2018combating,\n  title={Combating Adversarial Attacks Using Sparse Representations},\n  author={Soorya Gopalakrishnan and Zhinus Marzi and Upamanyu Madhow and Ramtin Pedarsani},\n  year={2018},\n  url={https://openreview.net/forum?id=S10qYwywf}\n}", "authorids": ["soorya@ucsb.edu", "zhinus_marzi@ucsb.edu", "madhow@ucsb.edu", "ramtin@ucsb.edu"], "authors": ["Soorya Gopalakrishnan", "Zhinus Marzi", "Upamanyu Madhow", "Ramtin Pedarsani"], "TL;DR": "We show via a theoretically grounded framework that sparsity in natural data can be exploited to combat adversarial attacks.", "pdf": "/pdf/d9dfac913db549bbbc7642926cc30090ea7db5e7.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582855645, "id": "ICLR.cc/2018/Workshop/-/Paper225/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper225/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper225/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper225/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper225/AnonReviewer1"], "reply": {"forum": "S10qYwywf", "replyto": "S10qYwywf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper225/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper225/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582855645}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573562091, "tcdate": 1521573562091, "number": 84, "cdate": 1521573561747, "id": "rJz6C00tz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S10qYwywf", "replyto": "S10qYwywf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combating Adversarial Attacks Using Sparse Representations", "abstract": "It is by now well-known that small adversarial perturbations can induce classification errors in deep neural networks (DNNs). In this paper, we make the case that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against l\u221e-bounded attacks, reducing output distortion due to the attack by a factor of roughly K/N where N is the data dimension and K is the sparsity level. We then extend this concept to DNNs, showing that a \u201clocally linear\u201d model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.", "paperhash": "gopalakrishnan|combating_adversarial_attacks_using_sparse_representations", "keywords": ["Adversarial examples", "sparse representations", "robust machine learning"], "_bibtex": "@misc{\n  gopalakrishnan2018combating,\n  title={Combating Adversarial Attacks Using Sparse Representations},\n  author={Soorya Gopalakrishnan and Zhinus Marzi and Upamanyu Madhow and Ramtin Pedarsani},\n  year={2018},\n  url={https://openreview.net/forum?id=S10qYwywf}\n}", "authorids": ["soorya@ucsb.edu", "zhinus_marzi@ucsb.edu", "madhow@ucsb.edu", "ramtin@ucsb.edu"], "authors": ["Soorya Gopalakrishnan", "Zhinus Marzi", "Upamanyu Madhow", "Ramtin Pedarsani"], "TL;DR": "We show via a theoretically grounded framework that sparsity in natural data can be exploited to combat adversarial attacks.", "pdf": "/pdf/d9dfac913db549bbbc7642926cc30090ea7db5e7.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}