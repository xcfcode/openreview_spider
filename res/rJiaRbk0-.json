{"notes": [{"tddate": null, "ddate": null, "tmdate": 1527745091765, "tcdate": 1509002946844, "number": 118, "cdate": 1518730188400, "id": "rJiaRbk0-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rJiaRbk0-", "original": "r1560WkRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260089405, "tcdate": 1517249706373, "number": 435, "cdate": 1517249706355, "id": "SJzhNy6Hz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper proposes training binary-values LSTMs for NLP using the Gumbel-softmax reparameterization.  The motivation is that this will generalize better, and this is demonstrated in a couple of instances.\n\nHowever, it's not clear how cherry-picked the examples are, since the training loss wasn't reported for most experiments.  And, if the motivation is better generalization, it's not clear why we would use this particular setup."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1516156376382, "tcdate": 1511836598499, "number": 3, "cdate": 1511836598499, "id": "HyA3jBqgG", "invitation": "ICLR.cc/2018/Conference/-/Paper118/Official_Review", "forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "signatures": ["ICLR.cc/2018/Conference/Paper118/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper propose a new \"gate\" function for LSTM to enable the values of the gates towards 0 or 1. The motivation behind is a  flat region of the loss surface is likely to generalize well. It shows the experimental results are comparable or better than vanilla LSTM and much more robust to low-precision approximation and low-rank approximation.\n\nIn section 3.2, the paper claimed using a smaller temperature cannot guarantee the outputs to be close to the boundary. Is there any experimental evidence to show it's not working? It also claimed pushing output gate to 0/1 will drop the performance. It actually quite interesting because there are bunch of paper claimed output gate is not important for language modeling, e.g. https://openreview.net/pdf?id=HJOQ7MgAW . \n\nIn the sensitive analysis, what if apply rounding / low-rank for all the parameters? \n\nHow was this approach compare to binarynet https://arxiv.org/abs/1602.02830 ? Applying the same idea, but only for forget gate/ input gate. Also, can we apply this idea to the binarynet? \n\nOverall, I think it's an interesting paper but I feel it should compare with some simple baseline to binarized the gate function.  \n\nUpdates: Thanks a lot for all the clarification. It do improve the paper quality but I'm still thinking it's higher than \"6\" but lower than \"7\". To me, improve ppl from \"52.8\" to \"52.1\" isn't very significant. For WMT, it improve on DE->EN but not for EN->DE (although it improve both for the author's own baseline). So I'm not fully convinced this approach could improve the generalization. But I feel this work can have many other applications such as \"binarynet\". ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642395115, "id": "ICLR.cc/2018/Conference/-/Paper118/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper118/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper118/AnonReviewer1", "ICLR.cc/2018/Conference/Paper118/AnonReviewer3", "ICLR.cc/2018/Conference/Paper118/AnonReviewer2"], "reply": {"forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642395115}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642395214, "tcdate": 1511683954491, "number": 1, "cdate": 1511683954491, "id": "S15OPlugz", "invitation": "ICLR.cc/2018/Conference/-/Paper118/Official_Review", "forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "signatures": ["ICLR.cc/2018/Conference/Paper118/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "The technical novelty is limited and experiments do not show much benefits of the proposed model.", "rating": "4: Ok but not good enough - rejection", "review": "This paper aims to push the LSTM gates to be binary. To achieve this, the paper proposes to employ the recent Gumbel-Softmax trick to obtain end-to-end trainable categorical distribution (taking 0 or 1 value). The resulted G2-LSTM is applied for language model and machine translation in the experiments. \n\nThe novelty of this paper is limited. Just directly apply the Gumbel-Softmax trick. \n\nThe motivation is not explained clearly and convincingly. Why need to pursue binary gates? According to the paper, it may give better generalization performance. But there is no theoretical or experimental evidence provided by this paper to support this argument. \n\nThe results of the new G2-LSTM are not significantly better than baselines in the experiments.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642395115, "id": "ICLR.cc/2018/Conference/-/Paper118/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper118/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper118/AnonReviewer1", "ICLR.cc/2018/Conference/Paper118/AnonReviewer3", "ICLR.cc/2018/Conference/Paper118/AnonReviewer2"], "reply": {"forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642395115}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642395174, "tcdate": 1511828226587, "number": 2, "cdate": 1511828226587, "id": "Syo-smqgf", "invitation": "ICLR.cc/2018/Conference/-/Paper118/Official_Review", "forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "signatures": ["ICLR.cc/2018/Conference/Paper118/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Interesting, but not impressive", "rating": "6: Marginally above acceptance threshold", "review": "The paper argues for pushing the input and forget gate\u2019s output toward 0 or 1, i.e., the LSTM tends to reside in flat region of surface loss, which is likely to generalize well. To achieve that, the sigmoid function in the original LSTM is replaced by a function G that is continuous and differentiable with respect to the parameters (by applying the Gumbel-Softmax trick). As a result, the model is still differentiable while the output gate is approximately binarized.  \n\nPros:\n-\tThe paper is clearly written\n-\tThe method is new and somehow theoretically guaranteed by the proof of the Proposition 1\n-\tThe experiments are clearly explained with detailed configurations\n-\tThe performance of the method in the model compression task is promising \n\nCons:\n-\tThe \u201csimple deduction\u201d which states that pushing the gate values toward 0 or 1 correspond to the region of the overall loss surface may need more theoretical analysis\n-\tIt is confusing whether the output of the gate is sampled based on or computed directly by the function G  \n-\tThe experiments lack many recent baselines on the same dataset (Penn Treebank: Melis et al. (2017) \u2013 On the State of the Art of Evaluation in Neural Language Models; WMT: Ashish et.al. (2017) \u2013 Attention Is All You Need) \n-\tThe experiment\u2019s result is only slightly better than the baseline\u2019s\n-\tTo be more persuasive, the author should include in the baselines other method that can \u201cbinerize\u201d the gate values such as the one sharpening the sigmoid function. \n\n\nIn short, this work is worth a read. Although the experimental results are not quite persuasive, the method is nice and promising. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642395115, "id": "ICLR.cc/2018/Conference/-/Paper118/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper118/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper118/AnonReviewer1", "ICLR.cc/2018/Conference/Paper118/AnonReviewer3", "ICLR.cc/2018/Conference/Paper118/AnonReviewer2"], "reply": {"forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642395115}}}, {"tddate": null, "ddate": null, "tmdate": 1515040888253, "tcdate": 1515040796829, "number": 4, "cdate": 1515040796829, "id": "rJBQl4j7z", "invitation": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "forum": "rJiaRbk0-", "replyto": "rJiaRbk0-", "signatures": ["ICLR.cc/2018/Conference/Paper118/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper118/Authors"], "content": {"title": "Revision to the paper (to all reviewers) : updates on experimental results", "comment": "Thanks all reviewers for their valuable comments, we updated a new version of the paper by including the following results:\n\n1. We make discussion about the sharpening sigmoid method proposed by the reviewers, and add the algorithm as one of the baselines in the experiments. The experimental results still show that our proposed method achieves the best performance in all tasks.\n\n2. We update the experimental results on language modelling task which achieves the best performance (52.1) as far as we know without using any hyperparameter search method.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738966, "id": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJiaRbk0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper118/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper118/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper118/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper118/Reviewers", "ICLR.cc/2018/Conference/Paper118/Authors", "ICLR.cc/2018/Conference/Paper118/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738966}}}, {"tddate": null, "ddate": null, "tmdate": 1515039729782, "tcdate": 1514376995142, "number": 2, "cdate": 1514376995142, "id": "BJj7JzW7G", "invitation": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "forum": "rJiaRbk0-", "replyto": "Syo-smqgf", "signatures": ["ICLR.cc/2018/Conference/Paper118/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper118/Authors"], "content": {"title": "Thanks for the relevant comments. We have improved PTB results according to the suggestions.", "comment": "\n[Regarding the computation of function G]\n\nDuring training, the output of the gate is computed directly by function G, while the function G contains some random noise U.\n\n[Regarding the sharpened sigmoid function experiment]\n\nThanks for figure this out. First, we want to point out that theoretically it doesn\u2019t help: Simply consider function f_{W,b}(x) =sigmoid((Wx+b)/tau), where tau is the temperature, it is computationally equivalent to f_{W\u2019,b\u2019}(x) =sigmoid(W\u2019x+b\u2019) by setting W\u2019=W/tau and b\u2019 = b/tau. Then using a small temperature is equivalent to rescale the initial parameter as well as gradient to a larger range. Usually, setting an initial point in a larger range with a larger learning rate will harm the optimization process.\n\nWe also did a set of experiments and updated the paper to show it doesn\u2019t help in practice.\n\n[Regarding the significance of experimental results]\n\nFor machine translation, we achieved the SOTA performance on German->English task and the improvement is significate (+ about 1 point) in the field of translation, not to mention that our model is much better than some other submissions https://openreview.net/forum?id=HktJec1RZ. For English->German task, we noticed that \u201cAttention is all you need\u201d is the state of the art but it is not LSTM-based; thus we didn\u2019t list that result in the paper.\n\nFor language model, thanks for the reference, we have studied the papers. By leveraging several tricks in literature, we significantly improve the performance from 77.4 to 52.1 (the best number as far as we know) without using any hyperparameter search method, we reported the detail in the paper. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738966, "id": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJiaRbk0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper118/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper118/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper118/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper118/Reviewers", "ICLR.cc/2018/Conference/Paper118/Authors", "ICLR.cc/2018/Conference/Paper118/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738966}}}, {"tddate": null, "ddate": null, "tmdate": 1515039702155, "tcdate": 1514376846821, "number": 1, "cdate": 1514376846821, "id": "BJvcAWbQG", "invitation": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "forum": "rJiaRbk0-", "replyto": "HyA3jBqgG", "signatures": ["ICLR.cc/2018/Conference/Paper118/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper118/Authors"], "content": {"title": "Thanks for the relevant comments. Here are the responses to the questions.", "comment": "[Regarding the small temperature experiment]\n\nThanks for figure this out. First, we want to point out that theoretically it doesn\u2019t help: Simply consider function f_{W,b}(x) =sigmoid((Wx+b)/tau), where tau is the temperature, it is computationally equivalent to f_{W\u2019,b\u2019}(x) =sigmoid(W\u2019x+b\u2019) by setting W\u2019=W/tau and b\u2019 = b/tau. Then using a small temperature is equivalent to rescale the initial parameter as well as gradient to a larger range. Usually, setting an initial point in a larger range with a larger learning rate will harm the optimization process.\n\nWe also did a set of experiments and updated the paper to show it doesn\u2019t help in practice.\n\n[Regarding the binary net]\n\nDespite the different between the model structure (gate-based LSTM v.s. CNN), the main difference is that we regularize the output of the activation of the gates to binary value only, but not to regularize the weights. One should notice that the accuracy of Binary Net is usually much worse than the baseline model. However, we show that (1) Our models generalize well among different tasks. (2) The accuracy of the models after low-rank/low-precision compression using our method is competitive to (or even better than) the baseline. Besides, our techniques can also be applied to binarynet training.\n\n[Regarding apply rounding / low-rank for all the parameters]\n\nWe will do the experiment but as our proposed method is focusing on LSTM unit. We are not sure whether the performance will drop a lot when we apply rounding/low-rank to embedding and attention."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738966, "id": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJiaRbk0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper118/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper118/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper118/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper118/Reviewers", "ICLR.cc/2018/Conference/Paper118/Authors", "ICLR.cc/2018/Conference/Paper118/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738966}}}, {"tddate": null, "ddate": null, "tmdate": 1514377291890, "tcdate": 1514377291890, "number": 3, "cdate": 1514377291890, "id": "rJ4UxzbQz", "invitation": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "forum": "rJiaRbk0-", "replyto": "S15OPlugz", "signatures": ["ICLR.cc/2018/Conference/Paper118/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper118/Authors"], "content": {"title": "We respectfully disagree with the comment about the novelty/experimental results of the paper. ", "comment": "\n[Regarding the experiment]\n\nWe are afraid that the reviewer makes a wrong judgement to the performance results, our model is much better than the baseline on two tasks. \n\nFor machine translation, we achieved the SOTA performance on German->English task and the improvement is significate (+ about 1 point) in the field of translation, not to mention that our model is much better than some other submissions https://openreview.net/forum?id=HktJec1RZ. \n\nFor language model, by leveraging several tricks in literature, we significantly improve the performance from 77.4 to 52.1 (the best number as far as we know). This number is achieved without using any hyperparameter search method, we reported the detail in the paper. \n\n[Regarding the motivation]\n\nWe have discussed in section 2.1 that there are a bunch of work empirically and theoretically studying the relationship between flat loss surface and generalization, not to mention that there are some continuous study and verification in ICLR 2018 submissions, e.g., https://openreview.net/forum?id=HkmaTz-0W . Thus our method is well motivated: by pushing the softmax operator towards its flat region will lead to better generalization. \n\n[Regarding the novelty of the paper]\n\nWe are regretful to see the reviewer claims that there is little novelty in the paper. First, we are the first to apply Gumbel-softmax trick for robust training of LSTM by pushing the value of the gate to the boundary.  We empirically show that our method achieves better accuracy even achieves the SOTA performance in some tasks. Second, we show that by different low-precision/low-rank compressions, our model is even still comparable to the baseline models before compressions. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Binary-Valued Gates for Robust LSTM Training ", "abstract": "Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.", "pdf": "/pdf/34e6b955f30f41fb5889241b09635755c0787952.pdf", "TL;DR": "We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.", "paperhash": "li|towards_binaryvalued_gates_for_robust_lstm_training", "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Wei Chen", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "keywords": ["recurrent neural network", "LSTM", "long-short term memory network", "machine translation", "generalization"], "authorids": ["lizhuohan@pku.edu.cn", "di_he@pku.edu.cn", "fetia@microsoft.com", "wche@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "_bibtex": "@misc{\nhe2018towards,\ntitle={Towards Binary-Valued Gates for Robust {LSTM} Training },\nauthor={Di He and Zhuohan Li and Fei Tian and Wei Chen and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2018},\nurl={https://openreview.net/forum?id=rJiaRbk0-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825738966, "id": "ICLR.cc/2018/Conference/-/Paper118/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJiaRbk0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper118/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper118/Authors|ICLR.cc/2018/Conference/Paper118/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper118/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper118/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper118/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper118/Reviewers", "ICLR.cc/2018/Conference/Paper118/Authors", "ICLR.cc/2018/Conference/Paper118/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825738966}}}], "count": 9}