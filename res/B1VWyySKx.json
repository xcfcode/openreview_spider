{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1492301139297, "tcdate": 1487363836149, "number": 128, "id": "B1VWyySKx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "B1VWyySKx", "signatures": ["~Jose_Sotelo1"], "readers": ["everyone"], "content": {"title": "Char2Wav: End-to-End Speech Synthesis", "abstract": "We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoder-decoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "pdf": "/pdf/59414e84d16bd514c3c5116a27a947fa9997817e.pdf", "TL;DR": "Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "paperhash": "sotelo|char2wav_endtoend_speech_synthesis", "keywords": ["Speech", "Deep learning", "Applications"], "conflicts": ["umontreal.ca", "inrs.ca", "iitk.ac.in"], "authors": ["Jose Sotelo", "Soroush Mehri", "Kundan Kumar", "Joao Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "authorids": ["rdz.sotelo@gmail.com", "soroush.mehris@umontreal.ca", "kundankumar2510@gmail.com", "kastnerkyle@gmail.com", "jfsantos@emt.inrs.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028616418, "tcdate": 1490028616418, "number": 1, "id": "ryeL_t6ox", "invitation": "ICLR.cc/2017/workshop/-/paper128/acceptance", "forum": "B1VWyySKx", "replyto": "B1VWyySKx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Char2Wav: End-to-End Speech Synthesis", "abstract": "We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoder-decoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "pdf": "/pdf/59414e84d16bd514c3c5116a27a947fa9997817e.pdf", "TL;DR": "Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "paperhash": "sotelo|char2wav_endtoend_speech_synthesis", "keywords": ["Speech", "Deep learning", "Applications"], "conflicts": ["umontreal.ca", "inrs.ca", "iitk.ac.in"], "authors": ["Jose Sotelo", "Soroush Mehri", "Kundan Kumar", "Joao Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "authorids": ["rdz.sotelo@gmail.com", "soroush.mehris@umontreal.ca", "kundankumar2510@gmail.com", "kastnerkyle@gmail.com", "jfsantos@emt.inrs.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028616958, "id": "ICLR.cc/2017/workshop/-/paper128/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1VWyySKx", "replyto": "B1VWyySKx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028616958}}}, {"tddate": null, "tmdate": 1489460296015, "tcdate": 1489460296015, "number": 2, "id": "H1x82RNog", "invitation": "ICLR.cc/2017/workshop/-/paper128/public/comment", "forum": "B1VWyySKx", "replyto": "B1JnhKlsx", "signatures": ["~Jose_Sotelo1"], "readers": ["everyone"], "writers": ["~Jose_Sotelo1"], "content": {"title": "Thank you for your comments!", "comment": "Thank you for your comments and for your review!\n\nSince we did not want to mislead the public about generated samples,  we did not add samples of the neural vocoder using true vocoder features. However, we added them after reading your review as this might help judge/attribute where the different artifacts are coming from. The section is called:\n'Neural Vocoder using ground-truth vocoder parameters'.\n\nAs you mention in your comment, samplernn adds some kind of background noise when generating the audio. We are now working on ways to solve this issue.\n\n(A personal note: I thought that wavenets produced this kind of artifact as well. I checked their samples some time ago and it was clear to me that they were also having this issue. I listened to them just right now and I couldn't hear this white-looking noise anymore. It's still a bit there, but not as much as I remember. Maybe they solved this in some way (reducing the temperature when sampling or some other thing) or maybe my memory is biased.\nHere's an example of their samples:\nhttps://storage.googleapis.com/deepmind-media/pixie/us-english/wavenet-1.wav )\n\nThis is a proof of concept work and we did not do a comprehensive hyperparameter optimization. We are currently working on this and hopefully results will be ready for the conference. Another thing to note is that we train on relatively small datasets (<10 h audio). So probably these results can be improved by using a larger dataset.\n\nThanks again for your review and we hope that this comments provided more information!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Char2Wav: End-to-End Speech Synthesis", "abstract": "We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoder-decoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "pdf": "/pdf/59414e84d16bd514c3c5116a27a947fa9997817e.pdf", "TL;DR": "Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "paperhash": "sotelo|char2wav_endtoend_speech_synthesis", "keywords": ["Speech", "Deep learning", "Applications"], "conflicts": ["umontreal.ca", "inrs.ca", "iitk.ac.in"], "authors": ["Jose Sotelo", "Soroush Mehri", "Kundan Kumar", "Joao Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "authorids": ["rdz.sotelo@gmail.com", "soroush.mehris@umontreal.ca", "kundankumar2510@gmail.com", "kastnerkyle@gmail.com", "jfsantos@emt.inrs.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487363837132, "tcdate": 1487363837132, "id": "ICLR.cc/2017/workshop/-/paper128/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper128/reviewers"], "reply": {"forum": "B1VWyySKx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487363837132}}}, {"tddate": null, "tmdate": 1489195860185, "tcdate": 1489195860185, "number": 1, "id": "BkhI7Clix", "invitation": "ICLR.cc/2017/workshop/-/paper128/public/comment", "forum": "B1VWyySKx", "replyto": "rJJjwqgie", "signatures": ["~Jose_Sotelo1"], "readers": ["everyone"], "writers": ["~Jose_Sotelo1"], "content": {"title": "Thank you for your comments!", "comment": "First of all, thank you for reading our paper and for your comments.\n\nUnfortunately 3 pages it's a bit constraining, so we could not add as much details as we would have liked. We are working on a longer arxiv version that should clarify all the details.\n\nWe have received several comments regarding the end-to-end issue. The main reason why we went for vocoder processing is because of computational efficiency. Basically, it reduces the time dimension of the problem by 80. However, we agree that it would be even better if we can train without it and we are currently exploring alternatives to do that."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Char2Wav: End-to-End Speech Synthesis", "abstract": "We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoder-decoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "pdf": "/pdf/59414e84d16bd514c3c5116a27a947fa9997817e.pdf", "TL;DR": "Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "paperhash": "sotelo|char2wav_endtoend_speech_synthesis", "keywords": ["Speech", "Deep learning", "Applications"], "conflicts": ["umontreal.ca", "inrs.ca", "iitk.ac.in"], "authors": ["Jose Sotelo", "Soroush Mehri", "Kundan Kumar", "Joao Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "authorids": ["rdz.sotelo@gmail.com", "soroush.mehris@umontreal.ca", "kundankumar2510@gmail.com", "kastnerkyle@gmail.com", "jfsantos@emt.inrs.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487363837132, "tcdate": 1487363837132, "id": "ICLR.cc/2017/workshop/-/paper128/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper128/reviewers"], "reply": {"forum": "B1VWyySKx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487363837132}}}, {"tddate": null, "tmdate": 1489180566569, "tcdate": 1489180566569, "number": 2, "id": "rJJjwqgie", "invitation": "ICLR.cc/2017/workshop/-/paper128/official/review", "forum": "B1VWyySKx", "replyto": "B1VWyySKx", "signatures": ["ICLR.cc/2017/workshop/paper128/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper128/AnonReviewer2"], "content": {"title": "TTS!", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper combines several ideas (seq2seq+attention, SampleRNN) to do TTS!\n\nThis reviewer is a little bit lost following the equations, especially where did \\phi come from (Eqn 6)? I know there is a space constraint, but more details are needed for the full paper version.\n\nThe model requires pre-training the reader and neural vocoder separately -- would be more impressive if this was trained end-to-end from scratch.\n\nOverall -- this paper is still good progress towards end-to-end TTS. I feel the title is still a bit misleading since the model isn't truely trained end-to-end... but the term end2end seems so overloaded in our community now...", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Char2Wav: End-to-End Speech Synthesis", "abstract": "We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoder-decoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "pdf": "/pdf/59414e84d16bd514c3c5116a27a947fa9997817e.pdf", "TL;DR": "Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "paperhash": "sotelo|char2wav_endtoend_speech_synthesis", "keywords": ["Speech", "Deep learning", "Applications"], "conflicts": ["umontreal.ca", "inrs.ca", "iitk.ac.in"], "authors": ["Jose Sotelo", "Soroush Mehri", "Kundan Kumar", "Joao Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "authorids": ["rdz.sotelo@gmail.com", "soroush.mehris@umontreal.ca", "kundankumar2510@gmail.com", "kastnerkyle@gmail.com", "jfsantos@emt.inrs.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489180567237, "id": "ICLR.cc/2017/workshop/-/paper128/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper128/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper128/AnonReviewer1", "ICLR.cc/2017/workshop/paper128/AnonReviewer2"], "reply": {"forum": "B1VWyySKx", "replyto": "B1VWyySKx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper128/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper128/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489180567237}}}, {"tddate": null, "tmdate": 1489177766948, "tcdate": 1489177766948, "number": 1, "id": "B1JnhKlsx", "invitation": "ICLR.cc/2017/workshop/-/paper128/official/review", "forum": "B1VWyySKx", "replyto": "B1VWyySKx", "signatures": ["ICLR.cc/2017/workshop/paper128/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper128/AnonReviewer1"], "content": {"title": "Proof of concept neural TTS", "rating": "7: Good paper, accept", "review": "The paper proposes a neural TTS system in which the synthesis happens in two stages. First, vocoder features are predicted by an attention-based seq2seq network. Second, the Vocoder features are synthesized into speech using a hierarchical RNN. The approach combines recent developments in waveform generation (Wavenet, SampleRNN) and simplifies the extraction of vocoder features: a single attention-based RNN replaces a pipeline that typically used a duration prediction RNN that does the alignment and a acoustic features prediction RNN that upsamples the phoneme/character sequence according to the duration prediction network.\n\nThe provided samples demonstrate the feasibility of the approach. However, many artefacts are present. Unfortunately, error attribution (frontend/backend network) is difficult because most of the samples use vocoder output, and (please correct me it I am wrong), no samples are generated using the SampleRNN over ground-truth vocoder features.\n1. Only Spanish samples use the SampleRNN output, for all other languages the network predicts vocoder features that are synthesized using the vocoder. Comparing the samples \"Reader over characters with vocoder output\" and \"Char2Wav\" shows that the SampleRNN introduces many artefacts and is in general inferior to the Vocoder.\n2. The frontend network produces slightly better features when applied to phonemes that to characters. The durations produced are unnatural, however this should be fixable since other LSTM-based approaches such as [Zen et al STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS] are used in production systems.\n\nDespite the deficiencies, I recommend acceptance if only to show the feasibility of assembling a TTS system using powerful recurrent neural networks and relatively limited resources.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Char2Wav: End-to-End Speech Synthesis", "abstract": "We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoder-decoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "pdf": "/pdf/59414e84d16bd514c3c5116a27a947fa9997817e.pdf", "TL;DR": "Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.", "paperhash": "sotelo|char2wav_endtoend_speech_synthesis", "keywords": ["Speech", "Deep learning", "Applications"], "conflicts": ["umontreal.ca", "inrs.ca", "iitk.ac.in"], "authors": ["Jose Sotelo", "Soroush Mehri", "Kundan Kumar", "Joao Felipe Santos", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio"], "authorids": ["rdz.sotelo@gmail.com", "soroush.mehris@umontreal.ca", "kundankumar2510@gmail.com", "kastnerkyle@gmail.com", "jfsantos@emt.inrs.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489180567237, "id": "ICLR.cc/2017/workshop/-/paper128/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper128/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper128/AnonReviewer1", "ICLR.cc/2017/workshop/paper128/AnonReviewer2"], "reply": {"forum": "B1VWyySKx", "replyto": "B1VWyySKx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper128/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper128/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489180567237}}}], "count": 6}