{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028568686, "tcdate": 1490028568686, "number": 1, "id": "SJbXuFajx", "invitation": "ICLR.cc/2017/workshop/-/paper46/acceptance", "forum": "BJcib5mFe", "replyto": "BJcib5mFe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delving into adversarial attacks on deep policies", "abstract": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.", "pdf": "/pdf/1889b4bf322f25c1e4809ee3ce7628eb47cb17ff.pdf", "TL;DR": "Study into adversarial attacks on deep reinforcemnt learning policies.", "paperhash": "kos|delving_into_adversarial_attacks_on_deep_policies", "conflicts": ["berkeley.edu"], "keywords": [], "authors": ["Jernej Kos", "Dawn Song"], "authorids": ["jernej@kos.mx", "dawnsong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028569207, "id": "ICLR.cc/2017/workshop/-/paper46/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJcib5mFe", "replyto": "BJcib5mFe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028569207}}}, {"tddate": null, "tmdate": 1489383868365, "tcdate": 1489383868365, "number": 2, "id": "SyVa-27se", "invitation": "ICLR.cc/2017/workshop/-/paper46/official/review", "forum": "BJcib5mFe", "replyto": "BJcib5mFe", "signatures": ["ICLR.cc/2017/workshop/paper46/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper46/AnonReviewer1"], "content": {"title": "Interesting investigation in efficiency of adversarial attacks on a DRL agent", "rating": "7: Good paper, accept", "review": "This work tests the robustness of DRL policies to adversarial examples. It also proposes a new method of reducing the number of adversarial 'injections' needed to disrupt the policy.\n\nMost of the results seem expected and in-line with the existing results on FGSM adversarial examples. For example, it is well known that FGSM adversarial examples have a much larger effect on image classifiers than examples with random noise added. However, using the value function to decide when to inject the perturbation for the highest effectiveness seems like a nice contribution and paves the way for most advanced injection timing algorithms. Overall, this seems like a nice contribution for the workshop.\n\nPros:\n- Nice idea for timing adversarial injections.\n- Good experiments and plots.\n\nCons:\n- It would be interesting to see if the VF method could result in much fewer injections compared to the attack in Fig. 1b.\n- Other results seem expected and not particularly novel.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delving into adversarial attacks on deep policies", "abstract": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.", "pdf": "/pdf/1889b4bf322f25c1e4809ee3ce7628eb47cb17ff.pdf", "TL;DR": "Study into adversarial attacks on deep reinforcemnt learning policies.", "paperhash": "kos|delving_into_adversarial_attacks_on_deep_policies", "conflicts": ["berkeley.edu"], "keywords": [], "authors": ["Jernej Kos", "Dawn Song"], "authorids": ["jernej@kos.mx", "dawnsong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489383869118, "id": "ICLR.cc/2017/workshop/-/paper46/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper46/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper46/AnonReviewer2", "ICLR.cc/2017/workshop/paper46/AnonReviewer1"], "reply": {"forum": "BJcib5mFe", "replyto": "BJcib5mFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper46/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper46/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489383869118}}}, {"tddate": null, "tmdate": 1489337030590, "tcdate": 1489337030590, "number": 1, "id": "rJ10qgmjg", "invitation": "ICLR.cc/2017/workshop/-/paper46/public/comment", "forum": "BJcib5mFe", "replyto": "ryAsJK1sl", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "writers": ["~Jernej_Kos1"], "content": {"title": "Reply", "comment": "Thank you for your review!\n\nWe would like to point out that the paper by Huang et al. is not from 2016, but was posted on arXiv the 8th of February 2017 and we noticed it just slightly before submitting to ICLR on 16th of February. Their paper has also been submitted to ICLR 2017 workshop track (https://openreview.net/forum?id=ryvlRyBKl). So both works are concurrent and independent and we've updated the introduction to make this more clear.\n\nAlso, as far as the analysis of the experimental results goes, the page limit unfortunately prevented us from including any more substantial analysis."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delving into adversarial attacks on deep policies", "abstract": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.", "pdf": "/pdf/1889b4bf322f25c1e4809ee3ce7628eb47cb17ff.pdf", "TL;DR": "Study into adversarial attacks on deep reinforcemnt learning policies.", "paperhash": "kos|delving_into_adversarial_attacks_on_deep_policies", "conflicts": ["berkeley.edu"], "keywords": [], "authors": ["Jernej Kos", "Dawn Song"], "authorids": ["jernej@kos.mx", "dawnsong@gmail.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487278499272, "tcdate": 1487278499272, "id": "ICLR.cc/2017/workshop/-/paper46/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper46/reviewers"], "reply": {"forum": "BJcib5mFe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487278499272}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1489336852962, "tcdate": 1487278498391, "number": 46, "id": "BJcib5mFe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "BJcib5mFe", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "content": {"title": "Delving into adversarial attacks on deep policies", "abstract": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.", "pdf": "/pdf/1889b4bf322f25c1e4809ee3ce7628eb47cb17ff.pdf", "TL;DR": "Study into adversarial attacks on deep reinforcemnt learning policies.", "paperhash": "kos|delving_into_adversarial_attacks_on_deep_policies", "conflicts": ["berkeley.edu"], "keywords": [], "authors": ["Jernej Kos", "Dawn Song"], "authorids": ["jernej@kos.mx", "dawnsong@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489108902383, "tcdate": 1489108902383, "number": 1, "id": "ryAsJK1sl", "invitation": "ICLR.cc/2017/workshop/-/paper46/official/review", "forum": "BJcib5mFe", "replyto": "BJcib5mFe", "signatures": ["ICLR.cc/2017/workshop/paper46/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper46/AnonReviewer2"], "content": {"title": "Official Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper continues on the work of Huang(2016), investigating further possible adversarial attacks on deep reinforcement learning policies. It also tests the performance of adversarial training on the robustification of the learning. Different from in the standard supervised learning setting, this paper,  to my best knowledge for the first time in the literature,  also looks into the question when to attack in the sequential learning setting. In general, the results of the paper are as expected, similar to the ones in the supervised learning setting.  \n\nThe questions that are investigated in the paper are important, but also to be expected in the sense that parallel questions have been investigated in the supervised learning setting. One significant contribution of the paper is the investigation of the \u2018low frequency\u2019 adversarial attacks that is unique in sequential learning. The paper is well written and easy to follow. \n\nPros:\n1. The highlight of the paper is the investigation of the \u2018low frequency\u2019 adversarial attacks\n2. Although in general  the questions and the results of this paper are as expected, these questions are important for further investigations.\n\nCons:\n1. This paper seems to be lack of novelty. Experimental results are reported with no insightful analyses.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delving into adversarial attacks on deep policies", "abstract": "Adversarial examples have been shown to exist for a variety of deep learning architectures. Deep reinforcement learning has shown promising results on training agent policies directly on raw inputs such as image pixels. In this paper we present a novel study into adversarial attacks on deep reinforcement learning polices. We compare the effectiveness of the attacks using adversarial examples vs. random noise. We present a novel method for reducing the number of times adversarial examples need to be injected for a successful attack, based on the value function. We further explore how re-training on random noise and FGSM perturbations affects the resilience against adversarial examples.", "pdf": "/pdf/1889b4bf322f25c1e4809ee3ce7628eb47cb17ff.pdf", "TL;DR": "Study into adversarial attacks on deep reinforcemnt learning policies.", "paperhash": "kos|delving_into_adversarial_attacks_on_deep_policies", "conflicts": ["berkeley.edu"], "keywords": [], "authors": ["Jernej Kos", "Dawn Song"], "authorids": ["jernej@kos.mx", "dawnsong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489383869118, "id": "ICLR.cc/2017/workshop/-/paper46/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper46/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper46/AnonReviewer2", "ICLR.cc/2017/workshop/paper46/AnonReviewer1"], "reply": {"forum": "BJcib5mFe", "replyto": "BJcib5mFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper46/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper46/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489383869118}}}], "count": 5}