{"notes": [{"id": "BYmFTEBshj", "original": null, "number": 1, "cdate": 1578643190867, "ddate": null, "tcdate": 1578643190867, "tmdate": 1578661182782, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Public_Comment", "content": {"comment": "Hi,\nCongratulations on your work! \n\nI was going through your paper and on page 5, section 3.1 Base Model, it is written  x0, x1, x2, ....... xL as post non-linearity activations and z1, z2, z3 .....zL as pre non linearity activations.\n\nI think it should be other way around. I am not confident on this, but I think this is a possible error on your part, or I might be interpreting things differently.\n\n@Authors: Please let me know your thoughts on this edit.\n\nThanks!", "title": "Great paper | Possible incorrect pairing of variables names"}, "signatures": ["~Deepak_Yadav1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Deepak_Yadav1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311789780, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HkNDsiC9KQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311789780}}}, {"id": "HkNDsiC9KQ", "original": "H1lm8tCKYm", "number": 632, "cdate": 1538087839326, "ddate": null, "tcdate": 1538087839326, "tmdate": 1550885450562, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xX9W7lgN", "original": null, "number": 1, "cdate": 1544724874555, "ddate": null, "tcdate": 1544724874555, "tmdate": 1545354473506, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Meta_Review", "content": {"metareview": "The reviewers all agree that the idea is interesting, the writing clear and the experiments sufficient. \n\nTo improve the paper, the authors should consider better discussing their meta-objective and some of the algorithmic choices. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Oral)", "title": "Well written paper with an interesting idea"}, "signatures": ["ICLR.cc/2019/Conference/Paper632/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper632/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353144481, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper632/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper632/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper632/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353144481}}}, {"id": "SJeJvkj5hX", "original": null, "number": 2, "cdate": 1541218135062, "ddate": null, "tcdate": 1541218135062, "tmdate": 1543276451104, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Official_Review", "content": {"title": "Novel idea of learning rules for unsupervised learning, need more theory/evidences on what/why meta objectives are sufficient for learning the unsupervised learning rules", "review": "This work brings a novel meta-learning approach that learns unsupervised learning rules for learning representations across different modalities, datasets, input permutation, and neural network architectures. The meta-objectives consist of few shot learning scores from several supervised tasks. The idea of using meta-objectives to learn unsupervised representation learning is a very interesting idea.\n\nAuthors mentioned that the creation of an unsupervised update rule is treated as a transfer learning problem, and this work is focused on learning a learning algorithm as opposed to structures of feature extractors. Can you elaborate on what aspect of learning rules and why they can be transferable among different modalities and datasets? For this type of meta-learning to be successful, can you discuss the requirements on the type of meta-objectives? Besides saving computational cost, does using smaller input dimensions favor your method over reconstruction type of semi-supervised learning, e.g. VAE?\n\nIn the section \"generalizing over datasets and domains\", the accuracy of supervised methods and VAE method are very close. This indicates those datasets may not be ideal to evaluate semi-supervised training.\n\nIn the section \"generalizing over network architectures\", what is the corresponding supervised/VAE learning accuracy?\n\nIn the experimentation section, can you describe in more details how input permutations are conducted? Are they re-sampled for each training session for tasks? If the input permutations are not conducted, will the comparison between this method, supervised and VAE be different?\n\nAfter reviewing the author response, I adjusted the rating up to focus more on novelty and less on polished results.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper632/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Official_Review", "cdate": 1542234415326, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper632/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335768179, "tmdate": 1552335768179, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper632/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJgRNO1Kp7", "original": null, "number": 3, "cdate": 1542154293932, "ddate": null, "tcdate": 1542154293932, "tmdate": 1542154293932, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "r1eIOmju3m", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your thoughtful review! Comments below:\n\n\"The section 5.4 is a bit hard to understand, with very very small images.\"\nWe apologize for the lack of clarity. We will improve this section and will increase the image size!\n\n\"cons only very modestly better than other methods. I would like to get a feel for why VAE is so good tbh (though the authors show that VAE has a problem with objective function mismatch).\"\nIn generative modeling, understanding what design principles lead to reusable representations is a huge open field of study, but many people have promoted compositional generative models[1,2] and information theoretic measures of how well the model captures structure in the data [3,4]. VAEs possess both of these attributes.\n\n\"One comment: the update rule takes as inputs pre and post activity and a backpropagated error; it seems natural to also use the local gradient of the neuron's transfer function here, as many three or four factor learning rules do.\"\nThis is a great suggestion! Thanks.\n\n[1]Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. 2013.\n[2] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013).\n[2]Hinton, Geoffrey E., et al. \"The\" wake-sleep\" algorithm for unsupervised neural networks.\" Science 268.5214 (1995): 1158-1161.\n[3]Roweis, Sam T. \"EM algorithms for PCA and SPCA.\" Advances in neural information processing systems. 1998."}, "signatures": ["ICLR.cc/2019/Conference/Paper632/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper632/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624111, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkNDsiC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper632/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper632/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper632/Authors|ICLR.cc/2019/Conference/Paper632/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624111}}}, {"id": "HJeD-O1Yam", "original": null, "number": 2, "cdate": 1542154239273, "ddate": null, "tcdate": 1542154239273, "tmdate": 1542154239273, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "SJeJvkj5hX", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your thoughtful review! Comments below:\n\n\"Can you elaborate on what aspect of learning rules and why they can be transferable among different modalities and datasets?\"\nThis is a hypothesis based on the observation that hand designed learning rules transfer across modalities and datasets. We structure our learning rule in such a way as to aid this generalization. The specifics are largely inspired by biological neural networks--for instance the use of a neuron-local learning rule, and by the challenges associated with making meta-training stable--for instance, the use of normalization in almost every part of the system was found to be necessary to prevent meta-training from diverging. A better understanding of what aspects of learned learning rules transfer across datasets is a fascinating question and definitely something we are pursuing in future work.\n\n\"For this type of meta-learning to be successful, can you discuss the requirements on the type of meta-objectives?\"\nIn general, the meta-objective has to be easily tractable and have a well defined derivative with respect to the final layer (e.g. from backpropagation during meta-training). It should also reflect, as well as possible, performance on the eventual task. In our case, we wanted the base network to learn a representation in an unsupervised way which easily exposed class labels or other high level attributes, so we chose our meta-objective to reward few-shot learning performance using the unsupervised representation. In early experiments, we explored a number of variations on our eventual meta-objective (e.g. clustering and softmax regression). We found similar performance for these variants, and chose the meta-objective we describe in the paper (least squares) because we believed it to be the simplest.\n\n\"Besides saving computational cost, does using smaller input dimensions favor your method over reconstruction type of semi-supervised learning, e.g. VAE?\"\nWe only meta-train on the datasets with the smaller input size, but we test on both sizes (Figure 4). The VAE performance is comparable for the two input sizes, while the learned optimizer decreases in performance on mnist and remains constant on fashion mnist.\n\n\"In the section \"generalizing over network architectures\", what is the corresponding supervised/VAE learning accuracy?\"\nWe have not run these experiments, but we would expect the performance of the VAE to go up with increased model size.\n\n\"In the experimentation section, can you describe in more details how input permutations are conducted? Are they re-sampled for each training session for tasks? If the input permutations are not conducted, will the comparison between this method, supervised and VAE be different?\"\nThey are re-sampled for each new instantiation of an inner problem and kept constant while training that task. While we have not removed them, if we did we would expect the learned update rule to overfit to the meta-training distribution, causing improved performance on non-permuted image tasks, but extremely poor performance on permuted image tasks. Doing this would make comparisons to VAEs and supervised learning misleading however, as these two methods have no notion of spatial locality (whereas the learned optimizer now would). As a result, the learned optimizer\u2019s relative performance would probably be a lot stronger. It would be very interesting in future work to use convnets for the base model--both for the learned update rule and the baselines. However, doing so would be a fairly involved process, requiring changes to the architecture of the learned update rule.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper632/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper632/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624111, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkNDsiC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper632/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper632/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper632/Authors|ICLR.cc/2019/Conference/Paper632/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624111}}}, {"id": "r1eK0P1F67", "original": null, "number": 1, "cdate": 1542154192590, "ddate": null, "tcdate": 1542154192590, "tmdate": 1542154192590, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "Bkgckkbah7", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your thoughtful review! Comments below:\n\n\"Motivations are not very clear in some parts. E.g., the reason for learning backward weights (V), and the choice of meta-objective.\"\nOriginally, we did not learn backward weights, but in an effort to make the learning rule more biologically inspired we removed the transposed weights in favor of learned backward weights [1]. In practice, performance is surprisingly quite similar with both versions.\n\nAs per meta-objective: Exploring alternative meta-objectives would be very interesting! We choose the least squares meta-objective as it is allows us to compute the optimal final layer weights in closed form. This is important in that it allows us to easily differentiate the meta-objective with respect to the representation at the final layer (necessary for meta-training). We have explored alternative few-shot classification objectives (e.g. logistic regression, using implicit differentiation to get the appropriate derivative) but found performance to be similar and thus stuck with the simpler meta-objective.\n\n\"Experimental evaluation is limited to few-shot classification, which is very close to the meta-learning objective used in this paper. \"\nFor simplicity, we used the same meta-objective at evaluation time. The use of different meta-objectives (at both meta-train and meta-test) is also very interesting to us and is something we would pursue in future work.\n\n\"The result of text classification is interesting, but not so informative given no further analysis. E.g., why domain mismatch does not occur in this case?\"\nDomain mismatch does occur--just later in meta-training. Because we are learning a learning rule, as opposed to features, we expect some generalization, after all, hand designed learning rules generalize across datasets. We get some transfer performance early in meta-training, but the meta-objective on text tasks diverges later in training. We will add a few sentences to this effect. Better understanding out of domain generalization is definitely of interest to us and we are pursuing it in future work.\n\nPaper Title: This is a good point and we plan to change the paper title to: \"Meta-Learning Update Rules for Unsupervised Representation Learning\".\n\n\n[1] Crick, F. The recent excitement about neural networks. Nature 337, 129\u2013132 (1989)."}, "signatures": ["ICLR.cc/2019/Conference/Paper632/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper632/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624111, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkNDsiC9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper632/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper632/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper632/Authors|ICLR.cc/2019/Conference/Paper632/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper632/Reviewers", "ICLR.cc/2019/Conference/Paper632/Authors", "ICLR.cc/2019/Conference/Paper632/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624111}}}, {"id": "Bkgckkbah7", "original": null, "number": 3, "cdate": 1541373665936, "ddate": null, "tcdate": 1541373665936, "tmdate": 1541533823434, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Official_Review", "content": {"title": "an interesting approach to meta-learning, clear accept", "review": "This paper introduces a novel meta-learning approach to unsupervised representation learning where an update rule for a base model (i.e., an MLP) is meta-learned using a supervised meta-objective (i.e., a few-shot linear regression from the learned representation to classification GTs). Unlike previous approaches, it meta-learns an update rule by directly optimizing the utility of the unsupervised representation using the meta-objective. In the phase of unsupervised representation learning, the learned update rule is used for optimizing a base model without using any other base model objective. Experimental evaluations on few-shot classification demonstrate its generalization performance over different base architectures, datasets, and even domains.  \n\n+  Novel and interesting formulation of meta-learning by learning an unsupervised update rule for representation learning. \n+  Technically sound, and well organized overall with details documented in appendixes. \n+  Clearly written overall with helpful schematic illustrations and, in particular, a good survey of related work. \n+ Good generalization performance over different (larger and deeper) base models, activation functions, datasets, and even a different modality (text classification).\n\n-  Motivations are not very clear in some parts. E.g., the reason for learning backward weights (V), and the choice of meta-objective.  \n- Experimental evaluation is limited to few-shot classification, which is very close to the meta-learning objective used in this paper. \n- The result of text classification is interesting, but not so informative given no further analysis. E.g., why domain mismatch does not occur in this case?\n\nI enjoyed reading this paper, and happy to recommend it as a clear accept paper. The idea of meta-learning update networks looks a promising direction worth exploring, indeed. \nI hope the authors to clarify the things I mentioned above. Experimental results are enough considering the space limit, but not great. Since the current evaluation task is quite similar to the meta-objective, evaluations on more diverse tasks would strengthen this paper. \n\nFinally, this paper aims at unsupervised representation learning, but it\u2019s not clear from the current title, which is somewhat misleading. I think that's quite an important feature of this paper, so I highly recommend the authors to consider a more informative title, e.g., `Learning Rules for Unsupervised Representation Learning\u2019 or else. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper632/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Official_Review", "cdate": 1542234415326, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper632/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335768179, "tmdate": 1552335768179, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper632/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1eIOmju3m", "original": null, "number": 1, "cdate": 1541088110231, "ddate": null, "tcdate": 1541088110231, "tmdate": 1541533823018, "tddate": null, "forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper632/Official_Review", "content": {"title": "substantial step towards good unsupervised and local learning", "review": "The paper describes unsupervised learning as a meta-learning problem: the observation is that unsupervised learning rules are effectively supervised by the quality of the representations that they yield relative to subsequent later semi-supervised (or RL) learning. The learning-to-learning algorithm allows for learning network architecture parameters, and also 'network-in-networks' that determine the unsupervised learning signal based on pre and post activations. \n\nQuality \nThe proposed algorithm is well defined, and it is compared against relevant competing algorithms on relevant problems. \nThe results show that the algorithm is competitive with other approaches like VAE (very slightly outperforms).\n\nClarity\nThe paper is well written and clearly structured. The section 5.4 is a bit hard to understand, with very very small images. \n\nOriginality\nThere is an extensive literature on meta-learning, which is expanded upon in Appendix A. The main innovation in this work is the parametric update rule for outer loop updates, which does have some similarity to the old work by Bengio in 1990 and 1992. \n\nSignificance\n- pros clear and seemingly state-of-the-art results, intuitive approach, \n-cons only very modestly better than other methods. I would like to get a feel for why VAE is so good tbh (though the authors show that VAE has a problem with objective function mismatch).\n\nOne comment: the update rule takes as inputs pre and post activity and a backpropagated error; it seems natural to also use the local gradient of the neuron's transfer function here, as many three or four factor learning rules do. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper632/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Update Rules for Unsupervised Representation Learning", "abstract": "A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks.  Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.", "keywords": ["Meta-learning", "unsupervised learning", "representation learning"], "authorids": ["lmetz@google.com", "nirum@google.com", "bcheung@berkeley.edu", "jaschasd@google.com"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We learn an unsupervised learning algorithm that produces useful representations from a set of supervised tasks. At test-time, we apply this algorithm to new tasks without any supervision and show performance comparable to a VAE.", "pdf": "/pdf/e572a334fe17a077ba8892af92200f9e00dd2656.pdf", "paperhash": "metz|metalearning_update_rules_for_unsupervised_representation_learning", "_bibtex": "@inproceedings{\nmetz2018learning,\ntitle={Learning Unsupervised Learning Rules},\nauthor={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkNDsiC9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper632/Official_Review", "cdate": 1542234415326, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkNDsiC9KQ", "replyto": "HkNDsiC9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper632/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335768179, "tmdate": 1552335768179, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper632/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}