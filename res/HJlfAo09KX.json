{"notes": [{"id": "HJlfAo09KX", "original": "SJec3AK5Fm", "number": 875, "cdate": 1538087882198, "ddate": null, "tcdate": 1538087882198, "tmdate": 1545355415918, "tddate": null, "forum": "HJlfAo09KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy", "abstract": "We study model recovery for data classification, where the training labels are generated from a one-hidden-layer fully -connected neural network with sigmoid activations, and the goal is to recover the weight vectors of the neural network. We prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground truth without requiring a fresh set of samples at each iteration. To the best of our knowledge, this is the first global convergence guarantee established for the empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, at the near-optimal sample and computational complexity with respect to the network input dimension.", "keywords": ["cross entropy", "neural networks", "parameter recovery"], "authorids": ["fu.436@osu.edu", "yuejiechi@cmu.edu", "liang.889@osu.edu"], "authors": ["Haoyu Fu", "Yuejie Chi", "Yingbin Liang"], "TL;DR": "We provide the first theoretical analysis of guaranteed recovery of one-hidden-layer neural networks under cross entropy loss for classification problems.", "pdf": "/pdf/28df4734fd8a152b0297b6c078605ef964201dc3.pdf", "paperhash": "fu|guaranteed_recovery_of_onehiddenlayer_neural_networks_via_cross_entropy", "_bibtex": "@misc{\nfu2019guaranteed,\ntitle={Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy},\nauthor={Haoyu Fu and Yuejie Chi and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlfAo09KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Skg0kCeTyE", "original": null, "number": 1, "cdate": 1544519141576, "ddate": null, "tcdate": 1544519141576, "tmdate": 1545354498954, "tddate": null, "forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper875/Meta_Review", "content": {"metareview": "This paper shows local convergence results for gradient descent on one hidden layer network with Gaussian inputs and sigmoid activations. Later it shows global convergence by using spectral initialization. All the reviewers agree that the results are similar to existing work in the literature with little novelty. There are also some concerns about the correctness of the statements expressed by some reviewers. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "ICLR 2019 decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper875/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper875/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy", "abstract": "We study model recovery for data classification, where the training labels are generated from a one-hidden-layer fully -connected neural network with sigmoid activations, and the goal is to recover the weight vectors of the neural network. We prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground truth without requiring a fresh set of samples at each iteration. To the best of our knowledge, this is the first global convergence guarantee established for the empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, at the near-optimal sample and computational complexity with respect to the network input dimension.", "keywords": ["cross entropy", "neural networks", "parameter recovery"], "authorids": ["fu.436@osu.edu", "yuejiechi@cmu.edu", "liang.889@osu.edu"], "authors": ["Haoyu Fu", "Yuejie Chi", "Yingbin Liang"], "TL;DR": "We provide the first theoretical analysis of guaranteed recovery of one-hidden-layer neural networks under cross entropy loss for classification problems.", "pdf": "/pdf/28df4734fd8a152b0297b6c078605ef964201dc3.pdf", "paperhash": "fu|guaranteed_recovery_of_onehiddenlayer_neural_networks_via_cross_entropy", "_bibtex": "@misc{\nfu2019guaranteed,\ntitle={Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy},\nauthor={Haoyu Fu and Yuejie Chi and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlfAo09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper875/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353052716, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper875/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper875/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper875/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353052716}}}, {"id": "rygK6cLo3m", "original": null, "number": 3, "cdate": 1541266112670, "ddate": null, "tcdate": 1541266112670, "tmdate": 1541533617666, "tddate": null, "forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper875/Official_Review", "content": {"title": "Lack of practicality and theoretical depth.", "review": "The paper presents theoretical analysis for recovering one-hidden-layer neural networks using logistic loss function. I have the following major concerns:\n\n(1.a) The paper does not mention identifiability at all. As has been known, neural networks with even only one hidden layer are not identifiable. The authors need to either prove the identifiability or cite existing references on the identifiability. Otherwise,  the parameter recovery does not make sense.\n\nExample: The linear network takes f(x) = 1'Wx/k, where 1 is a vector with every entry equal to one. Then two models with parameters W and V are identical as long 1'W = 1'V.\n\n(1.b) If the equivalent parameters are not isolated, the local strong convexity is impossible to hold. The authors need to carefully justify their claim.\n\n(2) When using Sigmoid or Tanh activation functions, the output is bounded between [0,1] or [-1,+1]. This is unrealistic for logistic regression: The output of [0,1] means that the posterior probability has to be bounded between 1/2 and e/(1+e); The output of [-1,1] means that the posterior probability has to be bounded between 1/(1+e) and e/(1+e).\n\n(3) The most challenging part of the logistic loss is the lack of curvature, when neural networks have large magnitude outputs. Since this paper assumes that the neural networks takes very small magnitude outputs, the extension from Zhong et al. 2017b to the logistic loss is very straightforward. \n\n(4) Spectral initialization is very impractical. Nobody is using it in practice. The spectral initialization avoids the challenging global convergence analysis.\n\n(5) Theorem 3 needs clarification. Please explicitly write the RHS of (7). The result would become meaningless, if under the scaling of Theorem 2, is the RHS of (7) smaller than RHS of (5).\n\nI also have the following minor concerns on some unrealistic assumptions, but these concerns do not affect my rating. These assumptions have been widely used in many other papers, due to the lack of theoretical understanding of neural networks in the machine learning community.\n\n(6)\tThe neural networks take independent Gaussian input.\n(7)\tThe model is assumed to be correct.\n(8)\tOnly gradient descent is considered.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper875/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy", "abstract": "We study model recovery for data classification, where the training labels are generated from a one-hidden-layer fully -connected neural network with sigmoid activations, and the goal is to recover the weight vectors of the neural network. We prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground truth without requiring a fresh set of samples at each iteration. To the best of our knowledge, this is the first global convergence guarantee established for the empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, at the near-optimal sample and computational complexity with respect to the network input dimension.", "keywords": ["cross entropy", "neural networks", "parameter recovery"], "authorids": ["fu.436@osu.edu", "yuejiechi@cmu.edu", "liang.889@osu.edu"], "authors": ["Haoyu Fu", "Yuejie Chi", "Yingbin Liang"], "TL;DR": "We provide the first theoretical analysis of guaranteed recovery of one-hidden-layer neural networks under cross entropy loss for classification problems.", "pdf": "/pdf/28df4734fd8a152b0297b6c078605ef964201dc3.pdf", "paperhash": "fu|guaranteed_recovery_of_onehiddenlayer_neural_networks_via_cross_entropy", "_bibtex": "@misc{\nfu2019guaranteed,\ntitle={Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy},\nauthor={Haoyu Fu and Yuejie Chi and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlfAo09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper875/Official_Review", "cdate": 1542234357136, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper875/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335822985, "tmdate": 1552335822985, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper875/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rklsJtp93m", "original": null, "number": 2, "cdate": 1541228770593, "ddate": null, "tcdate": 1541228770593, "tmdate": 1541533617445, "tddate": null, "forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper875/Official_Review", "content": {"title": "Incremental work, not strong enough", "review": "This paper studies the problem of learning the parameter of one hidden layer neural network with sigmoid activation function based on the negative log likelihood loss. The authors consider the teacher network setting with Gaussian input, and show that gradient descent can recover the teacher network\u2019s parameter up to certain statistical accuracy when the initialization is sufficiently close to the true parameter. The main contribution of this paper is that the authors consider the classification problem with negative log likelihood loss, and provide the local convergence result for gradient descent. However, based on the previous results in Mei et al., 2016 and Zhong et al., 2017, this work is incremental, and current results in this paper is not strong enough. To be more specific, the paper has the following weaknesses:\n\n1.\tThe authors show the uniformly strongly convex and smooth property of the objective loss function which can get rid of the sample splitting procedure used in Zhong et al., 2017. However, the method for proving this uniform result has been previously used in Mei et al., 2016. And the extension to the negative log likelihood objective function is straightforward since the derivate and Hessian of the log likelihood function can be easily bounded given the sigmoid activation function. \n2.\tThe authors employ a tensor initialization algorithm proposed by Zhong et al, 2017 to satisfy their initialization requirement. However, it seems like that the tensor initialization almost enables the recovery as it already lands on a point close to the ground truth, the role of GD is somehow not \nthat crucial. If the authors can prove the convergence of GD with random initialization, the results of this paper will be much stronger.\n3.\tThe presentation of the current paper needs to be improved. The authors should distinguish \\cite and \\citep. There are some incomplete sentences in the current paper, such as in page 3, \u201cMoreover, (Zhong et al., 2017b) shows\u2026the ground truth From a technical perspective, our\u2026\u201d.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper875/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy", "abstract": "We study model recovery for data classification, where the training labels are generated from a one-hidden-layer fully -connected neural network with sigmoid activations, and the goal is to recover the weight vectors of the neural network. We prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground truth without requiring a fresh set of samples at each iteration. To the best of our knowledge, this is the first global convergence guarantee established for the empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, at the near-optimal sample and computational complexity with respect to the network input dimension.", "keywords": ["cross entropy", "neural networks", "parameter recovery"], "authorids": ["fu.436@osu.edu", "yuejiechi@cmu.edu", "liang.889@osu.edu"], "authors": ["Haoyu Fu", "Yuejie Chi", "Yingbin Liang"], "TL;DR": "We provide the first theoretical analysis of guaranteed recovery of one-hidden-layer neural networks under cross entropy loss for classification problems.", "pdf": "/pdf/28df4734fd8a152b0297b6c078605ef964201dc3.pdf", "paperhash": "fu|guaranteed_recovery_of_onehiddenlayer_neural_networks_via_cross_entropy", "_bibtex": "@misc{\nfu2019guaranteed,\ntitle={Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy},\nauthor={Haoyu Fu and Yuejie Chi and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlfAo09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper875/Official_Review", "cdate": 1542234357136, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper875/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335822985, "tmdate": 1552335822985, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper875/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HylHS_0L37", "original": null, "number": 1, "cdate": 1540970557414, "ddate": null, "tcdate": 1540970557414, "tmdate": 1541533617241, "tddate": null, "forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper875/Official_Review", "content": {"title": "Review", "review": "Paper Summary:\nThis paper studies the problem of recovering a true underlying neural network (assuming there exists one) with cross-entropy loss. This paper shows if the input is standard Gaussian, within a small ball around the ground truth, the objective function is strongly convex and smooth if there is a sufficiently large number of samples. Furthermore, the global minimizer is actually the true neural network. This geometric analysis implies applying gradient descent within this neighborhood, one can recover the underlying neural network. This paper also proposed a provable method based on spectral learning to find a good initialization point. Lastly, this paper also provides some simulation studies.\n\nComments:\nThis paper closely follows a recent line of work on recovering a neural network under Gaussian input assumption. While studying cross-entropy loss is interesting, the analysis techniques in this paper are very similar to Zhong et al. 2017, so this paper is incremental. I believe studying the global convergence of the gradient descent or relaxing the Gaussian input assumption is more interesting.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper875/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy", "abstract": "We study model recovery for data classification, where the training labels are generated from a one-hidden-layer fully -connected neural network with sigmoid activations, and the goal is to recover the weight vectors of the neural network. We prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground truth without requiring a fresh set of samples at each iteration. To the best of our knowledge, this is the first global convergence guarantee established for the empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, at the near-optimal sample and computational complexity with respect to the network input dimension.", "keywords": ["cross entropy", "neural networks", "parameter recovery"], "authorids": ["fu.436@osu.edu", "yuejiechi@cmu.edu", "liang.889@osu.edu"], "authors": ["Haoyu Fu", "Yuejie Chi", "Yingbin Liang"], "TL;DR": "We provide the first theoretical analysis of guaranteed recovery of one-hidden-layer neural networks under cross entropy loss for classification problems.", "pdf": "/pdf/28df4734fd8a152b0297b6c078605ef964201dc3.pdf", "paperhash": "fu|guaranteed_recovery_of_onehiddenlayer_neural_networks_via_cross_entropy", "_bibtex": "@misc{\nfu2019guaranteed,\ntitle={Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy},\nauthor={Haoyu Fu and Yuejie Chi and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=HJlfAo09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper875/Official_Review", "cdate": 1542234357136, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJlfAo09KX", "replyto": "HJlfAo09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper875/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335822985, "tmdate": 1552335822985, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper875/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}