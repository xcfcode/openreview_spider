{"notes": [{"id": "ByGVui0ctm", "original": "SJlMk0PGF7", "number": 346, "cdate": 1538087788067, "ddate": null, "tcdate": 1538087788067, "tmdate": 1545355386922, "tddate": null, "forum": "ByGVui0ctm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkekvieWeN", "original": null, "number": 1, "cdate": 1544780630906, "ddate": null, "tcdate": 1544780630906, "tmdate": 1545354523438, "tddate": null, "forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Meta_Review", "content": {"metareview": "The authors have proposed 3 continual learning variants which are all based on MNIST and which vary in terms of whether task ids are given and what the classification task is, and they have proposed a method which incorporates a symmetric VAE for generative replay with a class discriminator. The proposed method does work well on the continual learning scenarios and the incorporation of the generative model with the classifier is more efficient than keeping them separate. The discussion of the different CL scenarios and of related work is nice to read. However, the authors imply that these scenarios cover the space of important CL variants, yet they do not consider many other settings, such as when tasks continually change rather than having sharp boundaries. The authors have also only focused on the catastrophic forgetting aspect of continual learning, without considering scenarios where, e.g., strong forward transfer (or backwards transfer) is very important. Regarding the proposed architecture that combines a VAE with a softmax classifier for efficiency, the reviewers all felt that this was not novel enough to recommend publication.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper346/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper346/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353249179, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353249179}}}, {"id": "r1xlfgHugE", "original": null, "number": 11, "cdate": 1545256967676, "ddate": null, "tcdate": 1545256967676, "tmdate": 1545256967676, "tddate": null, "forum": "ByGVui0ctm", "replyto": "HylBEpGrlN", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "content": {"title": "Three scenarios and not tied to specific architectural layout", "comment": "Thank you for the comment and for pointing us to these two papers, both of which indeed also discuss the need for defining different learning scenarios to evaluate continual learning algorithms.\n\nCompared with these two studies, our treatment of this important problem is more general. The mentioned papers both focus on the split MNIST task protocol (or other split dataset protocols), for which they discuss that there is a difference between whether models are evaluated with a multi-head layout (which requires task identity to be known) or with a single-head layout (which does not require task identity to be known). In our work we go further in two important ways.\nFirst, we note that when task-identity is not known, there is a further distinction depending on whether task-identity needs to be inferred by the model as well. The two mentioned papers do not discuss this, but we believe this to be important since two distinct scenarios arise, both with relevant real-world applications. When task identity does not need to be inferred, the resulting scenario is incrementally learning new domains. A real-world example is an agent who needs to learn to survive in different environments without the need to explicitly identify the environment it is confronted with. On the other hand, when task identity does need to be inferred, the resulting scenario corresponds to incrementally learning new classes. \u201cSingle-headed split MNIST\u201d is an example of this scenario. Importantly, in our paper we show\u2014for both the split MNIST and the permuted MNIST task protocol\u2014that there are substantial differences in difficulty between these two ethologically relevant scenarios, which are not captured by the multi-head / single-head split.\nSecond, while the multi-head / single-head distinction is tied to the architectural layout of the output layer, our identified scenarios are more general and reflect what information is available / what is required of a network. In the continual learning literature a multi-head layout (i.e., a model with a separate output layer for each task) might be the most common way to use task identity information, it is not the only way. Similarly, although a single-head layout (i.e., a model that uses the same output-layer for every task) by itself does not require task identity to be known, it is still possible for the model to use task identity information in other ways (e.g., in its hidden layers, as in context-dependent gating)."}, "signatures": ["ICLR.cc/2019/Conference/Paper346/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper346/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613896, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper346/Authors|ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613896}}}, {"id": "HylBEpGrlN", "original": null, "number": 1, "cdate": 1545051436754, "ddate": null, "tcdate": 1545051436754, "tmdate": 1545051436754, "tddate": null, "forum": "ByGVui0ctm", "replyto": "r1xKLRBvCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Public_Comment", "content": {"comment": "What you claim as the main/ novel contributions have already been discussed by [1] and [2], albeit with different terminologies. Can you please comment on how 'Incremental task learning' and 'Incremental class learning' is different from multi-head and single-head setting, respectively? \n\n[1] Chaudhry et al. Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence, ECCV, 2018.\n[2] Farquhar, S., & Gal, Y. (2018). Towards Robust Evaluations of Continual Learning. Lifelong Learning: A Reinforcement Learning Approach Workshop at ICML 2018.", "title": "Clarification on the novelty"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311861439, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByGVui0ctm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311861439}}}, {"id": "Bkxu3s1oyV", "original": null, "number": 10, "cdate": 1544383407853, "ddate": null, "tcdate": 1544383407853, "tmdate": 1544383407853, "tddate": null, "forum": "ByGVui0ctm", "replyto": "r1gOZ1IDAm", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "content": {"title": "Keep the score", "comment": "I feel the rebuttal is not sufficient for me to increase the score. \nI would suggest a rewrite of the paper to separate the two topics in future submission. \nExperimental study of the methods under different evaluation metrics deserve more length."}, "signatures": ["ICLR.cc/2019/Conference/Paper346/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper346/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613896, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper346/Authors|ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613896}}}, {"id": "HkgxzCoK1V", "original": null, "number": 7, "cdate": 1544302087833, "ddate": null, "tcdate": 1544302087833, "tmdate": 1544302087833, "tddate": null, "forum": "ByGVui0ctm", "replyto": "ByxIiAHwRm", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "content": {"title": "I still feel the same way about limited amount of novelty after the discussion", "comment": "Thank you for your response. However I do not feel I can support the paper for the same reasons I mentioned in my original review. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper346/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper346/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613896, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper346/Authors|ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613896}}}, {"id": "r1gOZ1IDAm", "original": null, "number": 4, "cdate": 1543098111719, "ddate": null, "tcdate": 1543098111719, "tmdate": 1543098111719, "tddate": null, "forum": "ByGVui0ctm", "replyto": "ryx-aMn_hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "content": {"title": "Response to reviewer 3", "comment": "We thank the reviewer for his/her review and for the positive feedback on our proposed evaluation framework. Please find our responses to the three \u201cCons\u201d below.\n\n1) Firstly, the reviewer is correct in that some important experimental details were missing from the main text. We sincerely apologize for this and thank the reviewer for pointing this out. We have now included these missing details towards the top of section 4.2.\nAs the reviewer points out, the original LwF method was implemented with a \u201cmultihead\u201d output layer with a different set of output units for each task. This implementation indeed requires task identity during testing and can thus only be used in the incremental task learning scenario. To be able to use (the core elements of) LwF (i.e., replaying current task inputs labelled with soft targets produced by the previous model) in the incremental domain learning scenario, for that scenario we instead use a \u201csinglehead\u201d output layer, whereby all tasks share the same set of output units. \nConversely, the original EWC method was implemented with a \u201csinglehead\u201d output layer. This implementation of EWC indeed does not use task identity during testing. As we have now described in our methods-section, to enable EWC to use the available task identity information in the incremental task learning scenario, for that scenario we use a \u201cmutlihead\u201d output layer instead.\n(Finally, note that in the incremental class learning scenario, all methods use an output layer with a separate output unit for each class and always all units of the classes seen so far are \u201cactive\u201d.)\n\n2) Our use of the term \u201cfeedback connections\u201d was motivated by Neuroscience, where connections from a higher processing region back to a lower processing region are referred to as feedback connections. We believe this parallel to be appropriate (to some degree) for the encoder/decoder structure that we used. But we do agree with the reviewer that there are other uses of the term \u201cfeedback\u201d that do not align with our proposed method and that it could indeed be misinterpreted. We have therefore now removed all mentions of \u201cfeedback\u201d from our paper and we changed the name \u201cReplay-through-Feedback (RtF)\u201d to \u201cIntegrated Generative Replay (IGR)\u201d. We have also made an effort to make sure it is clear that we use an encoder/decoder structure. \n\n3) We agree with the reviewer that it would also be possible to integrate the classifier into a GAN by letting it share a network with the discriminator, which would indeed save computation compared to DGR implemented with a GAN. We believe this to be along the same spirit as what we propose to do with a VAE. We had considered this option as well, but we expect that, due to the generally higher computational costs of training a GAN compared to a VAE, overall this would be more computationally expensive than our implementation. We also expect that it is more beneficial / less harmful to share weights between the classifier and a VAE-encoder than between the classifier and a GAN-discriminator, because we expect the representations extracted by a VAE-encoder (from which it should be possible to reconstruct the full input) to be more useful for robust classification than the representations extracted by a GAN-discriminator (from which it only needs to be possible to make a distinction between real and generated inputs). But these are merely expectations and a formal comparison between these two approaches would be interesting."}, "signatures": ["ICLR.cc/2019/Conference/Paper346/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613896, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper346/Authors|ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613896}}}, {"id": "HJlk11LvAQ", "original": null, "number": 3, "cdate": 1543098070741, "ddate": null, "tcdate": 1543098070741, "tmdate": 1543098070741, "tddate": null, "forum": "ByGVui0ctm", "replyto": "B1xLojt927", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "We thank the reviewer for his/her review. Note however that we are not completely sure whether we fully understood the specific concern raised by this reviewer; we would appreciate a correction if we missed or misinterpreted something.\n\nAs we understand it, the main concern raised by this reviewer is that the reduction in computational cost achieved by our RtF method is dependent on the NN architectures used. Specifically, the reviewer gives as example that it would be more difficult to combine a sophisticated classifier network with a simple generator network, while the reviewer thinks that this combination would be more accurate than any of our reported results.\nWe agree that our proposed framework likely yields most benefit when the sophistication (depth) of the classifier and generator are reasonably balanced, but we don\u2019t think that in practice this is an important limitation to the ML practitioner. Because although enhancing the sophistication of the classifier could increase the overall accuracy (by increasing the baseline accuracy on each individual task), so could enhancing the sophistication of the generator (by improving the quality of the replayed samples). For most practical applications, we expect the \u2018optimal sophistication\u2019 (in terms of accuracy vs. efficiency) of the classifier and generator to be reasonably biased.\n\nRegarding novelty, significance and broad impact of our work for the ML community, as discussed above we believe that the three continual learning scenarios we defined here are an important and much-needed contribution to the continual learning field that has already began to be adopted by the field [1]. In addition, our finding that currently only replay-based methods\u2014and not ones using regularization-based approaches\u2014are able to solve the most relevant catastrophic forgetting problem is novel and we believe of broad interest to the field. We have now rewritten the paper to better reflect these contributions, and we hope that the reviewer would be willing to reconsider her/his evaluation of our work. \n\n[1] Hsu, Liu & Kira (2018) \u201cRe-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines.\u201d arXiv preprint arXiv:1810.12488."}, "signatures": ["ICLR.cc/2019/Conference/Paper346/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613896, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper346/Authors|ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613896}}}, {"id": "ByxIiAHwRm", "original": null, "number": 2, "cdate": 1543098013579, "ddate": null, "tcdate": 1543098013579, "tmdate": 1543098013579, "tddate": null, "forum": "ByGVui0ctm", "replyto": "rJe-qMJA37", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We thank the reviewer for his/her review.\nThis reviewer\u2019s main criticism is that our proposed RtF method is very similar to a previous method (i.e., DGR+distill; although note that this is actually a hybrid of two published methods) and that besides approximately doubling efficiency there seem to be no significant contributions of the proposed model. We agree that our demonstration that RTF and DGR+distill have comparable performance is not necessarily surprising. However, we do believe that doubling efficiency is an important contribution given that a critical feature determining the practical applicability of any continual learning method is its computational efficiency. \n\nRegarding novelty, significance and broad impact of our work for the ML community, as discussed above we believe that the three continual learning scenarios we defined here are an important and much-needed contribution to the continual learning field that has already began to be adopted by the field [1]. In addition, our finding that currently only replay-based methods\u2014and not ones using regularization-based approaches\u2014are able to solve the most relevant catastrophic forgetting problem is novel and we believe of broad interest to the field. We have now rewritten the paper to better reflect these contributions, and we hope that the reviewer would be willing to reconsider her/his evaluation of our work. \n\n[1] Hsu, Liu & Kira (2018) \u201cRe-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines.\u201d arXiv preprint arXiv:1810.12488."}, "signatures": ["ICLR.cc/2019/Conference/Paper346/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613896, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper346/Authors|ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613896}}}, {"id": "r1xKLRBvCQ", "original": null, "number": 1, "cdate": 1543097937214, "ddate": null, "tcdate": 1543097937214, "tmdate": 1543097937214, "tddate": null, "forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "content": {"title": "General response to all reviewers", "comment": "We thank all reviewers for their time and effort for assessing our study and their valuable comments. We address them point by point below. Based on their feedback we have made changes to the manuscript, which we believe have substantially improved it. \n\nTheir main criticism is that our proposed method of \u201cReplay-through-Feedback\u201d is not a novel enough technical advance to warrant acceptance at ICLR. Although we have a few specific comments on this point (please see below), in general we agree with this assessment. However, the major novel contribution of our paper was not meant to be the technical advance of the \u201cReplay-through-Feedback\u201d architecture but instead: a) we identify three distinct continual learning scenarios that provide a much-needed structured and systematic evaluation framework of the proposed solutions to catastrophic forgetting; and b) we find that the three continual learning scenarios we defined significantly differentiate the previously proposed solutions to catastrophic forgetting grouping proposed solutions into two major classes. First, methods that use regularization-based approaches (i.e., synaptic metaplasticity) can solve the easy continual learning scenarios, but exhibit catastrophic forgetting under the more ethological relevant scenario where an agent also needs to recognize which task needs to be performed. Importantly, it is under this scenario that humans and animals typically have to continuously learn and execute in order to survive. Second, methods with generative replay can perform well under this more difficult scenario overcoming the problem of catastrophic forgetting. Given that solving catastrophic forgetting is a fundamental problem for the machine learning community in the quest to achieve human-level performance in lifelong learning, we believe our contribution is novel and of board interest. Our findings underscore the importance of evaluating proposed solutions under the different continual learning scenarios, which has not been widely used to-date (e.g., [1]), and they highlight that learning a generative model of the tasks seems to be a promising path forward for successfully creating lifelong learning AI systems. \n\nAdditionally, we also motivate and introduce a new way of evaluating continual learning methods: in terms of both their accuracy and their efficiency. In the lifelong learning setting, models need to be continually trained on new tasks, and in many practical applications this training even has to be done in real-time, for example in situations that can be unpredictable and where the ability to quickly adapt to dynamic circumstances is of primary importance. Efficiency is therefore very important for any continual learning method that is to be used in practice, and we believe it should be taken into account when comparing methods.\n \nWe acknowledge that the way we had written the paper did not make our novel contributions clear enough. We have now made major changes to better highlight our contributions and their significance. We hope that after these revisions, reviewers 1 and 2 are willing to reassess their evaluation of our study.\n\n[1] Masse, Grant & Freedman (2018) \u201cAlleviating catastrophic forgetting using context-dependent gating and synaptic stabilization.\u201d PNAS, 115(44): E10467-E10475."}, "signatures": ["ICLR.cc/2019/Conference/Paper346/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613896, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByGVui0ctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper346/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper346/Authors|ICLR.cc/2019/Conference/Paper346/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers", "ICLR.cc/2019/Conference/Paper346/Authors", "ICLR.cc/2019/Conference/Paper346/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613896}}}, {"id": "rJe-qMJA37", "original": null, "number": 3, "cdate": 1541431944928, "ddate": null, "tcdate": 1541431944928, "tmdate": 1541534072534, "tddate": null, "forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Review", "content": {"title": "An incremental method on lifelong learning.", "review": "This paper summarizes previous lifelong learning methods and identifies three different continual learning scenarios. Based on that, it draws a conclusion that DGR+distill outperforms other methods on all these scenarios. Further, the paper proposes unified model that combines a replay generator and a classification model. The proposed RTF model achieves comparable performance with DGR+distill and is approximately two times faster than DGR+distill.\n\nMy biggest concern is the novelty of the model, since RTF is still a replay-based method that is very similar as DGR+distill. Empirically it can be expected that RTF should behave similar as DGR+distill as well. And the result in this paper justifies that. So the main contribution comes from the efficiency boost by the integrated model strategy. That is, by replacing a separate generative model by a symmetric VAE. Besides that, there seem to be no significant contribution of the proposed model.\n\nIn my opinion, this paper look somewhat incremental. The first five pages are mostly reviews of previous methods, and the model it propose behave very similar to a previous method.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper346/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Review", "cdate": 1542234482145, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335703784, "tmdate": 1552335703784, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1xLojt927", "original": null, "number": 2, "cdate": 1541213085599, "ddate": null, "tcdate": 1541213085599, "tmdate": 1541534072329, "tddate": null, "forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Review", "content": {"title": "VAE with additional softmax classification layer for continual learning", "review": "summary: The paper claims to make three contributions\n1. It surveys the current literature on preventing catastrophic forgetting during lifelong learning. It explains the apparent inconsistencies in reported results by distinguishing three types of deployment scenarios, categorizing the evaluation procedures in the literature accordingly.\n2. The paper conducts two sets of simulated experiments on MNIST data to understand which existing methods (do not) work well. It finds that deep generative replay (DGR) that learns to generate imaginary new samples from previously seen training data, potentially augmented with soft labels seems to work best in these specific experiments but potentially doubles the computational cost. \n3. To reduce computational cost without sacrificing much accuracy it proposes to integrate the ability to learn to generate imaginary samples into the learning of the classifier itself. It does this by augmenting a symmetrical VAE with a softmax classification layer connected to the final hidden layer of the encoder. \n\nComments about significance:\n1. I'm not entirely sure if the paper does a good job separating contributions 2 & 3 above cleanly so that each can stand on its own and be fully trust-worthy.  \n2. In particular, the experimental evaluation depends on the NN architectures chosen. Here the choice of architectures that were used for the best performing approach in the experiment (DGR & the classifier) were simply combined together to motivate the new approach. However, this feels a bit too simplistic. for example, what would happen if you replaced the simple 2-hidden-layer NN with a much more sophisticated network for each classifier, but still had a simple VAE to generate samples? the combination is no longer likely to be this easy but it would likely work more accurately than anything shown in table 3. \n\nNovelty: This reviewer feels that augmenting a 2-hidden-layer VAE with a softmax classification layer does not seem to be a very significant new contribution by itself. The fact that it is being motivated for the specific problem of reducing catastrophic forgetting during lifelong learning is the main novelty here, but the relative amount of novelty might to be somewhat limited when viewed from this perspective. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper346/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Review", "cdate": 1542234482145, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335703784, "tmdate": 1552335703784, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryx-aMn_hQ", "original": null, "number": 1, "cdate": 1541092024858, "ddate": null, "tcdate": 1541092024858, "tmdate": 1541534072119, "tddate": null, "forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "invitation": "ICLR.cc/2019/Conference/-/Paper346/Official_Review", "content": {"title": "The comparison study needs some more detail, the RtF part is not significant enough.", "review": "This paper points out a important issue in current continual learning literature: Due to the different settings and different evaluation protocols of each method, comparison between methods are usually not fair, and lead to distinct conclusions.\nThe paper is in general easy to understand except a few drawbacks listed in the cons.\n\nPros:\n1. This paper investigates an important problem, aka, how does the methods compare to each other with the same evaluation protocol.\n2. Experiments are performed on the previous methods, which could be used as a baseline for future works in this field.\n3. Proposes to combine discriminative model with generative model to save computation when using generative model to store rehearsal examples.\n\nCons:\n1. Details of each experiments are missing. \nDifferent methods are evaluated under the \"incremental task learning\", \"incremental domain learning\",  \"incremental class learning\" settings. However, to my knowledge, some of the methods will not work under all of the three settings, as the author also suggest that XdG only works with task id. However, I think there are a few more. For example, the LwF methods has multiple sets of output neurons, which implicitly assumes the task id is known. It is not described in the paper how to evaluate it under \"incremental domain learning\", aka, how to decide which set of output to use if task id is not available during testing. Another example, the results in table 3 and 4 indicates that EWC with task id is better than without. However, original EWC does not take task id during testing, it is not described how to introduce dependency on the task id for EWC.\n2. Using the term feedback connection is misleading to the reader since the described method is just using an encoder/decoder structure. In my opinion this is different from feedback connection in which higher layer is an input for lower layers. Autoencoder or encoder/decoder structure is more appropriate.\n3. There is some contribution in the RtF part, namely the saved computation compared to DGR. However, subjectively, I think this contribution is not very significant. The same thing can be achieved with DGR by sharing the network between the discriminative model and the discriminator in GAN. In my opinion this is more a design bonus in using generative replay than a major methodology innovation.\n\nConclusion:\nThe first part that compares different methods is worth publishing given more details are provided. I'm more than happy to give a higher score if the authors are able to provide more details and the details are reasonable.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper346/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Three continual learning scenarios and a case for generative replay", "abstract": "Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic. Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \u201csoft targets\u201d) achieved superior performance in all three scenarios. In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model.", "keywords": ["continual learning", "generative models", "replay", "distillation", "variational autoencoder"], "authorids": ["gidovandeven@gmail.com", "astolias@bcm.edu"], "authors": ["Gido M. van de Ven", "Andreas S. Tolias"], "TL;DR": "A newly introduced structured comparison of recent methods for continual learning that turns into an argument for and extension of generative replay.", "pdf": "/pdf/0bb3c8813fc4b045f18fc87bc85df99c72b14353.pdf", "paperhash": "ven|three_continual_learning_scenarios_and_a_case_for_generative_replay", "_bibtex": "@misc{\nven2019three,\ntitle={Three continual learning scenarios and a case for generative replay},\nauthor={Gido M. van de Ven and Andreas S. Tolias},\nyear={2019},\nurl={https://openreview.net/forum?id=ByGVui0ctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper346/Official_Review", "cdate": 1542234482145, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByGVui0ctm", "replyto": "ByGVui0ctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper346/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335703784, "tmdate": 1552335703784, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper346/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}