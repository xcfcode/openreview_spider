{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392649080000, "tcdate": 1392649080000, "number": 3, "id": "AKHVlzzUO5Ao2", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "iiu7beeAJGnAl", "replyto": "iiu7beeAJGnAl", "signatures": ["Wenpeng Yin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We were happy to hear that the reviewer thinks that our idea of inducing representations for disjoint linguistic units is novel and holds potential!\r\n\r\n1)'strange to use word2vec'\r\nOur motivation for word2vec was to use the best currently available method for distributed representations. Embeddings perform better than other distributed representations on several tasks and embeddings induced by word2vec have been particularly successful.\r\nWe would appreciate further thoughts on why we should use word-document matrix factorization as opposed to a stronger method like word2vec for learning representations.\r\n\r\n2)'skip-gram learning algorithm doesn\u2019t make sense'\r\nWe would appreciate if the reviewer could expand on this point. Is the reason that representations of verbs should, in the reviewer's view, be induced using a 'sequence-sensitive' learning algorithm (since bag-of-words is often viewed as more appropriate for nouns)?\r\n\r\nThis is a good point. However, the skip-gram model seems to be successful to some extent in learning sequence-dependent information, possibly because the sampling is position dependent, giving a preference to close words. For example, singular and plural forms are systematically related, which one would not expect from a true bag-of-words model.\r\n\r\n3) 'why this task is chosen is unclear'\r\nAs we discuss in the paper, the task of predicting (human) animacy from context is useful for coreference resolution (because pronouns like 'him' and 'she' can only refer to human animate entities).\r\n\r\nWe agree though that we should also present results for a standard task. We are currently running experiments on paraphrase identification: http://www.aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_%28State_of_the_art%29   and will present results at the workshop if the paper gets accepted.\r\n\r\n4) 'paper is short'\r\nWe were trying to comply with the length restrictions of the ICLR 2014 workshop track. If the reviewer could be more specific as to which parts of the description of the evaluation task and of the conclusion need to be expanded, we would be very glad to fix these problems and submit a revised version to arxiv.\r\n\r\n5) 'No visualization or control experiments to understand the learned representations'\r\nOur control experiment was supposed to be the single-word baseline. A visualization would certainly improve the paper. Again, we were trying to comply with the length restrictions."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "decision": "submitted, no decision", "abstract": "Deep learning embeddings have been successfully used for many natural language processing (NLP) problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "pdf": "https://arxiv.org/abs/1312.5129", "paperhash": "yin|deep_learning_embeddings_for_discontinuous_linguistic_units", "keywords": [], "conflicts": [], "authors": ["Wenpeng Yin", "Hinrich Sch\u00fctze"], "authorids": ["mr.yinwenpeng@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392648720000, "tcdate": 1392648720000, "number": 1, "id": "azLZaQQbZvaIp", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "iiu7beeAJGnAl", "replyto": "GzFZGSMOZpzOk", "signatures": ["Wenpeng Yin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "1)'frequency of ... discontiguous units'\r\n\r\n     It may be the case that discontiguous units are more frequent in other languages. However, phrasal verbs are one of the most important verb groups in English, verbs like 'keep up' and 'take off'. Without discontiguous units, the 'vacation' meaning of 'I took the month off' is difficult to infer from the vectors of 'took' and 'off'. Our approach\r\nlearns a vector for 'took ... off', thus facilitating correct inference.\r\n           This shows that having appropriate representations for phrasal verbs is important.\r\n\r\n2) 'Neither the claim not the results are very surprising.'\r\n        We certainly agree that our results are not earth shattering. However, for all NLP work that represents linguistic input as embeddings or other distributed representations, the question of how the linguistic input should be parsed into units (which then are represented as vectors) must be addressed. We cite recent work on morphology and on phrases in this vein.\r\n        Within this line of work research on discontinuous units (as a third type of possible unit, in addition to stems/affices and continuous phrases) is well motivated.\r\n       Perhaps positive results for this new type of unit are to be expected. Still, we feel it is a contribution to confirm the hypothesis that they are useful."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "decision": "submitted, no decision", "abstract": "Deep learning embeddings have been successfully used for many natural language processing (NLP) problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "pdf": "https://arxiv.org/abs/1312.5129", "paperhash": "yin|deep_learning_embeddings_for_discontinuous_linguistic_units", "keywords": [], "conflicts": [], "authors": ["Wenpeng Yin", "Hinrich Sch\u00fctze"], "authorids": ["mr.yinwenpeng@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391850600000, "tcdate": 1391850600000, "number": 2, "id": "fC0gfomYgEfhk", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "iiu7beeAJGnAl", "replyto": "iiu7beeAJGnAl", "signatures": ["anonymous reviewer 6104"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Learning Embeddings for Discontinuous Linguistic Units", "review": "Summary\r\nThis paper proposes learning representations for discontinuous pairs of words in a sentence. Representations for such linguistic units such as \u201chelped*to\u201d are potentially more useful than bigrams or other units for particular NLP tasks. Rather than introducing a new algorithm to induce such representations, they alter a text corpus and use a skip-gram training algorithm. Representations are compared against previous word representation approaches on a task of classifying markables.\r\n\r\nReview\r\nGenerally the idea of inducing representations for disjoint linguistic units is novel, and seems to hold good potential. It seems strange to use word2vec which is a skip-gram algorithm to induce such representations. The process of creating fake \u2018sentences\u2019 with disjoint units to induce skip grams seems hacky. I would prefer to see a more straightforward approach, such as one based on token-document matrix factorization, to induce representations for the disjoint tokens. The evaluation task is obscure, and why this task is chosen is unclear. The authors should include experimental evaluation, visualization, or controlled experiments on at least one more standard task. Generally, there is a kernel of an interesting idea in this paper but the work needs a more thorough investigation into the representation learning algorithm used and evaluation.\r\n\r\nKey points\r\n+ Interesting linguistic idea\r\n+ Use of pre-existing word vector learning package makes experiments seemingly easy to reproduce\r\n- Using a skip-gram learning algorithm doesn\u2019t make sense. A matrix factorization or other similar approach seems more natural\r\n- Non-standard and somewhat difficult to understand evaluation\r\n- No visualization or control experiments to understand the learned representations\r\n- Paper is short to the point of lacking sufficient descriptions of the evaluation task and conclusions"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "decision": "submitted, no decision", "abstract": "Deep learning embeddings have been successfully used for many natural language processing (NLP) problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "pdf": "https://arxiv.org/abs/1312.5129", "paperhash": "yin|deep_learning_embeddings_for_discontinuous_linguistic_units", "keywords": [], "conflicts": [], "authors": ["Wenpeng Yin", "Hinrich Sch\u00fctze"], "authorids": ["mr.yinwenpeng@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391716200000, "tcdate": 1391716200000, "number": 1, "id": "GzFZGSMOZpzOk", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "iiu7beeAJGnAl", "replyto": "iiu7beeAJGnAl", "signatures": ["anonymous reviewer e7ba"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Learning Embeddings for Discontinuous Linguistic Units", "review": "This paper explores simple ways to embed linguistic units composed of discontiguous words such as 'HELP TO' in the sentence 'Paul HELPS me TO write my paper'.   The frequency of occurrence of such discontiguous units is very language dependent (high in German, lower in English). The authors propose a method that essentially amounts to rewriting the sentence in a manner that considers such units as a single word and using Mikolov's vec2word code. Experiments show that such embeddings perform better on a simple task, namely classifying entities are animated or non-animated.  In my opinion this is a very preliminary work at this stage.  Neither the claim not the results are very surprising."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "decision": "submitted, no decision", "abstract": "Deep learning embeddings have been successfully used for many natural language processing (NLP) problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "pdf": "https://arxiv.org/abs/1312.5129", "paperhash": "yin|deep_learning_embeddings_for_discontinuous_linguistic_units", "keywords": [], "conflicts": [], "authors": ["Wenpeng Yin", "Hinrich Sch\u00fctze"], "authorids": ["mr.yinwenpeng@gmail.com", "inquiries@cislmu.org"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387457160000, "tcdate": 1387457160000, "number": 1, "id": "iiu7beeAJGnAl", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "iiu7beeAJGnAl", "signatures": ["mr.yinwenpeng@gmail.com"], "readers": ["everyone"], "content": {"title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "decision": "submitted, no decision", "abstract": "Deep learning embeddings have been successfully used for many natural language processing (NLP) problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.", "pdf": "https://arxiv.org/abs/1312.5129", "paperhash": "yin|deep_learning_embeddings_for_discontinuous_linguistic_units", "keywords": [], "conflicts": [], "authors": ["Wenpeng Yin", "Hinrich Sch\u00fctze"], "authorids": ["mr.yinwenpeng@gmail.com", "inquiries@cislmu.org"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 5}