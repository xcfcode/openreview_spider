{"notes": [{"id": "HygS7n0cFQ", "original": "BJgsSv6qFm", "number": 1357, "cdate": 1538087965285, "ddate": null, "tcdate": 1538087965285, "tmdate": 1545355378756, "tddate": null, "forum": "HygS7n0cFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning", "abstract": "Humans learn to play video games significantly faster than the state-of-the-art reinforcement learning (RL) algorithms. People seem to build simple models that are easy to learn to support planning and strategic exploration. Inspired by this, we investigate two issues in leveraging model-based RL for sample efficiency. First we investigate how to perform strategic exploration when exact planning is not feasible and empirically show that optimistic Monte Carlo Tree Search outperforms posterior sampling methods. Second we show how to learn simple deterministic models to support fast learning using object representation. We illustrate the benefit of these ideas by introducing a novel algorithm, Strategic Object Oriented Reinforcement Learning (SOORL), that outperforms state-of-the-art algorithms in the game of Pitfall! in less than 50 episodes.", "keywords": ["Reinforcement Learning", "Strategic Exploration", "Model Based Reinforcement Learning"], "authorids": ["keramati@stanford.edu", "jaywhang@cs.stanford.edu", "patcho@cs.stanford.edu", "ebrun@cs.stanford.edu"], "authors": ["Ramtin Keramati", "Jay Whang", "Patrick Cho", "Emma Brunskill"], "TL;DR": "We studied exploration with imperfect planning and used object representation to learn simple models and introduced a new sample efficient RL algorithm that achieves state of the art results on Pitfall!", "pdf": "/pdf/55f6170d303ecb98c0eb5a6bded7145f616e4227.pdf", "paperhash": "keramati|fast_exploration_with_simplified_models_and_approximately_optimistic_planning_in_model_based_reinforcement_learning", "_bibtex": "@misc{\nkeramati2019fast,\ntitle={Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning},\nauthor={Ramtin Keramati and Jay Whang and Patrick Cho and Emma Brunskill},\nyear={2019},\nurl={https://openreview.net/forum?id=HygS7n0cFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Ske5y_n-eE", "original": null, "number": 1, "cdate": 1544828897787, "ddate": null, "tcdate": 1544828897787, "tmdate": 1545354530391, "tddate": null, "forum": "HygS7n0cFQ", "replyto": "HygS7n0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1357/Meta_Review", "content": {"metareview": "Pros:\n- rather novel approach to using optimistic MCTS for exploration with deterministic models\n- positive rewards on Pitfall\n\nCons:\n- lost of domain-specific knowledge\n- deteministic models\n- lacking clarity\n- lacking ablations\n- no rebuttal\n\nI agree with both reviewers that the paper is not good enough to be accepted.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper1357/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1357/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning", "abstract": "Humans learn to play video games significantly faster than the state-of-the-art reinforcement learning (RL) algorithms. People seem to build simple models that are easy to learn to support planning and strategic exploration. Inspired by this, we investigate two issues in leveraging model-based RL for sample efficiency. First we investigate how to perform strategic exploration when exact planning is not feasible and empirically show that optimistic Monte Carlo Tree Search outperforms posterior sampling methods. Second we show how to learn simple deterministic models to support fast learning using object representation. We illustrate the benefit of these ideas by introducing a novel algorithm, Strategic Object Oriented Reinforcement Learning (SOORL), that outperforms state-of-the-art algorithms in the game of Pitfall! in less than 50 episodes.", "keywords": ["Reinforcement Learning", "Strategic Exploration", "Model Based Reinforcement Learning"], "authorids": ["keramati@stanford.edu", "jaywhang@cs.stanford.edu", "patcho@cs.stanford.edu", "ebrun@cs.stanford.edu"], "authors": ["Ramtin Keramati", "Jay Whang", "Patrick Cho", "Emma Brunskill"], "TL;DR": "We studied exploration with imperfect planning and used object representation to learn simple models and introduced a new sample efficient RL algorithm that achieves state of the art results on Pitfall!", "pdf": "/pdf/55f6170d303ecb98c0eb5a6bded7145f616e4227.pdf", "paperhash": "keramati|fast_exploration_with_simplified_models_and_approximately_optimistic_planning_in_model_based_reinforcement_learning", "_bibtex": "@misc{\nkeramati2019fast,\ntitle={Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning},\nauthor={Ramtin Keramati and Jay Whang and Patrick Cho and Emma Brunskill},\nyear={2019},\nurl={https://openreview.net/forum?id=HygS7n0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1357/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352869615, "tddate": null, "super": null, "final": null, "reply": {"forum": "HygS7n0cFQ", "replyto": "HygS7n0cFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1357/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1357/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1357/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352869615}}}, {"id": "Syeo0AeX6Q", "original": null, "number": 2, "cdate": 1541766866696, "ddate": null, "tcdate": 1541766866696, "tmdate": 1541766866696, "tddate": null, "forum": "HygS7n0cFQ", "replyto": "HygS7n0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1357/Official_Review", "content": {"title": "Good sample-efficient performance on Pitfall using planning and imperfect models, but with limited impact due to simplifications that are hard to remove/circumvent.", "review": "-- Summary --\n\nThe paper proposes to learn (transition) models (for MDPs) in terms of objects and their interactions. These models are effectively deterministic and are compatible with algorithms for planning with count-based exploration. The paper demonstrates the performance of one such planning method in toy tasks and in Pitfall, as well as a comparison with other planning methods in the toy tasks. The proposed model-based method, called SOORL, yields agents that perform better on Pitfall with a small amount of data.\n\n-- Assessment --\n\nAs a positive, the results of the paper are favorable compared to previous work, with good sample efficiency, and they demonstrate the viability of the proposed approach. The most negative point is that SOORL relies on limiting domain-specific biases that are hard to remove or circumvent.\n\n-- Clarity --\n\nThe paper is somewhat clear. There are many typos and mistakes in writing, and at parts (for example, the second paragraph of Section 4.2) the explanations are not clear.\n\n-- Originality --\n\nI believe the work is original. The paper explores a natural idea and the claims/results are not surprising, but as far as I am aware it has not been tried before.\n\n-- Support --\n\nThe paper provides support for some of the claims made. The comparison to related work contains unsupported claims (\"we studied how imperfect planning can affect exploration\") and could be more upfront about the weaknesses of the proposed method. The claims in the introduction are sufficiently supported.\n\n-- Significance --\n\nIt would be hard to scale SOORL to other tasks, so it is unlikely to be adopted where end-to-end learning is wanted. Therefore I believe the impact of the paper to be limited.\n\nThere is also the question of whether the paper will attract interest and people will work on addressing the limitations of SOORL. I would like to hear more from the authors on this point.\n\n-- For the rebuttal --\n\nMy greatest doubt is whether the paper will attract enough interest if published, and it would be helpful to hear from the authors on why they think future work will build on the paper. Why is the proposed approach a step in the right direction?\n\n-- Comments --\n\nSample efficiency: The paper should be more clear about this point. It seems that 50 episodes were used for getting the positive reward in Pitfall, which is great.\n\nObject detection: I am happy with the motivation about how we can remove the hand-made object detection. It is important the other strong assumptions (object interaction matrix, for example) can be removed as well. My opinion on simplifications is this: They are ok if they are being used to make experiments viable and they can be removed when scaling up; but they are not ok if there is no clear way to remove them.\n\nKnown interaction matrix: It may be possible to remove this requirement using the tools in [1]\n\nDeterministic model: The use of no-ops to make the model deterministic seems right if the ultimate goal is to make the model deterministic, but it seems unsuited if the model is to be used for control. Maybe the model needs to be temporally extended as I thought the paper was proposing in Section 4.2 but Section 4.3 suggests that this temporal extension was not a good idea. Is my understanding correct?\n\nExploration: I was a bit confused about how the text discusses exploration. UCT uses OFU, but the text suggests that it does not. What are the components for exploration? Both a bonus on unseen transitions and the confidence interval bonus? Also, the paper would have to provide support for the claim that \"with limited number of rollouts, the agent might not observe the optimistic part of the model, in contrast to optimistic MCTS where optimism is build into every node of the tree\". However, it is fair to say that in the to domains MCTS seemed has performed better, and for that reason it has been chosen instead of Thompson Sampling for the later experiments.\n\nWriting: The paper has a number of typos and mistakes that need to be fixed. To point out a few:\n* I would suggest more careful use of \"much\" and \"very\"\n* For citations, \"Diuk et al. (2008) also proposed...\" and \"(UCT, Kocsis & Szepesvari, 2006)\"\n\nClaims: I think the claims made in the introduction could be stated more clearly in the conclusion. (Intro) \"We show how to do approximate planning\" --> (Conclusion) \"Our model learning produces effectively deterministic models that can then be used by usual planning algorithms\".\n\n-- References --\n\n[1] Santoro et al., 2017. \"A simple neural network module for relational reasoning\"", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1357/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning", "abstract": "Humans learn to play video games significantly faster than the state-of-the-art reinforcement learning (RL) algorithms. People seem to build simple models that are easy to learn to support planning and strategic exploration. Inspired by this, we investigate two issues in leveraging model-based RL for sample efficiency. First we investigate how to perform strategic exploration when exact planning is not feasible and empirically show that optimistic Monte Carlo Tree Search outperforms posterior sampling methods. Second we show how to learn simple deterministic models to support fast learning using object representation. We illustrate the benefit of these ideas by introducing a novel algorithm, Strategic Object Oriented Reinforcement Learning (SOORL), that outperforms state-of-the-art algorithms in the game of Pitfall! in less than 50 episodes.", "keywords": ["Reinforcement Learning", "Strategic Exploration", "Model Based Reinforcement Learning"], "authorids": ["keramati@stanford.edu", "jaywhang@cs.stanford.edu", "patcho@cs.stanford.edu", "ebrun@cs.stanford.edu"], "authors": ["Ramtin Keramati", "Jay Whang", "Patrick Cho", "Emma Brunskill"], "TL;DR": "We studied exploration with imperfect planning and used object representation to learn simple models and introduced a new sample efficient RL algorithm that achieves state of the art results on Pitfall!", "pdf": "/pdf/55f6170d303ecb98c0eb5a6bded7145f616e4227.pdf", "paperhash": "keramati|fast_exploration_with_simplified_models_and_approximately_optimistic_planning_in_model_based_reinforcement_learning", "_bibtex": "@misc{\nkeramati2019fast,\ntitle={Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning},\nauthor={Ramtin Keramati and Jay Whang and Patrick Cho and Emma Brunskill},\nyear={2019},\nurl={https://openreview.net/forum?id=HygS7n0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1357/Official_Review", "cdate": 1542234247362, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HygS7n0cFQ", "replyto": "HygS7n0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1357/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335929307, "tmdate": 1552335929307, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1357/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1liHsyThQ", "original": null, "number": 1, "cdate": 1541368643163, "ddate": null, "tcdate": 1541368643163, "tmdate": 1541533202042, "tddate": null, "forum": "HygS7n0cFQ", "replyto": "HygS7n0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1357/Official_Review", "content": {"title": "A heavily engineered approach which achieves good performance in limited settings", "review": "This paper proposes a model-based object-oriented algorithm, SOORL. \nIt assumes access to an object detector which returns a list of objects with their attributes, an interaction function which detects interactions between objects, and a set of high-level macro actions. Using a simplified state representation obtained through the object detector, it performs optimistic MCTS while simultaneously learning transition and reward models. The method is evaluated on two toy domains, PongPrime and miniPitfall, as well as the Atari game Pitfall. It achieves positive rewards on Pitfall, which previous methods have not been able to do. \n\nDespite good experimental results on a notoriously hard Atari game, I believe this work has limited significance due to the high amount of prior knowledge/engineering it requires (the authors note that this is why they only evaluate on one Atari game). I think this would make a good workshop paper, but it's not clear that the contributions are fundamental or generally applicable to other domains. Also, the paper is difficult to follow (see below). \n\nPros:\n- good performance on a difficult Atari game requiring exploration\n- sample efficient method\n\nCons:\n- paper is hard to follow\n- approach is evaluated on few environments\n- heavily engineered approach\n- unclear whether gains are due to algorithm or prior knowledge\n\n\nSpecific Comments:\n\n- Section 3 is hard to follow. The authors say that they are proposing a new optimistic MCTS algorithm to support deep exploration guided by models, but this algorithm is not described or written down explicitly anywhere. Is this the same as Algorithm 3 from Section 5? They say that at each step and optimistic reward bonus is given, but it's unclear which bonus this is (they mention several possibilities) or how it relates to standard MCTS.\nIn Section 3.1, it is unclear what the representation of the environment is. I'm guessing it is not pixels, but it is discrete states? A set of features? \nThe authors say \"we provided the right model class for both experiments\" - what is this model class? \n\n- Concerning the general organization of the paper, it would be clearer to first present the algorithm (i.e. Section 5), go over the different components (model learning, learning macro actions, and planning), and then group all the experiments together in the same section. \nThe first set of experiments in Sections 3.1 and 3.2 can be presented within the experiments section as ablations.  \n\n- Although the performance on Pitfall is good, it's unclear how much gains are due to the algorithm and how much are due to the extra prior knowledge. It would be helpful to include comparisons with other methods which have access to the same prior knowledge, for example with DQN/A3C and  pseudo-count exploration bonuses using the same feature set and macro actions as SOORL uses. \n\n\nMinor:\n- Page 2: \"Since the model...the new model estimates\": should this be part of the previous sentence?\n- Page 5: \"There are reasonable evidence\" -> \"There is reasonable evidence\"\n- Page 5: \". we define a set of...\" -> \". We define a set of...\"\n- Page 8: \"any function approximation methods\" -> \"method\"", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1357/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning", "abstract": "Humans learn to play video games significantly faster than the state-of-the-art reinforcement learning (RL) algorithms. People seem to build simple models that are easy to learn to support planning and strategic exploration. Inspired by this, we investigate two issues in leveraging model-based RL for sample efficiency. First we investigate how to perform strategic exploration when exact planning is not feasible and empirically show that optimistic Monte Carlo Tree Search outperforms posterior sampling methods. Second we show how to learn simple deterministic models to support fast learning using object representation. We illustrate the benefit of these ideas by introducing a novel algorithm, Strategic Object Oriented Reinforcement Learning (SOORL), that outperforms state-of-the-art algorithms in the game of Pitfall! in less than 50 episodes.", "keywords": ["Reinforcement Learning", "Strategic Exploration", "Model Based Reinforcement Learning"], "authorids": ["keramati@stanford.edu", "jaywhang@cs.stanford.edu", "patcho@cs.stanford.edu", "ebrun@cs.stanford.edu"], "authors": ["Ramtin Keramati", "Jay Whang", "Patrick Cho", "Emma Brunskill"], "TL;DR": "We studied exploration with imperfect planning and used object representation to learn simple models and introduced a new sample efficient RL algorithm that achieves state of the art results on Pitfall!", "pdf": "/pdf/55f6170d303ecb98c0eb5a6bded7145f616e4227.pdf", "paperhash": "keramati|fast_exploration_with_simplified_models_and_approximately_optimistic_planning_in_model_based_reinforcement_learning", "_bibtex": "@misc{\nkeramati2019fast,\ntitle={Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning},\nauthor={Ramtin Keramati and Jay Whang and Patrick Cho and Emma Brunskill},\nyear={2019},\nurl={https://openreview.net/forum?id=HygS7n0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1357/Official_Review", "cdate": 1542234247362, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HygS7n0cFQ", "replyto": "HygS7n0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1357/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335929307, "tmdate": 1552335929307, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1357/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 4}