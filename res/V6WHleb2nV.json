{"notes": [{"id": "V6WHleb2nV", "original": "_6hCKvpeKna", "number": 597, "cdate": 1601308071949, "ddate": null, "tcdate": 1601308071949, "tmdate": 1614985624565, "tddate": null, "forum": "V6WHleb2nV", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis", "authorids": ["~Katsuhiko_Ishiguro1", "ujihara@preferred.jp", "rsawada@preferred.jp", "~Hirotaka_Akita1", "kotera@preferred.jp"], "authors": ["Katsuhiko Ishiguro", "Kazuya Ujihara", "Ryohto Sawada", "Hirotaka Akita", "Masaaki Kotera"], "keywords": ["retrosynthesis", "data transfer", "transfer learninig", "pre-training", "fine-tuning", "self-training"], "abstract": "Retrosynthesis is a problem to infer reactant compounds to synthesize a given\nproduct compound through chemical reactions. Recent studies on retrosynthesis\nfocus on proposing more sophisticated prediction models, but the dataset to feed\nthe models also plays an essential role in achieving the best generalizing models.\nGenerally, a dataset that is best suited for a specific task tends to be small. In\nsuch a case, it is the standard solution to transfer knowledge from a large or\nclean dataset in the same domain. In this paper, we conduct a systematic and\nintensive examination of data transfer approaches on end-to-end generative models,\nin application to retrosynthesis. Experimental results show that typical data transfer\nmethods can improve test prediction scores of an off-the-shelf Transformer baseline\nmodel. Especially, the pre-training plus fine-tuning approach boosts the accuracy\nscores of the baseline, achieving the new state-of-the-art. In addition, we conduct a\nmanual inspection for the erroneous prediction results. The inspection shows that\nthe pre-training plus fine-tuning models can generate chemically appropriate or\nsensible proposals in almost all cases.", "one-sentence_summary": "Data Transfer improves the retrosynthesis models greatly, achieving new SotA with a simpler model. ", "pdf": "/pdf/67fc2c63d368a370b9f8b06c5d71a2553ced93e0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ishiguro|data_transfer_approaches_to_improve_seqtoseq_retrosynthesis", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QQsL9f70nb", "_bibtex": "@misc{\nishiguro2021data,\ntitle={Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis},\nauthor={Katsuhiko Ishiguro and Kazuya Ujihara and Ryohto Sawada and Hirotaka Akita and Masaaki Kotera},\nyear={2021},\nurl={https://openreview.net/forum?id=V6WHleb2nV}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_ousLf9zsAR", "original": null, "number": 1, "cdate": 1610040535090, "ddate": null, "tcdate": 1610040535090, "tmdate": 1610474145040, "tddate": null, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "invitation": "ICLR.cc/2021/Conference/Paper597/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "While the authors thought that the paper had some strong experimental comparisons, there were serious concerns with novelty and paper claims. For a stronger ML paper the authors would need to either: (a) design a new training methodology beyond pre-training that is better suited for leveraging multiple datasets for Retrosynthesis, (b) design a new model for Retrosynthesis that is better able to leverage mutliple datasets, (c) design new evaluation metrics to describe how well current methods perform in Retrosynthesis and/or metrics that describe how well methods can use data from different sources. That said, if the authors were interested to submit to non-ML venues then I agree with R2 that chemistry venues may be better suited to the paper in its current form. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis", "authorids": ["~Katsuhiko_Ishiguro1", "ujihara@preferred.jp", "rsawada@preferred.jp", "~Hirotaka_Akita1", "kotera@preferred.jp"], "authors": ["Katsuhiko Ishiguro", "Kazuya Ujihara", "Ryohto Sawada", "Hirotaka Akita", "Masaaki Kotera"], "keywords": ["retrosynthesis", "data transfer", "transfer learninig", "pre-training", "fine-tuning", "self-training"], "abstract": "Retrosynthesis is a problem to infer reactant compounds to synthesize a given\nproduct compound through chemical reactions. Recent studies on retrosynthesis\nfocus on proposing more sophisticated prediction models, but the dataset to feed\nthe models also plays an essential role in achieving the best generalizing models.\nGenerally, a dataset that is best suited for a specific task tends to be small. In\nsuch a case, it is the standard solution to transfer knowledge from a large or\nclean dataset in the same domain. In this paper, we conduct a systematic and\nintensive examination of data transfer approaches on end-to-end generative models,\nin application to retrosynthesis. Experimental results show that typical data transfer\nmethods can improve test prediction scores of an off-the-shelf Transformer baseline\nmodel. Especially, the pre-training plus fine-tuning approach boosts the accuracy\nscores of the baseline, achieving the new state-of-the-art. In addition, we conduct a\nmanual inspection for the erroneous prediction results. The inspection shows that\nthe pre-training plus fine-tuning models can generate chemically appropriate or\nsensible proposals in almost all cases.", "one-sentence_summary": "Data Transfer improves the retrosynthesis models greatly, achieving new SotA with a simpler model. ", "pdf": "/pdf/67fc2c63d368a370b9f8b06c5d71a2553ced93e0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ishiguro|data_transfer_approaches_to_improve_seqtoseq_retrosynthesis", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QQsL9f70nb", "_bibtex": "@misc{\nishiguro2021data,\ntitle={Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis},\nauthor={Katsuhiko Ishiguro and Kazuya Ujihara and Ryohto Sawada and Hirotaka Akita and Masaaki Kotera},\nyear={2021},\nurl={https://openreview.net/forum?id=V6WHleb2nV}\n}"}, "tags": [], "invitation": {"reply": {"forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040535076, "tmdate": 1610474145024, "id": "ICLR.cc/2021/Conference/Paper597/-/Decision"}}}, {"id": "bTMoOJMy8cp", "original": null, "number": 3, "cdate": 1603827009714, "ddate": null, "tcdate": 1603827009714, "tmdate": 1607122662293, "tddate": null, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "invitation": "ICLR.cc/2021/Conference/Paper597/-/Official_Review", "content": {"title": "Review", "review": "### Summary of the paper\nThis paper proposes to improve retrosynthesis models with pre-training and self-training techniques. For pre-training, the model is trained on the USPTO reaction dataset and fine-tuned on USPTO-50K dataset. For self-training, the model is trained on artificial reaction instances generated by the model (i.e., back-translation). The pre-training based approach greatly improves the seq2seq retrosynthesis model and achieves comparable performance against graph based methods.\n\n### Strength\n1) The paper conducts extensive experiments for various training paradigms: joint training, self-training and pre-training. For pre-training, authors try different options of the pre-training dataset (USPTO, USPTO-MIT). \n2) The self-training approach is an interesting application of back-translation.\n\n### Weakness\n1) Technical novelty is very weak. The pre-training approach is not novel. It is simply training on a larger training set, leveraging additional data. The self-training approach is not novel either -- it is a simple application of back-translation, which is well-studied in machine translation in NLP.\n2) The performance is weak. From Table 2, we can see that current state-of-the-art are graph-to-graph based approaches. The best result of this paper is worse than the state-of-the-art (for 1,3,5-best accuracy), even though it is trained on additional data.\n3) In principle, the pre-training method is not limited to seq2seq retrosynthesis models, but authors only apply pre-training for the seq2seq transformer architecture, which performs much worse than graph-to-graph methods.\n\n### Overall evaluation and suggestions\nI vote for rejection. Unfortunately, the weakness of the paper greatly outweighs the strength. I am afraid the technical novelty is too weak for ICLR. The results are unsatisfactory due to lower 1-best accuracy compared to prior methods. To improve the paper, authors should try to apply pre-training to graph-to-graph models, which may lead to new state-of-the-art results. Since your pre-training is just training on a larger dataset, it shouldn't be too hard to do.\n\n### Post rebuttal\nI would like to thank all the reviewers for valuable feedback. My review score stays the same.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper597/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper597/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis", "authorids": ["~Katsuhiko_Ishiguro1", "ujihara@preferred.jp", "rsawada@preferred.jp", "~Hirotaka_Akita1", "kotera@preferred.jp"], "authors": ["Katsuhiko Ishiguro", "Kazuya Ujihara", "Ryohto Sawada", "Hirotaka Akita", "Masaaki Kotera"], "keywords": ["retrosynthesis", "data transfer", "transfer learninig", "pre-training", "fine-tuning", "self-training"], "abstract": "Retrosynthesis is a problem to infer reactant compounds to synthesize a given\nproduct compound through chemical reactions. Recent studies on retrosynthesis\nfocus on proposing more sophisticated prediction models, but the dataset to feed\nthe models also plays an essential role in achieving the best generalizing models.\nGenerally, a dataset that is best suited for a specific task tends to be small. In\nsuch a case, it is the standard solution to transfer knowledge from a large or\nclean dataset in the same domain. In this paper, we conduct a systematic and\nintensive examination of data transfer approaches on end-to-end generative models,\nin application to retrosynthesis. Experimental results show that typical data transfer\nmethods can improve test prediction scores of an off-the-shelf Transformer baseline\nmodel. Especially, the pre-training plus fine-tuning approach boosts the accuracy\nscores of the baseline, achieving the new state-of-the-art. In addition, we conduct a\nmanual inspection for the erroneous prediction results. The inspection shows that\nthe pre-training plus fine-tuning models can generate chemically appropriate or\nsensible proposals in almost all cases.", "one-sentence_summary": "Data Transfer improves the retrosynthesis models greatly, achieving new SotA with a simpler model. ", "pdf": "/pdf/67fc2c63d368a370b9f8b06c5d71a2553ced93e0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ishiguro|data_transfer_approaches_to_improve_seqtoseq_retrosynthesis", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QQsL9f70nb", "_bibtex": "@misc{\nishiguro2021data,\ntitle={Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis},\nauthor={Katsuhiko Ishiguro and Kazuya Ujihara and Ryohto Sawada and Hirotaka Akita and Masaaki Kotera},\nyear={2021},\nurl={https://openreview.net/forum?id=V6WHleb2nV}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper597/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139577, "tmdate": 1606915809813, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper597/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper597/-/Official_Review"}}}, {"id": "xi4yr3esJ8K", "original": null, "number": 2, "cdate": 1605545679023, "ddate": null, "tcdate": 1605545679023, "tmdate": 1605545679023, "tddate": null, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "invitation": "ICLR.cc/2021/Conference/Paper597/-/Official_Comment", "content": {"title": "Thank you all reviewers", "comment": "We thank all reviewers for their insightful comments and interesting proposals. \nWe are encouraged that some reviewers feel positive to the topic (transfer learning) itself. \nWe will improve the manuscript following your advice and look for other venues. \n\nTo reviewer 1\n- We are not aware of the MEGAN paper. Thank you for pointing out missing important references. \n- As you suggest, we will consider submitting an improved manuscript for chemoinfo journals. \n- We would consider training graph2graph models with full dataset. \n\nTo reviewer 2\n- Our main message is that the training on the larger data drastically (20% is simply surprising)  improves the retrosynthesis prediction... it means we should work on the USPTO-FULL dataset instead of 50K subset to compete in model developments. \n- Try pertaining+fine-tune to graph2graphs is a reasonable proposal. Thank you. \n\nTo reviewer 3\n- Your comments on the minor domain shift is new and intersting for us. Thank you. \n- Thank you for factual correctness. These are very helpful for our future paper preparations. \n\nTo reviewer 4\n- We appreciate your interesting proposal to quantify the effectiveness of transfers by other metrics. We will check the referred articles and think about new metrics we can validate.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper597/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper597/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis", "authorids": ["~Katsuhiko_Ishiguro1", "ujihara@preferred.jp", "rsawada@preferred.jp", "~Hirotaka_Akita1", "kotera@preferred.jp"], "authors": ["Katsuhiko Ishiguro", "Kazuya Ujihara", "Ryohto Sawada", "Hirotaka Akita", "Masaaki Kotera"], "keywords": ["retrosynthesis", "data transfer", "transfer learninig", "pre-training", "fine-tuning", "self-training"], "abstract": "Retrosynthesis is a problem to infer reactant compounds to synthesize a given\nproduct compound through chemical reactions. Recent studies on retrosynthesis\nfocus on proposing more sophisticated prediction models, but the dataset to feed\nthe models also plays an essential role in achieving the best generalizing models.\nGenerally, a dataset that is best suited for a specific task tends to be small. In\nsuch a case, it is the standard solution to transfer knowledge from a large or\nclean dataset in the same domain. In this paper, we conduct a systematic and\nintensive examination of data transfer approaches on end-to-end generative models,\nin application to retrosynthesis. Experimental results show that typical data transfer\nmethods can improve test prediction scores of an off-the-shelf Transformer baseline\nmodel. Especially, the pre-training plus fine-tuning approach boosts the accuracy\nscores of the baseline, achieving the new state-of-the-art. In addition, we conduct a\nmanual inspection for the erroneous prediction results. The inspection shows that\nthe pre-training plus fine-tuning models can generate chemically appropriate or\nsensible proposals in almost all cases.", "one-sentence_summary": "Data Transfer improves the retrosynthesis models greatly, achieving new SotA with a simpler model. ", "pdf": "/pdf/67fc2c63d368a370b9f8b06c5d71a2553ced93e0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ishiguro|data_transfer_approaches_to_improve_seqtoseq_retrosynthesis", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QQsL9f70nb", "_bibtex": "@misc{\nishiguro2021data,\ntitle={Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis},\nauthor={Katsuhiko Ishiguro and Kazuya Ujihara and Ryohto Sawada and Hirotaka Akita and Masaaki Kotera},\nyear={2021},\nurl={https://openreview.net/forum?id=V6WHleb2nV}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "V6WHleb2nV", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper597/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper597/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper597/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper597/Authors|ICLR.cc/2021/Conference/Paper597/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper597/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869255, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper597/-/Official_Comment"}}}, {"id": "hisof0IqJln", "original": null, "number": 2, "cdate": 1603640544481, "ddate": null, "tcdate": 1603640544481, "tmdate": 1605024651252, "tddate": null, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "invitation": "ICLR.cc/2021/Conference/Paper597/-/Official_Review", "content": {"title": "Unsurprising empirical results given similarity in datasets; unclear if conclusions generalize beyond this application/task", "review": "This work examines different datasets and strategies for pretraining a Transformer model to perform one-step retrosynthesis as a SMILES-to-SMILES translation task. Three approaches are evaluated: joint training, joint training on self-labeled data, and pretraining/fine-tuning.\n\nThe related work section and problem formulation setting are rather long, but provide a reasonable overview of approaches used to date. It may behoove the authors to move some of the results and discussion from the appendix to the main text and shorten the introduction in a revision/resubmission.\n\nThe target dataset selected in this work is the USPTO-50K set, which has become a standard benchmark; that is a reasonable choice. The two larger datasets used for augmentation are \u201cUSPTO-Full\u201d and \u201cUSPTO-MIT\u201d. All three of these datasets were taken from the patent literature (if that wasn\u2019t obvious by the name). In particular, they should all be strict subsets of USPTO-Full if the same preprocessing steps are used.\n\nMy major concern is that the pre-training + fine-tuning approach might not be a reasonable approach to use with these datasets. The pretraining setting is merely training on a larger dataset for the same task of identical noise/quality. The breadth of transformations in USPTO-Full is larger, but it should contain a superset of reactions and reaction types compared to USPTO-50K. This is a relatively minor domain shift, so it\u2019s not surprising that the pretraining approach works well. I appreciate that the authors have removed exact matches where the product SMILES are identical (hopefully after removing atom mapping and canonicalizing \u2013 please confirm), but there will be an abundance of very similar training examples even without exact matches.\n\nI would certainly not consider this approach to achieve the new state-of-the-art, as is claimed in the abstract because it is merely using additional data for training, which other studies *intentionally* do not do to allow for a fair comparison. \n\nThe results in Figure 2 showing the tradeoffs between top-1 and top-20 test accuracy are interesting. This does suggest using the validation set to monitor for overfitting is worthwhile.\n\nThere is a claim that over 99% of top-1 predictions are reasonable and appropriate hypotheses. However, there is no way to evaluate this because\u2014as far as I can tell\u2014this data is not contained in the submission. \n\nThis work does not make any significant contributions from an algorithmic or theoretical perspective and requires additional clarifications and discussion to justify the significance of its empirical results. I do find aspects of the analysis novel, but those aspects might be more interesting to an application- or domain-focused venue.\n\nFactual corrections:\n-\tThe statement \u201cany compound can be uniquely represented as a SMILES format string\u201d is potentially confusing. Many distinct SMILES strings can describe the same compound; SMILES strings are not unique. \n-\tThe USPTO-50K dataset was not curated by Lowe (2012). It is from https://doi.org/10.1021/ci5006614 \n-\tThe dataset released by Lowe contains over one million reaction examples; is the 877K number after filtering?\n\n\nMinor corrections:\n-\tFigure 1 labels retrosynthesis as $p(X|Y)$ rather than $p(Y|X)$; the notation of $X$ and $Y$ is slightly inconsistent with the notation in 3.1 using $x$ and $y$.\n-\tParameters in the likelihood function are referred to as both $\\theta$ and $\\theta_\\mathcal{M}$\n-\tOptional parameters are referred to as both $\\theta^*$ and $\\theta_{\\mathcal{M}^*}$\n-\tThe notation for argmax expressions is a little unconventional\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper597/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper597/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis", "authorids": ["~Katsuhiko_Ishiguro1", "ujihara@preferred.jp", "rsawada@preferred.jp", "~Hirotaka_Akita1", "kotera@preferred.jp"], "authors": ["Katsuhiko Ishiguro", "Kazuya Ujihara", "Ryohto Sawada", "Hirotaka Akita", "Masaaki Kotera"], "keywords": ["retrosynthesis", "data transfer", "transfer learninig", "pre-training", "fine-tuning", "self-training"], "abstract": "Retrosynthesis is a problem to infer reactant compounds to synthesize a given\nproduct compound through chemical reactions. Recent studies on retrosynthesis\nfocus on proposing more sophisticated prediction models, but the dataset to feed\nthe models also plays an essential role in achieving the best generalizing models.\nGenerally, a dataset that is best suited for a specific task tends to be small. In\nsuch a case, it is the standard solution to transfer knowledge from a large or\nclean dataset in the same domain. In this paper, we conduct a systematic and\nintensive examination of data transfer approaches on end-to-end generative models,\nin application to retrosynthesis. Experimental results show that typical data transfer\nmethods can improve test prediction scores of an off-the-shelf Transformer baseline\nmodel. Especially, the pre-training plus fine-tuning approach boosts the accuracy\nscores of the baseline, achieving the new state-of-the-art. In addition, we conduct a\nmanual inspection for the erroneous prediction results. The inspection shows that\nthe pre-training plus fine-tuning models can generate chemically appropriate or\nsensible proposals in almost all cases.", "one-sentence_summary": "Data Transfer improves the retrosynthesis models greatly, achieving new SotA with a simpler model. ", "pdf": "/pdf/67fc2c63d368a370b9f8b06c5d71a2553ced93e0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ishiguro|data_transfer_approaches_to_improve_seqtoseq_retrosynthesis", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QQsL9f70nb", "_bibtex": "@misc{\nishiguro2021data,\ntitle={Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis},\nauthor={Katsuhiko Ishiguro and Kazuya Ujihara and Ryohto Sawada and Hirotaka Akita and Masaaki Kotera},\nyear={2021},\nurl={https://openreview.net/forum?id=V6WHleb2nV}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper597/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139577, "tmdate": 1606915809813, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper597/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper597/-/Official_Review"}}}, {"id": "BqLOEkw_01w", "original": null, "number": 1, "cdate": 1603178364439, "ddate": null, "tcdate": 1603178364439, "tmdate": 1605024651188, "tddate": null, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "invitation": "ICLR.cc/2021/Conference/Paper597/-/Official_Review", "content": {"title": "Review", "review": "Summary & Strengths\n- Recent works for retrosynthesis focus on developing neural architectures in the USPTO-50 benchmark. In contrast, this paper aims at improving existing models using additional reaction datasets with transfer learning techniques.\n- This paper tries to apply three techniques (joint-training, self-training, and pre-training+fine-tuning) into a seq-to-seq model, Transformer. As reported in the paper, pre-training+fine-tuning achieves significant improvements over the baseline. The obtained results are comparable with SOTA methods that utilize different knowledge (e.g., atom-mapping or templates) instead of the augmented datasets.\n\n---\n\nConcern \\#1: Limited contribution\n- This paper just applies the simplest transfer methods to the existing seq-to-seq model. Although transfer learning for retrosynthesis is an interesting topic, I think the methodology contribution is very limited.\n- Additionally, there is no insightful (experimental) analysis.\n\nConcern \\#2: Experiments are not enough\n- The provided experiments are insufficient to demonstrate the effectiveness of transfer learning. For example, it is hard to guarantee that the used transfer learning techniques are useful for other retrosynthesis frameworks (e.g., GLN or G2Gs). I want to note that the transfer learning literature in other domains often experiments with various scenarios (e.g., different architectures, tasks, datasets) to demonstrate the effectiveness.\n- Why the gap between pre-training+fine-tuning and joint-training is too large? I think the gap is somewhat surprising, but there is no explanation and analysis of this.\n- Is the top-k accuracy only one merit of transfer learning? In vision tasks, pre-training can improve robustness [Hendrycks'19]. If other metrics (e.g., diversity [Schwaller'19]) are evaluated, the paper would be stronger.\n\n[Hendrycks'19] Using Pre-Training Can Improve Model Robustness and Uncertainty, ICML 2019 \\\n[Schwaller'19] Evaluation Metrics for Single-Step Retrosynthetic Models, Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019)\n\n---\n\nConclusion: The topic is interesting, but contribution seems to be very limited. More extensive experiments/analyses and/or a novel transfer technique specialized in retrosynthesis should be presented.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper597/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper597/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis", "authorids": ["~Katsuhiko_Ishiguro1", "ujihara@preferred.jp", "rsawada@preferred.jp", "~Hirotaka_Akita1", "kotera@preferred.jp"], "authors": ["Katsuhiko Ishiguro", "Kazuya Ujihara", "Ryohto Sawada", "Hirotaka Akita", "Masaaki Kotera"], "keywords": ["retrosynthesis", "data transfer", "transfer learninig", "pre-training", "fine-tuning", "self-training"], "abstract": "Retrosynthesis is a problem to infer reactant compounds to synthesize a given\nproduct compound through chemical reactions. Recent studies on retrosynthesis\nfocus on proposing more sophisticated prediction models, but the dataset to feed\nthe models also plays an essential role in achieving the best generalizing models.\nGenerally, a dataset that is best suited for a specific task tends to be small. In\nsuch a case, it is the standard solution to transfer knowledge from a large or\nclean dataset in the same domain. In this paper, we conduct a systematic and\nintensive examination of data transfer approaches on end-to-end generative models,\nin application to retrosynthesis. Experimental results show that typical data transfer\nmethods can improve test prediction scores of an off-the-shelf Transformer baseline\nmodel. Especially, the pre-training plus fine-tuning approach boosts the accuracy\nscores of the baseline, achieving the new state-of-the-art. In addition, we conduct a\nmanual inspection for the erroneous prediction results. The inspection shows that\nthe pre-training plus fine-tuning models can generate chemically appropriate or\nsensible proposals in almost all cases.", "one-sentence_summary": "Data Transfer improves the retrosynthesis models greatly, achieving new SotA with a simpler model. ", "pdf": "/pdf/67fc2c63d368a370b9f8b06c5d71a2553ced93e0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ishiguro|data_transfer_approaches_to_improve_seqtoseq_retrosynthesis", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QQsL9f70nb", "_bibtex": "@misc{\nishiguro2021data,\ntitle={Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis},\nauthor={Katsuhiko Ishiguro and Kazuya Ujihara and Ryohto Sawada and Hirotaka Akita and Masaaki Kotera},\nyear={2021},\nurl={https://openreview.net/forum?id=V6WHleb2nV}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper597/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139577, "tmdate": 1606915809813, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper597/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper597/-/Official_Review"}}}, {"id": "2tjlrJeLtvM", "original": null, "number": 4, "cdate": 1603891123647, "ddate": null, "tcdate": 1603891123647, "tmdate": 1605024651043, "tddate": null, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "invitation": "ICLR.cc/2021/Conference/Paper597/-/Official_Review", "content": {"title": "Interesting but premature investigation of data transfer in retrosynthesis", "review": "The paper uses finetuning for SEQ2SEQ translation models to improve performance for retrosynthesis prediction. In particular, transformer models are finetuned on a large dataset to improve performance on a smaller dataset. \nSuch an investigation of fine tuning approaches for retrosynthesis is interesting for the experts working in the area and therefore it is much appreciated that the authors did this study.  \n \nWhile promising, however, the work is still premature. Unfortunately, the performed experiments do not fully support the claims made, in particular, the investigation is neither \"systematic and intensive\". Furthermore, a SOTA baseline is not cited (MEGAN, see below), which means the approach reported here actually does not achieve SOTA in any task, in contrast to what the authors claim. \n \nThe underlying idea of the paper is somewhat incremental and lacks novel insight on the ML side. With additional experiments, the paper might be better suited for a good chemoinformatics journal where it would find an expert audience, however, for a leading ML conference, this is too little. Transfer learning for reaction prediction transformer models has already been explored by Pesciullesi et al. (2020) and Chen et al. (2019). \n \nThis reviewer is not convinced that the comparison provided in the paper are meaningful. \nAll of the models in the study, not just Transformer, could be improved by finetuning. In a \"systematic and intensive\" study, the authors should apply the fine tuning to the other models as well, e.g. GLN, RetroXpert, G2G, GraphRetro, and others, and not just to Transformers, where all of the hard work of implementation has been done already by someone else. Given that Transformer without finetuning is outperformed by all of these models, it is likely that finetuning these other models on the larger dataset will also outperform finetuned Transformer models. These key experiments are missing. \n \nregarding the experimental setup used and data:\nThe USPTO50k dataset the authors use for validation is a random subset of the larger USPTO full dataset. It is therefore no surprise that a model (pre)trained on the full dataset performs better than a model trained only on a subset of that dataset. \n \nWhy do the authors not compare the performance on the full dataset then directly? In a realistic scenario, the user would train on the largest dataset available, and not on a small subset. \n \nMissing references: \n \n- Segler et al perform fine tuning of sequence based models on SMILES https://arxiv.org/abs/1701.01329 (2017)  \n- Winter et al perform pretraining using sequence to sequence models on SMILES https://doi.org/10.1039/C8SC04175J (2018) \n- Sacha et al report the MEGAN model https://arxiv.org/abs/2006.15426 which performs better than the fine-tuned transformer presented here in in top10 and top20 accuracy (87.6, 91.6) vs (87.4, 89.6). ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper597/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper597/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis", "authorids": ["~Katsuhiko_Ishiguro1", "ujihara@preferred.jp", "rsawada@preferred.jp", "~Hirotaka_Akita1", "kotera@preferred.jp"], "authors": ["Katsuhiko Ishiguro", "Kazuya Ujihara", "Ryohto Sawada", "Hirotaka Akita", "Masaaki Kotera"], "keywords": ["retrosynthesis", "data transfer", "transfer learninig", "pre-training", "fine-tuning", "self-training"], "abstract": "Retrosynthesis is a problem to infer reactant compounds to synthesize a given\nproduct compound through chemical reactions. Recent studies on retrosynthesis\nfocus on proposing more sophisticated prediction models, but the dataset to feed\nthe models also plays an essential role in achieving the best generalizing models.\nGenerally, a dataset that is best suited for a specific task tends to be small. In\nsuch a case, it is the standard solution to transfer knowledge from a large or\nclean dataset in the same domain. In this paper, we conduct a systematic and\nintensive examination of data transfer approaches on end-to-end generative models,\nin application to retrosynthesis. Experimental results show that typical data transfer\nmethods can improve test prediction scores of an off-the-shelf Transformer baseline\nmodel. Especially, the pre-training plus fine-tuning approach boosts the accuracy\nscores of the baseline, achieving the new state-of-the-art. In addition, we conduct a\nmanual inspection for the erroneous prediction results. The inspection shows that\nthe pre-training plus fine-tuning models can generate chemically appropriate or\nsensible proposals in almost all cases.", "one-sentence_summary": "Data Transfer improves the retrosynthesis models greatly, achieving new SotA with a simpler model. ", "pdf": "/pdf/67fc2c63d368a370b9f8b06c5d71a2553ced93e0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ishiguro|data_transfer_approaches_to_improve_seqtoseq_retrosynthesis", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QQsL9f70nb", "_bibtex": "@misc{\nishiguro2021data,\ntitle={Data Transfer Approaches to Improve Seq-to-Seq Retrosynthesis},\nauthor={Katsuhiko Ishiguro and Kazuya Ujihara and Ryohto Sawada and Hirotaka Akita and Masaaki Kotera},\nyear={2021},\nurl={https://openreview.net/forum?id=V6WHleb2nV}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "V6WHleb2nV", "replyto": "V6WHleb2nV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper597/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139577, "tmdate": 1606915809813, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper597/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper597/-/Official_Review"}}}], "count": 7}