{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573612368, "tcdate": 1521573612368, "number": 292, "cdate": 1521573612036, "id": "HkEeJ115G", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "B1XqwdoIz", "replyto": "B1XqwdoIz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions", "abstract": "We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.", "pdf": "/pdf/43c7f958449ab7111186e59d48121c63d5fc4cc0.pdf", "paperhash": "eidnes|shifting_mean_activation_towards_zero_with_bipolar_activation_functions", "_bibtex": "@misc{\nhiller2018shifting,\ntitle={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\nauthor={Lars Hiller Eidnes and Arild N\u00f8kland},\nyear={2018},\nurl={https://openreview.net/forum?id=BkoXnkWAb},\n}", "keywords": [], "authors": ["Lars Hiller Eidnes", "Arild N\u00f8kland"], "authorids": ["larseidnes@gmail.com", "arild.nokland@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730174709, "tcdate": 1518204811445, "number": 48, "cdate": 1518204811445, "id": "B1XqwdoIz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "B1XqwdoIz", "original": "BkoXnkWAb", "signatures": ["~Lars_H_Eidnes1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions", "abstract": "We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.", "pdf": "/pdf/43c7f958449ab7111186e59d48121c63d5fc4cc0.pdf", "paperhash": "eidnes|shifting_mean_activation_towards_zero_with_bipolar_activation_functions", "_bibtex": "@misc{\nhiller2018shifting,\ntitle={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\nauthor={Lars Hiller Eidnes and Arild N\u00f8kland},\nyear={2018},\nurl={https://openreview.net/forum?id=BkoXnkWAb},\n}", "keywords": [], "authors": ["Lars Hiller Eidnes", "Arild N\u00f8kland"], "authorids": ["larseidnes@gmail.com", "arild.nokland@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730174709, "tcdate": 1509125155343, "number": 518, "cdate": 1518730174698, "id": "BkoXnkWAb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BkoXnkWAb", "original": "rk1m31bA-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions", "abstract": "We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.", "pdf": "/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf", "paperhash": "eidnes|shifting_mean_activation_towards_zero_with_bipolar_activation_functions", "_bibtex": "@misc{\nhiller2018shifting,\ntitle={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\nauthor={Lars Hiller Eidnes and Arild N\u00f8kland},\nyear={2018},\nurl={https://openreview.net/forum?id=BkoXnkWAb},\n}", "keywords": [], "authors": ["Lars Hiller Eidnes", "Arild N\u00f8kland"], "authorids": ["larseidnes@gmail.com", "arild.nokland@gmail.com"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}