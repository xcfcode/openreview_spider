{"notes": [{"id": "rJxGLlBtwH", "original": "Hkg1jtgYPS", "number": 2315, "cdate": 1569439818277, "ddate": null, "tcdate": 1569439818277, "tmdate": 1592858325273, "tddate": null, "forum": "rJxGLlBtwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "vOTurDFmk", "original": null, "number": 1, "cdate": 1576798745969, "ddate": null, "tcdate": 1576798745969, "tmdate": 1576800890157, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper investigates how two means of learning natural language - supervised learning from labeled data and reward-maximizing self-play - can be combined. The paper empirically investigates this question, showing in two grounded visual language games that supervision followed by self-play works better than the reverse. \n\nThe reviewers found this paper interesting and well executed, though not especially novel. The last is a reasonable criticism but in this case I think a little beside the point. In any case, since all the reviewers are in agreement I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795703736, "tmdate": 1576800251173, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Decision"}}}, {"id": "ryxFUWSwoB", "original": null, "number": 5, "cdate": 1573503313399, "ddate": null, "tcdate": 1573503313399, "tmdate": 1573503313399, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "S1eg12kTcS", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment", "content": {"title": "Answers re: OR game", "comment": "Thanks for your interest in our work!\n\n1) By assigning each object a unique word, we simulate a perfectly compositional language. One population of speaker and listener are trained on a fixed order of words. Different populations are trained on different permutations of the language. So there is no varying permutation during training. \n\n2) During self-play, the message received by the listener is the same as the one sent by the sender.\n\nHope this helps!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxGLlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2315/Authors|ICLR.cc/2020/Conference/Paper2315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143182, "tmdate": 1576860561534, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment"}}}, {"id": "r1g_7brwsB", "original": null, "number": 4, "cdate": 1573503263754, "ddate": null, "tcdate": 1573503263754, "tmdate": 1573503263754, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "SkeTV81RKr", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the kind and insightful review. \n\n> Naming not consistent\n\nYes, we indeed mean ec2supervised as sp2sup and sched as sup2sp in the first paragraph of Section 5. Thanks for pointing this out, we will correct all the naming errors and ensure consistency throughout in the final version.\n\n> Adding error bars\n\nYes, the current values are the mean over runs from 5 different seeds. We will add error bars to show variance in the final version.\n\n> The conclusion in Section 7 that \u2018S2P performs much worse than the other options\u2019 is contrary to previous results.\n\nIn this Section, we are referring to the \u201cpoor performance of the *sup2sp* method\u201d. The sup2sp method is defined in Figure 2 and Section 3.3, and refers to one of the many instantiations of the (more general) S2P framework. So what we are saying is not that S2P performs worse, but that sup2sp performs worse than other methods of S2P. This is shown in Figure 7a (not Figure 6, as we mistakenly wrote in the text). The reason for this is that, since self-play is a form of regularizer (see Figure 7b), doing self-play updates often leads to worse performance on the task (until more supervised updates are done). So, if you finish training with self-play updates without performing supervised updates, your performance will be quite low. We will clarify this in the final version. \n\n> Add more hyperparameter details\n\nThank you for pointing this out, we will indeed update and clarify this in the final version. \n\n> Figure 3b, colour is representative of performance. Is this mean accumulated reward? Please clarify to increase how informative this visualisation is, as currently it is unclear if yellow or blue is the desired value.\n\nYes, it shows the performance (mean accumulated reward) over 50 pairs of speakers and listeners, so a higher value (yellow) is desirable. It shows that the performance of both the speakers and listeners when paired randomly is found to be quite variable, although we do observe a slight preference towards their own partner (yellow diagonal). Due to space constraints, we chose to omit this but we will clarify this in the final version.\n\n> if all these methods are \"well known ways to combine self-play and supervised learning\" can all be supported by an (or preferably multiple) exemplar publications that used these method previously. \n\nWe do refer to Lewis et al. 2017 published at EMNLP\u201917 (oral) and Lazaridou et al. 2016 published at ICLR\u201917 (oral) for two S2P methods, sched_frz and rand respectively. The sup2sp and sp2sup are the baseline models which we use for comparing other sophisticated approaches, and which we compare to show that supervised learning before self-play generally improves performance (Section 5). The sched_rand_frz is a novel extension to the sched_frz method which we found was more stable and sometimes performed slightly better. \n\n> Reference errors, fig refs/naming errors \nThanks for pointing out these, we will add references to the published versions of these papers and fix the references/naming to the figures in the text.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxGLlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2315/Authors|ICLR.cc/2020/Conference/Paper2315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143182, "tmdate": 1576860561534, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment"}}}, {"id": "H1xXheHvoH", "original": null, "number": 3, "cdate": 1573503147195, "ddate": null, "tcdate": 1573503147195, "tmdate": 1573503147195, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "HJgVjdDRFB", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment", "content": {"title": "Response to Reviewer #2 (part 2/2)", "comment": "Clarification questions\n\n> What exactly does Figure 4c compare? Are both methods distilled from ensembles or is the blue line normal S2P while the other is distilled from an ensemble of compositional languages? It's not clear since point (3) in section 5 refers to the S2P result (not Pop-S2P) in that plot. I'm also assuming that PB-S2P means the same thing as Pop-S2P, but that's not made clear anywhere. Does PB stand for Population Based?\n\nFigure 4c compares two distilled policies. One is distilled from S2P populations (trained with X samples), and one is distilled from \u2018perfect emergent communication languages\u2019 (defined in the text) and fine-tuned on X samples. So both are population-based. We apologize for the naming error, by PB-S2P we indeed mean Pop-S2P. \n\n> In the rand setting how is convergence defined? Do both objectives need to converge or just one?\n\nFor the rand setting, both the objectives need to converge since we define convergence based on the performance of the listener on $\\mathcal{D}_{val}$. Fig 9 in Appendix shows that they indeed converge after certain number of train steps.\n\n> In the sched_rand_frz setting what is r?\n\nWe define r as the probability of freezing the speaker parameters as mentioned in Section 3.3. The actual number was mistakenly commented out in the submitted version. For reference, we use l=50 and m=50 for sched, r=0.5 for sched_rand_frz, and q=0.75 for rand. We will update this in the final version.\n\n> In the IBR how are the distractor images picked?\n\nThey are picked using a uniform random distribution over all the images available in the dataset.\n\n> Can't both self-play and supervision be used at the same time (just use a weighted combination of the two objectives)? I don't think the paper ever did this but it seems like a very useful variation to consider.\n\nYes this can indeed be done, by mixing gradient updates from both self-play and supervision in a single batch. This is quite close to the \u2018random\u2019 schedule (which alternates every example), and we don\u2019t expect to see much difference, although it could indeed be tried. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxGLlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2315/Authors|ICLR.cc/2020/Conference/Paper2315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143182, "tmdate": 1576860561534, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment"}}}, {"id": "r1gUKlHviB", "original": null, "number": 2, "cdate": 1573503101815, "ddate": null, "tcdate": 1573503101815, "tmdate": 1573503101815, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "HJgVjdDRFB", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment", "content": {"title": "Response to Reviewer #2 (part 1/2)", "comment": "We thank the reviewer for the kind and insightful review. \n\n> Point 3 of section 5 isn\u2019t very surprising, since most emergent comm doesn\u2019t do fine-tuning to transfer to human language. The point would be stronger if \u2018translation layers\u2019 were considered.\n\nIn point 3 of section 5, we provide evidence for the claim that doing supervised learning before self-play is better than the converse. Indeed, while the question of how to bridge the gap from emergent communication to natural language is of significant interest to the emergent communication community (see e.g. the upcoming NeurIPS Workshop on \u2018bridging the gap from emergent communication to natural language\u2019 https://sites.google.com/view/emecom2019/), you are right that there is no consensus as to how this should be done, and very few papers (Lazaridou et al 2016, Havrylov et al 2017) have examined this explicitly (although there are many papers that examine how to make emergent language more compositional to be closer to human language). Two natural ways to do this are: (1) to fine-tune an emergent language using human data (similar to our single-agent sp2sup), or (2) to use a population of emergent languages as a kind of \u2018prior\u2019 that can then be fine-tuned with human data (our population-based sp2sup is inspired by this). You are correct that there is also some work (Andreas et al., 2017) on translating (e.g. computing an alignment) between the emergent language and natural language, and we think an interesting next step would be to compare the task performance of these models to various S2P schedules. \n\n\n> Unclear if the results in figure 7a are significant\n\nWe will add error bars to the final version of our paper, which will clarify these concerns. Indeed, the difference between some of the schedules are not significant (we found that the standard error was in the range of 1-2%). Our analysis shows that there is not one S2P method that is clearly superior to all others, but slight improvements could be gained from switching schedules (which may be task dependent). \n\n> Skepticism about whether these trends will generalize to other tasks / models\n\nWe are assuming that by \u2018trends\u2019, the reviewer is referring to the order of performance of different S2P schedules. Our belief is that there are some trends that will hold when moving to other tasks / models (the lower performance of sp2sup and sup2sp). However, as mentioned above, our analysis showed that while there is indeed some variation between different S2P methods, there is no method that is clearly superior to all others. We suspect that using a schedule (potentially with parameter freezing) will perform better than the random schedule, however this may be task-dependent. We will clarify this view in the paper. \n\n> Minor weaknesses\n\nThank you for pointing these out, we will fix them in the final copy. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxGLlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2315/Authors|ICLR.cc/2020/Conference/Paper2315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143182, "tmdate": 1576860561534, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment"}}}, {"id": "SyltwyrDsS", "original": null, "number": 1, "cdate": 1573502816556, "ddate": null, "tcdate": 1573502816556, "tmdate": 1573502816556, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "BJxoajDCtS", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for your kind and insightful review! \n\n> Considering other natural language tasks, e.g. negotiation or recommendation\n\nWe agree that there are other tasks in NLP that have more direct applications than the OR and image-based referential (IBR) games. The reason we selected the IBR game is because it\u2019s the most common game with natural language that has been used in previous work on emergent communication (e.g. [1, 2, 3]). Thus, it makes sense to compare various supervised and self-play schedules on this task. We agree that a strong step for future work would be expanding to other complex natural language tasks such as negotiation (note that the suggested \u2018language recommendation\u2019 paper came out on arXiv only two weeks before the ICLR submission deadline, and was accepted only after the deadline). \n\n> Adding non-task related metrics to study the language. \n\nThis is an interesting suggestion. To help understand the difference in language generation policies for different S2P schedules, we will add qualitative examples to the Appendix, and investigate which other metrics we could add to compare the generated languages (most likely perplexity on the validation set, as it\u2019s unclear how one would automatically measure \u2018fluency\u2019 and \u2018consistency\u2019). In our experience, adding self-play usually results in a decrease in perplexity (because you are adding an objective that\u2019s not maximum likelihood), in exchange for better performance on the task. \n\n> Population-based S2P is incremental and unrelated\n\nYes, we agree that introducing population-based S2P is somewhat orthogonal to the main theme of our paper. We think population-based approaches have a lot of potential for improving grounded language learning with self-play especially when language tasks become more complex. This is because, for more complex language tasks, we hypothesize that self-play will result in larger deviations from natural language, and population-based approaches can help alleviate this. While our current distilled Pop-S2P result doesn\u2019t yet reach the ensemble result, the S2P ensemble results in Figure 6 have an improvement at least as large over the single-agent S2P result (8.8% for 1k samples, and 4.1% for 10k samples), than single-agent S2P has over the supervised learning baseline without self-play (7.8% for 1k samples, and 4.4% for 10k samples). Also, population-based methods help with some parts of our analysis (for example, generating Figure 4c: the \u2018perfect emcomm baseline\u2019 wouldn\u2019t be as intuitive without having the distiller trained on a population of such languages \u2014 see our response to Reviewer #2). With this being said, we agree that the presentation of this result could be changed in our paper to de-emphasize it, and will work on this in the final version. We\u2019d love to hear if the reviewer has recommendations for this. \n\n> Test on more variations of data size for better visualization\n\nWe did run experiments with 2k and 5k samples and show the training curves for different S2P methods in the Appendix. Due to space constraints, we chose to show results on only 1k and 10k in the main text of the paper. However, we can go ahead and add another graph showing performance vs. # samples to the final version of the paper. \n\n> Figures and captions need to be improved\n\nWe thank the reviewer for these observations, and will make the changes in our final version. \n\n\nReferences:\n[1] Multi-agent communication and the emergence of (natural) language, Lazaridou et al, 2016. \n[2] Compositional obverter communication learning from raw visual input, Choi et al, 2018.\n[3] Emergent communication in a multi-step, multi-modal referential game, Evtimova et al, 2017. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxGLlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2315/Authors|ICLR.cc/2020/Conference/Paper2315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143182, "tmdate": 1576860561534, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Comment"}}}, {"id": "SkeTV81RKr", "original": null, "number": 1, "cdate": 1571841589096, "ddate": null, "tcdate": 1571841589096, "tmdate": 1572972354823, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper explores the effect of ordering supervised learning and self-play on the resultant language learnt between agents. The topic is of high relevance to the ICLR community and makes several interesting insights useful to anyone learning control of a multi-agent system where communication amongst agents is applicable. I have several suggestions for improvements below, but all I believe are feasible to make within the time period of the rebuttal with the most necessary being:\n\n1) The naming of methods in Section 5 is not consistent with those introduced in section 3.3. For example, in the first paragraph ec2supervised is presumably sp2sup and sched is presumably sup2sp? Similarly, on page 7 (S2P and Pop-S2P) and Figure 4. Please revise and ensure consistency throughout.\n\n2) Figures 4b, 6 and 7 only present single values. Are these average values from repeated runs? If so please quantify variance.\n\n3) The conclusion in Section 7 at the bottom of page 8 that \"S2P performs much worse than the other options\" is contrary to previous results. Can the authors please comment on what features of the environment caused this difference?\n\n4) Appendix A includes details of hyperparameters, but some details remain unclear. Specifically, hyperparameter ranges swept over are shown but how were they then chosen from? Are they optimised for each environment and algorithm? What does the bold text in the table represent? If it is chosen values, why do only some parameters have chosen values? These are important details to enable reproduction of the paper.\n\nMinor Comments:\nIn Section 3.3, if all these methods are \"well known ways to combine self-play and supervised learning\" can all be supported by an (or preferably multiple) exemplar publications that used these method previously. Directly linking each to the previous work will further clarify the contribution this specific paper makes and help readers new to the area gain insight across the multiple papers this work builds upon.\n\nFigure 3b, colour is representative of performance. Is this mean accumulated reward? Please clarify to increase how informative this visualisation is, as currently it is unclear if yellow or blue is the desired value.\n\nPage 5, small typo \"introduced in the Lee et al. (2017)\" should be \"introduced in Lee et al. (2017)\".\n\nFigure 4b, the legend is blocking two bars and their corresponding value. It looks like moving to the bottom right may help, or placing above the plot.\n\nFigure 4 caption refers to a subfigure (d) that is not included.\n\nOn Page 6, the reference to babbling equilibrium should include a citation for interested readers to learn more about this well established concept.\n\nOn Page 7 there is a reference to Figure r4b, is this intended to be a reference to Figure 4b right?\n\nFigure 6 appears after Figure 7. Maintaining ordered numbering would be preferable.\n\nOn Page 9 it is noted some experimental results are in the Appendix but as there is a page and a half of space remaining before the 10 page limit, I would encourage to include all results in the main body of the paper.\n\nMultiple references do not list a publication venue (e.g. Evtimova et al., Lazaridou et al. 2018, Tieleman et al. 2018) or cite Arxiv versions when the work has been later published (e.g. Jacques et al. 2018 was published at ICML 2018). \n\nFigure 9 caption should state the environment."}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575517997952, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Reviewers"], "noninvitees": [], "tcdate": 1570237724582, "tmdate": 1575517997964, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Review"}}}, {"id": "HJgVjdDRFB", "original": null, "number": 2, "cdate": 1571874972371, "ddate": null, "tcdate": 1571874972371, "tmdate": 1572972354773, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary\n---\n\n(motivation)\nTo develop language speaking agents we can teach them to mimic human language\nor to solve tasks that require communication. The latter is efficient, but\nthe former enables interpretability. Thus we combine the two in an attempt\nto take advantage of both advantages. This paper studies a variety of ways to\ncombine these approaches to inform future work that needs to make this tradeoff.\n\n(approach)\nThe trade-off is studied using reference games between a speaker and a\nlistener. Goal oriented _self-play_ and human _supervision_ are considered two contraints one\ncan put on a network during learning. This work considers algorithms that vary\nwhen self-play and supervision are used (e.g., training with self-play then supervision,\nor supervision then self-play, or alternating back and forth between the two).\nAdditional variations freeze the speaker or distill an ensemble of agents into one agent.\n\n(experiments)\nA synthetic Object Reference game (OR) and a Image-Base Reference game (IBR) with real images are used for evaluation. Performance is accuracy at image/object guessing.\n1. (OR) Like previous work, this work finds that emergent languages are imperfect at supporting their goals and cannot be understood by agents that only understand a human language like English.\n2. (OR) Pre-training with supervision then fine-tuning with self-play is superior to pre-training with self-play then fine-tuning with supervision. This is presented as surprising from the perspective of language emergence literature, which is though of as pre-training with self-play.\n3. (IBR) Distilling an agent from an ensemble of 50 independently trained agents outperforms training single agents from scratch, but is still not as good as the whole ensemble.\n\nSelf-play vs supervision schedules:\n4. (IBR) Supervision (using image captions) followed by self-play performs much worse than all other approaches.\n5. (IBR) Alternating between supervision and self play (e.g., randomly choosing supervision or self-play every iteration) performs best.\n\n\n\nStrengths\n---\n\nThe curricula considered by this paper seem to have a sigificant impact on performance. These are new and could be important for future work on language learning, which may have considered the sup2sp setting from figure 7a without considering the sched setting.\n\nThe diversity of experiments provided and the analysis help the reader get a better sense for how emergent communication models work.\n\nIt's nice to see experiments on both a toy setting and a setting with realistic images.\n\nFuture directions suggested throughout the paper are interesting.\n\n\nWeaknesses\n---\n\n\n* The 3rd point of section 5 is presented as a major conclusion of this paper, but it is not very surprising and I don't see how it's very useful. The perspective of language emergence literature is presented a bit strangely. The self-play to supervision baseline seems to be presented as an approach from the language emergence literature. I don't think this is what any of that literature promotes exactly, though it is close. Generally, I (and likely others) don't think it's too surprising that trying to fine-tune a self-play model with language supervision data doesn't work very well, for the same reasons cited in this paper (point 3 of section 5). I think the general strategy when trying to gain practical benefits from self-play pre-training is a translation approach where the learned language is translated into a known language like English rather than trying to directly align it to English as does the supervision approach in this paper. This particular baseline would be more useful if the paper considered learning some kind of translation layer on top of the self-play pre-trained model.\n\n* How significant are the performance differences in figure 7a, especially those between the frozen and non-frozen models? Is the frozen model really better or this performance difference just due to noise?\n\n* I'm somewhat skeptical that these trends will generalize to other tasks/models. The main goal of this paper is to inform future work. That makes it even more important than normal that the trends identified here are likely to generalize well. Are these trends likely to generalize well? Does the paper address when these trends are expected to hold anywhere?\n\n\nMinor Presentation Weaknesses:\n\n* Figure 4: I think the sub-figures are mis-labeled in the caption.\n\n* In the related work I'm not sure the concept of generations is right. I think it should refer to different languages of different agents across time rather than different languages of the same agent across time.\n\n\nMissing details / clarification questions:\n\n* What exactly does Figure 4c compare? Are both methods distilled from ensembles or is the blue line normal S2P while the other is distilled from an ensemble of compositional languages? It's not clear since point (3) in section 5 refers to the S2P result (not Pop-S2P) in that plot. I'm also assuming that PB-S2P means the same thing as Pop-S2P, but that's not made clear anywhere. Does PB stand for Population Based?\n\n* In the rand setting how is convergence defined? Do both objectives need to converge or just one?\n\n* In the sched_rand_frz setting what is r?\n\n* In the IBR how are the distractor images picked?\n\n\nSuggestions:\n\n* Can't both self-play and supervision be used at the same time (just use a weighted combination of the two objectives)? I don't think the paper ever did this but it seems like a very useful variation to consider.\n\n\nPreliminary Evaluation\n---\n\nClarity: The writing is fairly clear, though some details are lacking.\nSignificance: This work could help inspire some future models in the language emergence literature.\nQuality: Experiments are aligned with the paper's goals and support its conclusions.\nOriginality: The distillation approach and curricula are novel.\n\nOverall the work could prove to be an interesting and useful reference point inside the language emergence literature so I recommend it for acceptance.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575517997952, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Reviewers"], "noninvitees": [], "tcdate": 1570237724582, "tmdate": 1575517997964, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Review"}}}, {"id": "BJxoajDCtS", "original": null, "number": 3, "cdate": 1571875779201, "ddate": null, "tcdate": 1571875779201, "tmdate": 1572972354722, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigated how two conflicting learning objectives; supervised and self-play updates could be combined with a focus on visual-grounded language tasks. With a different set of their combinations, the authors empirically found that alternating two learning updates may result in the best equilibrium state; consistency with samples in the supervised dataset and optimal state with high rewards in the task environment.\n\nThe paper is very well-written, and I really enjoyed reading it overall. There are some typos, presentation issues, and minor format issues (e.g., wrong naming) though. I do like this kind of simple but insightful result with enough empirical observations and discussions. Even though there is not that novel method proposed, the overall message found from the experiments, their interpretation by the authors, and meaningful comparisons to the past works in emergent communication are fair enough to learn high scientific values from it. The design of the experiment is again very simple (e.g., changing the size of data, switching two setups in different ways) but clear to understand. This work is a good example of how well-designed hypotheses and their empirical validation could contribute to the field. I also appreciate the large spectrum of literature surveys including from the recent advances (Lewis et al., 2017, Lee et al., 17) to the past literature in emergent communications such as Littman (1994) and (Farrell & Rabin, 1996). \n\nOne of my concerns is the lack of applications, especially on the tasks using more natural language. The two tasks; OR and IBR, seem to be very limited settings to evaluate how self-play operates with data supervision. As pointed out by the authors, supervision from the training data itself may include most of the unexplored cases of the task, leading a less chance to learn policies from the high rewards. I think more realistic tasks using natural language need to be considered: negotiation (e.g., Lewi\u2019s task, \u201cDecoupling strategy and generation in negotiation dialogues\u201d), recommendation (e.g., \u201cRecommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue\u201d), and more. I agree with the point made by the authors that this work mainly focuses on investigation rather than exploitation. But, then it would be adding another emergent task where the self-play can learn many more policies than one in the supervised dataset. \n\nAdding to the point, I was expecting to see non-task related metrics to measure the effectiveness of their appropriate combinations. For example, it would be better to add language-side metrics (e.g., perplexity, fluency, consistency) to measure how language degeneration varies by the different combinations. This issue is not addressed in the paper, and I guess this is mainly because of the limited usage of language in the two limited tasks. If the paper is only focusing on emergent language which is related to specific tasks, it would be better to tone-down a little bit and state the major difference of it with natural language. \n\nThe population-based S2P seems to be a bit incremental and unrelated to the main theme of the paper. To me, the motivation of adding POP into S2P based on the policy variability is somewhat different from the original claim about the combination of supervised and selfplay. Also, the improvements on IBR in Figure 7 are incremental, making the major claim of this work little divergent. \n\nIn terms of presentation, if you like to show how performance changes over the different sizes of data, it would be better to show it by graphs over different variations instead of the bar charts only with 10k and 50k sizes. In addition, the figures and captions need to be improved for better interpretation. I think they are written in a hurry or changed a lot in the last minutes. Please see some minor formatting issues below. \n\n\nMinor comments:\nDuplicate reference of (Lewis et al., 2017)\nSome names defined in Section 3.3 and Section 5 are not exactly matched.\nFigures and fonts in Figures 4 and 7 are a little difficult to understand. Especially, I can\u2019t understand the two upper figures in Figure 4a\nCaptions in Figure 4 are not matched with the sub-figures. \nFigure r4b -> Figure 4b\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2315/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2315/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575517997952, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2315/Reviewers"], "noninvitees": [], "tcdate": 1570237724582, "tmdate": 1575517997964, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Official_Review"}}}, {"id": "S1eg12kTcS", "original": null, "number": 1, "cdate": 1572826072504, "ddate": null, "tcdate": 1572826072504, "tmdate": 1572826072504, "tddate": null, "forum": "rJxGLlBtwH", "replyto": "rJxGLlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2315/-/Public_Comment", "content": {"title": "About Implementation of the OR Game", "comment": "Dear Authors,\n\nThanks for the interesting work. I am currently trying to build upon your Obect Reconstruction settings. I realize that you mentioned the target language is in arbitaray order. E.g. \"blue triangle shaded\" would be the same as \"blue shaded triangle\". My question is\n1. During pretraining/supervised learning, is speaker/listener trained on different permutation of the language\n2. During selfplay, will the communication channel send the permuted message from speaker to listener?"}, "signatures": ["~Yuchen_Lu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yuchen_Lu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rlowe1@cs.mcgill.ca", "abhinav.gupta@umontreal.ca", "jakobfoerster@gmail.com", "dkiela@fb.com", "jpineau@cs.mcgill.ca"], "title": "On the interaction between supervision and self-play in emergent communication", "authors": ["Ryan Lowe*", "Abhinav Gupta*", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau"], "pdf": "/pdf/da56730d7142e8a0260f22bb5926200d5b7daab4.pdf", "abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.", "keywords": ["multi-agent communication", "self-play", "emergent languages"], "paperhash": "lowe|on_the_interaction_between_supervision_and_selfplay_in_emergent_communication", "code": "https://github.com/backpropper/s2p", "_bibtex": "@inproceedings{\nLowe*2020On,\ntitle={On the interaction between supervision and self-play in emergent communication},\nauthor={Ryan Lowe* and Abhinav Gupta* and Jakob Foerster and Douwe Kiela and Joelle Pineau},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxGLlBtwH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2ac50412acb3744ef0ec33f3d0dcc778401ee434.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxGLlBtwH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504182064, "tmdate": 1576860594615, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2315/Authors", "ICLR.cc/2020/Conference/Paper2315/Reviewers", "ICLR.cc/2020/Conference/Paper2315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2315/-/Public_Comment"}}}], "count": 11}