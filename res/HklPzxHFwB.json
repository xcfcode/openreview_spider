{"notes": [{"id": "HklPzxHFwB", "original": "H1gPHMlYPB", "number": 2176, "cdate": 1569439758781, "ddate": null, "tcdate": 1569439758781, "tmdate": 1577168276987, "tddate": null, "forum": "HklPzxHFwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "NEBj3SHKM0", "original": null, "number": 1, "cdate": 1576798742475, "ddate": null, "tcdate": 1576798742475, "tmdate": 1576800893739, "tddate": null, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "invitation": "ICLR.cc/2020/Conference/Paper2176/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a new method for zero-shot policy transfer in RL. The authors propose learning the policy over a disentangled representation that is augmented with attention. Hence, the paper is a simple modification of an existing approach (DARLA). The reviewers agreed that the novelty of the proposed approach and the experimental evaluation are limited. For this reason I recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713737, "tmdate": 1576800263414, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2176/-/Decision"}}}, {"id": "HJgQJGREsH", "original": null, "number": 3, "cdate": 1573343706919, "ddate": null, "tcdate": 1573343706919, "tmdate": 1573343706919, "tddate": null, "forum": "HklPzxHFwB", "replyto": "HyxtTsDjKH", "invitation": "ICLR.cc/2020/Conference/Paper2176/-/Official_Comment", "content": {"title": "Responses to your comments & Revision uploaded", "comment": "Thank you for your review. \n\nI have uploaded a revision which addresses your comments and also respond to them here.\n\nLimited applicability of the proposed methods:\n- I have now made my focus on this setting clear in the introduction. Other work has shown success in the  problem of transferring to domains with differing dynamics. Thus, I focus on the orthogonal (and yet unsolved) problem of visual domain transfer.\n- In the revised introduction and related work section, I show the importance of learning a dis-entangled representation for images. Not only has much work (now mentioned in related work section) focused on learning a dis-entangled representation, doing so would allow a (visual) Reinforcement Learning agent to learn a sufficient and transferable state representation. The question of how to best learn a state representation (often referred to as state abstraction) is an open problem in Reinforcement Learning\n\nLimited technical novelty:\n- In the related work section, I discuss the problems with DARLA (preserving domain shift, including irrelevant information in the state representation). The attention mechanism directly addresses these problems by ignoring irrelevant information and preserving only information needed to solve the RL task (reducing domain shift).\n- In the SADALA Training section (4.4), I now discuss the tradeoff between training in separate stages vs end-to-end and the importance of weight freezing.\n\nInsufficient experiments:\n- I am currently re-running the experiments to verify their outputs. Shortly, I will upload a revision with the updated results and discussion.\n\nQuantitative results:\n- Model Agnostic Meta Learning (MAML) and related model-agnostic methods do not solve the problem posed in my paper (zero shot transfer). They focus on meta-learning such that a model is able to learn a new task with few samples. This is few-shot learning, not zero-shot transfer.\n- I am not sure which different methods and different parameters refer to. I have compared against DARLA with the parameters used in its paper. Additionally, the parameters for my RL algorithms are fixed across different approaches (DARLA, Domain Randomization, SADALA). \n\nReproducibility:\n- The original code was open sourced and a link was included in submission to openreview along with the paper. Once the experiments are re-run, I will update the link to the open source code."}, "signatures": ["ICLR.cc/2020/Conference/Paper2176/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper2176/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklPzxHFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference/Paper2176/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2176/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2176/Reviewers", "ICLR.cc/2020/Conference/Paper2176/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2176/Authors|ICLR.cc/2020/Conference/Paper2176/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145226, "tmdate": 1576860536746, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference/Paper2176/Reviewers", "ICLR.cc/2020/Conference/Paper2176/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2176/-/Official_Comment"}}}, {"id": "SyxgXjaVir", "original": null, "number": 2, "cdate": 1573341976275, "ddate": null, "tcdate": 1573341976275, "tmdate": 1573341976275, "tddate": null, "forum": "HklPzxHFwB", "replyto": "H1gu5-h6Yr", "invitation": "ICLR.cc/2020/Conference/Paper2176/-/Official_Comment", "content": {"title": "Responses to your comments and Revision uploaded", "comment": "Thank you for your review.\n\nI have revised and re-uploaded the paper to address your comments.\n\n1) To my best knowledge, this paper is the first time in domain adaptation for RL that has explicitly learned a state representation that ignores irrelevant visual features, attention mechanism or not. I have made this clear in the introduction of the re-uploaded paper.\n\n2) \n- In the (re-uploaded) related work section, I have made the problems with the original approach clear: DARLA preserves domain shift due to the encoding of its state representation. Since its beta-VAE is incentivized to reconstruct the image, it preserves differences between the source and target domains, making transfer difficult. Our approach eliminates this domain shift by learning to attend to only the features relevant to solving the RL task and ignoring all others.\n- I am re-running the experimental results to verify their output (reconstruction and transfer performance). I will upload the results shortly.\n\n3) \n- I have made clear which of the citations are related work and which are baselines in the (new) related work section. Many of the approaches cited require samples from both the source and target domains and can only transfer to that target domain. \n- The only related works that do not have this problem are DARLA and Domain Randomization, both of which are compared against as baselines."}, "signatures": ["ICLR.cc/2020/Conference/Paper2176/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper2176/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklPzxHFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference/Paper2176/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2176/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2176/Reviewers", "ICLR.cc/2020/Conference/Paper2176/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2176/Authors|ICLR.cc/2020/Conference/Paper2176/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145226, "tmdate": 1576860536746, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference/Paper2176/Reviewers", "ICLR.cc/2020/Conference/Paper2176/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2176/-/Official_Comment"}}}, {"id": "BJxFF8pEjS", "original": null, "number": 1, "cdate": 1573340800792, "ddate": null, "tcdate": 1573340800792, "tmdate": 1573340800792, "tddate": null, "forum": "HklPzxHFwB", "replyto": "S1xiYtYZ9S", "invitation": "ICLR.cc/2020/Conference/Paper2176/-/Official_Comment", "content": {"title": "Related work discussion has been added", "comment": "Thank you for your review.\n\n- I have re-uploaded the paper and addressed your concerns. I have added a related work section, placing my work in the context of exiting literature, adding other relevant work. \n-- DANNs, ADDA, PixelIDA/SimGAN, and CycleGAN are discussed in this section. They focus on visual domain adaptation where samples from the target domain are present during training time and transfer is only to that target domain. \n-- As mentioned in this section, I am solving a different problem, where samples from the target domain are not present during training time, similar to DARLA. Thus, it is good to discuss these approaches, but not necessary to empirically compare against them.\n\n- I have added additional experimental details in the appendix. Specifically, methods requiring multiple source domains to train are evaluated (in fig 6) on one domain randomly sampled from the set of source domains. This sample is the same for the evaluation of all algorithms.\n\n- I am currently running domain randomization for deepmind lab and will upload results shortly.\n- I am re-running experiments (particularly for the reconstruction of figure 5) to verify their output and will upload results shortly."}, "signatures": ["ICLR.cc/2020/Conference/Paper2176/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper2176/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklPzxHFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference/Paper2176/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2176/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2176/Reviewers", "ICLR.cc/2020/Conference/Paper2176/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2176/Authors|ICLR.cc/2020/Conference/Paper2176/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145226, "tmdate": 1576860536746, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2176/Authors", "ICLR.cc/2020/Conference/Paper2176/Reviewers", "ICLR.cc/2020/Conference/Paper2176/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2176/-/Official_Comment"}}}, {"id": "HyxtTsDjKH", "original": null, "number": 1, "cdate": 1571679169084, "ddate": null, "tcdate": 1571679169084, "tmdate": 1572972373157, "tddate": null, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "invitation": "ICLR.cc/2020/Conference/Paper2176/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Pros:\nThis paper proposed a new method for zero-shot transfer learning under the reinforcement learning setting. The use of attention weights to regularize the latent states was fairly interesting.\n\nCons:\nLimited applicability of the proposed methods\n- The paper was restricted in a setting where rewards, actions, and true states were identical between source and target environments, and only the observed states differed due to differing renderers. Working under such a restricted setting was interesting in its own right, but it might also lead to limited applicability of the proposed method in the real-world setting.\n- The proposed method focused on solving a very specific problem: learning a dis-entangled latent representation for images. As a result, the potential impact of the proposed methods could be minimal.\n\nLimited technical novelty\n- The proposed method, SADALA, was built on top of Higgins et al., 2017 (DARLA). The only difference was an added attention layer to the learning of latent states. As a result, the novelty of the proposed method was very incremental and limited from a technology perspective.\n- Even with additional attention layer, the paper could have performed a more thorough study to help the readers understand and appreciate the idea. For example, this paper didn\u2019t discuss the tradeoff between training SADALA over separate stages, versus training it from end to end. For example, why the weights of the pre-trained beta-VAE had to be frozen and used as weights in the state representation stage.\n\nInsufficient experiments\n-More thorough discussion of the qualitative results should be helpful to understand whether the attention weights helped the model to focus on the right thing. For example, this paper did study the quality of reconstruction in Figure 3-5 of the proposed method. When comparing Figure 3 and Figure 5, it appeared to me that the reconstructed the angle of the pole was different from the original one. And it seemed like attention weights did successfully ignored the color of the cart and pole, but it ignored the angle of the pole, which should be important to the learning task. Unfortunately, the paper didn't further explain the implication of such misrepresentation.\n\n-Quantitative results \n* It would be interesting to all compare the proposed methods against model-agonistic methods like MAML\n* It would be useful to include confidence intervals over different tasks.\n* It would be useful to compare different methods with different parameter settings\n* The authors mentioned \u201cVisual Pendulum tasks\u201d but didn\u2019t include them in the paper\n\n\nReproducibility\n- It's unclear to me how reproducible the research conducted in this paper was, and it would be useful to open source the code used to conduct the experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper2176/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2176/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575370053209, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2176/Reviewers"], "noninvitees": [], "tcdate": 1570237726610, "tmdate": 1575370053223, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2176/-/Official_Review"}}}, {"id": "H1gu5-h6Yr", "original": null, "number": 2, "cdate": 1571828111937, "ddate": null, "tcdate": 1571828111937, "tmdate": 1572972373120, "tddate": null, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "invitation": "ICLR.cc/2020/Conference/Paper2176/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes adding an attention mechanism to the DARLA beta-VAE approach to transfer learning. The beta-VAE, soft attention and policy are trained on appropriate source tasks and evaluated zero-shot on target tasks, using two more difficult continuous control domains with RGB observations. Results indicate some improvements to compared to the immediate relevant baseline which may be statistically significant, but it is not clear whether over 10% in practice. \n\nI cannot at this stage recommend acceptance for the following reasons:\n1) The paper augments an existing method with a well understood attention mechanism, so the novelty of the approach is relatively low.\n2) The experimental results are interesting, but I don't find them compelling enough to recommend acceptance based on the results alone.  The paper does not solve a major problem with the approach it is based on. In fact, the improvement seems to be smaller when the environment is more complex.\n3) Several baselines which are cited in the paper are actually missing in the experiments, so it is hard to determine how important is that roughly 10% improvement compared to SOTA.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2176/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2176/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575370053209, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2176/Reviewers"], "noninvitees": [], "tcdate": 1570237726610, "tmdate": 1575370053223, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2176/-/Official_Review"}}}, {"id": "S1xiYtYZ9S", "original": null, "number": 3, "cdate": 1572080003331, "ddate": null, "tcdate": 1572080003331, "tmdate": 1572972373072, "tddate": null, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "invitation": "ICLR.cc/2020/Conference/Paper2176/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summarize what the paper claims to do/contribute.\n- The paper proposes a new method for zero-shot visual transfer for RL, SADALA. The method first learns a feature extractor with attention (to focus on realted features only) and then learns a policy in the source task and is able to transfer zero-shot int he target domain. The method is evaluated on two tasks: Cartpole-v1 (Gym) and \"Collect Good Objects\" (Deepmind Lab). It is compared against DARLA for both tasks and against Domain Randomization only for Cartpole. \n\nClearly state your decision (accept or reject) with one or two key reasons for this choice.\nReject.\n- The experiments of the paper were particularly weak. \n--More standard visual adaptation techniques like DANNs,ADDA, PixelDA/SimGAN, CycleGAN were not considered. \n--The results on domain randomization were not convincing: more details are necessary to determine what the experimental protocol was. One major question: what is the source domain in the case of domain randomization (for Fig. 6) In any case, I find it very hard to believe that simple domain randomization considered here can not fully solve this task for all visual pertrubations considered. \n-- In Fig. 5 the reconstruction is not correct.\n-- Domain randomization was not tried on the DeepMind Lab example because of compute. However, I'd encourage the authors to try this. Converging will surely not be  linear to the number of perturbations considered as it seems to be implied. Also the OpenAI paper cited as an example where domain randomization took 100 years of simulation required for transfer is a problem of rather different scale: the domain gap there is between simulation and reality for an anthropomorfic robotic hand, and not a simple visual gap where the color of an identical environment are changed. \n\n-Related work discussion was insufficient\n-- Related work section is missing and work is not adequately placed in the context of existing literature in the Introduction where some related work is indeed discussed.\n-- Related work at the last sentence of the introduction is not discussed correctly. It is implied that all these works are on domain randomization which is not true. Also one work (Chebotar et al) is not relevant as from what I recall there was no visual gap. Finally most of these works deal with much more complex visual gaps so sample complexity is hard to be compared.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2176/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2176/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-Shot Policy Transfer with Disentangled Attention", "authors": ["Josh Roy", "George Konidaris"], "authorids": ["josh_roy@brown.edu", "gdk@cs.brown.edu"], "keywords": ["Transfer Learning", "Reinforcement Learning", "Attention", "Domain Adaptation", "Representation Learning", "Feature Extraction"], "TL;DR": "We present an agent that uses a beta-vae to extract visual features and an attention mechanism to ignore irrelevant features from visual observations to enable robust transfer between visual domains.", "abstract": "Domain adaptation is an open problem in deep reinforcement learning (RL). Often, agents are asked to perform in environments where data is difficult to obtain. In such settings, agents are trained in similar environments, such as simulators, and are then transferred to the original environment. The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. We present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent). SADALA first learns a compressed state representation. It then jointly learns to ignore distracting features and solve the task presented. SADALA's separation of important and unimportant visual features leads to robust domain transfer. SADALA outperforms both prior disentangled-representation based RL and domain randomization approaches across RL environments (Visual Cartpole and DeepMind Lab).", "code": "https://drive.google.com/open?id=1KRaAjLofmAGpk2OhsmjUaXG1-DExkpOy", "pdf": "/pdf/322bb6c771a079984c6c04156b7df09e508bddf9.pdf", "paperhash": "roy|zeroshot_policy_transfer_with_disentangled_attention", "original_pdf": "/attachment/503df86ac6dcfa5dbfd9c88018f25d626bb35c1c.pdf", "_bibtex": "@misc{\nroy2020zeroshot,\ntitle={Zero-Shot Policy Transfer with Disentangled Attention},\nauthor={Josh Roy and George Konidaris},\nyear={2020},\nurl={https://openreview.net/forum?id=HklPzxHFwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklPzxHFwB", "replyto": "HklPzxHFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2176/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575370053209, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2176/Reviewers"], "noninvitees": [], "tcdate": 1570237726610, "tmdate": 1575370053223, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2176/-/Official_Review"}}}], "count": 8}