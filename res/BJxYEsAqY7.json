{"notes": [{"id": "BJxYEsAqY7", "original": "SkgN6zT_Y7", "number": 18, "cdate": 1538087728708, "ddate": null, "tcdate": 1538087728708, "tmdate": 1545355402524, "tddate": null, "forum": "BJxYEsAqY7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1lyvAMggN", "original": null, "number": 1, "cdate": 1544724054924, "ddate": null, "tcdate": 1544724054924, "tmdate": 1545354510357, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Meta_Review", "content": {"metareview": "The paper describes knowledge distillation methods. As noted by all reviewers, the methods are very similar to the prior art, so there is not enough novelty for the paper to be accepted. The reviewers' opinion didn't change after the rebuttal.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "lack of novelty"}, "signatures": ["ICLR.cc/2019/Conference/Paper18/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper18/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353367632, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper18/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper18/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper18/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353367632}}}, {"id": "H1eFJzwlyE", "original": null, "number": 7, "cdate": 1543692769385, "ddate": null, "tcdate": 1543692769385, "tmdate": 1543692769385, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "SklFTP690Q", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "content": {"title": "after the rebuttal", "comment": "I read the rebuttal. It didn't change my rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper18/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper18/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621806, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxYEsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper18/Authors|ICLR.cc/2019/Conference/Paper18/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621806}}}, {"id": "rJeOGv5pCX", "original": null, "number": 6, "cdate": 1543509775883, "ddate": null, "tcdate": 1543509775883, "tmdate": 1543509775883, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "r1xhmXmKRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "content": {"title": "Reply to authors", "comment": "Thank you for your reply.\n\nSo as far as I can tell, pFeed is an ensemble of activations from teachers (or some function thereof) . This is effectively FitNets (https://arxiv.org/abs/1412.6550)/attention transfer with an ensemble of teachers.\n\nIt is interesting to know that the BAN results aren't reproducible. I think when this happens, it's best to put your locally reproduced BAN results next to it and state that you weren't able to reproduce their results.\n\nThank you for providing CIFAR-10 results.\n\nI will, however, stick with my original review score as I am still concerned about the lack of novelty of the methods."}, "signatures": ["ICLR.cc/2019/Conference/Paper18/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper18/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621806, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxYEsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper18/Authors|ICLR.cc/2019/Conference/Paper18/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621806}}}, {"id": "r1xhmXmKRQ", "original": null, "number": 3, "cdate": 1543217955563, "ddate": null, "tcdate": 1543217955563, "tmdate": 1543217994477, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "HkehgnZ8nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "content": {"title": "Answer to reviewer #3 ", "comment": "Thank you for the constructive review.\n\nFirst, the sFEED, though the purpose of BANs[1] and ours are different, sFEED in our work and BANs ended up with similar training structure, and we admit that it lacks its novelty for their similarity in architecture with BANs. \nHowever, for the pFEED, we did not, and could not use \u2018ensemble of teachers\u2019 that you noted, because our purpose is to deliver the knowledge of ensemble at feature map level without using teachers\u2019 predictions.\n\nThe purpose of Table 6 is not to show that our method beat others, but to show that ours are fairly good. We actually tried to reproduce the BANs, but unfortunately we could not reproduce the base DenseNet models that BANs used. They reported that error of base DenseNet-80-80 is 17.16, but our reproduction of it only could achieve 17.71. To the best of our knowledge, their DenseNet models such as DenseNet-80-80 or DenseNet-80-120 are not public release, and they are modulation of BANs, so we could not find differences in detail. The WRN-28-10 with first iteration at sFEED can be fair to compare with WRN-28-10 at BAN-1, but sFEED with 1 iteration is just FT without paraphraser.\n\nHowever, we think that the Table 4. can give fair and meaningful comparison. The KD column in Table 4 are those who really use \u2018ensemble of teachers\u2019 for training. Comparing ours with them shows that ours can be beneficial with giving specific knowledge with knowledges of ensemble at feature map level.\n\nFor the CIFAR-10 experiment, I do not necessarily feels the importance to report since it already performs well on CIFAR-100, which is more difficult task, but I had experimented for few networks with sFEED, but did not report it on our paper. The result was \nResNet-56   6.97 -> 6.16\nResNet-110 6.43 -> 5.92\nWRN28-10   4.00 -> 3.62\n\nTesting on the tasks other than classification that you suggested would be good idea to try, and can be our future work.\n\n[1] Furlanello et al. Born-Again Neural Networks, 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper18/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621806, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxYEsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper18/Authors|ICLR.cc/2019/Conference/Paper18/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621806}}}, {"id": "rJgCQfmY0m", "original": null, "number": 2, "cdate": 1543217701883, "ddate": null, "tcdate": 1543217701883, "tmdate": 1543217701883, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "BJg-eYsw3m", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "content": {"title": "Answer to reviewer #2", "comment": "Thank you for your constructive feedback. I appreciate it and it was helpful in many points\n \nFirst, for the novelty issue, I admit that sFEED is not that novel because it is similar with BANs[1]. While we were working on our paper, we came across with BANs and found sFEED is similar with BANs, but I thought it is worth reporting it because sFEED performed better at ImageNet.\n \nSecond, for the motivations, sorry for our bad at writing it clearly. We mentioned our point again at Answer to reviewer #1.\n\nFor the related works and reference parts, we found that it was messy, and we will deliberately reflect what you pointed out.\n\nFor the experiments, we agree on your point that it would be better if we had compared ours with other knowledge transfer algorithms that use information from feature map level(you pointed out AT[3])\nFor the comparison on Table 6, since our purpose is to show that our method can deliver ensemble knowledge at feature map level, we just wanted to show that the performance is decent level. \nActually, we tried, but failed to reproduce the base DenseNet[2] models that BANs[1] used for their base network which are their own modulation. Maybe the only one that can be compared is WRN-28-10 with BAN-1 and our sFEED with one iteration.\n\nThanks for pointing out the typos.\n\n[1] Furlanello et al. Born-Again Neural Networks, 2018\n[2] Huang et al. Densely Connected Convolutional Networks, 2016\n[3] Zagoruyko et al. Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, 2016\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper18/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621806, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxYEsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper18/Authors|ICLR.cc/2019/Conference/Paper18/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621806}}}, {"id": "HJlQjZQYAX", "original": null, "number": 1, "cdate": 1543217563186, "ddate": null, "tcdate": 1543217563186, "tmdate": 1543217563186, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "rJgiuVx-6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "content": {"title": "Answer to reviewer # 1.", "comment": "Thanks for the review, but I think that your questions are out of focus. In the abstract of our paper, at the line 6, we noted that our algorithm is extension of FT[1], and we also mentioned in the third paragraph of the second page that we utilize the translator of FT. \nMost of the questions you ask are not obliged for us to answer them because our paper is not supposed to claim, but the FT paper could answer.\n\nOur point is:\nUsing stronger teacher has many drawbacks. As Lan et al.(2018)[3] mentioned and we noted on our paper that, cases are possible where stronger teacher may not exist. Knowledge Transfer(KT) methods such as KD[2] can use ensemble teacher instead of stronger teacher with large numbers of parameters, but KT methods that does not use predictions(labels) are incompatible with label ensemble.\nWhat we claim is that our method could manage these problems, and also delivering knowledge at feature map layer may have advantages with giving more specific information, and could get decent results.\n\n[1] Kim et al. Paraphrasing Complex Network: Network Compression via Factor Transfer, 2018\n[2] Hinton et al. Distilling the Knowledge in a Neural Network, 2015\n[3] Lan et al. Knowledge distillation by on-the-fly native ensemble, 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper18/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621806, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJxYEsAqY7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper18/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper18/Authors|ICLR.cc/2019/Conference/Paper18/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers", "ICLR.cc/2019/Conference/Paper18/Authors", "ICLR.cc/2019/Conference/Paper18/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621806}}}, {"id": "rJgiuVx-6Q", "original": null, "number": 3, "cdate": 1541633138944, "ddate": null, "tcdate": 1541633138944, "tmdate": 1541633138944, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Review", "content": {"title": "Potentially lack of true novelty", "review": "I do not necessarily see something wrong with the paper, but I'm not convinced of the significance (or sufficient novelty) of the approach. \n\nThe way I understand it, a translator is added on top of the top layer of the student, which is nothing but a few conv layers that project the output to potentially the size of the teacher (by the way, why do you need both a paraphraser and translator, rather than making the translator always project to the size of the teacher which basically will do the same thing !? )\nAnd then a distance is minimized between the translated value of the students and the teacher output layer. The distance is somewhat similar to L2 (though the norm is removed from the features -- which probably helps with learning in terms of gradient norm). \n\nComparing with normal distillation I'm not sure how significant the improvement is. And technically this is just a distance metric between the output of the student and teacher. Sure it is a more involved distance metric, however it is in the spirit of what the distillation work is all about and I do not see this as being fundamentally different, or at least not different enough for an ICLR paper.\n\nSome of the choices seem arbitrary to me (e.g. using both translator and paraphraser). Does the translator need to be non-linear? Could it be linear? What is this mapping doing (e.g. when teacher and student have the same size) ? Is it just finding a rotation of the features? Is it doing something fundamentally more interesting? \n\nWhy this particular distance metric between the translated features? Why not just L2? \n\nIn the end I'm not sure the work as is, is ready for ICLR.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper18/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Review", "cdate": 1542234556683, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335630560, "tmdate": 1552335630560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJg-eYsw3m", "original": null, "number": 2, "cdate": 1541023976577, "ddate": null, "tcdate": 1541023976577, "tmdate": 1541534356973, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Review", "content": {"title": "a good idea to try but the paper is not yet ready", "review": "In summary, I think this paper contains some reasonable results based on a reasonable, moderately novel, idea, but unfortunately, it is not yet ready for publication. Reading it made me rather confused. \n\nGood things:\n- The main idea is sensible, though distilling into the same architecture (sFEED) is not that novel. I think the pFEED is probably the more novel part.\n- The numerical results are quite good.\n- It's a fairly simple method. If others reproduced these results, I think it would be useful.\n\nProblems:\n- Some parts of the paper are written in a way that makes the reader confused about what this paper is about. For example the first paragraph. Some motivations I just did not understand.\n- Some parts of the paper are repeating itself. For example \"introduction\" and \"related works\". The section on related work also includes some quite unrelated papers.\n- The references in the paper are often pointing to work that came much later than the original idea or some pretty random recent papers. For example the idea of model compression (or knowledge distillation) is much older than Hinton et al. I believe it was first proposed by Bucila et al. [1] (which the authors mention later as if knowledge distillation and model compression were very different ideas), it definitely doesn't come from Kim et al. (2018). Learning from intermediate representations of the network is at least as old as Romero et al. [2]. Compression into a network of the same architecture is definitely older than Furnarello et al. (2018). It was done, for example, by Geras et al. [3]. The paper also cites Goodfellow et al. (2016) in some pretty random contexts. I don't want to be too petty about references, but unfortunately, this paper is just below a threshold that I would still find acceptable in this respect.\n- The comparison in Table 6 would make more sense if the same architectures would be clearly compared. As it is, it is difficult to be certain where the improvement is coming from and how it actually compares to different methods.\n\nTypos: Titap X, ResNext, prarphraser.\n\nReferences:\n[1] Bucila et al. Model Compression. 2006.\n[2] Romero et al. FitNets: Hints for Thin Deep Nets. 2014.\n[3] Geras et al. Blending LSTMs into CNNs. 2016.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper18/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Review", "cdate": 1542234556683, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335630560, "tmdate": 1552335630560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkehgnZ8nQ", "original": null, "number": 1, "cdate": 1540918260489, "ddate": null, "tcdate": 1540918260489, "tmdate": 1541534356761, "tddate": null, "forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "invitation": "ICLR.cc/2019/Conference/-/Paper18/Official_Review", "content": {"title": "A review", "review": "In this paper, the authors present two methods, Sequential and Parallel-FEED for learning student networks that share architectures with their teacher.\n\nFirstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.\n\nThe paper could do with some further grammar/spell checks.\n\nIt isn't clear to me where the novelty lies in this work. Sequential-FEED appears to be identical to BANs (https://arxiv.org/abs/1805.04770) with an additional non-linear transformation on the network outputs as in https://arxiv.org/abs/1802.04977. Parallel-FEED is just an ensemble of teachers; please correct me if I'm wrong.\n\nThe experimental results aren't convincing. There aren't any fair comparisons. For instance, in table 6 a WRN-28-10(sFEED) after 5 whole training iterations is compared to a WRN-28-1(BAN) after 1. It would be good to run BAN for as many iterations. A comparison to attention transfer (https://arxiv.org/abs/1612.03928) would be ideal for the ImageNet experiments. Furthermore, if one isn't interested in compression, then Table 4 indicates that an ensemble is largely preferable.\n\nThis work would benefit from a CIFAR-10 experiment as it's so widely used (interestingly, BANs perform poorly on CIFAR-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.\n\nIn summary I believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.\n\nPros:\n- Simple method\n- Largely written with clarity\n\nCons:\n- Method is not very novel\n- No compared thoroughly enough to other work", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper18/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEED: Feature-level Ensemble Effect for knowledge Distillation", "abstract": "This paper proposes a versatile and powerful training algorithm named Feature-level Ensemble Effect for knowledge Distillation(FEED), which is inspired by the work of factor transfer. The factor transfer is one of the knowledge transfer methods that improves the performance of a student network with a strong teacher network. It transfers the knowledge of a teacher in the feature map level using high-capacity teacher network, and our training algorithm FEED is an extension of it. FEED aims to transfer ensemble knowledge, using either multiple teachers in parallel or multiple training sequences. Adapting the peer-teaching framework, we introduce a couple of training algorithms that transfer ensemble knowledge to the student at the feature map level, both of which help the student network find more generalized solutions in the parameter space. Experimental results on CIFAR-100 and ImageNet show that our method, FEED, has clear performance enhancements,without introducing any additional parameters or computations at test time.", "paperhash": "park|feed_featurelevel_ensemble_effect_for_knowledge_distillation", "authorids": ["swpark0703@snu.ac.kr", "nojunk@snu.ac.kr"], "authors": ["SeongUk Park", "Nojun Kwak"], "keywords": ["Knowledge Distillation", "Ensemble Effect", "Knowledge Transfer"], "pdf": "/pdf/06272e082c548589ac291f2da2be2d6d70186f53.pdf", "_bibtex": "@misc{\npark2019feed,\ntitle={{FEED}: Feature-level Ensemble Effect for knowledge Distillation},\nauthor={SeongUk Park and Nojun Kwak},\nyear={2019},\nurl={https://openreview.net/forum?id=BJxYEsAqY7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper18/Official_Review", "cdate": 1542234556683, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJxYEsAqY7", "replyto": "BJxYEsAqY7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper18/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335630560, "tmdate": 1552335630560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper18/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}