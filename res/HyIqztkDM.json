{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124452479, "tcdate": 1518469773662, "number": 272, "cdate": 1518469773662, "id": "HyIqztkDM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HyIqztkDM", "signatures": ["~Abhishek_Sharma1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "On the Consistency of Spherical Z Loss", "abstract": "Extremely large and sparse output space in a deep net classifier induces two major challenges of high computational complexity and class ambiguity. Class ambiguity is usually tackled by optimizing top-k error instead of zero one loss. To deal with computational complexity, recent work of ~\\cite{Vincent2015EfficientEG} and ~\\cite{Brbisson15} introduced a family of spherical loss that comes with a weight update algorithm that is independent of output space size. In this family, Z loss is of particular interest since it outperforms other spherical losses and log-softmax on top-k scores. However, there exists no theoretical result on the top-k calibration of Z loss or any concrete connection between top-k scores and hyper-parameters of Z loss. This paper provides insights on the relationship between the two and answers how and why hyper-parameters of Z loss are essential to optimize top-k scores.", "paperhash": "sharma|on_the_consistency_of_spherical_z_loss", "_bibtex": "@misc{\n  sharma2018on,\n  title={On the Consistency of Spherical Z Loss},\n  author={Abhishek Sharma},\n  year={2018},\n  url={https://openreview.net/forum?id=HyIqztkDM}\n}", "authorids": ["kein.iitian@gmail.com"], "authors": ["Abhishek Sharma"], "keywords": ["Spherical Z Loss", "top-k calibration"], "pdf": "/pdf/c2dc891ad5ade6975c6b52a8bf54c0124ea0bbd9.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582894721, "tcdate": 1520513355600, "number": 1, "cdate": 1520513355600, "id": "HJNLWnRuz", "invitation": "ICLR.cc/2018/Workshop/-/Paper272/Official_Review", "forum": "HyIqztkDM", "replyto": "HyIqztkDM", "signatures": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer2"], "content": {"title": "Investigate relationship on how and why hyper-parameters of Z loss are essential to optimize top-k scores.", "rating": "4: Ok but not good enough - rejection", "review": "The paper is very badly written and lacks rigor at any analytical depth.\nThe derivations are tedious but quite immediate and easy.\nSymbols are badly used.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Consistency of Spherical Z Loss", "abstract": "Extremely large and sparse output space in a deep net classifier induces two major challenges of high computational complexity and class ambiguity. Class ambiguity is usually tackled by optimizing top-k error instead of zero one loss. To deal with computational complexity, recent work of ~\\cite{Vincent2015EfficientEG} and ~\\cite{Brbisson15} introduced a family of spherical loss that comes with a weight update algorithm that is independent of output space size. In this family, Z loss is of particular interest since it outperforms other spherical losses and log-softmax on top-k scores. However, there exists no theoretical result on the top-k calibration of Z loss or any concrete connection between top-k scores and hyper-parameters of Z loss. This paper provides insights on the relationship between the two and answers how and why hyper-parameters of Z loss are essential to optimize top-k scores.", "paperhash": "sharma|on_the_consistency_of_spherical_z_loss", "_bibtex": "@misc{\n  sharma2018on,\n  title={On the Consistency of Spherical Z Loss},\n  author={Abhishek Sharma},\n  year={2018},\n  url={https://openreview.net/forum?id=HyIqztkDM}\n}", "authorids": ["kein.iitian@gmail.com"], "authors": ["Abhishek Sharma"], "keywords": ["Spherical Z Loss", "top-k calibration"], "pdf": "/pdf/c2dc891ad5ade6975c6b52a8bf54c0124ea0bbd9.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582894536, "id": "ICLR.cc/2018/Workshop/-/Paper272/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper272/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper272/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper272/AnonReviewer1"], "reply": {"forum": "HyIqztkDM", "replyto": "HyIqztkDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582894536}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582649538, "tcdate": 1520791490985, "number": 2, "cdate": 1520791490985, "id": "Hkjayg7tf", "invitation": "ICLR.cc/2018/Workshop/-/Paper272/Official_Review", "forum": "HyIqztkDM", "replyto": "HyIqztkDM", "signatures": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer3"], "content": {"title": "Correct, but not a very exciting or surprising result", "rating": "6: Marginally above acceptance threshold", "review": "This paper sheds some light on the relationship between the top-k calibration property and the hyper-parameters of the Z loss. The derivation is correct, but the result is not surprising and doesn't have any strong important consequences; the exception may be that the result suggests that only one of the two hyper-parameters of the Z loss needs to be tuned in order to optimize the top-k score. \n\nIn terms of writing, the paper is missing the word \"the\" many times (in addition to other grammatical errors). For example: \"Within this family, Z loss is of particular interest...\" should be \"Within this family, the Z loss is of particular interest\"; \"...where a and b are two hyper-parameters controlling...\" should be \"...where a and b are the two hyper-parameters controlling...\"; \"Empirical results on Penn Tree Bank benchmark shows...\" should be \"Empirical results on the Penn Tree Bank benchmark shows...\"\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Consistency of Spherical Z Loss", "abstract": "Extremely large and sparse output space in a deep net classifier induces two major challenges of high computational complexity and class ambiguity. Class ambiguity is usually tackled by optimizing top-k error instead of zero one loss. To deal with computational complexity, recent work of ~\\cite{Vincent2015EfficientEG} and ~\\cite{Brbisson15} introduced a family of spherical loss that comes with a weight update algorithm that is independent of output space size. In this family, Z loss is of particular interest since it outperforms other spherical losses and log-softmax on top-k scores. However, there exists no theoretical result on the top-k calibration of Z loss or any concrete connection between top-k scores and hyper-parameters of Z loss. This paper provides insights on the relationship between the two and answers how and why hyper-parameters of Z loss are essential to optimize top-k scores.", "paperhash": "sharma|on_the_consistency_of_spherical_z_loss", "_bibtex": "@misc{\n  sharma2018on,\n  title={On the Consistency of Spherical Z Loss},\n  author={Abhishek Sharma},\n  year={2018},\n  url={https://openreview.net/forum?id=HyIqztkDM}\n}", "authorids": ["kein.iitian@gmail.com"], "authors": ["Abhishek Sharma"], "keywords": ["Spherical Z Loss", "top-k calibration"], "pdf": "/pdf/c2dc891ad5ade6975c6b52a8bf54c0124ea0bbd9.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582894536, "id": "ICLR.cc/2018/Workshop/-/Paper272/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper272/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper272/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper272/AnonReviewer1"], "reply": {"forum": "HyIqztkDM", "replyto": "HyIqztkDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582894536}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582648408, "tcdate": 1520793658774, "number": 3, "cdate": 1520793658774, "id": "B1mHueXtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper272/Official_Review", "forum": "HyIqztkDM", "replyto": "HyIqztkDM", "signatures": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer1"], "content": {"title": "Statistical consistency of Z-loss", "rating": "4: Ok but not good enough - rejection", "review": "This paper attempts to analyze the statistical consistency (namely, top-k consistecy) of the recently proposed spherical Z-loss, a loss function with some appealing properties that is an alternative to softmax.\n\nUnfortunately, the presentation lacks clarity and motivation for the several derivation steps, and the theoretical analysis seems to fall short. Notation is imprecise in some parts, and I have some doubts about the overall technical correctness.\n\nThe expression in n Eq 4 is erroneous and may impact the technical correctness of the derivation steps that follow. The righthand term should be\n\n\\sum_{r \\ne c} p_x(r) \\frac{\\nabla L(o, r)}{\\nabla o_c}\n\ninstead of\n\n\\sum_{r \\ne c} p_x(r) \\frac{\\nabla L(o, c)}{\\nabla o_r}.\n\nI don't know how this affects the correctness of Eq 7 (since I don't know if Eq 6 has the same problem), namely if the denominator inside the sum in Eq 7 should be 1 + exp(a(z_c - b)) instead of 1 + exp(a(z_r - b)).\n\nThe paper doesn't explain why it's trying to compute the \"critical points\" of the expected loss. Some motivation should be given, namely that the points o with zero gradient can correspond to minimizers of the loss for the given \"true\" distribution p_x(c). This will help the reader understand why the author is doing this.\n\nThe author then arrives at Eq 14, but unfortunately I don't think there's much we can conclude from this equation. As the author points out, the summation in the RHS is only guaranteed to be positive if a>0 (which is fine) and if z_k>0 (which will not happen for several k's, given that z is centered and standardized). What can we conclude for k's for which z_k is negative? The discussion argues that the choice of the hyperparameter \"a\" can fix this, but can it? It seems that there is no single \"a\" that can fix this for any possible k and p_x(c).\n\nIn my opinion, this is a \"half-baked\" work and it's hard to understand what the take-home message of this contribution should be.\n\nMinor comments:\n- use $a$, $b$, $k$, instead of a, b, k, when to refering to these variables inline.\n- remove the long space before the \"=\" sign in Eq 1\n- in Eq 3, it should be stated that p_x(c) means p(c | x)\n- in Eq 4, the shorthand E[L] for E[L(o, f(X) | X=x)] has not been explained", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Consistency of Spherical Z Loss", "abstract": "Extremely large and sparse output space in a deep net classifier induces two major challenges of high computational complexity and class ambiguity. Class ambiguity is usually tackled by optimizing top-k error instead of zero one loss. To deal with computational complexity, recent work of ~\\cite{Vincent2015EfficientEG} and ~\\cite{Brbisson15} introduced a family of spherical loss that comes with a weight update algorithm that is independent of output space size. In this family, Z loss is of particular interest since it outperforms other spherical losses and log-softmax on top-k scores. However, there exists no theoretical result on the top-k calibration of Z loss or any concrete connection between top-k scores and hyper-parameters of Z loss. This paper provides insights on the relationship between the two and answers how and why hyper-parameters of Z loss are essential to optimize top-k scores.", "paperhash": "sharma|on_the_consistency_of_spherical_z_loss", "_bibtex": "@misc{\n  sharma2018on,\n  title={On the Consistency of Spherical Z Loss},\n  author={Abhishek Sharma},\n  year={2018},\n  url={https://openreview.net/forum?id=HyIqztkDM}\n}", "authorids": ["kein.iitian@gmail.com"], "authors": ["Abhishek Sharma"], "keywords": ["Spherical Z Loss", "top-k calibration"], "pdf": "/pdf/c2dc891ad5ade6975c6b52a8bf54c0124ea0bbd9.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582894536, "id": "ICLR.cc/2018/Workshop/-/Paper272/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper272/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper272/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper272/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper272/AnonReviewer1"], "reply": {"forum": "HyIqztkDM", "replyto": "HyIqztkDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper272/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper272/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582894536}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573597208, "tcdate": 1521573597208, "number": 232, "cdate": 1521573596860, "id": "Hyry1kJ9f", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HyIqztkDM", "replyto": "HyIqztkDM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Consistency of Spherical Z Loss", "abstract": "Extremely large and sparse output space in a deep net classifier induces two major challenges of high computational complexity and class ambiguity. Class ambiguity is usually tackled by optimizing top-k error instead of zero one loss. To deal with computational complexity, recent work of ~\\cite{Vincent2015EfficientEG} and ~\\cite{Brbisson15} introduced a family of spherical loss that comes with a weight update algorithm that is independent of output space size. In this family, Z loss is of particular interest since it outperforms other spherical losses and log-softmax on top-k scores. However, there exists no theoretical result on the top-k calibration of Z loss or any concrete connection between top-k scores and hyper-parameters of Z loss. This paper provides insights on the relationship between the two and answers how and why hyper-parameters of Z loss are essential to optimize top-k scores.", "paperhash": "sharma|on_the_consistency_of_spherical_z_loss", "_bibtex": "@misc{\n  sharma2018on,\n  title={On the Consistency of Spherical Z Loss},\n  author={Abhishek Sharma},\n  year={2018},\n  url={https://openreview.net/forum?id=HyIqztkDM}\n}", "authorids": ["kein.iitian@gmail.com"], "authors": ["Abhishek Sharma"], "keywords": ["Spherical Z Loss", "top-k calibration"], "pdf": "/pdf/c2dc891ad5ade6975c6b52a8bf54c0124ea0bbd9.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}