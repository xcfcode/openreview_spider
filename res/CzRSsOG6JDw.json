{"notes": [{"id": "CzRSsOG6JDw", "original": "r86I82IjtKLE", "number": 2854, "cdate": 1601308316790, "ddate": null, "tcdate": 1601308316790, "tmdate": 1614985643459, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "UpTLC96Zy3", "original": null, "number": 1, "cdate": 1610040518439, "ddate": null, "tcdate": 1610040518439, "tmdate": 1610474126854, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Two very confident and fairly confident reviewers rate this paper ok but not good enough, and two other fairly confident reviewers rate the article below the acceptance threshold. Therefore I must reject the article. The reviewers provided encouraging comments and suggestions on how the manuscript could be improved, which I hope the authors will find useful. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040518426, "tmdate": 1610474126837, "id": "ICLR.cc/2021/Conference/Paper2854/-/Decision"}}}, {"id": "FPB-UldBqmu", "original": null, "number": 7, "cdate": 1606253153562, "ddate": null, "tcdate": 1606253153562, "tmdate": 1606253212414, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "iOMY1UgTjHg", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks for your detailed review! We\u2019re happy that you found our paper well written and easy to follow, agree that an exhaustive list of irrationality models has been tested, and also that the problem we are trying to answer is interesting. \n\nWe agree that our theoretical results are not necessarily surprising or technically difficult. Therefore, we place little emphasis on them, and instead use them to explain the more surprising (to us) empirical results. That being said:\n>  I would intuitively argue that it is then quite \"likely\" that reward inference is easier for some of the infinitely many irrational agents as opposed to the one perfectly rational agent...\n\nIndeed, while prop 1 makes this claim, prop 2 makes a stronger claim - that a particular, commonly used irrational planner model makes reward inference easier. And our empirical results show that *many* irrational planners make reward inference easier. \n\nIn fact, under many reasonable parameterizations of irrational planners, \u2018most\u2019 irrational planners will map fewer reward parameters to the same policies, therefore producing more informative behavior. Intuitively, we can think of this as optimal behavior being constrained in ways that irrational behavior is not. We will consider formalizing and including this result in the paper. \n\n> It is questionable how practically relevant the irrationality models, studied by this work, are. Results may be trivial and high-dimensional experiments are largely missing.\n\nAs with the other reviewers, this reviewer raises an important question of practicality of this finding: since we usually don\u2019t know the human\u2019s bias, and the inference procedures studied in this work don\u2019t scale to more realistic, high dimensional problems. This is a valid point, but even if we can\u2019t infer the bias and reward of humans today:\n* The finding itself has scientific value intrinsically, as it\u2019s pointing out a surprising fact about biased behavior. Note that our paper is not a typical \u201chere\u2019s a new method\u201d ML paper, it\u2019s merely an analysis, one that arguably produced a surprising finding about a well-studied topic and therefore has value in itself. \n* The finding encourages taking biases seriously: not just because we make the wrong inference if we don\u2019t, but because there is something to be gained by modeling them. It serves as an argument for further research in understanding and modeling enough about biases.  As other reviewers have pointed out, cognitive science, and behavioral economics have identified many biases. So while the approach in this work may not immediately scale, we believe the results can still inform research in this area. \n* The finding also points to future research directions where robots attempt to influence their demonstrators to exhibit a specific bias (like myopia) in order to make them more informative. While it\u2019s really hard for people to be pedagogical, it might be much easier for them to act myopically. Of course, how to do this well and whether it works out is an open question, but our paper uncovers that as a potentially new avenue for improving learning.\n\n\n> The experimental setting only considers low-dimensional grid worlds (as a recommendation, I would put more emphasis on the Frozen Lake setting rather than the simpler setting). Only autonomous driving is presented as a high-dimensional task---but in this context, irrationality is only considered in terms of the agent's planning horizon but not in terms of any of the irrationality types presented earlier for lower-dimensional settings.\n\nThanks for this suggestion. We can certainly place more emphasis on the Frozen Lake setting, though we will add that there are advantages to using random MDPs as well - for example, it suggests the results are more general than one or two particular gridworlds. We also considered analyzing more irrationalities in the autonomous driving task, but as the state is continuous and the planning is only local and approximate, the other irrationalities do not translate over as naturally as myopia. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CzRSsOG6JDw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2854/Authors|ICLR.cc/2021/Conference/Paper2854/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843823, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment"}}}, {"id": "P41iahsqq2K", "original": null, "number": 3, "cdate": 1606252908205, "ddate": null, "tcdate": 1606252908205, "tmdate": 1606253194021, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "ElExdznAsJq", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thanks for the detailed review! We are very happy that you agree that the problem of studying IRL for irrational demonstrators is an interesting problem, and also recognize the value in enumerating a large list of biases in a systematic way. \n\nWe would also like to thank you for the pointers to more related work in the Behavioural Economics/Psychology field. We agree that there has been a large amount of work in studying the impacts of specific biases (or families of biases), as discussed in our introduction, but (as recognized by the reviewer) unlike prior we provide a systematic covering of multiple biases (of various different types) in the same setting. \n\nMore importantly, we believe that another important finding from our work is not just that it is important to model biases, but that biases can actually be helpful for inference. That is, a biased demonstrator can be more informative than a rational demonstrator, if the bias is known. (See our top-level comment, \u201cBiases can be more informative - instead of merely important to model\u201d.)\n\n> The authors present their current contribution as more novel than it really is. \n\nWe\u2019re not aware of any prior work that claims that biases can be actually helpful for inference. \n\n> It is unclear from the paper whether the authors' current approach is doable in any real situation \n\nThe reviewer raises an important question of practicality of this finding: since we usually don\u2019t know the human\u2019s bias, we can\u2019t take advantage of their biased behavior being more informative. This is a valid point, but \n* The finding itself has scientific value intrinsically, as it\u2019s pointing out a surprising fact about biased behavior. Note that our paper is not a typical \u201chere\u2019s a new method\u201d ML paper, it\u2019s merely an analysis, one that arguably produced a surprising finding about a well-studied topic and therefore has value in itself. \n* The finding encourages taking biases seriously: not just because we make the wrong inference if we don\u2019t, but because there is something to be gained by modeling them. It serves as an argument for further research in understanding and modeling enough about biases.  As the reviewer points out, cognitive science, and behavioral economics have identified many biases. So while the approach in this work may not immediately scale, we believe the results can still inform research in this area. \n* The finding also points to future research directions where robots attempt to influence their demonstrators to exhibit a specific bias (like myopia) in order to make them more informative. While it\u2019s really hard for people to be pedagogical, it might be much easier for them to act myopically. Of course, how to do this well and whether it works out is an open question, but our paper uncovers that as a potentially new avenue for improving learning.\n\n>The current setup has the issue that when doing inference we must know exactly the way in which the actor was irrational in order to be able to get the gains from using the correct model. \n> however since the correct irrational model isn't ever known, ...\n\nWe do analyze this result in section 5 the paper. In Figure 11, we show that even when the inferer assumes the incorrect parameters for the irrationality, as long as they are not completely off base, the inferer still does better than assuming Boltzmann rationality. \n\n> if you do inverse RL assuming a rational model when individuals are irrational, you will learn exactly the wrong thing.\n\nAgain, one important finding from our work is not just that it is important to model biases, but that biases can actually be helpful for inference. (See our top-level comment, \u201cBiases can be more informative - instead of merely important to model\u201d.)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CzRSsOG6JDw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2854/Authors|ICLR.cc/2021/Conference/Paper2854/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843823, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment"}}}, {"id": "EpUcAHZeVxc", "original": null, "number": 4, "cdate": 1606252974855, "ddate": null, "tcdate": 1606252974855, "tmdate": 1606253174858, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "Wnn90NnmiCH", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thanks for the detailed review! \nWe\u2019re happy that you found our paper easy to read, and also agree that the research direction is fruitful and of interest to the community. \n\n> It seems that based on the results the author presented, we need to at least have prior knowledge about what type of irrationality humans exhibit (if not the exact parameter), this seems impractical to me and the authors did not discuss any potential approach to obtain this information. \n\nAs the reviewer says later in the review, psychologists and behavioral economists have identified several types of irrational behavior that humans exhibit. In addition, some recent work in machine learning has also looked at this problem (for example Shah et al 2019, which we cite). \n\nWe do agree that our paper doesn\u2019t discuss in detail the specific ways in which we could infer human irrationality. Instead, our work focuses on the effects of a known (through whatever method) or unknown bias on reward inference. We think that our analysis emphasizes why this is an important area of future work, and will add more discussion of this. \n\n>The authors also noted that humans exhibit a mix of irrationalities in 2.1, while the papers only considered cases where there\u2019s a single irrationality.\n\nThis is a good point. We agree that there is further work to be done involving mixes of irrationalities that could be added to the appendix. We expect our results to hold even for a mix of irrationalities. That being said, we feel that our paper makes an important contribution to understanding the effects of biases on reward learning without considering mixes of the different irrationalities we consider in our work. \n\nRegarding this point:\n> In 2.1, the authors reasoned that it seems to them impossible to use actual human data, thus the paper is based on simulation. While simulations are valuable before diving into real data, I don\u2019t think it \u201caddress these issues\u201d.\u2026\n\nWe agree that there are situations where researchers have experimentally observed biases. In some cases, the researchers use settings where the human subjects\u2019 reward and biases are known and can be easily inferred mathematically, but then this is the same situation as our work. In other cases, the bias is only described heuristically, which means we can\u2019t use it in our analysis without formalizing it with many researcher degrees of freedom. And in some cases the specific forms of the biases (or even their existence) are contested as the findings can be explained by different rewards. In addition, with human data, we won\u2019t be able to go into the type of detail we did in this work. For example, it would be significantly harder to set the parameter of different biases like the horizon for myopia.\n\nWe also agree that our arguments in 2.1 should not be construed to mean that we could never infer biases from human data. Instead, we see the arguments in 2.1 as explaining why we chose not to use human data in our analysis. And we also believe that our analysis could be used to motivate future work in this area. \n\n> ... In addition, if what the authors described in 2.1 paragraph 1 are all true, it\u2019s hard for me to see any value in this paper as it is not practical at all ...\n\nAs with reviewer 3, this reviewer raises an important question of practicality of this finding: since we usually don\u2019t know the human\u2019s bias, we can\u2019t take advantage of their biased behavior being more informative. This is a valid point, but even if we can\u2019t infer the bias and reward of humans today:\n* The finding encourages taking biases seriously: not just because we make the wrong inference if we don\u2019t, but because there is something to be gained by modeling them. It serves as an argument for further research in understanding and modeling enough about biases.  As the reviewer points out, cognitive science, and behavioral economics have identified many biases. So while the approach in this work may not immediately scale, we believe the results can still inform research in this area. \n* The finding also points to future research directions where robots attempt to influence their demonstrators to exhibit a specific bias (like myopia) in order to make them more informative. While it\u2019s really hard for people to be pedagogical, it might be much easier for them to act myopically. Of course, how to do this well and whether it works out is an open question, but our paper uncovers that as a potentially new avenue for improving learning.\n\n\n> ..., i.e. the authors performed all analyses based on a miracle condition\n\nIn addition to the above points, we do perform a sensitivity analysis in section 5 (and the appendix). "}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CzRSsOG6JDw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2854/Authors|ICLR.cc/2021/Conference/Paper2854/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843823, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment"}}}, {"id": "gvzU5p6hwBb", "original": null, "number": 6, "cdate": 1606253103267, "ddate": null, "tcdate": 1606253103267, "tmdate": 1606253103267, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "5BuqjaGj_fS", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We\u2019d like to thank you for your review of our work. We are pleased that you agree with us that the problem setting is important, and that our formalism allows for a systematic analysis of the effects of irrationality on reward inference.\n\nThe reviewer says:\n> It is not surprising that knowing the generative bias in data helps inference; assuming access to the underlying irrationality type is a strong assumption. \u2026 This work only demonstrates that (in contrast to the findings of prior work of Shah et al.) knowing the true irrationality model outperforms assuming Boltzmann-rationality by a lot ...\n\nWe agree! Knowing the generative bias unsurprisingly helps with reward inference. What is more surprising (to us), though, is that biases can actually be helpful for inference. That is, a biased demonstrator can be more informative than a rational demonstrator, if the bias is known. (See our top-level comment, \u201cBiases can be more informative - instead of merely important to model\u201d.)\n\n> \u2026 however, it does not experimentally show the effect of assuming the wrong irrationality type on learning (besides Botlzmann-rational); if inferring the type of irrationality is efficient and approximately, then it is desirable to do so whenever possible; on the other hand, if assuming the wrong type of irrationality performs worse than assuming Boltzmann-rational, then it is important to know the risk.\n\nWe agree that it is important to know the risks. We do investigate the effects of assuming incorrect irrationality parameters in section 5, and also investigate the effects of assuming the incorrect type of myopia. \n\n> In the abstract, it is indicated that the findings of \u201cmyopic behavior being more informative\u201d allow us to ask human demonstrators to be myopic when they demonstrate; this deviates from the motivation to correctly model human\u2019s irrationality in the data generation process but rather circumvent the problem of inferring human\u2019s irrationality by conditioning the demonstrator to be \u201cmyopic\u201d. Such claims should be avoided when there are no experimental results supporting their validity.\n\n\nWe will soften our language, since we don\u2019t want to claim that this will necessarily work. However we do want to point out the opportunity. While part of our work consists of investigating the costs of assuming the wrong irrationality type or parameter, another part of our work consists of investigating the effect of biases themselves. That is, we compare the situation where the human is biased in a particular way (and this bias is known) to the situation where the human is not biased in that way. Our results in that domain indicate that biased demonstrators can be more informative than an unbiased demonstrator, if the bias is known. (Again, see our top level comment, \u201cBiases can be more informative - instead of merely important to model\u201d.)\n\nRelevant to the practicality issue mentioned later, we also think that our finding points to future research directions where robots attempt to influence their demonstrators to exhibit a specific bias in order to make them more informative. While it\u2019s really hard for people to be pedagogical, it might be much easier for them to act myopically. Of course, how to do this well and whether it works out is an open question, but our paper uncovers that as a potentially new avenue for improving learning.\n\n>  it is important to discuss how this influences the contribution of this paper along with results from the above point demonstrating the effect of assuming wrong or incomplete irrationality when multiple irrationalities exist.\n\nAs with reviewer 3 and 4, this reviewer also raises questions of practicality of our finding: since we usually don\u2019t know the human\u2019s bias, we can\u2019t take advantage of their biased behavior being more informative. This is a valid point, but:\n* The finding itself has scientific value intrinsically, as it\u2019s pointing out a surprising fact about biased behavior. Note that our paper is not a typical \u201chere\u2019s a new method\u201d ML paper, it\u2019s merely an analysis, one that arguably produced a surprising finding about a well-studied topic and therefore has value in itself. \n* The finding encourages taking biases seriously: not just because we make the wrong inference if we don\u2019t, but because there is something to be gained by modeling them. It serves as an argument for further research in understanding and modeling enough about biases.  \n* The finding also points to future research directions where robots attempt to influence their demonstrators to exhibit a specific bias (like myopia) in order to make them more informative. While it\u2019s really hard for people to be pedagogical, it might be much easier for them to act myopically. Of course, how to do this well and whether it works out is an open question, but our paper uncovers that as a potentially new avenue for improving learning.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CzRSsOG6JDw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2854/Authors|ICLR.cc/2021/Conference/Paper2854/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843823, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment"}}}, {"id": "W0TCPTWMEqq", "original": null, "number": 5, "cdate": 1606253002961, "ddate": null, "tcdate": 1606253002961, "tmdate": 1606253002961, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "Wnn90NnmiCH", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment", "content": {"title": "Responses to the short questions at the end:", "comment": "* Yes, it should be V_i(s\u2019) - that is a typo on our part. We\u2019ve corrected it.\n* Failing to model the irrationality type tends to lead to really bad results, as shown in figure 4. We excluded the lines from figure 3 as the performance isn\u2019t on the same scale. \n* Beta = 10 was chosen because it was the best performing Beta for a Boltzmann-Rational human.\n* Yes, the parameters used are indicated by where the loss is minimal. However, this isn\u2019t very clear for some of the biases, so we will add the specific parameters used into the text.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CzRSsOG6JDw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2854/Authors|ICLR.cc/2021/Conference/Paper2854/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843823, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment"}}}, {"id": "NyEz1sun9nw", "original": null, "number": 2, "cdate": 1606252834482, "ddate": null, "tcdate": 1606252834482, "tmdate": 1606252834482, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment", "content": {"title": "Biases can be informative - instead of merely important to model. ", "comment": "We want to first and foremost clarify that we think that one of the major findings is that biases can make behavior more informative. Importantly, we are not (just) saying the trivial thing that if the human has a bias, it\u2019s better to know about it than to assume rationality. We are saying something much more surprising: that the human having a bias in the first place is better than them being rational. \n\nFor example, a human driver optimizing for a short horizon ends up revealing much more about their preferences than a rational human driver, optimizing for the full horizon. \n\nWhy is this interesting? It is tempting to assume that biases make the behavior more opaque, and we\u2019re finding the opposite. The most related finding to ours is that of pedagogical/legible behavior: that teachers trying to be informative will deviate from optimal behavior. But legible behavior is (in a sense) supra-rational: it requires the demonstrator to be modeling the learner and optimizing for their understanding. In contrast, biases like myopia are sub-rational, they're doing something. It is therefore somewhat surprising that despite polluting the optimality of the behavior, they in fact make it more informative without the demonstrator purposefully trying to be informative. \n\nWe just wanted to make sure this finding came across -- again, it\u2019s not that it\u2019s better to assume biases if they are there, instead it\u2019s that it\u2019s in a sense fortunate for biases to be there in the first place. We\u2019re concerned that this didn\u2019t come across in the paper, and would really appreciate some advice on how to emphasize it and explain it better. \n\nFor example, Reviewer 1 claims \u201cThis work only demonstrates that (in contrast to the findings of prior work of Shah et al.) knowing the true irrationality model outperforms assuming Boltzmann-rationality by a lot\u201d. However, in our work we actually demonstrated that the presence of the bias (when known) improves reward inference, not merely that correctly knowing the bias helps. Reviewer 3 characterized our results as showing \u201chow badly inference can fail when the model is misspecified\u201d. However, we also showed result that biases are helpful for inference.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "CzRSsOG6JDw", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2854/Authors|ICLR.cc/2021/Conference/Paper2854/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843823, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Comment"}}}, {"id": "iOMY1UgTjHg", "original": null, "number": 1, "cdate": 1603821870187, "ddate": null, "tcdate": 1603821870187, "tmdate": 1605024117760, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review", "content": {"title": "Interesting question and systematic manuscript outline, but concerns regarding applicability, triviality and absence of high-dimensional experiments", "review": "Summary\n\nThis paper investigates the effect different irrationality types have on reward inference. The irrationality types are modelled in the context of an Uncertain-Reward Markov Decision Process (URMDP) that is similar to an ordinary MDP but with a prior distribution over reward functions. Different irrationality types are expressed through different modifications of Bellman's optimality principle. In simple environment settings, given trajectories of an irrational agent with an optimal irrational policy (under the respective modified Bellman principle), Bayesian inference is feasible in order to identify a posterior over reward functions given the data (i.e. agent trajectories). \n\nThe authors provide a metric to evaluate the quality of the inference procedure. This metric is the expected log loss of the posterior evaluated at the optimal reward function (there is a further outer expectation over tasks with different reward functions). Based on this metric, the authors conclude that the quality of reward inference is higher for irrational agents than for fully rational agents, as evaluated in some grid world settings.\n\nQuality and Details\n\nI am on the verge when assessing the quality of this work. The manuscript tries to answer an interesting question and the outline and structure follow a systematic approach. However, I have 3 main concerns:\n\n1.) While there are a bunch of irrationality models presented in the context of an URMDP, it is unclear how realistic these models are for actual reward inference in practical problems.\n\n2.) I do appreciate both the empirical and theoretical analysis. However, I feel that the results may be trivial---especially in settings where an irrationality model can represent infinitely many decision-makers, one of which being a perfectly rational agent. I would intuitively argue that it is then quite \"likely\" that reward inference is easier for some of the infinitely many irrational agents as opposed to the one perfectly rational agent (recovered with a specific irrationality parameter value).\n\nSimilarly holds for the theoretical results. Proposition 1 is for example trivially true for a URMDP where the perfectly rational agent is the same for all reward functions, and where the optimal irrational agent assigns a different policy to each reward function. Proposition 2 seems to be a special case of the former for one-state-two-action settings and entropy-regularized irrationality. I simply don't know what to take away from these results...\n\n3.) The experimental setting only considers low-dimensional grid worlds (as a recommendation, I would put more emphasis on the Frozen Lake setting rather than the simpler setting). Only autonomous driving is presented as a high-dimensional task---but in this context, irrationality is only considered in terms of the agent's planning horizon but not in terms of any of the irrationality types presented earlier for lower-dimensional settings.\n\nClarity\n\nThe paper is clearly written and easy to follow.\n\nOriginality and Significance\n\nThe question, the paper asks, is original but the significance is limited. This is because it is not clear how reasonable the proposed irrationality models are, and because high-dimensional experiments with the presented irrationality models are missing.\n\nPros\n\nAn exhaustive list of irrationality models is tested.\n\nCons\n\nIt is questionable how practically relevant the irrationality models, studied by this work, are. Results may be trivial and high-dimensional experiments are largely missing.\n\n\nMinor\n\nSome more explanation in some places would help. For example, when mentioning Bayes' rule in low-dimensional settings, it could be explained more clearly how the likelihood looks like, and how the specific irrationality model affects the likelihood (through the optimal irrational policy I assume?).\n\nI would have also appreciated a bit more details about the lowest-dimensional setting: how exactly the reward parameter theta is chosen is a bit difficult to understand when reading the caption of Figure 3.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087349, "tmdate": 1606915805300, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2854/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review"}}}, {"id": "5BuqjaGj_fS", "original": null, "number": 2, "cdate": 1603838028338, "ddate": null, "tcdate": 1603838028338, "tmdate": 1605024117686, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review", "content": {"title": "Good presentation of the formalism for a diverse set of irrationalities; experiments are lacking", "review": "This work studies the effect of modeling systematic irrationality of the demonstrator for reward learning problems. By manipulating different factors of Bellman update, the authors simulate different irrational behavior in demonstrations. Experiments in gridworld and a 2D driving domain demonstrate that modeling irrationality helps reward inference. The authors also demonstrate that knowing the general/approximate type of irrationality, instead of knowing the exact irrationality model, might be enough to improve reward inference. \n\nPros:\nThe problem setting is important. It is significant to model human rationality and irrationality for learning from the data generated by human demonstrators; insights from studying human behavior helps construct better learning algorithms.\nThis work formalizes a diverse set of irrationality, as observed in human behavior, within the MDP formalism for systematic analysis of the effect of perfect modeling\nThe presented formalism for irrationality centers around Bellman update; such a unified presentation of the formalism for irrationality is novel, prior works only presented a few types of irrationality separately as different modifications to the value-iteration algorithm \n\nCons:\nIt is not surprising that knowing the generative bias in data helps inference; assuming access to the underlying irrationality type is a strong assumption. The more important task is to understand what types of irrationality exist in natural human data and how to infer them.\nThis work only demonstrates that (in contrast to the findings of prior work of Shah et al.) knowing the true irrationality model outperforms assuming Boltzmann-rationality by a lot, however, it does not experimentally show the effect of assuming the wrong irrationality type on learning (besides Botlzmann-rational); if inferring the type of irrationality is efficient and approximately, then it is desirable to do so whenever possible; on the other hand, if assuming the wrong type of irrationality performs worse than assuming Boltzmann-rational, then it is important to know the risk. \nThe authors also acknowledge that it is hard to infer irrationality directly from human data and real human data may contain multiple types of irrationality; it is important to discuss how this influences the contribution of this paper along with results from the above point demonstrating the effect of assuming wrong or incomplete irrationality when multiple irrationalities exist.\nIn the abstract, it is indicated that the findings of \u201cmyopic behavior being more informative\u201d allow us to ask human demonstrators to be myopic when they demonstrate; this deviates from the motivation to correctly model human\u2019s irrationality in the data generation process but rather circumvent the problem of inferring human\u2019s irrationality by conditioning the demonstrator to be \u201cmyopic\u201d. Such claims should be avoided when there are no experimental results supporting their validity.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087349, "tmdate": 1606915805300, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2854/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review"}}}, {"id": "Wnn90NnmiCH", "original": null, "number": 3, "cdate": 1603863256859, "ddate": null, "tcdate": 1603863256859, "tmdate": 1605024117604, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review", "content": {"title": "interesting but impractical", "review": "##########################################################################\n\nSummary:\nThis paper proposed modifications to the Bellman equation to capture known human irrationalities and showed that the reward under some conditions (the type of irrationality and parameter settings) can be better inferred compared to a rational agent. The authors demonstrated this through simulations in three different environments with different complexities and provided theoretical analyses to support this empirical finding. The authors further showed and discussed the effects on reward inference when the assumed parameter and assumed type of irrationality is misspecified. \n\n##########################################################################\n\nReasons for score: \nOverall, I vote for rejecting. The idea and results presenting are indeed very interesting and potentially promising, but I found it is to be impractical and based on lots of assumptions that easily fail. I elaborate on this below. \n \n##########################################################################\n\nPros: \n\n1. The writing is very clear and easy to follow.\n \n2.  The paper nicely applies results from studies on human behavior to AI. The idea is very interesting and can potentially inspire lots of new works in this direction. It opens up more questions than it answers. \n\n3. The authors showed the effects in three different environments, which demonstrates its generalizability to some degree. The authors also included theoretical results to support their empirical findings. \n\n##########################################################################\n\nCons: \n\n1. It seems that based on the results the author presented, we need to at least have prior knowledge about what type of irrationality humans exhibit (if not the exact parameter), this seems impractical to me and the authors did not discuss any potential approach to obtain this information. I think it is to some degree possible, as they were identified based on behavioral data empirically in human behavioral studies. \n\n2. The authors also noted that humans exhibit a mix of irrationalities in 2.1, while the papers only considered cases where there\u2019s a single irrationality. \n\n3. In 2.1, the authors reasoned that it seems to them impossible to use actual human data, thus the paper is based on simulation. While simulations are valuable before diving into real data, I don\u2019t think it \u201caddress these issues\u201d. In addition, if what the authors described in 2.1 paragraph 1 are all true, it\u2019s hard for me to see any value in this paper as it is not practical at all, i.e. the authors performed all analyses based on a miracle condition. Personally, I don\u2019t think what authors claimed in paragraph 1 are all true, as there is a large body of experimental and computational studies in economics/psychology/neuroscience that try to answer those questions, and also noted by the authors, those irrationalities were experimentally observed. \n\n4. The formulations of various irrationalities all look reasonable to me, but it would be better if the authors can validate that those formulations actually replicate human irrationalities in the same context as in the previous literature. Otherwise, it seems the choice of formulation kind of arbitrary to me.\n\n#########################################################################\n\nOther questions/suggestions:\nIn 2.3.3 Optimism/Pessimism, the first line after the equation, should it be s\u2019 in V_i(s) instead of s? I supposed the optimism is formulated as the agent is expected to transit into a good state with higher probability, thus it should depend on the value function of the state it transits into. \n\nI\u2019m wondering how worse it does for reward inference if we assume rationality under different irrationalities. It would be great to add another curve in all plots in Fig. 3 to indicate the performance when ignoring irrationality. This result might validate that it is crucial to model irrationality. \n\nFor figure 4, the choice of beta = 10 seems kind of arbitrary to me. Is it the best performing parameter?\n\nWhat are the parameters used in the simulation for figure 11 (the true parameter)? It would be good to put a vertical bar to indicate the case when the true and assumed parameters are the same. I supposed it is the value when the loss is minimal. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087349, "tmdate": 1606915805300, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2854/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review"}}}, {"id": "ElExdznAsJq", "original": null, "number": 4, "cdate": 1603900444502, "ddate": null, "tcdate": 1603900444502, "tmdate": 1605024117534, "tddate": null, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "invitation": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review", "content": {"title": "Great start but not quite there yet", "review": "The authors consider the following disconnect:\n1) Bellman equation based models make the implicit assumption that the agent making the decisions is rational in a very strong sense: they know the model correctly, the discount the future exponentially, they sum the (discounted) rewards as the total reward of a policy\n2) People don't behave in this way\n\nThe disconnect between 1 and 2 is a big problem when we use inverse RL to try to learn the underlying reward functions from demonstrations since we might learn completely the wrong reward function. This can be bad for many reasons including counterfactual inference for what the person would do as well as inference for welfare (which the authors do not discuss, more on this later).\n\nGood Parts\n- The authors recognize a major problem in inverse RL from human demonstrations\n- The authors have a list of known biases and how to parametrize them in reward inference, this list is larger than what is covered in past literature\n- The authors show how badly inference can fail when the model is misspecified\n\nPlaces to Improve\n- The authors present their current contribution as more novel than it really is. \nThere is a huge literature in psychology and behavioral economics (examples: Ainslie 2001 is a whole book on the topic, O'Donoghue and Rabin 1999 American Economic Review, Laibson 1997 Quarterly Journal of Econ.) that specifically focus on what happens if someone is rational (here, discounts exponentially) but is actually irrational (discount hyperbolically). \n\nThe overall point here is both one of being able to predict behavior and one of welfare. Here, welfare refers to the following problem: if a rational agent chooses to e.g. smoke cigarettes, then clearly they prefer smoking to not smoking. On the other hand, a hyperbolic discounter may choose to smoke in the moment but also choose to take actions like throw cigarette packs away because they don't want to be tempted smoke in the future. In this case, we would infer that the smoking action was, in some sense, a mistake. Something, that we can never learn if we assume that agents are rational.\n\nIn addition, there is recent literature in the AI ethics community on precisely this same question (Peysakhovich 2019, AI Ethics and Society) that makes exactly the same point: if you do inverse RL assuming a rational model when individuals are irrational, you will learn exactly the wrong thing. \n\n- It is unclear from the paper whether the authors' current approach is doable in any real situation\nThe current setup has the issue that when doing inference we must know exactly *the way* in which the actor was irrational in order to be able to get the gains from using the correct model. However, in practice we know that agents may have one of many biases, but don't know exactly which one they have. The current theorems tell us that \"there exist MDPs\"where we can tell which irrational model is correct, however it's not immediately clear what characteristics those MDPs have and whether, for example, just by having access to the MDP and a trajectory we can say that this trajectory is consistent or inconsistent with a rational model or a single type of irrationality. \n\nThe empirical counterpart to this is Figure 4 which compares irrational actors and inference with a rational vs the correct irrational model, however since the correct irrational model isn't ever known, it seems like the better comparison would be to show rational vs. all types of irrational and see whether the correct type of irrationality is picked up.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2854/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2854/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The impacts of known and unknown demonstrator irrationality on reward inference", "authorids": ["~Lawrence_Chan2", "~Andrew_Critch1", "~Anca_Dragan1"], "authors": ["Lawrence Chan", "Andrew Critch", "Anca Dragan"], "keywords": ["irrationality", "reward learning", "irl"], "abstract": "Algorithms inferring rewards from human behavior typically assume that people are (approximately) rational. In reality, people exhibit a wide array of irrationalities. Motivated by understanding the benefits of modeling these irrationalities, we analyze the effects that demonstrator irrationality has on reward inference. We propose operationalizing several forms of irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations affect inference. \n\nWe find that incorrectly assuming noisy-rationality for an irrational demonstrator can lead to remarkably poor reward inference accuracy, even in situations where inference with the correct model leads to good inference. This suggests a need to either model irrationalities or find reward inference algorithms that are more robust to misspecification of the demonstrator model. Surprisingly, we find that if we give the learner access to the correct model of the demonstrator's irrationality, these irrationalities can actually help reward inference. In other words, if we could choose between a world where humans were perfectly rational and the current world where humans have systematic biases, the current world might counter-intuitively be preferable for reward inference. We reproduce this effect in several domains. While this finding is mainly conceptual, it is perhaps actionable as well: we might ask human demonstrators for myopic demonstrations instead of optimal ones, as they are more informative for the learner and might be easier for a human to generate.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "chan|the_impacts_of_known_and_unknown_demonstrator_irrationality_on_reward_inference", "pdf": "/pdf/6745ac99de01d6cc70e4b816617cf957f6bfe236.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=2umsR3y6Vv", "_bibtex": "@misc{\nchan2021the,\ntitle={The impacts of known and unknown demonstrator irrationality on reward inference},\nauthor={Lawrence Chan and Andrew Critch and Anca Dragan},\nyear={2021},\nurl={https://openreview.net/forum?id=CzRSsOG6JDw}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CzRSsOG6JDw", "replyto": "CzRSsOG6JDw", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2854/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087349, "tmdate": 1606915805300, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2854/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2854/-/Official_Review"}}}], "count": 12}