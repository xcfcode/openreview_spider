{"notes": [{"id": "SJxJtiRqt7", "original": "Byg7mzt9tQ", "number": 405, "cdate": 1538087798567, "ddate": null, "tcdate": 1538087798567, "tmdate": 1545355377328, "tddate": null, "forum": "SJxJtiRqt7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Generating Images from Sounds Using Multimodal Features and GANs", "abstract": "Although generative adversarial networks (GANs) have enabled us to convert images from one domain to another similar one, converting between different sensory modalities, such as images and sounds, has been difficult. This study aims to propose a network that reconstructs images from sounds. First, video data with both images and sounds are labeled with pre-trained classifiers. Second, image and sound features are extracted from the data using pre-trained classifiers. Third, multimodal layers are introduced to extract features that are common to both the images and sounds. These layers are trained to extract similar features regardless of the input modality, such as images only, sounds only, and both images and sounds. Once the multimodal layers have been trained, features are extracted from input sounds and converted into image features using a feature-to-feature GAN. Finally, the generated image features are used to reconstruct images. Experimental results show that this method can successfully convert from the sound domain into the image domain. When we applied a pre-trained classifier to both the generated and original images, 31.9% of the examples had at least one of their top 10 labels in common, suggesting reasonably good image generation. Our results suggest that common representations can be learned for different modalities, and that proposed method can be applied not only to sound-to-image conversion but also to other conversions, such as from images to sounds.", "keywords": ["deep learning", "machine learning", "multimodal", "generative adversarial networks"], "authorids": ["app@live.jp", "tshino@nict.go.jp", "kaoruamano@nict.go.jp"], "authors": ["Jeonghyun Lyu", "Takashi Shinozaki", "Kaoru Amano"], "TL;DR": "We propose a method of converting from the sound domain into the image domain based on multimodal features and stacked GANs.", "pdf": "/pdf/4ceae2e37f600b40c20a1109ee3721196cc32e73.pdf", "paperhash": "lyu|generating_images_from_sounds_using_multimodal_features_and_gans", "_bibtex": "@misc{\nlyu2019generating,\ntitle={Generating Images from Sounds Using Multimodal Features and {GAN}s},\nauthor={Jeonghyun Lyu and Takashi Shinozaki and Kaoru Amano},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxJtiRqt7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkgsMy0ZgN", "original": null, "number": 1, "cdate": 1544834835057, "ddate": null, "tcdate": 1544834835057, "tmdate": 1545354531596, "tddate": null, "forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper405/Meta_Review", "content": {"metareview": "The work presents a new way to generate images from sounds. The reviewers found the problem ill-defined, the method not well-motivated and the results not compelling. There are a number of missing references and things to compare to, which the authors should change in a follow-up.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper405/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper405/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Images from Sounds Using Multimodal Features and GANs", "abstract": "Although generative adversarial networks (GANs) have enabled us to convert images from one domain to another similar one, converting between different sensory modalities, such as images and sounds, has been difficult. This study aims to propose a network that reconstructs images from sounds. First, video data with both images and sounds are labeled with pre-trained classifiers. Second, image and sound features are extracted from the data using pre-trained classifiers. Third, multimodal layers are introduced to extract features that are common to both the images and sounds. These layers are trained to extract similar features regardless of the input modality, such as images only, sounds only, and both images and sounds. Once the multimodal layers have been trained, features are extracted from input sounds and converted into image features using a feature-to-feature GAN. Finally, the generated image features are used to reconstruct images. Experimental results show that this method can successfully convert from the sound domain into the image domain. When we applied a pre-trained classifier to both the generated and original images, 31.9% of the examples had at least one of their top 10 labels in common, suggesting reasonably good image generation. Our results suggest that common representations can be learned for different modalities, and that proposed method can be applied not only to sound-to-image conversion but also to other conversions, such as from images to sounds.", "keywords": ["deep learning", "machine learning", "multimodal", "generative adversarial networks"], "authorids": ["app@live.jp", "tshino@nict.go.jp", "kaoruamano@nict.go.jp"], "authors": ["Jeonghyun Lyu", "Takashi Shinozaki", "Kaoru Amano"], "TL;DR": "We propose a method of converting from the sound domain into the image domain based on multimodal features and stacked GANs.", "pdf": "/pdf/4ceae2e37f600b40c20a1109ee3721196cc32e73.pdf", "paperhash": "lyu|generating_images_from_sounds_using_multimodal_features_and_gans", "_bibtex": "@misc{\nlyu2019generating,\ntitle={Generating Images from Sounds Using Multimodal Features and {GAN}s},\nauthor={Jeonghyun Lyu and Takashi Shinozaki and Kaoru Amano},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxJtiRqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper405/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353229846, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper405/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper405/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper405/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353229846}}}, {"id": "Skgbfz3Z6Q", "original": null, "number": 3, "cdate": 1541681673266, "ddate": null, "tcdate": 1541681673266, "tmdate": 1541681673266, "tddate": null, "forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper405/Official_Review", "content": {"title": "I think the problem is ill-posed; the image generations from image features are not great; baselines from class labels would have worked beter; lacks motivation.", "review": "PROS:\n* The paper was well-written and explained the method and the experiments well\n\nCONS:\n* The problem seems ill-posed to me. Sound is temporal and the problem should probably be sound-to-video conversion not sound-to-image. \n* A link to generated images from sounds where one could actually evaluate the generations would be useful. Currently the only way to evaluate the results is via labels.\n* Similarly, a baseline where images are generated given the classification labels of the sounds would probably produce better looking images. Such baseline is not provided, and it is not clear to me what a multi-modal feature extraction is providing on top of this.  For example, in the case of StackGAN, the GAN that was converting text to images, the text was describing something about the image that one could quantify in the resulting generation (eg a blue bird as opposed to a yellow one). Here such an advantage is not clear and if there is one, it should be clearly stated and discussed.\n* The results in Fig. 3 seem particularly poor and on par with current GAN generations. I think this part of the model should be improved before attempting to improve the rest.\n* In Figures 6 and 7, it is not clear what we are expected to see. Also, the labels do not correspond to the real images in many of hte cases (eg pajama, wing, volcano etc).\n\n\nFinally in the discussion, DiscoGAN is mentioned as something to look into for future work. I should note that DiscoGAN is converting samples between domains of the same modality (vision), in the context of domain adaptation, similarly to other works.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper405/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Images from Sounds Using Multimodal Features and GANs", "abstract": "Although generative adversarial networks (GANs) have enabled us to convert images from one domain to another similar one, converting between different sensory modalities, such as images and sounds, has been difficult. This study aims to propose a network that reconstructs images from sounds. First, video data with both images and sounds are labeled with pre-trained classifiers. Second, image and sound features are extracted from the data using pre-trained classifiers. Third, multimodal layers are introduced to extract features that are common to both the images and sounds. These layers are trained to extract similar features regardless of the input modality, such as images only, sounds only, and both images and sounds. Once the multimodal layers have been trained, features are extracted from input sounds and converted into image features using a feature-to-feature GAN. Finally, the generated image features are used to reconstruct images. Experimental results show that this method can successfully convert from the sound domain into the image domain. When we applied a pre-trained classifier to both the generated and original images, 31.9% of the examples had at least one of their top 10 labels in common, suggesting reasonably good image generation. Our results suggest that common representations can be learned for different modalities, and that proposed method can be applied not only to sound-to-image conversion but also to other conversions, such as from images to sounds.", "keywords": ["deep learning", "machine learning", "multimodal", "generative adversarial networks"], "authorids": ["app@live.jp", "tshino@nict.go.jp", "kaoruamano@nict.go.jp"], "authors": ["Jeonghyun Lyu", "Takashi Shinozaki", "Kaoru Amano"], "TL;DR": "We propose a method of converting from the sound domain into the image domain based on multimodal features and stacked GANs.", "pdf": "/pdf/4ceae2e37f600b40c20a1109ee3721196cc32e73.pdf", "paperhash": "lyu|generating_images_from_sounds_using_multimodal_features_and_gans", "_bibtex": "@misc{\nlyu2019generating,\ntitle={Generating Images from Sounds Using Multimodal Features and {GAN}s},\nauthor={Jeonghyun Lyu and Takashi Shinozaki and Kaoru Amano},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxJtiRqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper405/Official_Review", "cdate": 1542234468752, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper405/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335716659, "tmdate": 1552335716659, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper405/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgazwrThm", "original": null, "number": 2, "cdate": 1541392148922, "ddate": null, "tcdate": 1541392148922, "tmdate": 1541534023594, "tddate": null, "forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper405/Official_Review", "content": {"title": "Generating Images from Sounds Using Multimodal Features and GANs", "review": "Summary:\n\nThis paper addresses the problem of generating images from sound. The general idea is to use conditional GANs. In particular, two stacked conditional autoencoder GANs, where the autoencoders have a U-Net architecture. First, sound features are mapped into multimodal features that contain image feature/class information. Such multimodal features condition the generation of image features with an initial GAN, and the image features condition the generation of the output image with a second GAN. Although the problem itself is rather difficult, the solution is almost entirely based on previous work. The most novel part of the paper is the learning of the multimodal features. The final results are not very compelling, the literature review is very limited. There is a very high-level description of the approach with very few details, which leave the reader with a lot of unanswered questions. There is no attempt to compare with previous work, the architecture is not studied in depth with an ablation study, or compared with more interesting baselines, other than verifying that images can be generated from features (something not very surprising, given StackGUN!). Probably the more important verification is that the multimodal feature can embody some class information. Overall it seems to be a limited contribution.\n\nComments on quality, clarity, originality and significance:\n\nThis paper provides a high-level description for an approach to a problem that is relatively difficult to address. The paper is not very well motivated, and therefore lacks clarity and leaves the reader with a lot of unanswered questions. The literature review is limited as it is the set of results and comparison with previous works.\n\nThe paper addresses an important problem; however, I feel the work is not very significant because it does not reveal new techniques, nor produces compelling results, nor performs a deep analysis.\n\nI think the problem is interesting, but a deeper analysis is needed to lift this contribution up to a significant one.\n\nBelow is a summary of some of the additional questions gathered while reading the paper:\n\nWhy there is the need for the multimodal features at all? Why can\u2019t the sound be converted into class labels and then StackGAN can generate images?\n\nWhy do you need a feature-to-feature GAN? Why not generating images directly from the generated multimodal features? Motivations are not provided clearly.\n\nUnclear architecture from Figure 1. The Feature-to-Feature GAN and the Feature-to-Image GAN have the same architecture? What does the encoder and decoder do? How are they organized? \n\nLooks like every piece is trained alone, no end-to-end learning, right? Please clarify that point.\n\nNo attempt to compare with other approaches has been made. Also, no effort to formulate a baseline model. What would happen if one were to use solely the features generated by SoundNet?\n\nWould you be able to compare your multimodal features with those generated by Ngiam et al. (2011), for instance?\n\nSection 3 refers to a 90/10 training/evaluation split but then it is unclear in what experiments that exact split is used.\n\nNo description on hyperparameters.\n\nNo complexity, no architecture details, (also no equations that could provide more details).\n\nIt should be clarified what it means one-to-one conversion. It is brought up in several points in the paper, but it is never clear what it means and therefore how it relates to what the Author intends to stress. \n\nIt is unclear why by performing first a feature-to-feature mapping and later a feature-to-image mapping the one-to-one conversion problem should be addressed. In StackGAN the problem addressed is the resolution increase. The problem addressed by this paper is unclear.\n\nUnclear what is the \u201censemble effect\u201d, and what is the motivation for upsampling a feature vector in two dimensions.\n\nWhat image features were used to generate the images in Figure 3? Which architecture was used and how was it trained? Where these the same multimodal features used in the full architecture?\n\nThe paragraph motivating the need for multimodal features is unclear.\n\nHow are the three type of losses weighted for learning the multimodal layers? No discussion provided on that.\n\nUnclear why a multimodal vector should be upsampled in 2 dimensions. \n\nThe training procedure and loss for training the feature-to-feature conditional GAN is not explained.\n\nDespite the difficulty of the problem, the generated images do not look compelling.\n\n16 references do not seem enough by todays\u2019 high-quality standard conferences.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper405/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Images from Sounds Using Multimodal Features and GANs", "abstract": "Although generative adversarial networks (GANs) have enabled us to convert images from one domain to another similar one, converting between different sensory modalities, such as images and sounds, has been difficult. This study aims to propose a network that reconstructs images from sounds. First, video data with both images and sounds are labeled with pre-trained classifiers. Second, image and sound features are extracted from the data using pre-trained classifiers. Third, multimodal layers are introduced to extract features that are common to both the images and sounds. These layers are trained to extract similar features regardless of the input modality, such as images only, sounds only, and both images and sounds. Once the multimodal layers have been trained, features are extracted from input sounds and converted into image features using a feature-to-feature GAN. Finally, the generated image features are used to reconstruct images. Experimental results show that this method can successfully convert from the sound domain into the image domain. When we applied a pre-trained classifier to both the generated and original images, 31.9% of the examples had at least one of their top 10 labels in common, suggesting reasonably good image generation. Our results suggest that common representations can be learned for different modalities, and that proposed method can be applied not only to sound-to-image conversion but also to other conversions, such as from images to sounds.", "keywords": ["deep learning", "machine learning", "multimodal", "generative adversarial networks"], "authorids": ["app@live.jp", "tshino@nict.go.jp", "kaoruamano@nict.go.jp"], "authors": ["Jeonghyun Lyu", "Takashi Shinozaki", "Kaoru Amano"], "TL;DR": "We propose a method of converting from the sound domain into the image domain based on multimodal features and stacked GANs.", "pdf": "/pdf/4ceae2e37f600b40c20a1109ee3721196cc32e73.pdf", "paperhash": "lyu|generating_images_from_sounds_using_multimodal_features_and_gans", "_bibtex": "@misc{\nlyu2019generating,\ntitle={Generating Images from Sounds Using Multimodal Features and {GAN}s},\nauthor={Jeonghyun Lyu and Takashi Shinozaki and Kaoru Amano},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxJtiRqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper405/Official_Review", "cdate": 1542234468752, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper405/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335716659, "tmdate": 1552335716659, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper405/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryxapKiBhX", "original": null, "number": 1, "cdate": 1540893125392, "ddate": null, "tcdate": 1540893125392, "tmdate": 1541534023337, "tddate": null, "forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "invitation": "ICLR.cc/2019/Conference/-/Paper405/Official_Review", "content": {"title": "A good idea, poor development and results.", "review": "The authors present a novel method for generating images from sounds using a two parts model composed by a fusion network, aka. multi-modal layers, for learning sound and visual features in a common semantic space, and two conditional GANs for converting sound features into visual features and those into images. To validate their approach they created an ad-hoc dataset, based on Flickr-SoundNet dataset, which contains 104K pairs of sounds and images with matching scene content. Their model was trained as two separate models, the fusion network was trained to classify both images and sounds minimizing their cross-entropy and their L1 distance, while the two conditional GANs were trained until convergence penalizing the discriminator to prevent fast convergence.\n\nAlthough the idea of generating images from sounds with the aid of Generative Adversarial Networks is quite novel and interesting, the paper exhibits several problems starting with the lack of clarity explaining the purpose of the proposed method and the contributions of the work itself. Overall, the idea is good but not well developed. Introduction should present more clearly the problem and framework.\n\nIn the related work section the authors omitted some relevant recent prior works such as \u201cLook, Listen and Learn\u201d paper by Arandjelovi\u0107 and Zisserman presented on ICCV\u201917, \u201cObjects that Sound\u201d by Arandjelovi\u0107 and Zisserman presented on ECCV\u201918, \u201cAudio-Visual Scene Analysis with Self-Supervised Multisensory Features\u201d by Owens and Efros presented on ECCV\u201918, and \u201cJointly Discovering Visual Objects and Spoken Words from Raw Sensory Input\u201d by Harwath et al. also presented presented on ECCV\u201918. These works propose different methods for aligning visual and sound features.\n\nThere are also several concerns on the validity of the results: 1) none of the results achieved by training their multi-modal layers were validated against a baseline, e.g. evaluating the quality of the learned visual features against VGG or a simple GAN instead of two stacked conditional GANs, 2) it is not clear why they learned features minimizing L1 loss + Cross-Entropy while using L2 distance to address the quality of their learned features, a simple way of doing so would be evaluating their retrieval capabilities using any standard measure from the retrieval community, e.g. the normalized discriminative cumulative gain (nDCG) or the classical mean-average precision (mAP) as proposed in \u201cObjects that Sound\u201d, 3) the authors assume that using a conditional GAN is suitable for generating images from visual features, but they don\u2019t provide any quantitative results supporting this claim, they only provide a few successful qualitative results and elaborate their model from there. 4) Ablation is completely missing: it would be interesting to prove the effective contribution for i) the multi-modal fusion ii) the two-steps of image generation iii) the L_ losses for the two GANs.\n\nThere are many missing citations throughout the paper, in particular: 1) the concatenation of visual and sound features followed by a fusion network for learning features in a common semantic space was already proposed on \u201cLook, Listen and Learn\u201d, 2) when the authors describe their strategy for sound features extraction in section four, they never mentioned that the idea of using pool5 layer features was already introduced by SoundNet authors, and 3) in section 5.3 when they mention that using a conditional GAN to convert between two different feature domains it might be that the discriminator may converge too rapidly while the generator does not learn sufficiently.\n\nFinally although using an ad-hoc extremely simplified dataset with pairs of images and sounds matching scene content, the complete model is able to generate images which achieve only a 8,9% matching rate for the top 3 predicted classes. Given that the dataset was created with 100% matching on the top 3 scores for sound and images, the results are definitely  poor.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper405/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Images from Sounds Using Multimodal Features and GANs", "abstract": "Although generative adversarial networks (GANs) have enabled us to convert images from one domain to another similar one, converting between different sensory modalities, such as images and sounds, has been difficult. This study aims to propose a network that reconstructs images from sounds. First, video data with both images and sounds are labeled with pre-trained classifiers. Second, image and sound features are extracted from the data using pre-trained classifiers. Third, multimodal layers are introduced to extract features that are common to both the images and sounds. These layers are trained to extract similar features regardless of the input modality, such as images only, sounds only, and both images and sounds. Once the multimodal layers have been trained, features are extracted from input sounds and converted into image features using a feature-to-feature GAN. Finally, the generated image features are used to reconstruct images. Experimental results show that this method can successfully convert from the sound domain into the image domain. When we applied a pre-trained classifier to both the generated and original images, 31.9% of the examples had at least one of their top 10 labels in common, suggesting reasonably good image generation. Our results suggest that common representations can be learned for different modalities, and that proposed method can be applied not only to sound-to-image conversion but also to other conversions, such as from images to sounds.", "keywords": ["deep learning", "machine learning", "multimodal", "generative adversarial networks"], "authorids": ["app@live.jp", "tshino@nict.go.jp", "kaoruamano@nict.go.jp"], "authors": ["Jeonghyun Lyu", "Takashi Shinozaki", "Kaoru Amano"], "TL;DR": "We propose a method of converting from the sound domain into the image domain based on multimodal features and stacked GANs.", "pdf": "/pdf/4ceae2e37f600b40c20a1109ee3721196cc32e73.pdf", "paperhash": "lyu|generating_images_from_sounds_using_multimodal_features_and_gans", "_bibtex": "@misc{\nlyu2019generating,\ntitle={Generating Images from Sounds Using Multimodal Features and {GAN}s},\nauthor={Jeonghyun Lyu and Takashi Shinozaki and Kaoru Amano},\nyear={2019},\nurl={https://openreview.net/forum?id=SJxJtiRqt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper405/Official_Review", "cdate": 1542234468752, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJxJtiRqt7", "replyto": "SJxJtiRqt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper405/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335716659, "tmdate": 1552335716659, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper405/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}