{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487088813426, "tcdate": 1478282245349, "number": 269, "id": "HkpbnH9lx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkpbnH9lx", "signatures": ["~Laurent_Dinh1"], "readers": ["everyone"], "content": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396471797, "tcdate": 1486396471797, "number": 1, "id": "SJgBnfI_e", "invitation": "ICLR.cc/2017/conference/-/paper269/acceptance", "forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.\n \n The main weakness of this paper is that it did not explore the computational tradeoffs of this approach against related methods. However, the paper already had a lot of content.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396472322, "id": "ICLR.cc/2017/conference/-/paper269/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396472322}}}, {"tddate": null, "tmdate": 1484775959189, "tcdate": 1484775959189, "number": 6, "id": "B1JXfvaUx", "invitation": "ICLR.cc/2017/conference/-/paper269/public/comment", "forum": "HkpbnH9lx", "replyto": "ryV5E2-4e", "signatures": ["~Laurent_Dinh1"], "readers": ["everyone"], "writers": ["~Laurent_Dinh1"], "content": {"title": "Answer to AnonReviewer2", "comment": "Thanks for your input!\n\"Given that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\"\nEfficient inference is necessary for tractable training in most probabilistic models and efficient sampling is useful for application that would require fast sampling, e.g. music and speech synthesis and real-time rendering. Having exact inference allows us to accurately and exactly evaluate our models, and could be highly beneficial for tasks such as semi-supervised learning, which require accurately estimating a latent state. Having exact sampling as opposed to MCMC sampling efficiently prevents us from having highly correlated samples (and poor diversity of samples).\nWe have added this to the submission as an additional argument.\n\n\"The authors claim that \u201cunlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.\u201d Where is the evidence for this claim? I didn\u2019t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\"\nThank you for pointing out this claim. The aim of this sentence was to focus more on the \"as high dimensional as the input space\" aspect as opposed to VAEs and GANs, which previously used effectively low dimensional latent codes for their models.\nAs for showing that the latent space is semantically meaningful, based on your feedback we have added an additional experiment to the paper (Appendix F), showing that in class-conditional image generation latent variables have a consistent semantic meaning across classes.\n\n\"I still think the author\u2019s intuitions around the \u201cfixed reconstruction cost of L2\u201d are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.\"\nWhat you are saying is true but your statement is about the class of generative models associated to Variational Auto-Encoders. The Variational Auto-Encoder learning also includes an encoder for the approximate posterior. Often this approximation is not enough which often results in under exploiting the capacity that these generative models (the decoders) have. The issue here is trainability.\nThis lack of trainability is due to the gap between the approximate posterior and the real posterior. As the reconstruction cost becomes more flexible, the *real* posterior p(z|x) = p(z)p(x|z)/p(x) becomes more able to fit the family of approximate posterior considered.\nSince the problem of using fixed form reconstruction cost in VAEs is already mentioned in \"Autoencoding beyond pixels using a learned similarity metric\", we chose not to put a focus on explaining this issue."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287647471, "id": "ICLR.cc/2017/conference/-/paper269/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpbnH9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper269/reviewers", "ICLR.cc/2017/conference/paper269/areachairs"], "cdate": 1485287647471}}}, {"tddate": null, "tmdate": 1484333563051, "tcdate": 1484333563051, "number": 5, "id": "Sk7bzoLUl", "invitation": "ICLR.cc/2017/conference/-/paper269/public/comment", "forum": "HkpbnH9lx", "replyto": "HkxVRCfNg", "signatures": ["~Laurent_Dinh1"], "readers": ["everyone"], "writers": ["~Laurent_Dinh1"], "content": {"title": "Answer to AnonReviewer3", "comment": "Hi,\n\nThanks you for reviewing our paper. \n\"The paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\"\nOur code is now included in the Tensorflow models open source repository (https://github.com/tensorflow/models/tree/master/real_nvp), in addition to that, our model has been reimplemented  by Taesung Park on Github: https://github.com/taesung89/real-nvp\n\nWe will definitely correct the typo you mentioned."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287647471, "id": "ICLR.cc/2017/conference/-/paper269/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpbnH9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper269/reviewers", "ICLR.cc/2017/conference/paper269/areachairs"], "cdate": 1485287647471}}}, {"tddate": null, "tmdate": 1482241109657, "tcdate": 1482241109657, "number": 3, "id": "SJ08V3IEe", "invitation": "ICLR.cc/2017/conference/-/paper269/official/review", "forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "signatures": ["ICLR.cc/2017/conference/paper269/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper269/AnonReviewer4"], "content": {"title": "Interesting and important work", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents a clever way of training a generative model which allows for exact inference, sampling and log likelihood evaluation. The main idea here is to make the Jacobian that comes when using the change of variables formula (from data to latents) triangular - this makes the determinant easy to calculate and hence learning possible. \nThe paper nicely presents this core idea and a way to achieve this - by choosing special \"routings\" between the latents and data such that part of the transformation is identity and part some complex function of the input (a deep net, for example) the resulting Jacobian has a tractable structure. This routing can be cascaded to achieve even more complex transformation. \n\nOn the experimental side, the model is trained on several datasets and the results are quite convincing, both in sample quality and quantitive measures. \nI would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration.\n\nIn summary - the paper is nicely written, results are quite good and the model is interesting - I'm happy to recommend acceptance.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512642599, "id": "ICLR.cc/2017/conference/-/paper269/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper269/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper269/AnonReviewer2", "ICLR.cc/2017/conference/paper269/AnonReviewer3", "ICLR.cc/2017/conference/paper269/AnonReviewer4"], "reply": {"forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512642599}}}, {"tddate": null, "tmdate": 1481989672472, "tcdate": 1481989672472, "number": 2, "id": "HkxVRCfNg", "invitation": "ICLR.cc/2017/conference/-/paper269/official/review", "forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "signatures": ["ICLR.cc/2017/conference/paper269/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper269/AnonReviewer3"], "content": {"title": "Interesting ideas, great empirical work, overall good contribution", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.\nThe authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.\n\nGenerative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.\n\nWhile not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.\n\nThe authors clearly mention the difference and similarities with other types of generative models that are being actively researched.\nCompared to autoregressive models, the proposed approach offers fast sampling.\nCompared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.\nCompared to variational autoencoders, the inference is exact.\nCompared to deep Boltzmann machines, the learning of the proposed method is tractable.\nIt is clear that Real NVP goal is to bridge the gap between existing and popular generative models.\n\nThe paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\n\nTypo: (Section 3.7) We also \"use apply\" batch normalization", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512642599, "id": "ICLR.cc/2017/conference/-/paper269/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper269/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper269/AnonReviewer2", "ICLR.cc/2017/conference/paper269/AnonReviewer3", "ICLR.cc/2017/conference/paper269/AnonReviewer4"], "reply": {"forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512642599}}}, {"tddate": null, "tmdate": 1481913484389, "tcdate": 1481913484389, "number": 1, "id": "ryV5E2-4e", "invitation": "ICLR.cc/2017/conference/-/paper269/official/review", "forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "signatures": ["ICLR.cc/2017/conference/paper269/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper269/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.\n\nGiven that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\n\nThe authors claim that \u201cunlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.\u201d Where is the evidence for this claim? I didn\u2019t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\n\nI still think the author\u2019s intuitions around the \u201cfixed reconstruction cost of L2\u201d are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512642599, "id": "ICLR.cc/2017/conference/-/paper269/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper269/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper269/AnonReviewer2", "ICLR.cc/2017/conference/paper269/AnonReviewer3", "ICLR.cc/2017/conference/paper269/AnonReviewer4"], "reply": {"forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512642599}}}, {"tddate": null, "tmdate": 1481555849685, "tcdate": 1481552653337, "number": 3, "id": "BJBMQVnQe", "invitation": "ICLR.cc/2017/conference/-/paper269/public/comment", "forum": "HkpbnH9lx", "replyto": "SJRCCZ0Ge", "signatures": ["~Laurent_Dinh1"], "readers": ["everyone"], "writers": ["~Laurent_Dinh1"], "content": {"title": "Re: Latent space interpolation, batch norm with moving average and close train/valid results", "comment": "Hi, \n\nI used cosine interpolation because, in high dimension, the mass of the gaussian distribution is mainly concentrated on a sphere (the norm-2 of a multivariate standard gaussian variable follows a chi-squared distribution). Hence, a cosine interpolation would mainly travel in area of high probability according to the prior distribution.\nWe are using batch normalization with moving average only on CIFAR-10 because this is where it had noticeable effect, the choice of the batch normalization scheme for the other datasets did not seem to make a difference.\nThe train and valid results remained close during training on large scale datasets, which seems to suggest that we are not overfitting."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287647471, "id": "ICLR.cc/2017/conference/-/paper269/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpbnH9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper269/reviewers", "ICLR.cc/2017/conference/paper269/areachairs"], "cdate": 1485287647471}}}, {"tddate": null, "tmdate": 1481555119167, "tcdate": 1481555119159, "number": 4, "id": "ryP23EnXe", "invitation": "ICLR.cc/2017/conference/-/paper269/public/comment", "forum": "HkpbnH9lx", "replyto": "S1Ivr4kXl", "signatures": ["~Laurent_Dinh1"], "readers": ["everyone"], "writers": ["~Laurent_Dinh1"], "content": {"title": "Re: Likelihood vs L2 reconstruction", "comment": "Hi, \n\nTheir objective functions _can be_ very similar indeed as Real NVP optimizes the exact log-likelihood whereas VAE optimizes the Evidence Lower Bound of the log-likelihood and their closeness depends on how much the approximate posterior q(z|x) matches the true posterior p(z|x)=p(z)p(x|z)/p(x). The Variational Lossy Autoencoder paper provides further discussion on the subject: https://openreview.net/forum?id=BysvGP5ee. \n\nHowever, we are referring to the standard VAE algorithms from (Kingma and Welling, 2013) and (Rezende et al., 2014) where p(x|z) is factorial. This results in a fixed form reconstruction loss -log(p(x|z)), a L2 loss for a conditional gaussian p(x|z). Even if you have a small decoder noise, i.e. p(x|z) has small entropy, this reconstruction loss remains fixed nonetheless. I agree that it would not be the case if p(x|z) was a more complex conditional distribution as in \"PixelVAE: A Latent Variable Model for Natural Images\" and \"Variational Lossy Autoencoder\", two papers that you will find among ICLR submissions this year. \n\nRelying on fixed form reconstruction cost is not an issue in an infinite capacity setting for either algorithm. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287647471, "id": "ICLR.cc/2017/conference/-/paper269/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpbnH9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper269/reviewers", "ICLR.cc/2017/conference/paper269/areachairs"], "cdate": 1485287647471}}}, {"tddate": null, "tmdate": 1480701277652, "tcdate": 1480701277647, "number": 2, "id": "S1Ivr4kXl", "invitation": "ICLR.cc/2017/conference/-/paper269/pre-review/question", "forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "signatures": ["ICLR.cc/2017/conference/paper269/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper269/AnonReviewer2"], "content": {"title": "Likelihood vs L2 reconstruction", "question": "The authors claim that \u201cas opposed to [variational autoencoders], real NVP does not rely on fixed form reconstruction cost like an L2.\u201d I can see where the authors get their intuition, nevertheless this statement seems overly simplified/wrong. For small decoder noise, a larger latent space and assuming a strong enough encoder so that the lower bound is close to the likelihood, the differences between what real NVP and a VAE optimize seem to become very small*. Both use a very similar generative model and (approximately) optimize its likelihood. Am I missing something/can you clarify what you mean?\n\n* Note that I am not trying to say that VAE and real NVP are the same, just that their objective functions _can be_ very similar."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959369947, "id": "ICLR.cc/2017/conference/-/paper269/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper269/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper269/AnonReviewer3", "ICLR.cc/2017/conference/paper269/AnonReviewer2"], "reply": {"forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959369947}}}, {"tddate": null, "tmdate": 1480625878097, "tcdate": 1480625878092, "number": 1, "id": "SJRCCZ0Ge", "invitation": "ICLR.cc/2017/conference/-/paper269/pre-review/question", "forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "signatures": ["ICLR.cc/2017/conference/paper269/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper269/AnonReviewer3"], "content": {"title": "Latent space interpolation, batch norm with moving average and close train/valid results", "question": "Why use a cosine interpolation instead of a linear one (Fig. 6)?\nWhy aren't you using your batch normalization with moving average on other datasets than CIFAR-10 (Appendix E)?\nSome train and valid results in Table 1 seem pretty close. Are they statistically different?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959369947, "id": "ICLR.cc/2017/conference/-/paper269/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper269/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper269/AnonReviewer3", "ICLR.cc/2017/conference/paper269/AnonReviewer2"], "reply": {"forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper269/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959369947}}}, {"tddate": null, "tmdate": 1479851590806, "tcdate": 1479840125106, "number": 2, "id": "HJrtbGMze", "invitation": "ICLR.cc/2017/conference/-/paper269/public/comment", "forum": "HkpbnH9lx", "replyto": "HJ5ml-Gfl", "signatures": ["~Laurent_Dinh1"], "readers": ["everyone"], "writers": ["~Laurent_Dinh1"], "content": {"title": "There are different interpretations of batch normalization", "comment": "Thanks for noticing that!\n\nThere are two views of batch normalization. One is the non-linear mapping from a batch of inputs to a batch of outputs.  In this case, the Jacobian determinant should indeed include the dependencies of the batch statistics with respect to the input x. However, this function is not bijective, so this interpretation cannot be used when learning a bijective map.\n\nThe other view of batch normalization is a linear mapping whose parameters are estimated at every iteration using the current batch statistics. During evaluation time, the quantities mu and sigma are replaced by accumulated statistics over the whole dataset. \n\nWe are using the second view: we consider batch normalization as a linear mapping with parameters estimated from the batch statistics. We therefore do not include the functional dependence of mu and sigma on x in the log determinant of the Jacobian when computing the log likelihood. During test time, batch norm is applied using accumulated values for mu and sigma, with no x dependence.\n\nNote that during training when we compute the gradient of the log likelihood w.r.t. the parameters, the dependence of mu and sigma on x *is* taken into account via automatic differentiation.\n\nWe will clarify this in the next version of the text. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287647471, "id": "ICLR.cc/2017/conference/-/paper269/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpbnH9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper269/reviewers", "ICLR.cc/2017/conference/paper269/areachairs"], "cdate": 1485287647471}}}, {"tddate": null, "tmdate": 1479835682547, "tcdate": 1479835682542, "number": 1, "id": "HJ5ml-Gfl", "invitation": "ICLR.cc/2017/conference/-/paper269/public/comment", "forum": "HkpbnH9lx", "replyto": "HkpbnH9lx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Equation 18 - missing derivatives of batch statistics?", "comment": "In equations 17-18 the quantities $\\mu$ and $\\sigma$ depend on the current batch $x$. Shouldn't their derivatives with respect to $x$ be part of the Jacobian?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Density estimation using Real NVP", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "pdf": "/pdf/10807df6f626b504c0d7abe90bb81aef3fb9fb13.pdf", "TL;DR": "Efficient invertible neural networks for density estimation and generation", "paperhash": "dinh|density_estimation_using_real_nvp", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["umontreal.ca", "google.com"], "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287647471, "id": "ICLR.cc/2017/conference/-/paper269/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpbnH9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper269/reviewers", "ICLR.cc/2017/conference/paper269/areachairs"], "cdate": 1485287647471}}}], "count": 13}