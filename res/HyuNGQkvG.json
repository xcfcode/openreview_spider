{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124478528, "tcdate": 1518445104235, "number": 123, "cdate": 1518445104235, "id": "HyuNGQkvG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HyuNGQkvG", "signatures": ["~Joel_Owen_Nicholls1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Progressive prune network for memory efficient continual learning", "abstract": "We present a method for the transfer of knowledge between tasks in memory-constrained devices. In this setting, the per-parameter performance over multiple tasks is a critical objective. Specifically, we consider continual training and pruning of a progressive neural network. This type of multi-task network was introduced in Rusu et al. (2016)a, which optimised for performance, while the number of parameters grew quadratically with the number of tasks. Our preliminary results demonstrates that it is possible to limit the parameter growth to be linear, while still achieving a performance boost, and sharing knowledge across different tasks.", "paperhash": "nicholls|progressive_prune_network_for_memory_efficient_continual_learning", "_bibtex": "@misc{\n  nicholls2018progressive,\n  title={Progressive prune network for memory efficient continual learning},\n  author={Joel Nicholls and Sakyasingha Dasgupta},\n  year={2018},\n  url={https://openreview.net/forum?id=HyuNGQkvG}\n}", "authorids": ["joel@leapmind.io", "sakya@ascent.ai"], "authors": ["Joel Nicholls", "Sakyasingha Dasgupta"], "keywords": ["Deep learning", "transfer learning", "compression", "classification"], "pdf": "/pdf/69df9394daafa0646c7789dba6fd1fd58fc71d8f.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582939558, "tcdate": 1520278104175, "number": 1, "cdate": 1520278104175, "id": "HJev5zs_f", "invitation": "ICLR.cc/2018/Workshop/-/Paper123/Official_Review", "forum": "HyuNGQkvG", "replyto": "HyuNGQkvG", "signatures": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer1"], "content": {"title": "Simple yet effective pruning of progressive networks can lead to improved generalization", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a simple parameter pruning technique, based on small absolute values, as a way to reduce the quadratic growth of standard progressive neural networks to linear in the number of tasks, with subunitary coefficients.. Experiments show that, in certain cases, this simple approach delivers competitive generalization with much reduced model growth; the added columns are indeed much smaller than the first, suggesting that similar tasks can indeed leverage representations in previous columns.\n\nPros:\nAs far as I can tell the experiments are sound and interesting.\n\nCons:\nBackground work section is missing. Please add references to at least: [1, 2, 3]\nLack of details in method description, especially about pruning strategy. These should be moved to the main text.\nDiagrams of the baselines considered would be useful.\nPlease add a table with actual numbers for each column and task. It\u2019s not clear what accuracies over the 3 datasets really are from the average.\n\nPlease clarify:\nHow does your approach compare to state-of-the-art results on each dataset? Are there other ways to get similar performance within the same numbers of parameters?\n\n[1]  A. V. Terekhov, G. Montone, and J. K. O\u2019Regan, Knowledge Transfer in Deep Block-Modular Neural Networks. Cham: Springer Interna- tional Publishing, 2015, pp. 268\u2013279.\n[2] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert, \u201cCross-stitch networks for multi-task learning,\u201d in Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, 2016, pp. 3994\u2013 4003.\n[3] Ark Anderson, Kyle Shaffer, Artem Yankov, Court D. Corley, Nathan O. Hodas. Beyond Fine Tuning: A Modular Approach to Learning on Small Data. Arxiv https://arxiv.org/abs/1611.01714\n\n\nI am willing to increase my score if substantial clarifications are made.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive prune network for memory efficient continual learning", "abstract": "We present a method for the transfer of knowledge between tasks in memory-constrained devices. In this setting, the per-parameter performance over multiple tasks is a critical objective. Specifically, we consider continual training and pruning of a progressive neural network. This type of multi-task network was introduced in Rusu et al. (2016)a, which optimised for performance, while the number of parameters grew quadratically with the number of tasks. Our preliminary results demonstrates that it is possible to limit the parameter growth to be linear, while still achieving a performance boost, and sharing knowledge across different tasks.", "paperhash": "nicholls|progressive_prune_network_for_memory_efficient_continual_learning", "_bibtex": "@misc{\n  nicholls2018progressive,\n  title={Progressive prune network for memory efficient continual learning},\n  author={Joel Nicholls and Sakyasingha Dasgupta},\n  year={2018},\n  url={https://openreview.net/forum?id=HyuNGQkvG}\n}", "authorids": ["joel@leapmind.io", "sakya@ascent.ai"], "authors": ["Joel Nicholls", "Sakyasingha Dasgupta"], "keywords": ["Deep learning", "transfer learning", "compression", "classification"], "pdf": "/pdf/69df9394daafa0646c7789dba6fd1fd58fc71d8f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582939321, "id": "ICLR.cc/2018/Workshop/-/Paper123/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper123/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper123/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper123/AnonReviewer2"], "reply": {"forum": "HyuNGQkvG", "replyto": "HyuNGQkvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582939321}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582777867, "tcdate": 1520634682467, "number": 2, "cdate": 1520634682467, "id": "SkMHiFgYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper123/Official_Review", "forum": "HyuNGQkvG", "replyto": "HyuNGQkvG", "signatures": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer3"], "content": {"title": "Controlling parameter growth in neural nets for shared tasks", "rating": "5: Marginally below acceptance threshold", "review": "The paper presents a method for controlling the growth in the number of parameters when dealing with shared tasks in which sharing of knowledge between tasks is crucial for overall performance. It is argued that the earlier work of Rusu et al. (2016)a for achieving the same goal had a quadratic growth in the number of parameters. \n\nIn this work, the authors prune the small weights based on their magnitudes as done is Han etal. 2015. In this sense, there is limited novelty in the paper. Also, it is not clear how the parameter are linear in the number of tasks after pruning as claimed by the authors. These still seem to be quadratic wrt number of tasks over the reduced set of parameters after pruning.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive prune network for memory efficient continual learning", "abstract": "We present a method for the transfer of knowledge between tasks in memory-constrained devices. In this setting, the per-parameter performance over multiple tasks is a critical objective. Specifically, we consider continual training and pruning of a progressive neural network. This type of multi-task network was introduced in Rusu et al. (2016)a, which optimised for performance, while the number of parameters grew quadratically with the number of tasks. Our preliminary results demonstrates that it is possible to limit the parameter growth to be linear, while still achieving a performance boost, and sharing knowledge across different tasks.", "paperhash": "nicholls|progressive_prune_network_for_memory_efficient_continual_learning", "_bibtex": "@misc{\n  nicholls2018progressive,\n  title={Progressive prune network for memory efficient continual learning},\n  author={Joel Nicholls and Sakyasingha Dasgupta},\n  year={2018},\n  url={https://openreview.net/forum?id=HyuNGQkvG}\n}", "authorids": ["joel@leapmind.io", "sakya@ascent.ai"], "authors": ["Joel Nicholls", "Sakyasingha Dasgupta"], "keywords": ["Deep learning", "transfer learning", "compression", "classification"], "pdf": "/pdf/69df9394daafa0646c7789dba6fd1fd58fc71d8f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582939321, "id": "ICLR.cc/2018/Workshop/-/Paper123/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper123/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper123/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper123/AnonReviewer2"], "reply": {"forum": "HyuNGQkvG", "replyto": "HyuNGQkvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582939321}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582739514, "tcdate": 1520664965565, "number": 3, "cdate": 1520664965565, "id": "rJCt-bZYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper123/Official_Review", "forum": "HyuNGQkvG", "replyto": "HyuNGQkvG", "signatures": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer2"], "content": {"title": "Very preliminary work, claims not properly substantiated.", "rating": "5: Marginally below acceptance threshold", "review": "Building on progressive NNs for multi-task learning, this work aims at reducing the number of weights (and therefore memory) used by the solution. The proposal is simply to use a previously existing and straightforward pruning mechanism on the weights, so it doesn't convey a particularly novel idea.\n\nProgressive NNs suffer from a quadratic increase in the number of weights with the number of tasks. In this paper, the authors claim that that memory increase is only linear. However this is not justified, since the number of adaptor networks is still quadratic. No reason is given for this linear order. One possibility is that the authors are adjusting the pruning level in a way that achieves the desired scaling, but then we might as well say that it is sublinear, superlinear, etc, so that the linear scaling is not a property of the approach\n\nIn Figure 2(a) it seems that \"No conn\" networks result in better accuracy with the intermediate number of parameters, meaning that the first network is overfitting. This is not the regime in which the test should have been done.\n\nFigure 2(b) doesn't include a comparison with the progressive unpruned networks, which should be the reference.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive prune network for memory efficient continual learning", "abstract": "We present a method for the transfer of knowledge between tasks in memory-constrained devices. In this setting, the per-parameter performance over multiple tasks is a critical objective. Specifically, we consider continual training and pruning of a progressive neural network. This type of multi-task network was introduced in Rusu et al. (2016)a, which optimised for performance, while the number of parameters grew quadratically with the number of tasks. Our preliminary results demonstrates that it is possible to limit the parameter growth to be linear, while still achieving a performance boost, and sharing knowledge across different tasks.", "paperhash": "nicholls|progressive_prune_network_for_memory_efficient_continual_learning", "_bibtex": "@misc{\n  nicholls2018progressive,\n  title={Progressive prune network for memory efficient continual learning},\n  author={Joel Nicholls and Sakyasingha Dasgupta},\n  year={2018},\n  url={https://openreview.net/forum?id=HyuNGQkvG}\n}", "authorids": ["joel@leapmind.io", "sakya@ascent.ai"], "authors": ["Joel Nicholls", "Sakyasingha Dasgupta"], "keywords": ["Deep learning", "transfer learning", "compression", "classification"], "pdf": "/pdf/69df9394daafa0646c7789dba6fd1fd58fc71d8f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582939321, "id": "ICLR.cc/2018/Workshop/-/Paper123/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper123/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper123/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper123/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper123/AnonReviewer2"], "reply": {"forum": "HyuNGQkvG", "replyto": "HyuNGQkvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper123/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582939321}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573584705, "tcdate": 1521573584705, "number": 178, "cdate": 1521573584358, "id": "ryY0ARAFG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HyuNGQkvG", "replyto": "HyuNGQkvG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive prune network for memory efficient continual learning", "abstract": "We present a method for the transfer of knowledge between tasks in memory-constrained devices. In this setting, the per-parameter performance over multiple tasks is a critical objective. Specifically, we consider continual training and pruning of a progressive neural network. This type of multi-task network was introduced in Rusu et al. (2016)a, which optimised for performance, while the number of parameters grew quadratically with the number of tasks. Our preliminary results demonstrates that it is possible to limit the parameter growth to be linear, while still achieving a performance boost, and sharing knowledge across different tasks.", "paperhash": "nicholls|progressive_prune_network_for_memory_efficient_continual_learning", "_bibtex": "@misc{\n  nicholls2018progressive,\n  title={Progressive prune network for memory efficient continual learning},\n  author={Joel Nicholls and Sakyasingha Dasgupta},\n  year={2018},\n  url={https://openreview.net/forum?id=HyuNGQkvG}\n}", "authorids": ["joel@leapmind.io", "sakya@ascent.ai"], "authors": ["Joel Nicholls", "Sakyasingha Dasgupta"], "keywords": ["Deep learning", "transfer learning", "compression", "classification"], "pdf": "/pdf/69df9394daafa0646c7789dba6fd1fd58fc71d8f.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}