{"notes": [{"id": "rkl2s34twS", "original": "HylWQMfZwH", "number": 167, "cdate": 1569438884170, "ddate": null, "tcdate": 1569438884170, "tmdate": 1577168250086, "tddate": null, "forum": "rkl2s34twS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_RbuvlBtnG", "original": null, "number": 1, "cdate": 1576798689181, "ddate": null, "tcdate": 1576798689181, "tmdate": 1576800945943, "tddate": null, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Decision", "content": {"decision": "Reject", "comment": "The authors proposed a new problem setting called Wildly UDA (WUDA) where the labels in the source domain are noisy. They then proposed the \"butterfly\" method, combining co-teaching with pseudo labeling and evaluated the method on a range of WUDA problem setup. In general, there is a concern that Butterfly as the combination between co-teaching and pseudo labeling is weak on the novelty side. In this case the value of the method can be assessed by strong empirical result.  However as pointed out by Reviewer 3, a common setup (SVHN<-> MNIST) that appeared in many UDA paper was missing in the original draft. The author added the result for SVHN<-> MNIST  as  a response to review 3, however they only considered the UDA setting, not WUDA, hence the value of that experiment was limited. In addition, there are other UDA methods that achieve significantly better performance on SVHN<->MNIST that should be considered among the baselines. For example DIRT-T (Shu et al 2018) has a second phase where the decision boundary on the target domain is adjusted, and that could provide some robustness against a decision boundary affected by noise.\n\nShu et al (2018) A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR 2018. https://arxiv.org/abs/1802.08735\n\nI suggest that the authors consider performing the full experiment with WUDA using SVHN<->MNIST, and also consider the use of stronger UDA methods among the baseline. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707833, "tmdate": 1576800256111, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper167/-/Decision"}}}, {"id": "H1e6wnNniB", "original": null, "number": 11, "cdate": 1573829732957, "ddate": null, "tcdate": 1573829732957, "tmdate": 1573831197294, "tddate": null, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "The revision has been uploaded", "comment": "According to valuable comments from three reviewers, we have revised our paper. In this revision, we demonstrate more details related to our method, which should help reviewers better understand our proposal. We also revise our previous responses according to the revision. Please have a look."}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "r1gWdfkBsr", "original": null, "number": 4, "cdate": 1573347945400, "ddate": null, "tcdate": 1573347945400, "tmdate": 1573829336134, "tddate": null, "forum": "rkl2s34twS", "replyto": "HylUhej6KH", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to unclear descriptions", "comment": "The major concern of R3 is that he/she thinks we do not clearly describe 1) the introduced loss, 2) how to obtain u_i, 3) the underlying meaning of R(t) and 4) notations in Algorithm 2. We address these concerns here and update these new descriptions in the revision.\n\nQuestion 1: The introduced losses should be better justified (the nature of loss).\nAnswer 1: In loss function \\mathcal{L}, we have n instances: (x_i, y_i), where i = 1, 2, \u2026, n. For the ith instance, we will compute its cross-entropy loss (i.e., \\ell(F(x_i),y_i)), and we will denote this instance as \"selected\" if u_i = 1. Thus, the nature of \\mathcal{L} is actually the average value of cross-entropy loss of these \"selected\" instances. \nPlease see \"nature of the loss \\mathcal{L}\" at the end of page 4 in the revision.\n\nQuestion 2: How to obtain u_i is still not clear to me after several re-reading of the paper.\nAnswer 2: In lines 2-3 in Algorithm 2, we show that we can obtain u_i by solving a minimization problem: $\\textbf{u} = \\arg\\min_{\\textbf{u}^{\\prime}:\\textbf{1}\\textbf{u}^{\\prime}>\\alpha|D|}\\mathcal{L}(\\theta,\\textbf{u}^{\\prime};F, D)$, where $\\textbf{1}$ is a 1-by-n vector whose elements are 1, $\\textbf{u}=[$u_1,...,u_n$]^T$, n=|D| and $\\textbf{1}\\textbf{u}^{\\prime}$ represents the number of 1 in the vector $\\textbf{u}^{\\prime}$. Recall the nature of the loss \\mathcal{L} (Answer 1), we know \\mathcal{L} is the average value of cross-entropy losses of \"selected\" instances, and  $\\textbf{1}\\textbf{u}'$ is the number of these \"selected\" instances. Thus this minimization problem is equivalent to \"given a fixed network F (F_1 or F_2) and n instances in D, how to select at least k instances such that \\mathcal{L} is minimized\", where k = $\\lceil \\alpha|D| \\rceil$. To solve this problem, we first use a sorting algorithm (top_k function in TensorFlow) to sort these n instances according to their cross-entropy losses \\ell(F_1(x_i),y_i). Then, we select k instance with the smallest cross-entropy losses. Finally, let u_i of these k instances be 1 and u_i of the other instances be 0, and we can get the best $\\textbf{u}=[$u_1,...,u_n$]^T$. The average value of cross-entropy losses of these k instances is the minimized value of $\\mathcal{L}(\\theta,\\textbf{u}^{\\prime};F, D)$ under the constrain $\\textbf{1}\\textbf{u}^{\\prime}>\\alpha|D|$. \nPlease see \"Solution to minimization problems in Algorithm 2\" at page 5 in the revision.\n\nQuestion 3: R(T) could be more clearly defined.\nAnswer 3: R(T) is defined in line 11 of Algorithm 1 in page 5. We will explain it in detail here. In general, R(T) is a piecewise-defined linear function. Specifically, when T>=T_k, R(T) = 1-\\tau; when T<T_k, R(T) = 1 - T/T_k * \\tau. \nPlease see the end of the fourth paragraph at page 4 in the revision.\n\nQuestion 4: Algorithm 2 is a key component of the method and is in appendix, notation is not always clear nor explained. \nAnswer 4: Many thanks for your suggestions. We have moved Algorithm 2 to the main context (please see page 6 in the revision). We also explained each line in Algorithm 2 in \"checking process in Butterfly\" at page 5 in the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "BkeaKtESiB", "original": null, "number": 7, "cdate": 1573370244941, "ddate": null, "tcdate": 1573370244941, "tmdate": 1573829074327, "tddate": null, "forum": "rkl2s34twS", "replyto": "HJlcZBJLYH", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to clarity", "comment": "We thank R2 for listing some unclear points in our paper. We will clarify these points here and update them in the revision.\n\nQuestion 1: In Alg. 1, the authors do not explain how they initialize the values R(T) and R_t(T).\nAnswer 1: R(T) and R_t(T) are two functions with respective to T. We do not need to initialize them. We have demonstrated how to select hyper-parameters of R(T) and R_t(T) in Section G.2. Please see page 20 in the revision.  \n\nQuestion 2: I would expect that, to obtain meaningful results from the Checking method, the parameters of the networks F_1, F_2, F_{t1} and F_{t2} should already be initialized to reasonable values. Can the authors comment on the initialization procedure?\nAnswer 2: We do not use some special ways to initialize parameters of four networks. All parameters are initialized by the default procedures in TensorFlow on Python 3.6.  \n\nQuestion 3: In Alg. 2, how are the inner minimization problems solved? Are u_1 and u_2 truly enforced to be binary variables? How fast can one obtain the solutions to these problems?\nAnswer 3: u_1 and u_2 are binary variables and we can easily solve this minimization problem via using a sorting algorithm. \nSpecifically, in \\mathcal{L}, we have n instances: (x_i, y_i), where i = 1, 2, \u2026, n. For the ith instance, we will compute its cross-entropy loss (i.e., \\ell(F(x_i),y_i)), and we will denote this instance as \"selected\" if u_i = 1. So, the nature of \\mathcal{L} is actually the average value of cross-entropy losses of \"selected\" instances. In lines 2-3 in Alg. 2, we show that we can obtain u_i by solving a minimization problem: $\\textbf{u} = \\arg\\min_{\\textbf{u}':\\textbf{1}\\textbf{u}'>\\alpha|D|}\\mathcal{L}(\\theta,\\textbf{u}';F, D)$, where $\\textbf{1}$ is a 1-by-n vector whose elements are 1, $\\textbf{u}=[$u_1,...,u_n$]^T$, n=|D| and $\\textbf{1}\\textbf{u}'$ is the number of 1 in vector $\\textbf{u}'$. Recall the nature of the loss \\mathcal{L}, we know \\mathcal{L} is the average value of cross-entropy losses of \"selected\" instances, and  $\\textbf{1}\\textbf{u}'$ is the number of these \"selected\" instances. Thus this minimization problem is equivalent to \"given a fixed network F (F_1 or F_2) and n instances in D, how to select at least k instances such that \\mathcal{L} is minimized\", where k = $\\lceil \\alpha|D| \\rceil$. To solve this problem, we first use a sorting algorithm (top_k function in TensorFlow) to sort these n instances according to their cross-entropy losses \\ell(F_1(x_i),y_i). Then, we select k instance with the smallest cross-entropy losses. Finally, let u_i of these k instances be 1 and u_i of the other instances be 0, and we can get the best $\\textbf{u}=[$u_1,...,u_n$]^T$. The average value of cross-entropy losses of these k instances is the minimized value of $\\mathcal{L}(\\theta,\\textbf{u}^{\\prime};F, D)$ under the constrain $\\textbf{1}\\textbf{u}^{\\prime}>\\alpha|D|$.\nSince we only need to sort |$D$| numbers, we can solve this minimization problem quickly.\nPlease see \"Solution to minimization problems in Algorithm 2\" at page 5 in the revision.\n\nQuestion 4: One has to go to the appendix to find Alg. 2, which describes one of the key components (the Checking method in Alg. 1). The steps performed by Alg. 2 are not explained anywhere.\nAnswer 4: Alg. 2 has been moved back to main context (see page 6 in the revision).  We also explained each line in Alg. 2 in \"checking process in Butterfly\" at page 5 in the revision.\n\nQuestion 5: In Fig. 1, it is not clear to me what the term Interaction between the two branches represent. From the text, I could not find any reference to explicit interactions between the branches. In Alg. 1, it seems surprising that \\tilde{D}_T^l is initialized as \\tilde{D}_s, since, according to Fig. 3, the second branch should act on the target data only. \nAnswer 5: The interaction between DIR and TSR happens via the shared CNN. Since we do not have any pseudo-labeled target data, we need to use source data as the pseudo-labeled target data in the first step (i.e. T=1), which follows the ATDA method. We have explain this in Fig. 3 (please see page 4 in the revision) and the description of Alg. 1 in the revision (please see the third paragraph at page 5 in the revision).\n\nQuestion 6: In Eq. 1, the loss function \\ell() is not defined (although I imagine that it is the cross-entropy).\nAnswer 6: \\ell() is indeed the cross-entropy loss. We have explained this at page 4 in the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "rkxN3FEBsB", "original": null, "number": 8, "cdate": 1573370283958, "ddate": null, "tcdate": 1573370283958, "tmdate": 1573828515962, "tddate": null, "forum": "rkl2s34twS", "replyto": "HJlcZBJLYH", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to experiments", "comment": "R2 suggests a baseline and wish to see what will happen if we turn off the branch II. We are running our method on the baseline. We actually have justified what will happen if we turn off the branch II in our ablation study.\n\nQuestion 1: The experiments show the good behavior of the method. However, while I understand the motivation behind defining the two-stage baseline using ATDA, which is used in the proposed method, there seem to be no strict constrain on using this specific method in the two-stage scenario. For example, based on the results in Table 1, one might rather want to use TCL as the second stage, i.e., have a baseline Co+TCL. \nAnswer 1: It is a good suggestion for us. We have added Co+TCL as a baseline (see \"baselines\" at page 7 in the revision). We also updated the results in the revision.\nPlease see Table 1 at page 8, Table 2 at page 9, Tables 5 and 6 at page 18 in the revision.\n\nQuestion 2: As I mentioned before, the motivation behind the second branch in the framework is not clear to me. I would appreciate it if the authors could explain the reasoning behind this branch and evaluate their method without it.\nAnswer 2: we have already justified the necessity of Branch II in our ablation study in Table 4, page 9. In Table 4, B-Net-M represents the situation that we only check the mixed source-target data. From the reported results, Branch II is meaningful and can help improve the target-domain accuracy.\nIf we only consider the first branch, we can only obtain the high-quality domain-invariant representation. However, if we consider two branches, we can obtain not only the high-quality domain-invariant representation (DIR) but also the high-quality target-specific representation (TSR). Since we consider to accurately annotate the target data, it is necessary to obtain the high-quality TSR, which is also verified in ATDA method.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "rJxilcNriB", "original": null, "number": 9, "cdate": 1573370355415, "ddate": null, "tcdate": 1573370355415, "tmdate": 1573828355937, "tddate": null, "forum": "rkl2s34twS", "replyto": "HJlcZBJLYH", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to \"necessity of Branch II\"", "comment": "One major concern of R2 is that he/she cannot agree with us about the necessity of Branch II of our method since the first branch already acts on the mixed source-target data. \nHowever, we have already justified the necessity of Branch II in our ablation study in Table 4, page 9. In Table 4, B-Net-M represents the method that we only check the mixed source-target data. From the reported results, Branch II is meaningful and can help improve the target-domain accuracy. \nIf we only consider the first branch, we can only obtain the high-quality domain-invariant representation. However, if we consider two branches, we can obtain not only the high-quality domain-invariant representation (DIR) but also the high-quality target-specific representation (TSR). Since we consider to accurately annotate the target data, it is necessary to obtain the high-quality TSR, which is also verified in ATDA method."}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "SJxXdByrsH", "original": null, "number": 5, "cdate": 1573348714950, "ddate": null, "tcdate": 1573348714950, "tmdate": 1573828030035, "tddate": null, "forum": "rkl2s34twS", "replyto": "rJeaTeMitr", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to R1", "comment": "We appreciate that R1 strongly supports our paper and argue that WUDA, as a new problem, could lead a new research direction in the domain adaptation field. We will address your major concerns here and update the revision.\n\nQuestion 1: The interaction between DIR and TSR happens via shared CNN, right? The authors should explain this in the caption of Figure 3.\nAnswer 1: Yes, your understanding is correct. The interaction between DIR and TSR happens via shared CNN. We have explained this in the caption of Figure 3 in the revision.\nPlease see Figure 3, page 4, in the revision.\n\nQuestion 2: When T = 1, you directly use source data as the pseudo-labeled target data since there are no pseudo-labeled target data in the first step (based on Algorithm 1). Is that correct? If yes, the authors should explain this in Figure 3 and the description of Algorithm 1. If no, please give a detailed explanation.\nAnswer 2: Yes, your understanding is correct. Since we do not have any pseudo-labeled target data, we need to use source data as the pseudo-labeled target data in the first step (i.e. T=1). We have explained this in Figure 3 and the description of Algorithm 1 in the revision.\nPlease see Figure 3 at page 4, and third paragraph at page 5 in the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "S1xdXfJHir", "original": null, "number": 3, "cdate": 1573347872115, "ddate": null, "tcdate": 1573347872115, "tmdate": 1573827869454, "tddate": null, "forum": "rkl2s34twS", "replyto": "HylUhej6KH", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Response to experiments", "comment": "The other concerns of R3 are related to experiments. We will address these concerns here and update the revision.\nQuestion 1: Did the authors observe the same evolution of the accuracy (decreasing and increasing after a few epochs) also on the \"real world\" datasets as in MNIST-SYND?\nAnswer 1: Yes, we also observe this phenomenon on the real world datasets.   \n\nQuestion 2:  Could the authors provide results on MNIST-SVHN to help compare with other papers in literature? \nAnswer 2: We have added this experiment. \nPlease see the target-domain accuracy on both UDA tasks in Table 3, page 9, in the revision. We also describe this table at page 8 in the revision (\"Results on two UDA tasks\").\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "BJlGrhaEiH", "original": null, "number": 2, "cdate": 1573342266118, "ddate": null, "tcdate": 1573342266118, "tmdate": 1573827411630, "tddate": null, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Revision of figures", "comment": "According to valuable comments from R1 and R3, \n1) we have added the networks F_1, F_2, F_t1, F_t2 in Figure 3 for more clear understanding (see Figure 3, page 4, in the revision);\n2) we have explained interaction between DIR and TSR in the caption of figure 3 (see Figure 3, page 4, in the revision);\n3) we have explained that, when T = 1, we directly use source data as the pseudo-labeled target data in the caption of figure 3 (see Figure 3, page 4, in the revision);\n4) we have changed color scheme of figures such that they are friendly to color-blind people (see Figure 2, page 3, and Figure 4, page 7, and Figure 5, page 8, in the revision)."}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "HJgdZVvHjB", "original": null, "number": 10, "cdate": 1573381120076, "ddate": null, "tcdate": 1573381120076, "tmdate": 1573827181593, "tddate": null, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment", "content": {"title": "Thank reviewers for their valuable comments", "comment": "We first thank all three reviewers for their valuable comments. We have tried our best to address your concerns. Please see our responses below. In the revision, we highlighted the revised content by changing the color of font to blue. Please check our revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper167/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkl2s34twS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper167/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper167/Authors|ICLR.cc/2020/Conference/Paper167/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175358, "tmdate": 1576860547134, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper167/Authors", "ICLR.cc/2020/Conference/Paper167/Reviewers", "ICLR.cc/2020/Conference/Paper167/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Comment"}}}, {"id": "HJlcZBJLYH", "original": null, "number": 1, "cdate": 1571316994213, "ddate": null, "tcdate": 1571316994213, "tmdate": 1572972630263, "tddate": null, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces the idea of wildly unsupervised domain adaptation, where the source labels are noisy and the target data is unsupervised. To tackle this, the authors propose an architecture based one two branches: one acting on the mixed source-target data and the other on the target data only. During training, each branch is updated using the idea of co-teaching, by finding the samples with the lowest loss values. Pseudo-labeling is then applied to the target data, and the process iterates.\n\nOriginality:\n- In essence, the proposed method combines co-teaching (Han et al., 2018) and pseudo-labeling (Saito et al., 2017). While it goes beyond the naive two-stage approach, used here as a baseline, the technical novelty remains limited.\n- The main novelty consists of using two branches to model the two domains. However, the necessity for the second branch is not very clearly explained, and remains obscure to me, since the first branch already acts on the mixed source-target data.\n\nClarity:\nIn addition the fact that, as mentioned above, the design of the overall framework is not entirely well motivated, I found the paper somewhat hard to follow. In particular:\n- One has to go to the appendix to find Alg. 2, which describes one of the key components (the Checking method in Alg. 1). The steps performed by Alg. 2 are not explained anywhere.\n- In Alg. 1, it seems surprising that \\tilde{D}_T^l is initialized as \\tilde{D}_s, since, according to Fig. 3, the second branch should act on the target data only.\n- In Alg. 1, the authors do not explain how they initialize the values R(T) and R_t(T).\n- I would expect that, to obtain meaningful results from the Checking method, the parameters of the networks F_1, F_2, F_{t1} and F_{t2} should already be initialized to reasonable values. Can the authors comment on the initialization procedure?\n- In Alg. 2, how are the inner minimization problems solved? Are u_1 and u_2 truly enforced to be binary variables? How fast can one obtain the solutions to these problems?\n- In Fig. 1, it is not clear to me what the term Interaction between the two branches represent. From the text, I could not find any reference to explicit interactions between the branches.\n- In Eq. 1, the loss function \\ell() is not defined (although I imagine that it is the cross-entropy).\n\nExperiments:\n- The experiments show the good behavior of the method. However, while I understand the motivation behind defining the two-stage baseline using ATDA, which is used in the proposed method, there seem to be no strict constrain on using this specific method in the two-stage scenario. For example, based on the results in Table 1, one might rather want to use TCL as the second stage, i.e., have a baseline Co+TCL. \n- As I mentioned before, the motivation behind the second branch in the framework is not clear to me. I would appreciate it if the authors could explain the reasoning behind this branch and evaluate their method without it.\n\nSummary:\nThe novelty of the proposed method, relying on a combination of co-teaching and pseudo-labeling, is limited. Furthermore, the clarity of the paper could be significantly improved.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper167/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576442605497, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper167/Reviewers"], "noninvitees": [], "tcdate": 1570237756043, "tmdate": 1576442605514, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Review"}}}, {"id": "rJeaTeMitr", "original": null, "number": 2, "cdate": 1571655876938, "ddate": null, "tcdate": 1571655876938, "tmdate": 1572972630223, "tddate": null, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a new problem setting in the domain adaptation field. Since it is impossible to obtain perfectly clean labeled source data in the real world, existing UDA methods cannot well handle real-world data. However, in wildly unsupervised domain adaptation (WUDA), we do not need a perfectly clean source data, which means that WUDA problem is a more general and realistic problem than existing ones. \n\nTo address WUDA problem, the authors proposed Butterfly framework (based on dual checking principle) to simultaneously reduce the 1) noise effects in source domain and 2) distributional discrepancy between source and target domains. They tested the proposed method on simulated and real-world WUDA tasks (35 tasks in total), and the accuracy of proposed method is higher than those of representative UDA methods. They claim that Butterfly can eliminate noise effect, which is strongly supported by Figures 2, 4 and 5. They also present the ablation study to show that each part in Butterfly is meaningful. \n\nIn general, this paper is easy to follow and clearly presents the main idea and learning procedures of Butterfly. Since WUDA, as a new problem, could lead a new research direction in the domain adaptation field, this paper should be presented in ICLR 2020. Detailed comments can be seen below.\n\nPros:\n\n+ WUDA, as a new problem, is very important for the domain adaptation field.\n+ Butterfly, as a solution to WUDA, outperforms representative UDA methods on simulated and real-world WUDA tasks.\n+ All claims are strongly supported by experimental results, and ablation study shows that each part in Butterfly is a necessary component.\n+ Following Ben-David's paper, this paper also presents an upper bound of the target domain risk. This is 1) the first WUDA bound and 2) probably the first DA bound related to pseudo labelling function. The conditions in Remark 3 are very interesting.\n+ It is very nice to use noise effect \\Delta to explain the abnormal phenomenon in experiments.\n\nCons:\n\n- The color scheme of figures is not friendly to color-blind people. The authors should do different line styles or marker styles.\n- The interaction between DIR and TSR happens via shared CNN, right? The authors should explain this in the caption of Figure 3.\n- When T = 1, you directly use source data as the pseudo-labeled target data since there are no pseudo-labeled target data in the first step (based on Algorithm 1). Is that correct? If yes, the authors should explain this in Figure 3 and the description of Algorithm 1. If no, please give a detailed explanation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper167/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576442605497, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper167/Reviewers"], "noninvitees": [], "tcdate": 1570237756043, "tmdate": 1576442605514, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Review"}}}, {"id": "HylUhej6KH", "original": null, "number": 3, "cdate": 1571823789792, "ddate": null, "tcdate": 1571823789792, "tmdate": 1572972630189, "tddate": null, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "invitation": "ICLR.cc/2020/Conference/Paper167/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for unsupervised domain adaptation. The problem is well known in literature and follows the setting of labeled source and unlabeled target set. This work proposes the \u201cbutterfly network\u201d suitable to train on noisy data (labels) and assign pseudo-labels to the target set. The butterfly network consists in two branches one for source+target and one for target only data. Both use the same optimization objective and a \u201cchecking\u201d mechanism has been devised for pseudo-labelling the data.\nDecision: weak reject\nMotivation: This paper has some merit but does not present the method in a clear way, it requires some additional effort to go through the method and retrieve from the appendix information useful for full understanding. For example algorithm 2 is a key component of the method and is in appendix, notation is not always clear nor explained (the loss in algorithm 2 is ), the networks F_1, F_2, F_t1, F_t2 could be added in figure 3 for more clear understanding and R(t) along with u_i could be more clearly defined (how to obtain u_i is still not clear to me after several re-reading of the paper). The introduced losses should be better justified. \nAnyway, as I said the paper has some merit, it provides many insights and extensive analysis on \u201cbutterfly\u201d method for unsupervised domain adaptation. The experimental section is extensive and demonstrates improvement in performance using this method compared to state-of-the-art, even though for some \"real world\" datasets (e.g. SUN, Caltech, ImageNet) the improvement is not so significant as in the case of MNIST-SYND. Could the authors provide results on MNIST-SVHN to help compare with other papers in literature? Did the authors observe the same evolution of the accuracy (decreasing and increasing after a few epochs) also on the \"real world\" datasets as in MNIST-SYND?\nReplicability: as I said the method is not really clearly explained and therefore I am not confident I could implement and replicate the results. This not because I think the method is complex but because some key components that I pointed out previously about the method are not clear and I strogly believe these are key components to replicate the results.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper167/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper167/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution", "authors": ["Feng Liu", "Jie Lu", "Bo Han", "Gang Niu", "Guangquan Zhang", "Masashi Sugiyama"], "authorids": ["feng.liu-2@student.uts.edu.au", "jie.lu@uts.edu.au", "bo.han@riken.jp", "gang.niu@riken.jp", "guangquan.zhang@uts.edu.au", "sugi@k.u-tokyo.ac.jp"], "keywords": [], "abstract": "In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.", "pdf": "/pdf/5d4363f65f80ef0786970034fb1dc46384734a76.pdf", "paperhash": "liu|wildly_unsupervised_domain_adaptation_and_its_powerful_and_efficient_solution", "original_pdf": "/attachment/7fd360ac5925e393781dac84b9196e50db24ddd3.pdf", "_bibtex": "@misc{\nliu2020wildly,\ntitle={Wildly Unsupervised Domain Adaptation and Its Powerful and Efficient Solution},\nauthor={Feng Liu and Jie Lu and Bo Han and Gang Niu and Guangquan Zhang and Masashi Sugiyama},\nyear={2020},\nurl={https://openreview.net/forum?id=rkl2s34twS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkl2s34twS", "replyto": "rkl2s34twS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576442605497, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper167/Reviewers"], "noninvitees": [], "tcdate": 1570237756043, "tmdate": 1576442605514, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper167/-/Official_Review"}}}], "count": 14}