{"notes": [{"id": "ByxZdj09tX", "original": "BygnC_L-F7", "number": 327, "cdate": 1538087784756, "ddate": null, "tcdate": 1538087784756, "tmdate": 1545355397154, "tddate": null, "forum": "ByxZdj09tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.", "abstract": "Contrary to most reinforcement learning studies, which emphasize on training a deep neural network to approximate its output layer to certain strategies, this paper proposes a reversed method for reinforcement learning. We call this \u201cDeep Deducing\u201d. In short, after adequately training a deep neural network according to a strategy-environment-to-payoff table, then we initialize randomized strategy\ninput and propagate the error between the actual output and the desired output back to the initially-randomized strategy input in the \u201cinput layer\u201d of the trained deep neural network gradually to perform a task similar to \u201chuman deduction\u201d. And we view the final strategy input in the \u201cinput layer\u201d as the fittest strategy for a neural network when confronting the observed environment input from the world outside.", "keywords": ["Reinforcement Learning", "Deep Feed-forward Neural Network", "Recurrent Neural Network", "Game Theory", "Control Theory", "Nash Equilibrium", "Optimization"], "authorids": ["brownwang0426@gmail.com"], "authors": ["Brown Wang"], "TL;DR": "FROM DEEP LEARNING TO DEEP DEDUCING", "pdf": "/pdf/bae28453950d6cc5d571551031e04ea73f2b4442.pdf", "paperhash": "wang|from_deep_learning_to_deep_deducing_automatically_tracking_down_nash_equilibrium_through_autonomous_neural_agent_a_possible_missing_step_toward_general_ai", "_bibtex": "@misc{\nwang2019from,\ntitle={{FROM} {DEEP} {LEARNING} {TO} {DEEP} {DEDUCING}: {AUTOMATICALLY} {TRACKING} {DOWN} {NASH} {EQUILIBRIUM} {THROUGH} {AUTONOMOUS} {NEURAL} {AGENT}, A {POSSIBLE} {MISSING} {STEP} {TOWARD} {GENERAL} A.I.},\nauthor={Brown Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZdj09tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HklXHLwxlV", "original": null, "number": 1, "cdate": 1544742459301, "ddate": null, "tcdate": 1544742459301, "tmdate": 1545354514727, "tddate": null, "forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper327/Meta_Review", "content": {"metareview": "The paper presents \"deep deducing\", which means learning the state-action value function of 2 player games from a payoff table, and using the value function by maximizing over the (actionable) inputs at test time.\n\nThe paper lacks clarity overall. The method does not contain any new model nor algorithm. The experiments are too weak (easy environments, few/no comparisons) to support the claims.\n\nThe paper is not ready for publication at this time.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Not yet ready for publication"}, "signatures": ["ICLR.cc/2019/Conference/Paper327/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper327/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.", "abstract": "Contrary to most reinforcement learning studies, which emphasize on training a deep neural network to approximate its output layer to certain strategies, this paper proposes a reversed method for reinforcement learning. We call this \u201cDeep Deducing\u201d. In short, after adequately training a deep neural network according to a strategy-environment-to-payoff table, then we initialize randomized strategy\ninput and propagate the error between the actual output and the desired output back to the initially-randomized strategy input in the \u201cinput layer\u201d of the trained deep neural network gradually to perform a task similar to \u201chuman deduction\u201d. And we view the final strategy input in the \u201cinput layer\u201d as the fittest strategy for a neural network when confronting the observed environment input from the world outside.", "keywords": ["Reinforcement Learning", "Deep Feed-forward Neural Network", "Recurrent Neural Network", "Game Theory", "Control Theory", "Nash Equilibrium", "Optimization"], "authorids": ["brownwang0426@gmail.com"], "authors": ["Brown Wang"], "TL;DR": "FROM DEEP LEARNING TO DEEP DEDUCING", "pdf": "/pdf/bae28453950d6cc5d571551031e04ea73f2b4442.pdf", "paperhash": "wang|from_deep_learning_to_deep_deducing_automatically_tracking_down_nash_equilibrium_through_autonomous_neural_agent_a_possible_missing_step_toward_general_ai", "_bibtex": "@misc{\nwang2019from,\ntitle={{FROM} {DEEP} {LEARNING} {TO} {DEEP} {DEDUCING}: {AUTOMATICALLY} {TRACKING} {DOWN} {NASH} {EQUILIBRIUM} {THROUGH} {AUTONOMOUS} {NEURAL} {AGENT}, A {POSSIBLE} {MISSING} {STEP} {TOWARD} {GENERAL} A.I.},\nauthor={Brown Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZdj09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper327/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353254309, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper327/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper327/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper327/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353254309}}}, {"id": "Byg8FCttCQ", "original": null, "number": 3, "cdate": 1543245437596, "ddate": null, "tcdate": 1543245437596, "tmdate": 1543245437596, "tddate": null, "forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper327/Official_Review", "content": {"title": "Poorly written paper with preliminary experiments", "review": "The paper proposes a method of searching for a Nash equilibrium strategy in games where the strategy-to-payoff mapping is defined by a neural network. The idea is to perform gradient optimization of the payoff w.r.t. the strategy. Preliminary results on tic-tac-toe and variations of the prisoner\u2019s dilemma task are presented. The paper has an interesting idea at the core. However, it is poorly written, does not properly discuss the related works and does not present a convincing method or experimental results. \n\nPros:\n* The paper considers an interesting question of exploring the applications of neural networks to the game theory problems.\n* The idea of the paper is reasonable. It makes sense to me to perform gradient-based search over the strategies (assuming that the payoff is differentiable).\n\nCons:\n* Writing\n- The paper is over the mandatory length limit of 10 pages.\n- The paper makes a grandiose claim: \u201cthis paper provides a revolutionary way for reinforcement learning and a possible road toward general A.I.\u201d However, there are arguably no revolutionary ideas, and certainly no reinforcement learning experiments!\n- Despite the claim, the novelty of the paper is limited. There is no discussion of the related work: optimization of the neural networks w.r.t. the inputs [1]; related RL ideas such as model-based learning [2,3] and Monte-Carlo Tree Search [4].\n- The problem being solved is never formally stated. As far as I understand, Nash equilibrium is usually defined (1) in mixed strategies, while the paper seems to consider pure strategies; (2) in the scenario where every player attempts to maximize their payoff, while in the paper the players attempt to achieve some pre-fixed value of the payoff.\n- The flow of the paper is generally poor. Instead of presenting a general solution and then showcasing its applications, the paper iterates on similar ideas multiple times. For example, all four algorithms are just gradient-based optimization of either the weights or the inputs to a model.\n- The paper provides extremely misleading analogies and explanations. I am quite sure that a mosquito brain is not a one hidden layer fully-connected neural network! Also, the example of avoiding a moving hand is poor: since the outcome is life or death, the learning should happen via evolution, not during the lifetime of a single insect. The claim that the neural networks with sigmoid activation functions are less prone to local optima is questionable as well.\n\n* Method and experiments\n- The proposed method is essentially a greedy gradient-based planning procedure. For this to work, we need to have a very good environment model. This is a strong assumption that is not discussed.\n- The experiments are performed on very simple synthetic problems: matrix games and tic-tac-toe. They do not suggest that the method is general and can work on harder problems, say, Sokoban [2].\n- The experiments do not present any baselines, so it is unclear how well the method performs compared to the alternatives. One obvious candidate is gradient-free optimization, such as Nelder-Mead, and gradient descent with momentum, which can be less prone to local optima.\n\n[1] Brandon Amos, Lei Xu, J. Zico Kolter \u201cInput Convex Neural Networks\u201d, ICML 2017\n[2] Racani\u00e8re et al. \u201cImagination-Augmented Agents for Deep Reinforcement Learning\u201d, NIPS 2017\n[3] David Ha, J\u00fcrgen Schmidhuber \u201cRecurrent World Models Facilitate Policy Evolution\u201d, NIPS 2018\n[4] Thomas Anthony, Zheng Tian, David Barber \u201cThinking Fast and Slow with Deep Learning and Tree Search\u201d, NIPS 2017", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper327/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.", "abstract": "Contrary to most reinforcement learning studies, which emphasize on training a deep neural network to approximate its output layer to certain strategies, this paper proposes a reversed method for reinforcement learning. We call this \u201cDeep Deducing\u201d. In short, after adequately training a deep neural network according to a strategy-environment-to-payoff table, then we initialize randomized strategy\ninput and propagate the error between the actual output and the desired output back to the initially-randomized strategy input in the \u201cinput layer\u201d of the trained deep neural network gradually to perform a task similar to \u201chuman deduction\u201d. And we view the final strategy input in the \u201cinput layer\u201d as the fittest strategy for a neural network when confronting the observed environment input from the world outside.", "keywords": ["Reinforcement Learning", "Deep Feed-forward Neural Network", "Recurrent Neural Network", "Game Theory", "Control Theory", "Nash Equilibrium", "Optimization"], "authorids": ["brownwang0426@gmail.com"], "authors": ["Brown Wang"], "TL;DR": "FROM DEEP LEARNING TO DEEP DEDUCING", "pdf": "/pdf/bae28453950d6cc5d571551031e04ea73f2b4442.pdf", "paperhash": "wang|from_deep_learning_to_deep_deducing_automatically_tracking_down_nash_equilibrium_through_autonomous_neural_agent_a_possible_missing_step_toward_general_ai", "_bibtex": "@misc{\nwang2019from,\ntitle={{FROM} {DEEP} {LEARNING} {TO} {DEEP} {DEDUCING}: {AUTOMATICALLY} {TRACKING} {DOWN} {NASH} {EQUILIBRIUM} {THROUGH} {AUTONOMOUS} {NEURAL} {AGENT}, A {POSSIBLE} {MISSING} {STEP} {TOWARD} {GENERAL} A.I.},\nauthor={Brown Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZdj09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper327/Official_Review", "cdate": 1542234486553, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper327/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335699476, "tmdate": 1552335699476, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper327/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgCQeuUaQ", "original": null, "number": 3, "cdate": 1541992486075, "ddate": null, "tcdate": 1541992486075, "tmdate": 1541992486075, "tddate": null, "forum": "ByxZdj09tX", "replyto": "Hke-a03-6m", "invitation": "ICLR.cc/2019/Conference/-/Paper327/Official_Comment", "content": {"title": "Thank you, reviewer 2", "comment": "Thank you for your opinion. Indeed, there is much need to be done. We agree with your opinion.\n\n(1) The name of this machine is inappropriate since we didn't really invent a new kind of neural network, and we only exploit back propagation in an nontraditional way. We will cherish your opinion and re-coin the name of the machine.\n\n(2) The experiment result is not enough, indeed. We are working on the experiment result. The experiment result requires some time to come by (if there are randomly generated 100 tables, it takes quite a time), and we are working on it.\n\nThank you again for your time reviewing this paper. We sincerely thank you for your time and precious opinion. Thank you again."}, "signatures": ["ICLR.cc/2019/Conference/Paper327/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper327/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper327/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.", "abstract": "Contrary to most reinforcement learning studies, which emphasize on training a deep neural network to approximate its output layer to certain strategies, this paper proposes a reversed method for reinforcement learning. We call this \u201cDeep Deducing\u201d. In short, after adequately training a deep neural network according to a strategy-environment-to-payoff table, then we initialize randomized strategy\ninput and propagate the error between the actual output and the desired output back to the initially-randomized strategy input in the \u201cinput layer\u201d of the trained deep neural network gradually to perform a task similar to \u201chuman deduction\u201d. And we view the final strategy input in the \u201cinput layer\u201d as the fittest strategy for a neural network when confronting the observed environment input from the world outside.", "keywords": ["Reinforcement Learning", "Deep Feed-forward Neural Network", "Recurrent Neural Network", "Game Theory", "Control Theory", "Nash Equilibrium", "Optimization"], "authorids": ["brownwang0426@gmail.com"], "authors": ["Brown Wang"], "TL;DR": "FROM DEEP LEARNING TO DEEP DEDUCING", "pdf": "/pdf/bae28453950d6cc5d571551031e04ea73f2b4442.pdf", "paperhash": "wang|from_deep_learning_to_deep_deducing_automatically_tracking_down_nash_equilibrium_through_autonomous_neural_agent_a_possible_missing_step_toward_general_ai", "_bibtex": "@misc{\nwang2019from,\ntitle={{FROM} {DEEP} {LEARNING} {TO} {DEEP} {DEDUCING}: {AUTOMATICALLY} {TRACKING} {DOWN} {NASH} {EQUILIBRIUM} {THROUGH} {AUTONOMOUS} {NEURAL} {AGENT}, A {POSSIBLE} {MISSING} {STEP} {TOWARD} {GENERAL} A.I.},\nauthor={Brown Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZdj09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper327/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614289, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByxZdj09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper327/Authors", "ICLR.cc/2019/Conference/Paper327/Reviewers", "ICLR.cc/2019/Conference/Paper327/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper327/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper327/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper327/Authors|ICLR.cc/2019/Conference/Paper327/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper327/Reviewers", "ICLR.cc/2019/Conference/Paper327/Authors", "ICLR.cc/2019/Conference/Paper327/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614289}}}, {"id": "Hke-a03-6m", "original": null, "number": 2, "cdate": 1541684921413, "ddate": null, "tcdate": 1541684921413, "tmdate": 1541684921413, "tddate": null, "forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper327/Official_Review", "content": {"title": "weak experimental evidence and unconvincing arguments", "review": "The paper presents an approach to infer optimal strategies by learning the payoff function of 2 player games with a neural network Q(s, a), where a are the agent actions and s the context (e.g., action of the other player). The strategy is inferred by considering the inputs as free variables at test time and maximizing the learnt Q over its input variables by backpropagation.\n\nThe structure of the neural network in itself is not particularly original. The originality of the paper is to show that experimentally, in some 2-player games (and small sequential games, using an RNN), the policy that is inferred is close to a Nash equilibrium. While this is an interesting result in itself, the games used in the experiment are pretty easy to solve with existing algorithms, so the experimental evidence that this approach can work in practice in difficult cases is weak.\n\nThe motivation and intuition fail to be convincing. There is an excessive usage of analogies with intelligence and life in general that are not particularly enlighting (e.g., \"Even though its hardware is damaged, the software in\nthe cloud (or the eternal soul, arguably) of the mosquito [...]\", the value of the analogy between the cloud and the soul is unclear), and in the end there is no clear explanation of why it should work in practice. \n\nI think the paper in its current form is not ready for publication. More formal arguments and/or stronger experimental evidence are necessary.\n ", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper327/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.", "abstract": "Contrary to most reinforcement learning studies, which emphasize on training a deep neural network to approximate its output layer to certain strategies, this paper proposes a reversed method for reinforcement learning. We call this \u201cDeep Deducing\u201d. In short, after adequately training a deep neural network according to a strategy-environment-to-payoff table, then we initialize randomized strategy\ninput and propagate the error between the actual output and the desired output back to the initially-randomized strategy input in the \u201cinput layer\u201d of the trained deep neural network gradually to perform a task similar to \u201chuman deduction\u201d. And we view the final strategy input in the \u201cinput layer\u201d as the fittest strategy for a neural network when confronting the observed environment input from the world outside.", "keywords": ["Reinforcement Learning", "Deep Feed-forward Neural Network", "Recurrent Neural Network", "Game Theory", "Control Theory", "Nash Equilibrium", "Optimization"], "authorids": ["brownwang0426@gmail.com"], "authors": ["Brown Wang"], "TL;DR": "FROM DEEP LEARNING TO DEEP DEDUCING", "pdf": "/pdf/bae28453950d6cc5d571551031e04ea73f2b4442.pdf", "paperhash": "wang|from_deep_learning_to_deep_deducing_automatically_tracking_down_nash_equilibrium_through_autonomous_neural_agent_a_possible_missing_step_toward_general_ai", "_bibtex": "@misc{\nwang2019from,\ntitle={{FROM} {DEEP} {LEARNING} {TO} {DEEP} {DEDUCING}: {AUTOMATICALLY} {TRACKING} {DOWN} {NASH} {EQUILIBRIUM} {THROUGH} {AUTONOMOUS} {NEURAL} {AGENT}, A {POSSIBLE} {MISSING} {STEP} {TOWARD} {GENERAL} A.I.},\nauthor={Brown Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZdj09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper327/Official_Review", "cdate": 1542234486553, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper327/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335699476, "tmdate": 1552335699476, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper327/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJx6WXTa2m", "original": null, "number": 1, "cdate": 1541423876580, "ddate": null, "tcdate": 1541423876580, "tmdate": 1541534089795, "tddate": null, "forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper327/Official_Review", "content": {"title": "Not good enough", "review": "This paper tries to build a neural net to learn Nash equilibrium of games, even though it has been proved that no uncoupled algorithm can do that, except on specific games, as the ones considered in the example (0-sum, potentiel, solvable by iterated elimination of dominated strategies, etc.).\n\nThe algorithm proposed is a classical neural net without any insight (I believe its behavio must more or less be similar to regret matching).\n\nIn table 10, I do not think that the underline case is the NE. \n\nIn table 12, the algorithm si conveniently initiated at the NE.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper327/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FROM DEEP LEARNING TO DEEP DEDUCING: AUTOMATICALLY TRACKING DOWN NASH EQUILIBRIUM THROUGH AUTONOMOUS NEURAL AGENT, A POSSIBLE MISSING STEP TOWARD GENERAL A.I.", "abstract": "Contrary to most reinforcement learning studies, which emphasize on training a deep neural network to approximate its output layer to certain strategies, this paper proposes a reversed method for reinforcement learning. We call this \u201cDeep Deducing\u201d. In short, after adequately training a deep neural network according to a strategy-environment-to-payoff table, then we initialize randomized strategy\ninput and propagate the error between the actual output and the desired output back to the initially-randomized strategy input in the \u201cinput layer\u201d of the trained deep neural network gradually to perform a task similar to \u201chuman deduction\u201d. And we view the final strategy input in the \u201cinput layer\u201d as the fittest strategy for a neural network when confronting the observed environment input from the world outside.", "keywords": ["Reinforcement Learning", "Deep Feed-forward Neural Network", "Recurrent Neural Network", "Game Theory", "Control Theory", "Nash Equilibrium", "Optimization"], "authorids": ["brownwang0426@gmail.com"], "authors": ["Brown Wang"], "TL;DR": "FROM DEEP LEARNING TO DEEP DEDUCING", "pdf": "/pdf/bae28453950d6cc5d571551031e04ea73f2b4442.pdf", "paperhash": "wang|from_deep_learning_to_deep_deducing_automatically_tracking_down_nash_equilibrium_through_autonomous_neural_agent_a_possible_missing_step_toward_general_ai", "_bibtex": "@misc{\nwang2019from,\ntitle={{FROM} {DEEP} {LEARNING} {TO} {DEEP} {DEDUCING}: {AUTOMATICALLY} {TRACKING} {DOWN} {NASH} {EQUILIBRIUM} {THROUGH} {AUTONOMOUS} {NEURAL} {AGENT}, A {POSSIBLE} {MISSING} {STEP} {TOWARD} {GENERAL} A.I.},\nauthor={Brown Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=ByxZdj09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper327/Official_Review", "cdate": 1542234486553, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByxZdj09tX", "replyto": "ByxZdj09tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper327/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335699476, "tmdate": 1552335699476, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper327/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}