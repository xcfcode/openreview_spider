{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028632996, "tcdate": 1490028632996, "number": 1, "id": "rJWDutTse", "invitation": "ICLR.cc/2017/workshop/-/paper150/acceptance", "forum": "SJGIC1BFe", "replyto": "SJGIC1BFe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Contextual Discretization framework for compressing Recurrent Neural Networks", "abstract": "In this paper, we address the issue of training Recurrent Neural Networks with binary weights and introduce a novel  Contextualized Discretization (CD) framework and showcase its effectiveness across multiple RNN architectures and two disparate tasks. We also propose a modified GRU architecture that allows harnessing the CD method and reclaim the exclusive usage of weights in $\\{-1, 1\\}$, which in turn reduces the number of power-two bit multiplications from $O(n^3)$ to $O(n^2)$.", "pdf": "/pdf/e53af45c2a4b5113485eebfffa4d4a0568bdfcfe.pdf", "TL;DR": "A new technique for weight binarized compression of  Recurrent Neural Networks", "paperhash": "clark|a_contextual_discretization_framework_for_compressing_recurrent_neural_networks", "conflicts": ["none@none.com"], "keywords": ["Deep learning", "Supervised Learning", "Applications"], "authors": ["Aidan Clark", "Vinay Uday Prabhu", "John Whaley"], "authorids": ["aidanbclark@berkeley.edu", "vinay@unify.id", "john@unify.id"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028633538, "id": "ICLR.cc/2017/workshop/-/paper150/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJGIC1BFe", "replyto": "SJGIC1BFe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028633538}}}, {"tddate": null, "tmdate": 1489110689117, "tcdate": 1489110689117, "number": 2, "id": "SyFiItyse", "invitation": "ICLR.cc/2017/workshop/-/paper150/official/review", "forum": "SJGIC1BFe", "replyto": "SJGIC1BFe", "signatures": ["ICLR.cc/2017/workshop/paper150/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper150/AnonReviewer1"], "content": {"title": "A simple trick for binary recurrent nets", "rating": "4: Ok but not good enough - rejection", "review": "The authors introduce a simple trick to improve the training of binary-weighted RNNs.\nThe motivation of this work is to consider the difference in the distribution of weights between the input-to-hidden connection and hidden-to-hidden connection. The main idea is to use different scaling terms for each weight (i.e., input-to-hidden and hidden-to-hidden). This is shown with GRU-RNN implementation.\n\nAs shown in Fig. 1 (b), the performance is dramatically improved by using this simple scaling trick. I will complain a bit about the term 'contextual discretization'. It seems a bit overselling the idea after I learned that it is about hand-picking different scaling variables to different multiplication terms in the update function of GRU-RNN. The scaling terms are chosen after observing the distribution of weights in a GRU-RNN using full-precision, therefore the values reported in this paper may not apply in general. However, the method is surprisingly effective based on the reported results. The paper is clearly written, and the method is tested on two different domains, text and audio. \n\nMinor comments: citation to GRU-RNN is missing.\n\nPros:\nA simple trick that works well.\nCons:\nThe method is not entirely new, I believe a similar trick was already proposed for non-recurrent nets.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Contextual Discretization framework for compressing Recurrent Neural Networks", "abstract": "In this paper, we address the issue of training Recurrent Neural Networks with binary weights and introduce a novel  Contextualized Discretization (CD) framework and showcase its effectiveness across multiple RNN architectures and two disparate tasks. We also propose a modified GRU architecture that allows harnessing the CD method and reclaim the exclusive usage of weights in $\\{-1, 1\\}$, which in turn reduces the number of power-two bit multiplications from $O(n^3)$ to $O(n^2)$.", "pdf": "/pdf/e53af45c2a4b5113485eebfffa4d4a0568bdfcfe.pdf", "TL;DR": "A new technique for weight binarized compression of  Recurrent Neural Networks", "paperhash": "clark|a_contextual_discretization_framework_for_compressing_recurrent_neural_networks", "conflicts": ["none@none.com"], "keywords": ["Deep learning", "Supervised Learning", "Applications"], "authors": ["Aidan Clark", "Vinay Uday Prabhu", "John Whaley"], "authorids": ["aidanbclark@berkeley.edu", "vinay@unify.id", "john@unify.id"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489110689775, "id": "ICLR.cc/2017/workshop/-/paper150/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper150/AnonReviewer2", "ICLR.cc/2017/workshop/paper150/AnonReviewer1"], "reply": {"forum": "SJGIC1BFe", "replyto": "SJGIC1BFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper150/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489110689775}}}, {"tddate": null, "tmdate": 1489090065451, "tcdate": 1489090065451, "number": 1, "id": "S1FG841je", "invitation": "ICLR.cc/2017/workshop/-/paper150/official/review", "forum": "SJGIC1BFe", "replyto": "SJGIC1BFe", "signatures": ["ICLR.cc/2017/workshop/paper150/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper150/AnonReviewer2"], "content": {"title": "", "rating": "3: Clear rejection", "review": "In this paper, the authors propose a weight discretization scheme for training recurrent neural networks. The work shows that RNNs can be successfully trained on language modelling and music generation tasks when the binary weights are chosen to be much smaller and different for the hidden-to-hidden and input-to-hidden connections.\n\nThe idea of the paper is very simple but apparently effective. Some of the earlier research about binary and ternary weights did involve different choices for different parts of networks (e.g, the different GRU gates in Ott et al., 2016). Since recurrent neural networks are complicated non-linear dynamical systems, it makes sense that the scaling of the weights should be chosen carefully to allow for useful dynamics which are useful for solving practical problems. It's good that the paper draws attention to this. I think that the branding of this method as 'contextual discretization' is making it seem more elaborate than it really is. \n\nWhile the authors cite the work by Rastegari et al. (2016), they don't point out that the Binary-Weight-Networks described there are almost identical to the method proposed for implementing binary GRU's. In Binary-Weight-Networks, a multiplication with a binary weight vector is followed by a multiplication with a scalar which has been chosen to make the end result as similar as possible to a multiplication with the original real-valued weights. While that method has apparently not been applied to RNNs yet, it is more elegant than picking these values by hand and it can be seen as a more general method than the one proposed in this paper. I think that this similarity severely undermines the novelty of the ideas presented.\n\nThe paper is clearly written and easy to understand. Given how short the paper is, there could have been a more elaborate discussion of related work.\n\nPros:\nThe method is very simple yet effective for the tasks it is evaluated on and this is an interesting result.\n\nCons:\nThe method is almost identical to Binary-Weight-Nets.\nThe hand-picked scaling values could be very task specific.\nThe paper exaggerates the novelty of the method by giving it a new name.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Contextual Discretization framework for compressing Recurrent Neural Networks", "abstract": "In this paper, we address the issue of training Recurrent Neural Networks with binary weights and introduce a novel  Contextualized Discretization (CD) framework and showcase its effectiveness across multiple RNN architectures and two disparate tasks. We also propose a modified GRU architecture that allows harnessing the CD method and reclaim the exclusive usage of weights in $\\{-1, 1\\}$, which in turn reduces the number of power-two bit multiplications from $O(n^3)$ to $O(n^2)$.", "pdf": "/pdf/e53af45c2a4b5113485eebfffa4d4a0568bdfcfe.pdf", "TL;DR": "A new technique for weight binarized compression of  Recurrent Neural Networks", "paperhash": "clark|a_contextual_discretization_framework_for_compressing_recurrent_neural_networks", "conflicts": ["none@none.com"], "keywords": ["Deep learning", "Supervised Learning", "Applications"], "authors": ["Aidan Clark", "Vinay Uday Prabhu", "John Whaley"], "authorids": ["aidanbclark@berkeley.edu", "vinay@unify.id", "john@unify.id"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489110689775, "id": "ICLR.cc/2017/workshop/-/paper150/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper150/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper150/AnonReviewer2", "ICLR.cc/2017/workshop/paper150/AnonReviewer1"], "reply": {"forum": "SJGIC1BFe", "replyto": "SJGIC1BFe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper150/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper150/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489110689775}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487368737163, "tcdate": 1487367753896, "number": 150, "id": "SJGIC1BFe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "SJGIC1BFe", "signatures": ["~vinay_uday_prabhu1"], "readers": ["everyone"], "content": {"title": "A Contextual Discretization framework for compressing Recurrent Neural Networks", "abstract": "In this paper, we address the issue of training Recurrent Neural Networks with binary weights and introduce a novel  Contextualized Discretization (CD) framework and showcase its effectiveness across multiple RNN architectures and two disparate tasks. We also propose a modified GRU architecture that allows harnessing the CD method and reclaim the exclusive usage of weights in $\\{-1, 1\\}$, which in turn reduces the number of power-two bit multiplications from $O(n^3)$ to $O(n^2)$.", "pdf": "/pdf/e53af45c2a4b5113485eebfffa4d4a0568bdfcfe.pdf", "TL;DR": "A new technique for weight binarized compression of  Recurrent Neural Networks", "paperhash": "clark|a_contextual_discretization_framework_for_compressing_recurrent_neural_networks", "conflicts": ["none@none.com"], "keywords": ["Deep learning", "Supervised Learning", "Applications"], "authors": ["Aidan Clark", "Vinay Uday Prabhu", "John Whaley"], "authorids": ["aidanbclark@berkeley.edu", "vinay@unify.id", "john@unify.id"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}