{"notes": [{"id": "SJxZnR4YvB", "original": "rygTHUYOPr", "number": 1345, "cdate": 1569439400672, "ddate": null, "tcdate": 1569439400672, "tmdate": 1583912046800, "tddate": null, "forum": "SJxZnR4YvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "n18iTdrYgn", "original": null, "number": 1, "cdate": 1576798721114, "ddate": null, "tcdate": 1576798721114, "tmdate": 1576800915486, "tddate": null, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "invitation": "ICLR.cc/2020/Conference/Paper1345/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper tackles the problem of regret minimization in a multi-agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret. More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost. The goal is therefore to design protocols with little communication cost. The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near-optimal regret.\n\nThe only concern with the paper is that ICLR may not be the appropriate venue given that this work lacks representation learning contributions. However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727087, "tmdate": 1576800279305, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1345/-/Decision"}}}, {"id": "S1lV0dNpYS", "original": null, "number": 1, "cdate": 1571797195715, "ddate": null, "tcdate": 1571797195715, "tmdate": 1574038577159, "tddate": null, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "invitation": "ICLR.cc/2020/Conference/Paper1345/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The paper considers the problem of distributed multi-arm bandit, where M players are playing in the same stochastic environment. The goal of the paper is to have small over-all regret for all the players without a significant amount of communication between the players. \n\n\n\nThe main contribution of this paper is obtaining regret ~root(M KT) with ~M bits of communication in MAB, and regret ~d*root(MT) with ~Md bits of communication in linear bandit setting. \n\nThe main intuition of the algorithms in this paper is to do \"best arm identification\" with epoching: At every epoch t, the central server sends the set of possible best arms to each player and each player pulls it for 2^t /M times, followed by a communication round. Thus, the cumulative regret is comparable to having one player doing this epoch strategy for MT iterations, where the regret follows.\n\nThe problem considered in this paper is interesting and the result is new, the technique looks simple on paper but it requires a masterful combination of known tricks in (linear) MAB to obtain the best bound.  \n\n\nIt seems that in the MAB setting, the lower bound could be further strengthened with a log(K) factor, since removing this factor would ultimately require \"dynamic epoching\" which is not possible with limited communication. This would mostly complete the picture in the distributed MAB regime.\n\n\nMissing citation:\nThe authors are missing citations relevant to distributed MAB with collisions, see for example \n\"Non-Stochastic Multi-Player Multi-Armed Bandits: Optimal Rate With Collision Information, Sublinear Without\"\n\n\nAfter Rebuttal: I have read the authors' responses and acknowledge the sensibility of the statement.\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1345/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1345/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667092409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1345/Reviewers"], "noninvitees": [], "tcdate": 1570237738726, "tmdate": 1575667092428, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1345/-/Official_Review"}}}, {"id": "S1ekI4WBsH", "original": null, "number": 5, "cdate": 1573356615175, "ddate": null, "tcdate": 1573356615175, "tmdate": 1573356615175, "tddate": null, "forum": "SJxZnR4YvB", "replyto": "S1lV0dNpYS", "invitation": "ICLR.cc/2020/Conference/Paper1345/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank anonymous reviewer 3 for the review. \n\nRegarding the lower bound: We greatly thank reviewer 3 for the comments on the lower bound. We are considering this and may improve the lower bound in the final version.\n\nRegarding the missing citation: Thanks for bringing this paper to our attention. We will add it to the references.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1345/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxZnR4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference/Paper1345/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1345/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1345/Reviewers", "ICLR.cc/2020/Conference/Paper1345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1345/Authors|ICLR.cc/2020/Conference/Paper1345/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157411, "tmdate": 1576860560136, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference/Paper1345/Reviewers", "ICLR.cc/2020/Conference/Paper1345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1345/-/Official_Comment"}}}, {"id": "SyejNmWror", "original": null, "number": 4, "cdate": 1573356339096, "ddate": null, "tcdate": 1573356339096, "tmdate": 1573356339096, "tddate": null, "forum": "SJxZnR4YvB", "replyto": "HJlMtWL79H", "invitation": "ICLR.cc/2020/Conference/Paper1345/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank anonymous reviewer 2 for the review. \n\nRegarding the load balancing: The main issue of load balancing is that the length of each phase is determined by the agent with maximum number of remaining arms, since all agents run concurrently and need to synchronize at the end of each epoch. As a result, agents with too many arms need to donate arms to other agents if the numbers of arms remained in agents are not balanced. We will make it clearer in the final version.\n\nRegarding the experiments: We tested the algorithms for distributed linear bandits. We compared performance of our algorithms with two baselines: LinUCB with no communication (LinUCBnoComm) and adapted LinUCB with naive communication strategy (LinUCBnaiveComm), which means we allow agents to communicate their full information at some time steps. In our experiment, we enforced a communication budget of 200kb. The result shows that the regret of DELB protocol is 4 times smaller than that of LinUCBComm or LinUCBnaiveComm. In detail, we ran the algorithms using M = 100 agents on synthesized datasets with 1000 actions, d = 10. We used T ranging from 100 to 10000. We will update the final version accordingly.\n\nRegarding the UCB algorithm: In fact, for multi-armed bandits, the first protocol we designed with near-optimal regret and efficient communication is based on UCB approach. The main idea is to maintain global counts for pulling each arms and synchronize the average rewards for a certain arm only when the number of pulling that arm is doubled. However, the communication cost of this UCB-based protocol is $O(MK\\log M)$, which is worse than DEMAB. For linear bandits, our DisLinUCB protocol is based on optimism principle, which also achieves near-optimal regret with efficient communication.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1345/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxZnR4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference/Paper1345/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1345/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1345/Reviewers", "ICLR.cc/2020/Conference/Paper1345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1345/Authors|ICLR.cc/2020/Conference/Paper1345/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157411, "tmdate": 1576860560136, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference/Paper1345/Reviewers", "ICLR.cc/2020/Conference/Paper1345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1345/-/Official_Comment"}}}, {"id": "B1eOKfbroH", "original": null, "number": 3, "cdate": 1573356159792, "ddate": null, "tcdate": 1573356159792, "tmdate": 1573356159792, "tddate": null, "forum": "SJxZnR4YvB", "replyto": "HJeHVjyb5S", "invitation": "ICLR.cc/2020/Conference/Paper1345/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank anonymous reviewer 1 for the review. \n\nRegarding the ICLR venue: Our work focuses on parallelization and distributed learning, which is also a research topic in representation learning. We believe our results can bring insight to other distributed learning problems. Besides, the linear bandit studied in our paper is a special case of contextual bandit. We notice that several contextual bandit papers have been published on ICLR in the past a few years. Extending our work to contextual bandit is a future direction.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1345/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJxZnR4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference/Paper1345/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1345/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1345/Reviewers", "ICLR.cc/2020/Conference/Paper1345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1345/Authors|ICLR.cc/2020/Conference/Paper1345/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157411, "tmdate": 1576860560136, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1345/Authors", "ICLR.cc/2020/Conference/Paper1345/Reviewers", "ICLR.cc/2020/Conference/Paper1345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1345/-/Official_Comment"}}}, {"id": "HJeHVjyb5S", "original": null, "number": 2, "cdate": 1572039469055, "ddate": null, "tcdate": 1572039469055, "tmdate": 1572972480818, "tddate": null, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "invitation": "ICLR.cc/2020/Conference/Paper1345/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors study a bandit problem where there are multiple agents (say, M) and each of the agents is playing a multi-armed bandit problem for T rounds. The agents can communicate with each other in order to achieve small regret. The problem is to design a strategy for arm-playing and communication so that the agents all combined can achieve a small regret w/o communicating a lot. The authors study this bandit problem in 2 settings: 1. multi-armed bandit setting and 2.  bandit linear optimization. For both these settings, the authors establish elimination style algorithms with communication. upon communication the sub-optimal arms are eliminated and the game continues with the remaining arms. \nThe authors establish regret guarantees as well as communication guarantees. The interesting result is that with constant communication the regret scales as if full communication was available.  \n\nThe results are interesting and I do not have any objections with the paper, except that ICLR might not be the right avenue for such work given that it lacks any ideas regarding representation learning."}, "signatures": ["ICLR.cc/2020/Conference/Paper1345/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1345/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667092409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1345/Reviewers"], "noninvitees": [], "tcdate": 1570237738726, "tmdate": 1575667092428, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1345/-/Official_Review"}}}, {"id": "HJlMtWL79H", "original": null, "number": 3, "cdate": 1572196729557, "ddate": null, "tcdate": 1572196729557, "tmdate": 1572972480771, "tddate": null, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "invitation": "ICLR.cc/2020/Conference/Paper1345/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper considers the problem of obtaining an optimal regret algorithm in a distributed setting without incurring a large communication cost. In the standard and linear MAB settings, the authors propose algorithms and show that they achieve optimal regret up to logarithmic factors with communication costs that are almost independent of the horizon T. In addition the authors establish interesting lower bounds on the communication cost to obtain sublunar regret.\n\nOverall I found the paper very well motivated and clearly written. I did not go through the proofs in the appendix carefully, but I did check a few sections and found them to be correct. I did have a few concerns. \n\nI found the discussion around load balancing very confusing. Perhaps the authors could provide a picture to explain the issue? In addition, there is a lack of experiments- it is always nice to see comparisons to baseline even though the theory implies you would do better. Finally, the algorithm doesn\u2019t seem extremely practical from an applied point of view - for a linear amount of time all the bandits are pulling the same arms and there is no communication at all. I understand this repeated work doesn\u2019t affect the regret - but it is an artifact of using elimination. In general optimism based approaches (such as UCB) tend to work significantly better than elimination-style methods. I\u2019d be curious to hear the authors comment on whether they think a UCB style algorithm is possible in this setting.\n\nOverall I recommend the paper for acceptance. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1345/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1345/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication", "authors": ["Yuanhao Wang", "Jiachen Hu", "Xiaoyu Chen", "Liwei Wang"], "authorids": ["yuanhao-16@mails.tsinghua.edu.cn", "nickh@pku.edu.cn", "cxy30@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "keywords": ["Theory", "Bandit Algorithms", "Communication Efficiency"], "abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.", "pdf": "/pdf/d768f63a63c71f1196c7304231b92983f9c040c0.pdf", "paperhash": "wang|distributed_bandit_learning_nearoptimal_regret_with_efficient_communication", "_bibtex": "@inproceedings{\nWang2020Distributed,\ntitle={Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication},\nauthor={Yuanhao Wang and Jiachen Hu and Xiaoyu Chen and Liwei Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJxZnR4YvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/665f546f0a25fe784d59049f7f4415870a48669f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJxZnR4YvB", "replyto": "SJxZnR4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575667092409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1345/Reviewers"], "noninvitees": [], "tcdate": 1570237738726, "tmdate": 1575667092428, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1345/-/Official_Review"}}}], "count": 8}