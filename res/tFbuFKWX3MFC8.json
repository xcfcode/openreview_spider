{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1367022720000, "tcdate": 1367022720000, "number": 5, "id": "lcfIcbYPqX3P7", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["Ryan Kiros"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewers,\r\n\r\nTo better account for the mentioned weaknesses of the paper, I've re-implemented SHF with GPU compatibility and evaluated the algorithm on the CURVES and MNIST deep autoencoder tasks. I'm using the same setup as in Chapter 7 of Ilya Sutskever's PhD thesis, which allows for comparison against SGD, HF, Nesterov's accelerated gradient and momentum methods. I'm going to make one final update to the paper before the conference to include these new results."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1364786880000, "tcdate": 1364786880000, "number": 1, "id": "nYshYtAXG48ze", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["Ryan Kiros"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I want to say thanks again to the conference organizers, reviewers and openreview.net developers for doing a great job.\r\n\r\nI have updated the code on my webpage to include two additional features: max norm weight clipping and training deep autoencoders. Autoencoder training uses symmetric encoding / decoding and supports denoising and L2 penalties."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363601400000, "tcdate": 1363601400000, "number": 4, "id": "mm_3mNH6nD4hc", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["Ryan Kiros"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I have submitted an updated version to arxiv and should appear shortly. My apologies for the delay. From the suggestion of reviewer 0a71 I've renamed the paper to 'Training Neural Networks with Dropout Stochastic Hessian-Free Optimization'."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363585560000, "tcdate": 1363585560000, "number": 1, "id": "3nHzayPmAI5r1", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "tFbuFKWX3MFC8", "replyto": "av7x0igQwD0M-", "signatures": ["anonymous reviewer 0a71"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Regarding using HF for classification. My point was that lack of results in the literature about classification error with HF might be just due to the fact that this is a new method, arguably hard to implement and hence not many had a chance to play with it. I'm not sure that just using HF (the way James introduced it) would not do well on classification. I feel I didn't made this clear in my original comment. I would just remove that statement. Looking back on [R2] I couldn't find a similar statement, it only says that empirically KSD seems to do better on classification. \r\n\r\nAlso I see you have not updated the arxiv papers. I would urge you to do so, even if you do not have all the new experiments ready. It would be helpful for us the reviewers to see how you change the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362494640000, "tcdate": 1362494640000, "number": 8, "id": "av7x0igQwD0M-", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["Ryan Kiros"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you for your comments! \r\n\r\nTo Anonymous 0a71:\r\n---------------------------------\r\n\r\n(1,8): I agree. Indeed, it is straightforward to add an additional experiment without the use of dropout. At the least, the experimental section can be modified to indicate whether the method is using dropout or not instead of simply referring to 'stochastic HF'.\r\n\r\n(2): Fair point. It would be interesting trying this method out in a similar experimental setting as [R1]. Perhaps it may give some insight on the paper's hypothesis that the optimization is the culprit to underfitting.\r\n\r\n(3): Correct me if I'm wrong but the only classification results of HF I'm aware of are from [R2] in comparison with Krylov subspace decent, not including methods that refer to themselves as natural gradient. Minibatch overfitting in batch HF is problematic and discussed in detail in [R5], pg 50. Given the development of [R3], the introduction could be modified to include additional discussion regarding the relationship with natural gradient and classification settings.\r\n\r\n(5): Section 4.5 of [R4] discusses the benefits of non-zero CG initializations. In batch HF, it's completely reasonable to fix gamma throughout training (James uses 0.95). This is problematic in stochastic HF due to such a small number of CG iterations. Given a non-zero CG initialization and a near-one gamma, hat{M}_\theta may be more likely to remain positive after CG and assuming f_k - f_{k-1} < 0, means that the reduction ratio will be negative and thus lambda will be increased to compensate. This is not necessarily a bad thing, although if it happens too frequently the algorithm will began to behave more like SGD (and in some cases the linesearch will reject the step). Setting gamma to some smaller initial value and incrementing at each epoch, based on empirical performance, allows for near-one delta values late in training without negating the reduction ratio. I refer the reader to pg.28 and pg.39 in [R5], which give further motivation and discussion on these topics.\r\n\r\n(6): Using the same batches for gradients and curvature have some theoretical advantages (see section 12.1, pg.48 of [R5] for derivations). While lambda -> 0 is indeed an empirical observation, James and Ilya also report similar behaviour for shorter CG runs (although longer than what I use) using the same batches for gradients and curvature (pg.54 of [R5]). Within the proposed stochastic setting, having lambda -> 0 doesn't make too much sense to me (at least for non-convex f). It could allow for much more aggressive steps which may or may not be problematic given how small the curvature minibatches are. One solution is to simply increase the batch sizes, although this was something I was intending to avoid.\r\n\r\n(7): The motivation behind \beta_e was to help achieve more stable training over the stochastic networks induced using dropout. You are probably right that 'not requiring early stopping' is way too strong of a statement.\r\n\r\n\r\nTo Anonymous 4709:\r\n---------------------------------\r\n\r\nDue to the additional complexity of HF compared to SGD, I attempted to make my available (Matlab) code as easy as possible to read and follow through in order to understand and reproduce the key features of the method.\r\n\r\nWhile an immediate advantage of stochastic HF is not requiring tuning learning rate schedules, I think it is also a promising approach in further investigating the effects of overfitting and underfitting with optimization in neural nets, as [R1] motivates. The experimental evaluation does not attack this particular problem, as the goal was to make sure stochastic HF was at least competitive with SGD dropout on standard benchmarks. This to me was necessary to justify further experimentation.\r\n\r\nThere is no comparison with the results of [R4] since the goal of the paper was to focus on classification (and [R4] only trains on deep autoencoders). Future work includes extending to other architectures, as discussed in the conclusion.\r\n\r\nI mention on pg. 7 that the per epoch update times were similar to SGD dropout (I realize this is not particularly rigorous). \r\n\r\nIn regards to evaluating each of the modifications, I had hoped that the discussion was enough to convey the importance of each design choice. I realize now that there might have been too much assumption of information discussed in [R5]. These details will be made clear in the updated version of the paper with appropriate references.\r\n\r\n\r\nTo Anonymous f834:\r\n--------------------------------\r\n\r\n- Thanks for the reference clarifications. In regards to classification tasks, see (3) in my response to Anonymous 0a71.\r\n\r\n- Indeed, much of the motivation of the algorithm, particularly the momentum interpretation, came from studying [R5] which expands on HF concepts in significantly more detail then the first publications allowed for. I will be sure to make this more clear in the relevant sections of the paper.\r\n\r\n- I agree that not comparing against other adaptive methods is a weakness and discussed this briefly in the conclusion. To accommodate for this, I tried to use an SGD implementation that would at least be as competitive (dropout, max-norm weight clipping with large initial rates, momentum and learning rate schedules). Weight clipping was also shown to improve SGD dropout, at least on MNIST [R6]. \r\n\r\n- Unfortunately, I don't have too much more insight on the behaviour of lambda though it appears to be quite consistent. The large initial decrease is likely to come from conservative initialization of lambda which works well as a default.\r\n\r\n- I did not test on deeper nets largely due to time constraints (it made more sense to me to start on shallower networks then to 'jump the gun' and go straight for very deep nets) . Should I not have done this? As alluded to in the conclusion, I wouldn't be expecting any significant gain on these datasets (perhaps I'm wrong here). It would be cool to try on some speech data where deeper nets have made big improvements but I haven't worked with speech before. Reuters didn't use hidden layers due to the high dimensionality of the inputs (~19000 log word count features). Applying this to RNNs is a work in progress.\r\n\r\n\r\n----------------------------------------------\r\n\r\nTo summarize (modifications for the paper update):\r\n- include additional references\r\n- add results for stochastic HF with no dropout\r\n- some additional discussion on the relationship with natural gradient (and classification results)\r\n- better detail section 4, including additional references to [R5]\r\n\r\nThese modifications will be made by the start of next week (March 11).\r\n\r\n\r\nOne additional comment: after looking over [R6], I realized the MNIST dropout SGD results (~110 errors) were due to a combination of dropout and the max-norm weight clipping and not just dropout alone. I have recently been exploring using weight clipping with stochastic HF and it is advantageous to include it. This is because it allows one to start training with smaller lambda values, likely in the same sense as it allows SGD to start with larger learning rates. I will be updating the code shortly to include this option.\r\n\r\n\r\n[R1] Yann N. Dauphin, Yoshua Bengio, Big Neural Networks Waste Capacity, arXiv:1301.3583\r\n[R2] O. Vinyals and D. Povey. Krylov subspace descent for deep learning. arXiv:1111.4259, 2011\r\n[R3] Razvan Pascanu, Yoshua Bengio, Natural Gradient Revisited, arXiv:1301.3584\r\n[R4] J. Martens. Deep learning via hessian-free optimization. In ICML 2010.\r\n[R5] J. Martens and I. Sutskever. Training deep and recurrent networks with hessian-free optimization. Neural Networks: Tricks of the Trade, pages 479\u2013535, 2012.\r\n[R6] N. Srivastava. Improving Neural Networks with Dropout. Master's thesis, University of Toronto, 2013."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362400260000, "tcdate": 1362400260000, "number": 3, "id": "TF3miswPCQiau", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["anonymous reviewer f834"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Training Neural Networks with Stochastic Hessian-Free Optimization", "review": "This paper looks at designing an SGD-like version of the 'Hessian-free' (HF) optimization approach which is applied to training shallow to moderately deep neural nets for classification tasks.  The approach consists of the usual HF algorithm, but with smaller minibatches and with CG terminated after only 3-5 iterations.   As advocated in [20], more careful attention is paid to the 'momentum-constant' gamma.\r\n\r\nIt is somewhat interesting to see a very data intensive method like HF made 'lighter' and more SGD-like, since this could perhaps provide benefits unique to both HF and SGD, but it's not clear to me from the experiments if there really is an advantage over variants of SGD that would perform some kind of automatic adaptation of learning rates (or even a fixed schedule!).   The amount of novelty in the paper isn't particularly high since many of these ideas have been proposed before ([20]), although perhaps in less extreme or less developed forms.  \r\n\r\n\r\nPros:\r\n- takes the well-known approach HF in a different (if not entirely novel) direction\r\n- seems to achieves performance competitive with versions of SGD used in [3] with dropout\r\nCons:\r\n- experiments don't look at particularly deep models and aren't very thorough\r\n- comparisons to other versions of SGD are absent (this is my primary issue with the paper)\r\n\r\n----\r\n\r\n\r\nThe introduction and related work section should probably clarify that HF is an instance of the more general family of methods sometimes known as 'truncated-Newton methods'.\r\n\r\nIn the introduction, when you state:  'HF has not been as successful for classification tasks', is this based on your personal experience, particularly negative results in other papers, or lack of positive results in other papers?\r\n\r\nMissing from your review are papers that look at the performance of pure stochastic gradient descent applied to learning deep networks, such as [15] did, and the paper by Glorot and Bengio from AISTATS 2010.   Also, [18] only used L-BFGS to perform 'fine-tuning' after an initial layer-wise pre-training pass. \r\n\r\nWhen discussing the generalized Gauss-Newton matrix you should probably cite [7].\r\n\r\nIn section 4.1, it seems like a big oversimplification to say that the stopping criterion and overall convergence rate of CG depend on mostly on the damping parameter lambda.  Surely other things matter too, like the current setting of the parameters (which determine the local geometry of the error surface).  A high value of lambda may be a sufficient condition, but surely not a necessary one for CG to quickly converge.  Moreover, missing from the story presenting in this section is the fact that lambda *must* decrease if the method is to ever behave like a reasonable approximation of a Newton-type method.\r\n\r\nThe momentum interpretation discussed in the middle of section 4, and overall the algorithm discussed in this paper, sounds similar to ideas discussed in [20] (which were perhaps not fully explored there).   Also, a maximum iteration for CG is was used in the original HF paper (although it only appeared in the implementation, and was later discussed in [20]).  This should be mentioned.\r\n\r\nCould you provide a more thorough explanation of why lambda seems to shrink, then grow, as optimization proceeds?  The explanation in 4.2 seems vague/incomplete.\r\n\r\nThe networks trained seem pretty shallow (especially Reuters, which didn't use any hidden layers).  Is there a particular reason why you didn't make them deeper?  e.g. were deeper networks overfitting more, or perhaps underfitting due to optimization problems, or simply not providing any significant advantage for some other reasons?  SGD is already known to be hard to beat for these kinds of not-very-deep classification nets, and while it seems plausible that the much more SGD-like HF which you are proposing would have some advantage in terms of automatic selection of learning rates, it invites comparison to other methods which do this kind of learning rate tuning more directly (some of which you even discuss in the paper).  The lack of these kinds of comparisons seems like a serious weakness of the paper.\r\n\r\nAnd how important to your results was the use of this 'delta-momentum' with the particular schedule of values for gamma that you used?  Since this behaves somewhat like a regular momentum term, did you also try using momentum in your SGD implementation to make the comparison more fair?\r\n\r\nThe experiments use drop-out, but comparisons to implementations that don't use drop-out, or use some other kind of regularization instead (like L2) are noticeably absent.  In order understand what the effect of drop-out is versus the optimization method in these models it is important to see this.\r\n\r\nI would have been interested to see how well the proposed method would work when applied to very deep nets or RNNs, where HF is thought to have an advantage that is perhaps more significant/interesting than what could be achieved with well tuned learning rates."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362391800000, "tcdate": 1362391800000, "number": 2, "id": "UJZtu0oLtcJh1", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["anonymous reviewer 4709"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Training Neural Networks with Stochastic Hessian-Free Optimization", "review": "This paper makes an attempt at extending the Hessian-free learning work to a stochastic setting.  In a nutshell, the changes are:\r\n\r\n- shorter CG runs\r\n- cleverer information sharing across CG runs that has an annealing effect\r\n- using differently-sized mini-batches for gradient and curvature estimation (former sizes being larger)\r\n- Using a slightly modified damping schedule for lamdba than Martens' LM criteria, which encourages fewer oscillations.\r\n\r\nAnother contribution of the paper is the integration of dropouts into stochastic HF in a sensible way. The authors also include an exponentially-decaying momentum-style term into the parameter updates.\r\n\r\nThe authors present but do not discuss results on the Reuters dataset (which seem good). There is also no comparison with the results from [4], which to me would be a natural thing to compare to.\r\n\r\nAll in all, a series of interesting tricks for making HF work in a stochastic regime, but there are many questions which are unanswered. I would have liked to see more discussion *and* experiments that show which of the individual changes that the author makes are responsible for the good performance. There is also no discussion on the time it takes the stochastic HF method to make on step / go through one epoch / reach a certain error. \r\n\r\nSGD dropout is a very competitive method because it's fantastically simple to implement (compared to HF, which is orders of magnitude more complicated), so I'm not yet convinced by the insights of this paper that stochastic HF is worth implementing (though it seems easy to do if one has an already-running HF system)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362161760000, "tcdate": 1362161760000, "number": 6, "id": "gehZgYtw_1v8S", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["anonymous reviewer 0a71"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Training Neural Networks with Stochastic Hessian-Free Optimization", "review": "Summary and general overview:\r\n----------------------------------------------\r\nThe paper tries to explore an online regime for Hessian Free as well as using drop outs. The new method is called Stochastic Hessian Free and is tested on a few datasets (MNIST, USPS and Reuters). \r\nThe approach is interesting and it is a direction one might need to consider in order to scale to very large datasets. \r\n\r\nQuestions:\r\n---------------\r\n(1) An aesthetic point. Stochastic Hessian Free does not seem as a suitable name for the algorithm, as it does not mention the use of drop outs. I think scaling to a stochastic regime is an orthogonal issue to using drop outs, so maybe Drop-out Stochastic Hessian Free would be more suitable, or something rather, that makes the reader aware of the use of drop-outs.\r\n\r\n(2) Page 1, first paragraph. Is not clear to me that SGD scales well for large data. There are indications that SGD could suffer, for e.g., from under-fitting issues (see [1]) or early over-fitting (see [2]). I'm not saying you are wrong, you are probably right, just that the sentence you use seems a bit strong and we do not yet have evidence that SGD scales well to very large datasets, especially without the help of things like drop-outs (which might help with early-overfitting or other phenomena). \r\n\r\n(3) Page 1, second paragraph. Is not clear to me that HF does not do well for classification. Is there some proof for this somewhere? For e.g. in [3] a Hessian Free like approach seem to do well on classification (note that the results are presented for Natural Gradient, but the paper shows that Hessian Free is Natural Gradient due to the use of Generalized Gauss-Newton matrix).\r\n\r\n(4) Page 3, paragraph after the formula. R-operator is only needed to compute the product of the generalized Gauss-Newton approximation of the Hessian with some vector `v`. The product between the Hessian and some vector 'v' can easily be computed as d sum((dC/dW)*v)/dW (i.e. without using the R-op).\r\n \r\n(5) Page 4, third paragraph. I do not understand what you mean when you talk about the warm initialization of CG (or delta-momentum as you call it). What does it mean that hat{M}_\theta is positive ? Why is that bad? I don't understand what this decay you use is suppose to do? Are you trying to have some middle ground between starting CG from 0 and starting CG from the previous found solution? I feel a more detailed discussion is needed in the paper. \r\n\r\n(6) Page 4, last paragraph. Why does using the same batch size for the gradient and for computing the curvature results in lambda going to 0? Is not obvious to me. Is it some kind of over-fitting effect? If it is just an observation you made through empirical experimentation, just say so, but the wording makes it sound like you expect this behaviour due to some intuitions you have.\r\n \r\n(7) Page 5, section 4.3. I feel that the affirmation that drop-outs do not require early stopping is too strong. I feel the evidence is too weak at the moment for this to be a statement. For one thing, \beta_e goes exponentially fast to 0. \beta_e scales the learning rate, and it might be the reason you do not easily over-fit (when you reach epoch 50 or so you are using a extremely small learning rate). I feel is better to make this as an observation. Also could you maybe say something about this decaying learning rate, is my understanding of \beta_e correct? \r\n \r\n(8) I feel a important comparison would be between your version of stochastic HF with drop-outs vs stochastic HF (without the drop outs) vs just HF. From the plots you give, I'm not sure what is the gain from going stochastic, nor is it clear to me that drop outs are important. You seem to have the set-up to run this additional experiments easily. \r\n \r\nSmall corrections:\r\n--------------------------\r\nPage 1, paragraph 1, 'salable` -> 'scalable'\r\nPage 2, last paragraph. You wrote : 'B is a curvature matrix suc as the Hessian'. The curvature of a function `f` at theta is the Hessian (there is no choice) and there is only one curvature for a given function and theta. There are different approximations of the Hessian (and hence you have a choice on B) but not different curvatures. I would write only 'B is an approximation of the curvature matrix` or `B is the Hessian`.\r\n\r\nReferences: \r\n[1] Yann N. Dauphin, Yoshua Bengio, Big Neural Networks Waste Capacity,  arXiv:1301.3583\r\n[2] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent and Samy Bengio, Why Does Unsupervised Pre-training Help Deep Learning? (2010), in: Journal of Machine Learning Research, 11(625--660)\r\n[3] Razvan Pascanu, Yoshua Bengio, Natural Gradient Revisited, arXiv:1301.3584"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360514640000, "tcdate": 1360514640000, "number": 7, "id": "CUXbqkRcJWqcy", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "tFbuFKWX3MFC8", "replyto": "tFbuFKWX3MFC8", "signatures": ["Ryan Kiros"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Code is now available: http://www.ualberta.ca/~rkiros/\r\n\r\nIncluded are scripts to reproduce the results in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358410500000, "tcdate": 1358410500000, "number": 48, "id": "tFbuFKWX3MFC8", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "tFbuFKWX3MFC8", "signatures": ["rkiros@ualberta.ca"], "readers": ["everyone"], "content": {"title": "Training Neural Networks with Stochastic Hessian-Free Optimization", "decision": "conferencePoster-iclr2013-conference", "abstract": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with small gradient and curvature mini-batches independent of the dataset size for classification. We modify Martens' HF for this setting and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. On classification tasks, stochastic HF achieves accelerated training and competitive results in comparison with dropout SGD without the need to tune learning rates.", "pdf": "https://arxiv.org/abs/1301.3641", "paperhash": "kiros|training_neural_networks_with_stochastic_hessianfree_optimization", "authors": ["Ryan Kiros"], "authorids": ["rkiros@ualberta.ca"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 10}