{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487892985371, "tcdate": 1478191225945, "number": 71, "id": "r1fYuytex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1fYuytex", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "content": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396338215, "tcdate": 1486396338215, "number": 1, "id": "r1cnsfIOe", "invitation": "ICLR.cc/2017/conference/-/paper71/acceptance", "forum": "r1fYuytex", "replyto": "r1fYuytex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "After discussion, the reviewers unanimously recommend accepting the paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396338753, "id": "ICLR.cc/2017/conference/-/paper71/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1fYuytex", "replyto": "r1fYuytex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396338753}}}, {"tddate": null, "tmdate": 1485733764659, "tcdate": 1485733764659, "number": 9, "id": "HkpY1WhPe", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "r1aVeyMEe", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Re: Baseline results...", "comment": "We have added some results on a new baseline using data augmentation (affine transformation method) for the CIFAR10 dataset in Table 3 of the revised manuscript. Our simulation results show that binarized/ternarized networks acquired with our pruning technique still result in a better MCR while using data augmentation. Based on our experimental results, binarized/ternarized networks yield a better MCR than Single-Precision Floating-Point networks since these networks prevent the model from overfitting as it was also shown in the original binarized/ternarized paper. In this paper, we experimentally showed that our pruning technique can also yield a further improvement in accuracy while having a significantly lower connections. Of course, the reason behind this improvement requires more investigations and we will include this in our future works."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1484576789301, "tcdate": 1484576789301, "number": 8, "id": "SyazdLcLl", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "r1aVeyMEe", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Results on a new baseline using data augmentation", "comment": "We have added some results on a new baseline using data augmentation (affine transformation method) for the MNIST dataset in Table 2 of the revised manuscript. Our simulation results show that when data augmentation is used, our method can drop up to 50% and 70% of connections from BinaryConnect and TernaryConnect networks, respectively, without any compromise in performance. However, using data augmentation results in a better misclassification rate when it is used on networks trained with single-precision floating-point weights, as shown in Table 2. In this case, our method can drop up to 90% of connections without any performance degradation.\nAt the moment, we are are running simulations on the CIFAR 10 dataset with data augmentation. We hope to be able to include the results before the deadline."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1484278997831, "tcdate": 1484278997831, "number": 4, "id": "ryAChprLx", "invitation": "ICLR.cc/2017/conference/-/paper71/official/comment", "forum": "r1fYuytex", "replyto": "r1ttK25Eg", "signatures": ["ICLR.cc/2017/conference/paper71/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper71/AnonReviewer1"], "content": {"title": "Thank you for the detailed responses and changes on the paper", "comment": "Increased my grade to 6. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741335, "id": "ICLR.cc/2017/conference/-/paper71/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper71/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper71/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741335}}}, {"tddate": null, "tmdate": 1484278849941, "tcdate": 1482033768103, "number": 2, "id": "B1x_qtQNl", "invitation": "ICLR.cc/2017/conference/-/paper71/official/review", "forum": "r1fYuytex", "replyto": "r1fYuytex", "signatures": ["ICLR.cc/2017/conference/paper71/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper71/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. \nThe paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.\nThe paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. \nThis explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.\nAlgorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text.\nUsing random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before. The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up. Also the author's VLSI implementation does not speed up the network inference.\nHow are the results of this work compared to Network in Network (NiN)? https://arxiv.org/abs/1312.4400 In NiN, the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance. Are the results presented here better? It would be more convincing to see this method tested on ImageNet, which actually uses a big fully connected layer. \n\nIncreased my rating from 5-6 after rebuttal.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512709074, "id": "ICLR.cc/2017/conference/-/paper71/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper71/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper71/AnonReviewer3", "ICLR.cc/2017/conference/paper71/AnonReviewer1", "ICLR.cc/2017/conference/paper71/AnonReviewer2"], "reply": {"forum": "r1fYuytex", "replyto": "r1fYuytex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512709074}}}, {"tddate": null, "tmdate": 1482504640384, "tcdate": 1482504640384, "number": 7, "id": "SJuTY254e", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "S1FsmyvEl", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Re: Neural Nets for embedded devices", "comment": "We sincerely thank the reviewer for pointing this out. The mentioned paper is now added to the manuscript. We will also double check the references to make sure that the original papers are cited."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1482504577091, "tcdate": 1482504577091, "number": 6, "id": "r1ttK25Eg", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "B1x_qtQNl", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Re: no title", "comment": "Reviewer comment: The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.\n\nResponse: We sincerely thank the reviewer for the comments.\n\n--------------------------\n\nReviewer comment: The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation.\nThis explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.\n\nResponse: We thank the reviewer for pointing out this unclear statement. We have modified the explanation according to your comment.\n\n--------------------------\n\nReviewer comment: Algorithm 1 is basically training a network with back-propagation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text.\n\nResponse: We thank the reviewer for the comment. We agree with the reviewer that Alg. 1 is the back-propagation algorithm with a binary mask. We believe that summarizing the proposed method as an algorithm would be clearer to the public reviewer. As suggested, we added the explanation to the text.\n\n--------------------------\n\nReviewer comment: Using random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before. \n\nResponse: We thank the reviewer for pointing this out. Regarding the 1998 paper, we agree with the reviewer that a pruning technique was used. However, we believe that the presented pruning technique follows a pattern provided in Table I of the 1998 paper: it is not declared that the pattern was generated randomly, and no explanation is provided on this matter. Moreover, this pruning technique was only used for one convolutional layer (from S2 to C3) with 37% less connections. Besides, the method presented in the 1998 paper only applies a high level pruning technique in a \u201cbinary\u201d way: it either eliminates or maintains each complete 5*5 filter, and it does not drop connections within the filter. Also, in the aforementioned paper no explanation is provided on the effect of the presented pruning technique in terms of both misclassification rate (MCR) and hardware implementation.\n\n--------------------------\n\nReviewer comment: The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up. Also the author's VLSI implementation does not speed up the network inference.\n\nResponse: We thank the reviewer for the comment. The current VLSI implementations of fully-connected layers mainly suffer from copious power/energy consumption due to the energy/power-hungry memory accesses [*], [**]. That is why many works recently have focused on pruning techniques to reduce the memory accesses and consequently power/energy consumption. We agree with the reviewer that VLSI implementation of our proposed pruning method does not speed up the process compared to conventional VLSI implementations. However, the proposed VLSI implementation provides other advantages. First, we show that the proposed pruning technique can be simply performed using the conventional back-propagation algorithm where each layer has a binary mask, as opposed to other pruning techniques that usually require an additional training stage. Secondly, we show that the proposed technique can drop up to 90% of connections while improving MCR. More importantly, the proposed technique can be easily implemented using LFSR units as shown in Fig. 3 while highly reducing power/energy consumption and required memory size, and thus making fully connected network implementable in practice. VLSI implementations of fully-connected layers generally speed up the computations compared to GPUs/CPUs as shown in [*].\n\n[*] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally. EIE: Efficient inference engine on compressed deep neural network. In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), pp. 243\u2013254, June 2016. doi: 10.1109/ISCA.2016.30.\n\n[**] Matthieu Courbariaux and Yoshua Bengio. BinaryNet: Training deep neural networks with weights and activations constrained to +1 or -1. CoRR, abs/1602.02830, 2016.\n\n--------------------------\n\nReviewer comment: How are the results of this work compared to Network in Network (NiN)? https://arxiv.org/abs/1312.4400 In NiN, the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance. Are the results presented here better? It would be more convincing to see this method tested on ImageNet, which actually uses a big fully connected layer.\n\nResponse: We thank the reviewer for the comment. The MCRs of the NiN paper are summarized in Table 5 of our paper. The simulation results show that the proposed technique has a better MCR compared the NiN paper on CIFAR10 and SVHN datasets. \nRegarding the ImageNet dataset, we are currently working on the extension on this paper: it will include tests results obtained on ImageNet.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1482504172104, "tcdate": 1482504172104, "number": 5, "id": "B1NxOn9El", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "S1tTNizEg", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Re: related work", "comment": "We sincerely thank the reviewer for pointing this out. The mentioned paper is now properly cited and a discussion is provided in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1482504114640, "tcdate": 1482504114640, "number": 4, "id": "rkjhw2q4g", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "r1aVeyMEe", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Re: Baseline results...", "comment": "We sincerely thank the reviewer for the comments. We are currently running simulations on more challenging scenarios, and we will do our best to upload the results before the submission deadline."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1482253217321, "tcdate": 1482253217321, "number": 3, "id": "S1FsmyvEl", "invitation": "ICLR.cc/2017/conference/-/paper71/official/review", "forum": "r1fYuytex", "replyto": "r1fYuytex", "signatures": ["ICLR.cc/2017/conference/paper71/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper71/AnonReviewer2"], "content": {"title": "Neural Nets for embedded devices", "rating": "7: Good paper, accept", "review": "Experimental results look reasonable, validated on 3 tasks. \nReferences could be improved, for example I would rather see\nRumelhart's paper cited for back-propagation than the Deep Learning book.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512709074, "id": "ICLR.cc/2017/conference/-/paper71/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper71/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper71/AnonReviewer3", "ICLR.cc/2017/conference/paper71/AnonReviewer1", "ICLR.cc/2017/conference/paper71/AnonReviewer2"], "reply": {"forum": "r1fYuytex", "replyto": "r1fYuytex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512709074}}}, {"tddate": null, "tmdate": 1481924661543, "tcdate": 1481924661543, "number": 1, "id": "r1aVeyMEe", "invitation": "ICLR.cc/2017/conference/-/paper71/official/review", "forum": "r1fYuytex", "replyto": "r1fYuytex", "signatures": ["ICLR.cc/2017/conference/paper71/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper71/AnonReviewer3"], "content": {"title": "Baseline results...", "rating": "6: Marginally above acceptance threshold", "review": "From my original comments:\n\nThe results looks good but the baselines proposed are quite bad.\n\nFor instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nIn CIFAR-10 experiments, i do not understand  why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. \n\nAgain i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. \n\nIn fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\n----\n\nThe authors reply still does not convince me.\n\nI still think that the same technique should be applied on more challenging scenarios.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512709074, "id": "ICLR.cc/2017/conference/-/paper71/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper71/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper71/AnonReviewer3", "ICLR.cc/2017/conference/paper71/AnonReviewer1", "ICLR.cc/2017/conference/paper71/AnonReviewer2"], "reply": {"forum": "r1fYuytex", "replyto": "r1fYuytex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512709074}}}, {"tddate": null, "tmdate": 1481045396694, "tcdate": 1481045396686, "number": 2, "id": "HJaqSuN7e", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "B1GluvyQe", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Re: Results better but not good baselines", "comment": "We sincerely thank the reviewer for the comments.\n\n\n-----------------------\nReviewer comment: For instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nResponse: We fully agree with the reviewer that misclassification rate (MCR) of 0.8% can be obtained for a purely fully-connected network with the configuration of 784-1024-1024-1024-10 if data augmentation methods, unsupervised pre-training or other fine-tuning techniques are used. Despite the good MCR that the floating point networks with data augmentation methods, unsupervised pre-training or other fine-tuning techniques provide, they require a high number of bits, huge amount of memories and costly multipliers for hardware implementation. In order to address the aforementioned issues, different pruning and binarization/ternarization techniques were introduced in literature to make the hardware implementation of neural networks feasible. In this paper, we also mainly focused on reducing the connections of binarized/ternarized networks while improving their MCR. Of course, the proposed method is not restricted to only binarized/ternarized networks and can be used also in floating point networks.\n\n\n-----------------------\nReviewer comment: In CIFAR-10 experiments, i do not understand why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float.\n\nResponse: In the original paper on binary precision [*], it was shown that the binary precision can yield a better performance than the floating point precision. This is because the binarization technique works as a regularization method to prevent over-fitting. In our work, we observed the sparsification of the network connections can work as a regularization method too.\n\n[*] Matthieu Courbariaux,\u00a0Yoshua Bengio,\u00a0Jean-Pierre David, \u201cBinaryConnect: Training Deep Neural Networks with binary weights during propagations\u201d,\u00a0NIPS 2015.\n\n\n-----------------------\nReviewer comment: Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision.\n\nResponse: The main goal of this paper is to propose a method to significantly reduce the number of parameters of fully-connected networks for hardware implementations. Literature mainly focuses on floating-point precision: however, floating point is very costly to implement in hardware. On the other hand,  binarized/ternarized networks are well-suited for hardware implementations, and thus we focused on these kind of networks. \nAs mentioned earlier, no data augmentation or regularization was used in this paper. We have not investigated the effect of using the augmentation techniques yet. However, while we mostly focus on and compare to binarized/ternarized networks due to their more efficient hardware implementation, the proposed technique is not restricted to binarized/ternarized networks and can be applied to the floating point precision networks on top of any other optimization technique.\n\n\n-----------------------\nReviewer Comment: In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\nResponse: We agree that with the convolutional network we used in the paper, floating point precision can get a MCR below 9% on CIFAR10 while using data augmentation or regularization methods. In this paper, we showed that we can still get a MCR below 9% on CIFAR10 while using the proposed pruning technique and binarized/ternarized weights without any data augmentation or regularization method. Although the floating point networks yield state-of-the-art results on different datasets while using data-augmentation techniques, they require a high number of bits and costly multipliers for hardware implementation. However, binarized networks require only a single bit for representation of weights and no multiplication is required for hardware implementation of these networks. Therefore, binarized/ternarized networks are mainly used for hardware implementation of the inference engine, and we showed that we can optimize these networks by pruning the weights in an efficient way and achieving a better MCR.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1480714217901, "tcdate": 1480714217898, "number": 2, "id": "B1GluvyQe", "invitation": "ICLR.cc/2017/conference/-/paper71/pre-review/question", "forum": "r1fYuytex", "replyto": "r1fYuytex", "signatures": ["ICLR.cc/2017/conference/paper71/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper71/AnonReviewer3"], "content": {"title": "Results better but not good baselines", "question": "The results looks good but the baselines proposed are quite bad.\n\nFor instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nIn CIFAR-10 experiments, i do not understand  why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. \n\nAgain i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. \n\nIn fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959480413, "id": "ICLR.cc/2017/conference/-/paper71/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper71/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper71/AnonReviewer1", "ICLR.cc/2017/conference/paper71/AnonReviewer3"], "reply": {"forum": "r1fYuytex", "replyto": "r1fYuytex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959480413}}}, {"tddate": null, "tmdate": 1480697434689, "tcdate": 1480697434683, "number": 1, "id": "BJQD8QyXx", "invitation": "ICLR.cc/2017/conference/-/paper71/public/comment", "forum": "r1fYuytex", "replyto": "SybQwI0Ge", "signatures": ["~Arash_Ardakani1"], "readers": ["everyone"], "writers": ["~Arash_Ardakani1"], "content": {"title": "Re: Request for more numbers", "comment": "We thank the reviewer for the comments.\n\nThe number of inputs of each neuron determines the latency (or speed) of the architecture. Regardless of the output value of SNG, inputs are fed into each neuron sequentially in each clock cycle in the proposed architecture. Therefore, the latency of the proposed sparsely-connected architecture is the same as the conventional fully-connected architecture. The detailed discussion is provided in Section 5, paragraph 4. The latency value has also been added to Table 6 based on your comment.\n\nWe have not considered ImageNet dataset for this conference paper due to the time constraints we have. Currently, we are extending the proposed architecture to an accelerator for sparsely-connected networks which will be tested using ImageNet dataset."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741460, "id": "ICLR.cc/2017/conference/-/paper71/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1fYuytex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper71/reviewers", "ICLR.cc/2017/conference/paper71/areachairs"], "cdate": 1485287741460}}}, {"tddate": null, "tmdate": 1480644377429, "tcdate": 1480644377425, "number": 1, "id": "SybQwI0Ge", "invitation": "ICLR.cc/2017/conference/-/paper71/pre-review/question", "forum": "r1fYuytex", "replyto": "r1fYuytex", "signatures": ["ICLR.cc/2017/conference/paper71/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper71/AnonReviewer1"], "content": {"title": "Request for more numbers", "question": "Does the Asic implementation of the sparsely-connected network also provide a speed-up over fully connected layers? Can you add the speed numbers to Table 6?\nDo you have any plans to experiment with Imagenet dataset?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.", "pdf": "/pdf/ae297fea4af26c7ec220b0b744098a7e7ef26a0a.pdf", "TL;DR": "We show that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance.", "paperhash": "ardakani|sparselyconnected_neural_networks_towards_efficient_vlsi_implementation_of_deep_neural_networks", "conflicts": ["mcgill.ca"], "keywords": ["Deep learning", "Applications", "Optimization"], "authors": ["Arash Ardakani", "Carlo Condo", "Warren J. Gross"], "authorids": ["arash.ardakani@mail.mcgill.ca", "carlo.condo@mail.mcgill.ca", "warren.gross@mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959480413, "id": "ICLR.cc/2017/conference/-/paper71/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper71/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper71/AnonReviewer1", "ICLR.cc/2017/conference/paper71/AnonReviewer3"], "reply": {"forum": "r1fYuytex", "replyto": "r1fYuytex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper71/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959480413}}}], "count": 16}