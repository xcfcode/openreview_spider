{"notes": [{"id": "rJlg1n05YX", "original": "Sklkvtp5Km", "number": 957, "cdate": 1538087896451, "ddate": null, "tcdate": 1538087896451, "tmdate": 1545355429645, "tddate": null, "forum": "rJlg1n05YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Penetrating the Fog: the Path to Efficient CNN Models", "abstract": "With the increasing demand to deploy convolutional neural networks (CNNs) on mobile platforms, the sparse kernel approach was proposed, which could save more parameters than the standard convolution while maintaining accuracy. However, despite the great potential, no prior research has pointed out how to craft an sparse kernel design with such potential (i.e., effective design), and all prior works just adopt simple combinations of existing sparse kernels such as group convolution. Meanwhile due to the large design space it is also impossible to try all combinations of existing sparse kernels. In this paper, we are the first in the field to consider how to craft an effective sparse kernel design by eliminating the large design space. Specifically, we present a sparse kernel scheme to illustrate how to reduce the space from three aspects. First, in terms of composition we remove designs composed of repeated layers. Second, to remove designs with large accuracy degradation, we find an unified property named~\\emph{information field} behind various sparse kernel designs, which could directly indicate the final accuracy. Last, we remove designs in two cases where a better parameter efficiency could be achieved. Additionally, we provide detailed efficiency analysis on the final 4 designs in our scheme. Experimental results validate the idea of our scheme by showing that our scheme is able to find designs which are more efficient in using parameters and computation with similar or higher accuracy.", "keywords": ["Efficient CNN models", "Computer Vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "shuyang1995@ucsb.edu", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Shu Yang", "Yufei Ding"], "TL;DR": "We are the first in the field to show how to craft an effective sparse kernel design from three aspects: composition, performance and efficiency.", "pdf": "/pdf/09dd80df69fc22779a3fa0bb14dedb8f31fa2232.pdf", "paperhash": "wan|penetrating_the_fog_the_path_to_efficient_cnn_models", "_bibtex": "@misc{\nwan2019penetrating,\ntitle={Penetrating the Fog: the Path to Efficient {CNN} Models},\nauthor={Kun Wan and Boyuan Feng and Shu Yang and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlg1n05YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJxKk2wEeV", "original": null, "number": 1, "cdate": 1545006049406, "ddate": null, "tcdate": 1545006049406, "tmdate": 1545354486652, "tddate": null, "forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper957/Meta_Review", "content": {"metareview": "This paper points out methods to obtain sparse convolutional operators. The reviewers have a consensus on rejection due to clarity and lack of support to the claims.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "lack of support"}, "signatures": ["ICLR.cc/2019/Conference/Paper957/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper957/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Penetrating the Fog: the Path to Efficient CNN Models", "abstract": "With the increasing demand to deploy convolutional neural networks (CNNs) on mobile platforms, the sparse kernel approach was proposed, which could save more parameters than the standard convolution while maintaining accuracy. However, despite the great potential, no prior research has pointed out how to craft an sparse kernel design with such potential (i.e., effective design), and all prior works just adopt simple combinations of existing sparse kernels such as group convolution. Meanwhile due to the large design space it is also impossible to try all combinations of existing sparse kernels. In this paper, we are the first in the field to consider how to craft an effective sparse kernel design by eliminating the large design space. Specifically, we present a sparse kernel scheme to illustrate how to reduce the space from three aspects. First, in terms of composition we remove designs composed of repeated layers. Second, to remove designs with large accuracy degradation, we find an unified property named~\\emph{information field} behind various sparse kernel designs, which could directly indicate the final accuracy. Last, we remove designs in two cases where a better parameter efficiency could be achieved. Additionally, we provide detailed efficiency analysis on the final 4 designs in our scheme. Experimental results validate the idea of our scheme by showing that our scheme is able to find designs which are more efficient in using parameters and computation with similar or higher accuracy.", "keywords": ["Efficient CNN models", "Computer Vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "shuyang1995@ucsb.edu", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Shu Yang", "Yufei Ding"], "TL;DR": "We are the first in the field to show how to craft an effective sparse kernel design from three aspects: composition, performance and efficiency.", "pdf": "/pdf/09dd80df69fc22779a3fa0bb14dedb8f31fa2232.pdf", "paperhash": "wan|penetrating_the_fog_the_path_to_efficient_cnn_models", "_bibtex": "@misc{\nwan2019penetrating,\ntitle={Penetrating the Fog: the Path to Efficient {CNN} Models},\nauthor={Kun Wan and Boyuan Feng and Shu Yang and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlg1n05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper957/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353021046, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper957/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper957/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper957/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353021046}}}, {"id": "B1x1mxD5h7", "original": null, "number": 1, "cdate": 1541201942895, "ddate": null, "tcdate": 1541201942895, "tmdate": 1542576031078, "tddate": null, "forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper957/Official_Review", "content": {"title": "this paper gives a recipe for efficient CNN, especially tailored for mobile platforms", "review": "The paper considers sparse kernel design in order to reduce the space complexity of  a convolutional neural network. In specifics, the proposed procedure is composed of following steps: 1) remove repeated layers, 2) remove designs with large degradation design, and 3) further remove design for better parameter efficiency.\n\nThe paper proposed the composition of group convolution, pointwise convolution, and depthwise convolution  for the sparse kernel design, which seems pretty promising. In addition, the authors discussed the efficiency of each convolution compositions.\n\nI failed to appreciate the idea of information field, I didn't understand the claims that \"For one output tensor, sizes of information fields for all activations are usually the same\". When introducing a new concept, it's very important to make it clear and friendly. The author could consider more intuitive, high level, explanation, or some graphic demonstrations. Also, I couldn't see why this notion is important in the rest of the paper.\n\nPersonally I'm so confused by the theorem. It looks like a mathematical over-claim to me. It claims that the best efficiency is achieved when M N = C. However, is it always the case? What is M N \\neq C? What does the theorem mean for real applications?\n\nAll the reasoning and derivation are assuming the 3 x 3 spatial area and 4 way tensor. I would assume these constant are not important, the paper could be much stronger if there is a clear notion of general results.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper957/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Penetrating the Fog: the Path to Efficient CNN Models", "abstract": "With the increasing demand to deploy convolutional neural networks (CNNs) on mobile platforms, the sparse kernel approach was proposed, which could save more parameters than the standard convolution while maintaining accuracy. However, despite the great potential, no prior research has pointed out how to craft an sparse kernel design with such potential (i.e., effective design), and all prior works just adopt simple combinations of existing sparse kernels such as group convolution. Meanwhile due to the large design space it is also impossible to try all combinations of existing sparse kernels. In this paper, we are the first in the field to consider how to craft an effective sparse kernel design by eliminating the large design space. Specifically, we present a sparse kernel scheme to illustrate how to reduce the space from three aspects. First, in terms of composition we remove designs composed of repeated layers. Second, to remove designs with large accuracy degradation, we find an unified property named~\\emph{information field} behind various sparse kernel designs, which could directly indicate the final accuracy. Last, we remove designs in two cases where a better parameter efficiency could be achieved. Additionally, we provide detailed efficiency analysis on the final 4 designs in our scheme. Experimental results validate the idea of our scheme by showing that our scheme is able to find designs which are more efficient in using parameters and computation with similar or higher accuracy.", "keywords": ["Efficient CNN models", "Computer Vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "shuyang1995@ucsb.edu", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Shu Yang", "Yufei Ding"], "TL;DR": "We are the first in the field to show how to craft an effective sparse kernel design from three aspects: composition, performance and efficiency.", "pdf": "/pdf/09dd80df69fc22779a3fa0bb14dedb8f31fa2232.pdf", "paperhash": "wan|penetrating_the_fog_the_path_to_efficient_cnn_models", "_bibtex": "@misc{\nwan2019penetrating,\ntitle={Penetrating the Fog: the Path to Efficient {CNN} Models},\nauthor={Kun Wan and Boyuan Feng and Shu Yang and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlg1n05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper957/Official_Review", "cdate": 1542234338510, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper957/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335841226, "tmdate": 1552335841226, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper957/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BylleIdbaX", "original": null, "number": 3, "cdate": 1541666280133, "ddate": null, "tcdate": 1541666280133, "tmdate": 1541666280133, "tddate": null, "forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper957/Official_Review", "content": {"title": "Interesting topic but the paper is not well explained", "review": "This paper addressed an interesting problem of reducing the kernel to achieve CNN models, which is important and attracts lots of research work. However, the methods don't have very good justifications. \nFor example, in Section 3.1, the authors mentioned that \"Specifically, in normal CNNs it is quite common to have multiple stages/blocks which contain repeated patterns such as layers or structures.\" It is still unclear why it is better to replace these so-called repeated patterns. \nThe defined \"information field\" is not clearly explained and the benefits are also not demonstrated.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper957/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Penetrating the Fog: the Path to Efficient CNN Models", "abstract": "With the increasing demand to deploy convolutional neural networks (CNNs) on mobile platforms, the sparse kernel approach was proposed, which could save more parameters than the standard convolution while maintaining accuracy. However, despite the great potential, no prior research has pointed out how to craft an sparse kernel design with such potential (i.e., effective design), and all prior works just adopt simple combinations of existing sparse kernels such as group convolution. Meanwhile due to the large design space it is also impossible to try all combinations of existing sparse kernels. In this paper, we are the first in the field to consider how to craft an effective sparse kernel design by eliminating the large design space. Specifically, we present a sparse kernel scheme to illustrate how to reduce the space from three aspects. First, in terms of composition we remove designs composed of repeated layers. Second, to remove designs with large accuracy degradation, we find an unified property named~\\emph{information field} behind various sparse kernel designs, which could directly indicate the final accuracy. Last, we remove designs in two cases where a better parameter efficiency could be achieved. Additionally, we provide detailed efficiency analysis on the final 4 designs in our scheme. Experimental results validate the idea of our scheme by showing that our scheme is able to find designs which are more efficient in using parameters and computation with similar or higher accuracy.", "keywords": ["Efficient CNN models", "Computer Vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "shuyang1995@ucsb.edu", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Shu Yang", "Yufei Ding"], "TL;DR": "We are the first in the field to show how to craft an effective sparse kernel design from three aspects: composition, performance and efficiency.", "pdf": "/pdf/09dd80df69fc22779a3fa0bb14dedb8f31fa2232.pdf", "paperhash": "wan|penetrating_the_fog_the_path_to_efficient_cnn_models", "_bibtex": "@misc{\nwan2019penetrating,\ntitle={Penetrating the Fog: the Path to Efficient {CNN} Models},\nauthor={Kun Wan and Boyuan Feng and Shu Yang and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlg1n05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper957/Official_Review", "cdate": 1542234338510, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper957/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335841226, "tmdate": 1552335841226, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper957/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syejycz037", "original": null, "number": 2, "cdate": 1541446115275, "ddate": null, "tcdate": 1541446115275, "tmdate": 1541533546952, "tddate": null, "forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "invitation": "ICLR.cc/2019/Conference/-/Paper957/Official_Review", "content": {"title": "A good start", "review": "Standard dense 2D convolution (dense in space and channels) may waste parameters. This paper points out the many ways that sparser convolutional operators (\u201ckernels\u201d) may be combined into a single combined operator that may be used in place of dense convolution.\n\nThe paper waxes grandiose about the exponentially many ways that operations may be combined but then defines and tries only four. While trying four alternatives may be quite interesting, the paper could have avoided grandiose language by just stating: \u201cWe tried four things. If you restrict yourself to kernels with 3x3 receptive field and no repeated operations <and probably other assumptions>, there are only four unique combinations to be tried.\u201d Perhaps a page of text could have been saved.\n\nThe paper also defines \u201cinformation field\u201d as the product of the operator\u2019s (spatial) receptive field and the number of channels that each unit can see. Authors proceed to make broad claims about how information field is an important concept that predicts performance. While this may indeed turn out to be an important concept, it is not shown as such by the paper.\n\nClaims:\n\n\u201c\u2026we identify a easily measurable quantity named information field behind various sparse kernel designs, which is closely related to the model accuracy.\u201d\n\n\u201cDuring the process to reduce the design space, we find an unified property named information field behind various designs, which could directly indicate the final accuracy.\u201d\n\nBut the paper does not substantiate these claims.\n\nSince information field is defined as the product of the receptive field and the number of channels seen, it would seem necessary to show, say, at least some experiments with varying receptive field sizes and number of channels. Then it might be shown, for example, that across a wide array of network sizes, widths, depths, holding all but information field constant, information field is predictive of performance. But these experiments are not done.\n\nReceptive fields: the paper *only ever tries 3x3 receptive fields* (Table 2, 3, 4). So absolutely no support is given for the relevance of two out of the three components (i size, j size) comprising information field!\n\nNumber of channels: as far as I can tell, Table 2 and 3 contain the only results in this direction. Reading off of Table 2: for networks of the same depth (98), info size 256 works a bit better than 128*, and 512 works a bit better than 256. \n\n* (see also Table 3 lines 4 vs 5 show the same 256 vs 128 effect.)\n\nCool. But *two comparisons* are not even close to enough to support the statement \u201cwe find an unified property named information field behind various designs\u201d. It is enough to support the statement \u201cfor this single network we tried and using 3x3 receptive fields, we found that letting units see more channels seemed to help.\u201d Unfortunately, this conclusion on its own is not a publishable result.\n\n\n\nTo make this paper great, you will have to close the gap between what you believe and what you have shown.\n\n(1) You believe that information field is predictive of accuracy. So show it is predictive of accuracy across sufficiently many well-controlled experiments.\n\n(2) It may also be that the PWGConv+DW+PWGConv combination is a winning one; in this case, show that swapping it in for standard convolution helps in a variety of networks (not just ResNet) and tasks (not just ImageNet).\n\n\n\nOther minor notes:\n\n - Equations are critical in some parts of some papers, but e.g. triple nested sums probably aren\u2019t the easiest way of describing group convolution.\n\n - The part about regexes seemed unnecessary. If 1000 different designs were tried in a large automated study where architectures were generated and pruned automatically, this detail might be important (but put it in SI). But if only four are tried this detail isn\u2019t needed: we can see all four are different.\n\n - Figure 1 is a great diagram!\n\n - How efficient are these kernels to compute on the GPU? Include computation time.\n\n - \u201cEfficiency given the total amount of parameters.\u201d These equations and scaling properties seemed to miss the point. For example, \u201cIt can be easily verified that given the total number of parameters the greatest width is reached when the best efficiency is achieved.\u201d This is just saying that standard convolution scales poorly as F -> infinity. This doesn\u2019t seem like the most useful definition of efficiency. A better one might be \u201cHow many params do you need to get to x% accuracy on ImageNet?\u201d Then show curves (# of params vs accuracy) for variants of a few popular model architectures (like ResNet or Xception with varying width and depth).\n\n - 3.3.2: define M and N\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper957/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Penetrating the Fog: the Path to Efficient CNN Models", "abstract": "With the increasing demand to deploy convolutional neural networks (CNNs) on mobile platforms, the sparse kernel approach was proposed, which could save more parameters than the standard convolution while maintaining accuracy. However, despite the great potential, no prior research has pointed out how to craft an sparse kernel design with such potential (i.e., effective design), and all prior works just adopt simple combinations of existing sparse kernels such as group convolution. Meanwhile due to the large design space it is also impossible to try all combinations of existing sparse kernels. In this paper, we are the first in the field to consider how to craft an effective sparse kernel design by eliminating the large design space. Specifically, we present a sparse kernel scheme to illustrate how to reduce the space from three aspects. First, in terms of composition we remove designs composed of repeated layers. Second, to remove designs with large accuracy degradation, we find an unified property named~\\emph{information field} behind various sparse kernel designs, which could directly indicate the final accuracy. Last, we remove designs in two cases where a better parameter efficiency could be achieved. Additionally, we provide detailed efficiency analysis on the final 4 designs in our scheme. Experimental results validate the idea of our scheme by showing that our scheme is able to find designs which are more efficient in using parameters and computation with similar or higher accuracy.", "keywords": ["Efficient CNN models", "Computer Vision"], "authorids": ["kun@cs.ucsb.edu", "boyuan@cs.ucsb.edu", "shuyang1995@ucsb.edu", "yufeiding@cs.ucsb.edu"], "authors": ["Kun Wan", "Boyuan Feng", "Shu Yang", "Yufei Ding"], "TL;DR": "We are the first in the field to show how to craft an effective sparse kernel design from three aspects: composition, performance and efficiency.", "pdf": "/pdf/09dd80df69fc22779a3fa0bb14dedb8f31fa2232.pdf", "paperhash": "wan|penetrating_the_fog_the_path_to_efficient_cnn_models", "_bibtex": "@misc{\nwan2019penetrating,\ntitle={Penetrating the Fog: the Path to Efficient {CNN} Models},\nauthor={Kun Wan and Boyuan Feng and Shu Yang and Yufei Ding},\nyear={2019},\nurl={https://openreview.net/forum?id=rJlg1n05YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper957/Official_Review", "cdate": 1542234338510, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJlg1n05YX", "replyto": "rJlg1n05YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper957/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335841226, "tmdate": 1552335841226, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper957/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}