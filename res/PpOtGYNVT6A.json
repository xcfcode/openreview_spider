{"notes": [{"id": "PpOtGYNVT6A", "original": "ot-kd8mxyTC", "number": 3212, "cdate": 1601308356829, "ddate": null, "tcdate": 1601308356829, "tmdate": 1614985741696, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xdNb7fVbvtu", "original": null, "number": 1, "cdate": 1610040396145, "ddate": null, "tcdate": 1610040396145, "tmdate": 1610473991313, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Reviewers agree that this is a very promising paper, with an excellent overview of existing techniques for semi-supervised and neuro-symbolic learning. However, reviewers also agree that the paper is not ready. With one more revision for clarity, some limited empirical validation and illustration of the theory, and focus on the essential message, this could become a seminal paper for our understanding of semi-supervised learning. Luckily the reviews provided ample feedback, and the authors should be able to submit a very competitive paper next time around."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040396130, "tmdate": 1610473991292, "id": "ICLR.cc/2021/Conference/Paper3212/-/Decision"}}}, {"id": "jsJLJPLJZA", "original": null, "number": 2, "cdate": 1603102065740, "ddate": null, "tcdate": 1603102065740, "tmdate": 1606754858492, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review", "content": {"title": "promises more than it delivers", "review": "The paper aims at proposing a theoretical rationale for discriminative semi-supervised learning that is comparable with that of generative models. Moreover,\nthe paper aims at theoretically justifying a family of neuro-symbolic SSL approaches.\nFor the first task, the paper states that the proposal justifies entropy minimisation (Grandvalet &\nBengio, 2005), mutual exclusivity (Sajjadi et al., 2016a; Xu et al., 2018) and pseudo-labelling (Lee,\n2013).\nFor the second task, the paper states that the proposal justifies  (Serafini & Garcez, 2016; van Krieken et al., 2019, Marra et al., 2019, Xu et al., 2018).\n\nWith respect to the first task, while I agree the the paper provides a justification for mutual exclusivity (Sajjadi et al., 2016a) \nand pseudo-labelling (Lee, 2013), I have doubts on entropy minimisation (Grandvalet & Bengio, 2005):\nhow do you model an entropy with terms of the form $-\\sum\\log p(\\theta|\\alpha)$?\nThe authors also say that previous discriminative SSL lack theoretical justification but (Xu et al., 2018) proves that their loss is a consequence of a \nnumber of assumptions, thus theoretically justifying it.\n\nThe paper misunderstood the scope of neuro-symbolic integration (NeSy): it is not the integration of statistics and logic but the integration of \nneural networks with (possibly probabilistic) logic. The integration of statistics and logic is well established in the field of\nStatistical Relational Learning.  For example, (De Raedt et al., 2007) is not NeSy while (Manhaeve et al., 2018) is.\nAs such, the paper does not provide \"a theoretically principled understanding of integrating \u2018connectionist\u2019 and \u2018symbolic\u2019 methods.\" in general, only\nfor some specific methods (not for example for (Manhaeve et al., 2018)). In fact, the justification is only for NeSy that are based on fuzzy logic, \nwhich is much simpler to manipulate than probabilistic logic as in (Manhaeve et al., 2018). Fuzzy logic is not a probabilistic logic because the\nfirst is truth functional (the value of A and B depends only on the values of A and B) while the latter is not (A and B depends on whether A and B are\nindependent, mutually exclusive,...). Moreover, (Xu et al., 2018) is more than simply impositive mutual exclusivity as it  also allow a form of probabilistic reasoning by \nrepresenting the loss formula as an arithmetic circuit obtained with automated reasoning. So overall I think that the paper claims more than it effectively provides.\n\nThe presentation of the paper makes following the exposition unnecessarily difficult. \nFirst, the notation is confusing, with a nonuniform representation\nof vectors, which are sometimes bold and sometimes not ($\\mathbb{z}$ and $\\theta$), while sometimes also scalars are bold ($\\mathbb{z}_k$).\nMoreover, random variables and their values are also not typographically distinct.\nAll integrals have the integrating variables as subscripts of the integral symbol instead of as $dxdy$...\nIn Figure 1 the left and center subfigures are equal, I guess in the left one x and y should be exchanged.\nFigure 1 left should represent the model of eq 1 but $\\psi$ and $\\pi$ are absent from the figure.\n\nIn eq. 6, shouldn't the integrating variable be $\\tilde{\\theta}^X$?\nIt is not clear how you derive the formulas [Gen.] from eq 1: since Figure 1 does not show parameters\n$\\psi$ and $\\pi$, it is difficult to judge the conditional independences among variables.\nThe symbol $\\Delta^{2^K}$ is confusing since it can also be interpreted as the simplex over a set with $2^K$ elements. Better\n$(\\Delta^2)^K$.\nYou say that Figure 3 represents a mix of two univariate Gaussians but the variance of the two Gaussians do not seem equal: if only\nthe mean changes, then the Gaussians should have the same height.\nThe paper mentions Table 4 which is absent from the paper.\n\nMinor comments:\n\"proportionality constant obsolete\"->\"proportionality constant irrelevant\"\nA the bottom of page 6, last formula: the subscript of the sum should be $\\mathbb{y}$\nPage 7 \"Note that each $\\theta\\in\\{0,1\\}^K\\subseteq \\Delta^{2^K}$ (from a continuous space): $\\theta$ does not seem from a continuous space.\nBibliography: reference Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning. IEEE\nTransactions on Neural Networks, 20(3):542\u2013542, 2009. is a book review, you want to cite the book, not the review\nIn Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Broeck. A semantic loss function for\ndeep learning with symbolic knowledge. In International Conference on Machine Learning, 2018.\nthe last author is Guy van den Broeck\n\n\n---After reading the other reviews and the authors' comments, I sill think that the paper promises more than it delivers, even if the paper was extensively rewritten as a consequence of many problems in the original version, so I will keep my score.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080025, "tmdate": 1606915771509, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3212/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review"}}}, {"id": "KZVXlj7iPbr", "original": null, "number": 10, "cdate": 1606242274221, "ddate": null, "tcdate": 1606242274221, "tmdate": 1606242274221, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "2025ctAZgzK", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "Response to rebuttal feedback", "comment": "Thank you for your further response. Please note:\n\n1. [re: Distinguishing from prior works in SSL] we include 2 pages of Background material, in which we summarise: \n     - existing theory behind Semi-Supervised Learning,\n     - example SSL methods that are covered by our theoretical model; and \n     - relevant Neuro-symbolic approaches, including relevant high-level perspectives from the less recent (e.g. Valiant (2000), see Fig 2) through to very recent (e.g. Garcez at al (2019)).  \n\n   Our work exclusively addresses SSL, providing a theoretical explanation that unifies families of methods from both traditional SSL and neuro-symbolic SSL, hence we devote space to providing sufficient related background for the paper to be broadly self-contained.\n\n   We carefully distinguish the proposed model for SSL from prior theoretical explanations, showing a novel symmetry with the more familiar generative model for SSL (see equations labelled [Gen] & [Disc], Sec 3).\n\n2. [re: Distinguishing from prior works in Bayesian learning] we do not *distinguish* from Bayesian learning as such, since we propose a Bayesian model, but we are unaware of this model being used to explain semi-supervised learning or neuro-symbolic SSL anywhere in the literature (including the several extensive SSL reviews referenced). The only Bayesian explanation for SSL that we are aware of is the generative model with which we draw direct contrast (noted above).\n\n3. [re: References mentioned previously] There are many machine learning works with which comparisons might be drawn (e.g. any that involve supervised model with a prior) but it is not possible to discuss all such works. However, as suggested, we acknowledge that a broader backdrop exists. Note that, of the references suggested previously:\n   - Mei et al. (2014) impose spike-and-slab-type regularising constraints in a latent variable model that applies approximate inference for unsupervised learning (i.e. LDA topic modelling). The authors motivate their quasi-bayesian approach on the basis that when minimising KL divergence between true & approximate posteriors, \"it is often difficult to make sure that the posterior satisfies all domain knowledge constraints\". In contrast, we consider a fully Bayesian model in which domain knowledge constraints correspond to mathematical properties of the prior, i.e. restricting its support. Where the prior's support is zero, so too is any posterior's, which the considered MAP approaches maximise. We have expanded the suggested reference to summarise this, but do not include further detail since they do not consider (i) semi-supervised learning, (ii) the effect of y|x being deterministic or (iii) how logical rules directly manifest in the prior. \n   - Tsochantaridis et al. (2004) considers various structured output spaces, but focuses on adapting large-margin classifiers (SVMs) to such output spaces (e.g. formulating dual programs) and is inherently non-probabilistic in nature. It is difficult to see how it relates to our probabilistic explanation for semi-supervised learning.\n   - Cesa-Bianchi et al. (2006) focus on making \"hierarchical classifications\" over a taxonomy, devise a loss function specific to taxonomic data and consider its theoretical properties. We find it difficult to make any insightful comparison between that work and our own (we do not focus on hierarchy or any *particular* logical relationship between label attributes).\n   - Zhang & Zhou (2014) provide a general review of \"multi-label\" learning, i.e. where label features may co-occur. A main focus of the paper is to categorising approaches (of the time), primarily by the number of label attributes between which interactions are considered. Reviewed methods include nearest-neighbour, decision tree and SVM approaches. There is little clear relationship to any of our work beyond that both consider multi-label leaning to an extent. We reference them in that regard (as suggested).\n\n4. [re: working example & experiments] We have made the running example of multi-class classification more prevalent throughout the theory (Sec 3), to give more clear intuition. We have also included an explicit simple diagram in the neuro-symbolic part (Sec 4) to make this more tangible. So there are *two* working examples. Further, various specific methods (e.g. entropy minimisation, mutual exclusivity, semantic loss) are explained and shown to be instantiations of our general theoretical framework. Each of those works include experiments that demonstrate their relative performance on a number of tasks. As such, there is significant existing empirical evidence that these methods \"work\", we provide a unifying theoretical basis to give clearer insight why. Similarly with neuro-symbolic SSL, several works show that using logical rules as a regulariser improves performance, but mathematical understanding of how such rules fit with a probabilistically interetable supervised learning loss function has been lacking."}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "2025ctAZgzK", "original": null, "number": 9, "cdate": 1606222730895, "ddate": null, "tcdate": 1606222730895, "tmdate": 1606222730895, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "_JhSZwk6Ms1", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "Feedback after reading the rebuttal", "comment": "Thank you for the detailed clarification, and I appreciate the authors' revision. However, I still think the presented framework is not distinguished from prior works in SSL and Bayesian learning. The lack of working example and experiment weakens the contribution of this work. Therefore I will keep my rating."}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "ClLRXmerarC", "original": null, "number": 7, "cdate": 1606126147932, "ddate": null, "tcdate": 1606126147932, "tmdate": 1606126147932, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "vA3KLg0nWY6", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "response to answer 1", "comment": "Considering the (K-dimensional) prediction $\\theta$ (drop superscript here, just for ease of notation) for a single unlabelled data sample $x_j$, the min entropy term (Eq 2) is: \n   - $ \n     -\\sum_k \\theta_k \\log \\theta_k   \n=   -\\sum_k  \\log \\theta_k^{\\theta_k}    \n=   -\\log \\prod_k\\theta_k^{\\theta_k}\n$\n\nEquating this to the $-\\log p(\\theta|\\alpha)$ term in Eq (7) shows that $\\prod_k\\theta_k^{\\theta_k}$ describes the implied form of $p(\\theta|\\alpha)$ (up to proportionality) assumed under min entropy.  Importantly, this function is convex over [0,1] and maximal when each $\\theta_k$ is 0 or 1, hence it acts as a relaxation of the \"discrete\" distribution under the assumption of deterministic $y|x$."}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "vA3KLg0nWY6", "original": null, "number": 6, "cdate": 1606119552816, "ddate": null, "tcdate": 1606119552816, "tmdate": 1606119552816, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "5sPI_yMITMG", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "answer 1", "comment": "I dont understand the formula \\prod_k (\\theta_k)^{\\theta_k} , are you sure that it is correct? You exponentiate a parameter vectore by itself?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "5sPI_yMITMG", "original": null, "number": 4, "cdate": 1606077354720, "ddate": null, "tcdate": 1606077354720, "tmdate": 1606078652275, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "jsJLJPLJZA", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "Response to Reviewer 2 ", "comment": "Many thanks for your review. We address each of your comments below and in the revised paper.\n\n1. \u201centropy minimisation (Grandvalet & Bengio, 2005): how do you model an entropy with terms of the form -\\sum\\log p(\\theta | \\alpha)?\u201d\n    - Under gradient based optimisation, any strictly convex function over the simplex that is maximal at its vertices (ie for one-hot vectors) serves as a relaxation of the true distribution p(\\theta), (a weighted sum of delta functions at simplex vertices). The negative entropy of a multinomial is known to satisfy those requirements. Mathematically: let p(\\theta | alpha) \\propto \\prod_k (\\theta_k)^{\\theta_k} aligns Eqs 2, 6.\n\n2. \u201c(Xu et al., 2018) proves that their loss is a consequence of a number of assumptions, thus theoretically justifying it\u201d\n    - Agreed, existing works have separate rationale (which we make more clear), whereas we unify those works under a single probabilistic model. Interesting future work might consider which axioms of Xu et al are satisfied by *any* relaxation of the prior, our g(\\theta).\n\n3. \u201cThe paper misunderstood the scope of neuro-symbolic integration\u201d\n    - Understood, we have addressed this.\n\n4. \u201cthe paper does not provide \"a theoretically principled understanding of integrating \u2018connectionist\u2019 and \u2018symbolic\u2019 methods\u201d in general, only for some specific methods\"\n    - We did not intend this to read so broadly (emphasis was meant on \u201c*a* theoretically\u2026\u201d), we have reworded to clarify and limit the claim.\n\n5. \u201c(Xu et al., 2018) is more than mutual exclusivity, it also allow a form of probabilistic reasoning\u201d\n    - Agreed, Xu et al. covers mutual exclusivity (Sec 4) & logical reasoning (Sec 5), so we reference it for both, showing that it can be interpreted under a probabilistic framework and unified with other approaches.\n\n6. \"notation is confusing\"\n   - We have revised the notation to be more consistent, in particular (non-italicised) \\uptheta is used for the random variable, consistent with other RVs.\n\n7. \"All integrals have the integrating variables as subscripts of the integral symbol instead of as dxdy\"\n    - We believe this is fairly common shorthand notation, e.g. Barber, \u201cBayesian Reasoning & Machine Learning\u201d. However, we will review this for the camera ready version (if necessary).\n\n8. (i) \"Figure 1 left should represent the model of eq 1\" (ii) \"In eq. 6, shouldn't the integrating variable be \\tilde\\theta^X?\"\n    - Agreed, amended\n\n9. \"It is not clear how you derive the formulas [Gen.] from eq 1\"\n    - The rearrangement is now explained in Appendix B.\n\n10. \"The symbol \\Delta^{2^K} is confusing since it can also be interpreted as the simplex over a set with 2^K elements\"\n    - Without assuming attribute independence, there are 2^K distinct labels and an arbitrary multinomial distribution over them has (2^K) - 1 free parameters. Assuming attribute independence reduces the domain to [0,1]^K hypercube (isomorphic to a region of the simplex).\n\n11. \"Figure 3 the Gaussians should have the same height.\"\n    - The plots show p(x,y) = p(x|y)p(y) and so are Gaussians weighted by p(y).\n\n12. (i) \"The paper mentions Table 4 which is absent.\" (ii) \"obsolete\"->\"irrelevant\". (iii) \u201cpage 6, last formula: subscript should be \ud835\udd6a\u201c\n    - Amended.\n\n13. \u201cPage 7 Note that each \\theta \\in {0,1}^K \\subseteq \\Delta^(2^K) (from a continuous space):  does not seem from a continuous space\u201d\n    - Clarified.\n\n14. \"Bibliography: references to Chapelle et al., Xu et al.\"\n    - Amended."}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "qMV_4clWQo", "original": null, "number": 5, "cdate": 1606077856711, "ddate": null, "tcdate": 1606077856711, "tmdate": 1606077856711, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "tY6jOWxVsgx", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Many thanks for your review. We have updated the paper throughout to make the central idea more accessible and the connection between SSL neuro-symbolic methods more clear. We have also included (Fig. 5) a simple example of how a set of logical rules can translate into an approximation to the distribution p(theta) that enables semi-supervised learning."}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "_JhSZwk6Ms1", "original": null, "number": 3, "cdate": 1606076576375, "ddate": null, "tcdate": 1606076576375, "tmdate": 1606077579207, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "Ri6de9pFn9a", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Many thanks for your review. We address each of your comments below and in the revised paper.\n1. \u201cwriting of this paper could be improved\u201d\n    - Secs 1-3 have been re-worded for greater clarity.\n2. \u201cThe meaning of parameter \\theta^x is not clearly explained ... the authors should improve the notations and provide some simple examples\u201d\n    - We have made this more clear. p(Y|x) is a distribution over (all) labels for a given sample of x (note: no two distributions p(Y|x_i), p(Y|x_j) need be the same). Each distribution is defined by a parameter \\theta^x (e.g. a vector on the simplex in the case of multi-class classification). Since each distribution is specific to some x, we use that x to index (by a superscript) the corresponding parameter. \n    - For clarity: we now never use \u201cparameter\u201d to refer to the \u201cweights\u201d of a model, only ever parameters of a probability distribution (e.g. its mean). \n    - Under a hierarchical Bayesian model, we consider the distribution p(\\theta) over those parameters (superscript x is dropped when referring to the random variable, which is now denoted \\uptheta for greater clarity). \n    - We denote estimates of parameter (e.g. as output by a model) with \\tilde. \n    - Throughout, we focus on the special case where each x only ever occurs with a single y, rather than perhaps appearing with several distinct y_1, y_2, y_3 in the dataset (e.g. as possible in the mixture of Gaussians case). In this case, every distribution p(Y|x) is a delta distribution and so can be parameterised by a single label y  (or its one-hot equivalent), i.e. \\theta^x=y  (hence the 1-1 correspondence between \\theta^x and y mentioned).\n\n3. \u201cusing logic rules as constraints (or other non-differentiable constraints) on label space is not a novel idea\u201d. \n    - Agreed. Note, we do not introduce logical rules as a constraint. Rather, for a family of existing SSL methods that do, we show that they fall under a unifying probabilistic model for DSSL.\n\n4. \u201cIn [multi-class learning & multi-label learning], there are many works talking about constraining the label space with prior knowledge \u2026 the authors should discuss the relationship between this work and those early works in statistical learning\u201d\n    - Thankyou for the references. The paper is updated to acknowledge extensive prior work in related areas. Since our work specifically addresses SSL to unify specific families of methods under a probabilistic model, we restrict our detailed review to works most closely related. \n\n5. \u201cthis paper lacks experiments to support the authors' claims\u201d\n    - Our work is completely theoretic, describing a new theoretical explanation for an approach to SSL that also provides a unifying probabilistic rationale for two families of existing methods including a theoretical link between neuro-symbolic and statistical machine learning methods. We have added an example into the neuro-symbolic section to give clearer insight.\n\n6. \u201cprovides little insight into this area\u201d\n    - We have reworded the paper to make insights more clear.\n\n7. \u201cinclude some examples\u201d\n    - We have made the running example of multi-class classification more clear in Sec 3. Figure 3 also aims to give a visual understanding how p(x) corresponds to p(\\theta). We have also added an example to the neuro-symbolic section (new Fig. 5).\n\n8. \"The authors propose three methods to compute the non-differentiable ...\"\n    - We have removed this section in order to make the text more clear and to include Fig. 5. The reviews have shown that it is most fundamental to make the central idea of the paper more clear, i.e. how the single probabilistic model unifies families of models and those families (from SSL & NSL) with each other.\n\n9. \u201cThe first two sub-figures in Fig. 1 are the same.\u201d\n    - Thanks and apologies. An error in the latex referenced the wrong subfigure.\n\n10. \u201cThe first should be \\tilde\\theta^x.\u201d\n    - Agreed, amended.\n\n11. \u201c\\alpha ... no explanation about what is it.\u201d\n    - Agreed, amended."}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "8Z-bMoTKKEz", "original": null, "number": 2, "cdate": 1606075665999, "ddate": null, "tcdate": 1606075665999, "tmdate": 1606075665999, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "bjrSuO8tCpn", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Many thanks for your review, we address each of your comments below and in the revised paper:\n\n1. \u201cnot clear in terms of concrete contributions \u2026 connection with neuro-symbolic learning is interesting but feels a bit much; why is it needed in this framework?\u201d\n    - We have revised the paper to be more clear and connected, including adding an example into the neuro-symbolic section.\n    - We show that several discriminative SSL methods can be theoretically justified and unified by a single probabilistic model in which a prior over the parameters of p(y|x) encodes the assumption that y|x is deterministic (each x has only one label). Example: in multiclass classification, such parameters (\\theta^x) are vectors on the simplex. Given test input x, a typical classifier can output any simplex vector, whereas the imposed prior confines parameters to the simplex vertices (i.e. one-hot), a continuous relaxation of that \u201cencourages\u201d model predictions to be one-hot. \nThe DSSL methods considered each have rationale (e.g. minimising entropy) but we show that *all* approximate the \u201cdeterministic\u201d prior and so are unified under one probabilistic model.\n    - Considering *the same* probabilistic model when labels are binary vectors, an equivalent deterministic prior over the parameter space again restricts parameters to its vertices. If logical rules determine attribute combinations, they restrict the prior to \u201cvalid\u201d vertices and so can be \u201cnaturally\u201d built into the prior in a rigorously justified way.\n    - Summarising: *one probabilistic model* unifies a family of DSSL methods *and* a family of neuro-symbolic SSL methods. We believe it is important to include both to show that connection.\n\n2. \u201clong introduction \u2026 quickly goes through Expression (6)\u201d\n    - Secs 1-3 have been reworded for greater clarity.\n\n3. \u201cpoint of Table 1?\u201d\n    - Table 1 shows combinations of data attributes (with examples) that dictate the form of p(\\theta), i.e. its specific form in an SSL loss function. We explore several cases (now italicised) in detail. We have made this more clear in the paper.\n\n4. \u201cFigure 1 left and center identical\u201d\n    - Thanks, amended.\n\n5. \u201c(Van Engelen and Hoos 2020) in the references?\u201d\n    - Yes, 5th from last, p.10"}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PpOtGYNVT6A", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3212/Authors|ICLR.cc/2021/Conference/Paper3212/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839920, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Comment"}}}, {"id": "tY6jOWxVsgx", "original": null, "number": 1, "cdate": 1602782982869, "ddate": null, "tcdate": 1602782982869, "tmdate": 1605024046135, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review", "content": {"title": "Discriminative semi-supervised learning", "review": "The authors introduce a discriminative model for semi-supervised learning for which several existing methods are special cases. In their model, for each data value, there is a distribution from which the label is sampled. Although this distribution is unknown, in their framework the sampling distribution's parameters are approximately produced by a discriminative model such as a neural network trained on the labeled data.\n\nWhen the labels are vectors of features, the prior distribution of this parameter distribution can be defined so as to enforce logical constraints about which combinations of features are valid. This is a neat direction and in keeping with recent trends to integrate statistical and logical reasoning, although the paper would be strengthened if the authors gave concrete examples of a real-world dataset for which their innovation would be helpful. Likewise for their overall model, it's nice to help unify previous work, but additional discussion of impact or potential applications beyond what's been done would strengthen the paper.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080025, "tmdate": 1606915771509, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3212/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review"}}}, {"id": "Ri6de9pFn9a", "original": null, "number": 3, "cdate": 1603736922468, "ddate": null, "tcdate": 1603736922468, "tmdate": 1605024045978, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review", "content": {"title": "Recommendation to Reject", "review": "#### Summary\n\nThis paper proposes a probabilistic model to describe semi-supervised/unsupervised learning, which is further applied to model neuro-symbolic learning. Comparing to traditional unsupervised/semi-supervised learning formulations, the proposed model imposes a prior on the label distribution instead of input features. When applying this formulation to neuro-symbolic learning, the symbolic part can be regarded as a prior on label space to constrain the learning process. Finally, the authors propose three methods to calculate the loss of violating the symbolic prior constraints on label space.\n  \n#### Pros\n\n+ This paper is motivated by a very important problem. Combining statistical learning and symbolic reasoning becomes a trend recently, many algorithms and models have been proposed. However, there lacks a unified theoretical framework to understand this combination fundamentally.\n+ Modelling neuro-symbolic learning as a traditional semi-supervised statistical learning problem is an interesting idea. The resulted model integrates symbolic constraints as a prior on label distribution, which is very natural.\n\n#### Cons\n- The writing of this paper could be improved. For example, the notation in this paper is sometimes confusing. The meaning of parameter $\\theta^x$ is not clearly explained: In section 2, it is explained as a parameter of the conditional distribution $p(\\mathrm{y}|x)$, so I took it as the parameters of a classifier $y=h_\\theta^x(x)$. But in Section 4 and 5, the authors state that $\\mathbf{y}$ and $\\theta$ has a 1-1 corresponds in the space of $\\{0,1\\}^K$, so it seems that $\\theta^x$ should be interpreted as pseudo-labels of examples. In section 3, the authors introduce another variable $\\tilde{\\theta^x}$ as estimations of $\\theta^x$, which makes me more confused. Although this is a theoretical paper, I think the authors should improve the notations and provide some simple examples for helping readers understand this framework.\n- The idea of using logic rules as constraints (or other non-differentiable constraints) on label space is not a novel idea in statistical learning (Zhang and Zhou, 2013). In section 3, the first category of \"deterministic, distinct classes\" is multi-class learning; the \"deterministic non-exclusive features\" is multi-label learning. In both contexts, there are many works talking about constraining the label space with prior knowledge, for example, Cesa-Bianchi et al. (2006) learns multi-label classifier with hierarchical labels; Tsochantaridis et al. (2004) proposes SVM-struct deal with structural output with support vector machines; Mei et al. (2014) introduces logic rules as label constraints within a Bayesian learning framework. The authors have surveyed a wide range of related works in the area of semi-supervised learning and neuro-symbolic learning. However, I think the works I have mentioned above is highly related to this work; the authors should discuss the relationship between this work and those early works in statistical learning.\n- Last but not least, this paper lacks experiments to support the authors' claims. Meanwhile, although this paper presents a general framework, it provides little insight into this area. The authors propose three methods to compute the non-differentiable $p(\\theta)$, while (B) ignores the relationship between rules (e.g., recursive theories) and treat rules independently; (C) is equivalent to multi-label learning setting or the ECOC method in multi-class learning.\n  \n#### Recommendation\n\nOverall, I appreciate that this paper is trying to solve a crucial problem in neuro-symbolic learning, and the idea of viewing this problem as semi-supervised statistical learning is natural and reasonable. However, this paper should include more discussion about related works in statistical learning. Furthermore, it would be better if the authors can include some examples and experiments to demonstrate the idea of the proposed model.\n\n#### Additional questions and comments\n\n- The first to sub-figures in Fig. 1 are the same; the generative model should have arrows from $y$ and $\\theta$ to $x$.\n- In section 3, first paragraph, line 6: \"... outputting $\\theta^x$, an estimate of $\\theta^x$\". The first $\\theta^x$ should be $\\tilde{\\theta^x}$.\n- $\\alpha$ first appears in Eq. 6 and Fig. 1. However, there is no explanation about what is it.\n  \n\n#### References\n- Zhang, Min-Ling, and Zhi-Hua Zhou. \"A review on multi-label learning algorithms.\" IEEE transactions on knowledge and data engineering 26.8 (2013): 1819-1837.\n- Cesa-Bianchi, Nicol\u00f2, Claudio Gentile, and Luca Zaniboni. \"Incremental algorithms for hierarchical classification.\" Journal of Machine Learning Research 7.Jan (2006): 31-54.\n- Tsochantaridis, Ioannis, et al. \"Support vector machine learning for interdependent and structured output spaces.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n- Mei, Shike, Jun Zhu, and Jerry Zhu. \"Robust regbayes: Selectively incorporating first-order logic domain knowledge into bayesian models.\" International Conference on Machine Learning. 2014.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080025, "tmdate": 1606915771509, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3212/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review"}}}, {"id": "bjrSuO8tCpn", "original": null, "number": 4, "cdate": 1603946802507, "ddate": null, "tcdate": 1603946802507, "tmdate": 1605024045915, "tddate": null, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "invitation": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review", "content": {"title": "Relevant theme but still preliminary: many ideas but still scattered results.", "review": "The authors discuss several ideas aimed at improved semi-supervised learning by adopting an appropriate \"plate model\" with probabilistic content, and then examining various techniques and variants. The theme is relevant but the whole effort seems a bit preliminary: there are many keywords and many discussed techniques, but the whole picture is not clear in terms of concrete contributions (and the provided testing does not clarify whether gains are realized). \n\nThe paper contains a somewhat long introduction that in a sense includes Sections 1 and 2, then quickly goes through the proposed Expression (6), and derives consequences that are not always clearly articulated; for instance what is the point of Table 1? Also the connection with neuro-symbolic learning is interesting but it feels a bit too much; why exactly is it needed in this framework? Or is it just an optional add-on? (Besides, for the proposed approach to work I believe more testing is needed.)\n\nA problem: as far as I can tell, Figure 1 left and center are identical. What is the difference?\n\nA problem: is the citation (Van Engelen and Hoos 2020) indeed in the references? I could not find it.\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3212/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3212/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning", "authorids": ["~Carl_Allen1", "~Ivana_Balazevic1", "~Timothy_Hospedales1"], "authors": ["Carl Allen", "Ivana Balazevic", "Timothy Hospedales"], "keywords": ["semi-supervised learning", "probabilistic model", "neuro-symbolic learning"], "abstract": "Strong progress has been achieved in semi-supervised learning (SSL) by combining several methods, some of which relate to properties of the data distribution p(x), others to the model outputs p(y|x), e.g. minimising the entropy of unlabelled predictions. Focusing on the latter, we fill a gap in the standard text by introducing a probabilistic model for discriminative semi-supervised learning, mirroring the classical generative model. Several SSL methods are theoretically explained by our model as inducing (approximate) strong priors over parameters of p(y|x). Applying this same probabilistic model to tasks in which labels represent binary attributes, we theoretically justify a family of neuro-symbolic SSL approaches, taking a step towards bridging the divide between statistical learning and logical reasoning.", "one-sentence_summary": "A probabilistic model for discriminative and neuro-symbolic semi-supervised learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "allen|a_probabilistic_model_for_discriminative_and_neurosymbolic_semisupervised_learning", "pdf": "/pdf/30b18dd6bcdb1f64b4ec6b477aecfcc729d489ac.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U9PIbxt_Zp", "_bibtex": "@misc{\nallen2021a,\ntitle={A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning},\nauthor={Carl Allen and Ivana Balazevic and Timothy Hospedales},\nyear={2021},\nurl={https://openreview.net/forum?id=PpOtGYNVT6A}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PpOtGYNVT6A", "replyto": "PpOtGYNVT6A", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3212/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538080025, "tmdate": 1606915771509, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3212/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3212/-/Official_Review"}}}], "count": 14}