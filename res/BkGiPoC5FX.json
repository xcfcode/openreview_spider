{"notes": [{"id": "BkGiPoC5FX", "original": "Bye9rTuVtX", "number": 297, "cdate": 1538087779506, "ddate": null, "tcdate": 1538087779506, "tmdate": 1545355409200, "tddate": null, "forum": "BkGiPoC5FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SylNfxHJxN", "original": null, "number": 1, "cdate": 1544667148431, "ddate": null, "tcdate": 1544667148431, "tmdate": 1545354504854, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Meta_Review", "content": {"metareview": "This paper proposes a training algorithm for ConvNet architectures in which the final few layers are fully connected.  The main idea is to use direct feedback alignment with carefully chosen binarized (\u00b11) weights to train the fully connected layers and backpropagation to train the convolutional layers. The binarization reduces the memory footprint and computational cost of direct feedback alignment, while the careful selection of feedback weights improves convergence. Experiments on CIFAR-10, CIFAR-100, and an object tracking task are provided to show that the proposed algorithm outperforms backpropagation, especially when the amount of training data is small. The reviewers felt that the paper does a terrific job of introducing the various training algorithms --- backpropagation, feedback alignment, and direct feedback alignment --- and that the paper clearly explained what the novel contributions were. However, the reviewers felt the paper had limited novelty because it combines ideas that were already known, that it has limited applicability because it will not work with fully convolutional architectures, that the baselines in the experiments were somewhat weak, and that the paper provided no insights on why the proposed algorithm might be better than backpropagation in some cases. Regrettably, only one reviewer (R2) participated in the discussion, though this was the reviewer who provided the most constructive review. The AC read the revised paper, and agrees with R2's concerns about the limited applicability of the proposed algorithm and lack of insight or analysis explaining why the proposed training algorithm would improve over backpropagation.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Great explanation of prior work, but limited applicability and no insight or analysis"}, "signatures": ["ICLR.cc/2019/Conference/Paper297/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper297/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353265242, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper297/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper297/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper297/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353265242}}}, {"id": "rklrp6fry4", "original": null, "number": 6, "cdate": 1544003005063, "ddate": null, "tcdate": 1544003005063, "tmdate": 1544003005063, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "rkeKcIiO6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "content": {"title": "Final comments", "comment": "I am impressed by how quickly the authors addressed some of the issues in the paper.\n\nDespite this, I feel that the method has limited applicability (only on networks with a combination of dense and conv layers).\nAs mentioned by one of the other reviewers, the insight into why this approach would work better is not provided.\nFor this reason, I do not think the current manuscript can be accepted. "}, "signatures": ["ICLR.cc/2019/Conference/Paper297/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper297/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620767, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkGiPoC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper297/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper297/Authors|ICLR.cc/2019/Conference/Paper297/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620767}}}, {"id": "S1efJssO6X", "original": null, "number": 4, "cdate": 1542138586321, "ddate": null, "tcdate": 1542138586321, "tmdate": 1542138586321, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "BJxID8GNh7", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "Thank you for your comments. \n\nFollowings are our response about your comments.\n\n1) Effectiveness of DFA\nReduced amount of computation by BDFA can be much smaller when we include the computations due to the covolutional layers. Therefore, computational efficiency caused by BDFA can be larger only for the specific NN models. For example, MDNet (referred in the paper) uses only FC fine-tuning, and BDFA can improve the computational efficiency significantly. \nHowever, when the BDFA is applied instead of BP, we can reduce the required size of the dataset to achieve same test accuracy. Finally, training with the small dataset can additionally improve the effectiveness of the BDFA based training.\n\n2) Limited Novelty\nAs you pointed out, we suggest the combination of two well-known training methods, BP and binarized DFA. Although the proposed solution is simple, the effect of the training is powerful especially when the training dataset is small. To emphasize the effect of the algorithm, we added the simulation of data augmentation (table 2).\n\n3) Low baseline performance\nThere are three reasons why the baseline model in the manuscript shows lower accuracy compared with the state-of-the-art CIFAR results.\n  (1) There was no data augmentation. \n  (2) Simple optimization, momentum is used.\n  (3) We used one additional FC layer for VGG-16.\nWe added the simulation result with data augmentation in table 2. The explanation of the different network configuration is added in section 4."}, "signatures": ["ICLR.cc/2019/Conference/Paper297/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620767, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkGiPoC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper297/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper297/Authors|ICLR.cc/2019/Conference/Paper297/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620767}}}, {"id": "rkeKcIiO6Q", "original": null, "number": 3, "cdate": 1542137489480, "ddate": null, "tcdate": 1542137489480, "tmdate": 1542137489480, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "HklxaibB2m", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "content": {"title": "Response about your valuable comments", "comment": "Thanks to your valuable comments, we can improve the quality of the manuscript more than before. \n\nBefore answering your main concerns, the manuscripts are revised as follow.\n\n1) The contents of the 2nd paragraph was gradient vanishing problem but it is replaced with the overfitting problem to emphasize the effect of the CDFA. \n\n2) We made the figures smaller to secure more manuscript spaces.\n\n3) Minor typos in the manuscripts were revised.\n\n----------------------------------------\n\nFollowings are our response about your comments.\n\n1) Data augmentation strategy and the details of the experiment (hyper parameter, network configuration, data augmentation, optimization method, etc)\nAs you pointed out, the data augmentation startegy was not mentioned in the previous manuscript. We added the explantion about the data augmentaion clearly and added more details of the experiments as follow.\n  - The base network follows the VGG-16 configuration, but has one additional FC layer.\n  - The simulation was based on mini-batch gradient descent with the batch size 100 and uses momentum for the optimization method.\n  - The parameters of the network is initialized as introduced by He et al. (2015), and the learning rate decay and the weight decay method is adopted. Other hyper parameters are not changed for fair comparison.\n\nIn this experiments result in the previous manuscripts was evaluated with no data augmentation. Even though the data augmentation is not used, CBDFA shows more robust training performance compared with the BP. We emphasized this strength into the manuscript. Moreover, the related simulation is added in table 2. \n\n2) Complexity and speedup estimation\nThe complexity of the BP training highly depends on the network configurations. Instead, we add the reference, Han et al (ISCAS 2018) which explains the BP based online learning can be the obstacle of real-time object tracking implementation. \n\n3) Comparison with \"Conv. only training\"\nIn table 2, we added the comparison results with conv. only training suggested by Hoffer et al. (2018). CBDFA shows much better training performance compared with the conv. only training.\n\n4) Explanation of gradient vanishing problem in intorudction.\nWe removed the second paragraph, but add the overfitting problem.\n\n5) Low baseline performance\nThere are three reasons why the baseline model in the manuscript shows lower accuracy compared with the state-of-the-art CIFAR results.\n  (1) There was no data augmentation. \n  (2) Simple optimization, momentum is used.\n  (3) We used one additional FC layer for VGG-16.\nWe added the simulation result with data augmentation in table 2. The explanation of the different network configuration is added in section 4."}, "signatures": ["ICLR.cc/2019/Conference/Paper297/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620767, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkGiPoC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper297/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper297/Authors|ICLR.cc/2019/Conference/Paper297/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620767}}}, {"id": "BklM51jOT7", "original": null, "number": 2, "cdate": 1542135689530, "ddate": null, "tcdate": 1542135689530, "tmdate": 1542135689530, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "rJgvU8g3h7", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "content": {"title": "Thank you for valuable comments", "comment": "Thanks to your valuable comments, we can improve the quality of the manuscript more than before. \n\nBefore answering your main concerns, the manuscript was revised as follow.\n\n1) The contents of the 2nd paragraph was gradient vanishing problem but it is replaced with the overfitting problem to emphasize the effect of the CDFA. \n\n2) We made the figures smaller to secure more manuscript spaces.\n\n3) Minor typos in the manuscripts were revised.\n\nFollowings are our response to your comments.\n\n1) Limited Novelty\nAs you pointed out, we suggest the combination of two well-known training methods, BP and binarized DFA. Although the proposed solution is simple, the effect of the training is powerful especially when the training dataset is small. To emphasize the effect of the algorithm, we added the simulation of data augmentation (table 2).\n\n2) Performance contribution\nReduced amount of computation by BDFA can be much smaller when we include the computations due to the covolutional layers. Therefore, computational efficiency caused by BDFA can be larger only for the specific NN models. For example, MDNet (referred in the paper) uses only FC fine-tuning, and BDFA can improve the computational efficiency significantly. \nHowever, when the BDFA is applied instead of BP, we can reduce the required size of the dataset to achieve same test accuracy. Finally, training with the small dataset can additionally improve the effectiveness of the BDFA based training.\n\n3) Limited evaluations\nWe are really sorry that we could not add the evaluations with the different NN models and datasets. Our computation resources are limited so, it is hard to get the results before the deadline. Instead, we added the simulation results with data augmentation, and convolution layer only training. We are still evaluating the more various network configurations and datasets."}, "signatures": ["ICLR.cc/2019/Conference/Paper297/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620767, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkGiPoC5FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper297/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper297/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper297/Authors|ICLR.cc/2019/Conference/Paper297/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper297/Reviewers", "ICLR.cc/2019/Conference/Paper297/Authors", "ICLR.cc/2019/Conference/Paper297/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620767}}}, {"id": "rJgvU8g3h7", "original": null, "number": 3, "cdate": 1541305935090, "ddate": null, "tcdate": 1541305935090, "tmdate": 1541534114120, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Official_Review", "content": {"title": "The contribution is limited", "review": "This paper targets at developing new DFA method to replace BP for neural network model optimization, in order to speed up the training process. The paper is generally written clearly and relatively easy to follow. \n\nMy main concern is about significance of the contribution of this paper.\n\n1. the novelty is limited. This paper only simply combines two well-known approach BP and DFA together. \n\n2. performance contribution seems not significant from the proposed approach. In the implementation, the authors only apply their approach to optimize a few top layers. A majority of the layers in the NN model are still optimized via BP. \n\n3. the authors should provide more evaluations on different NN backbones and datasets, to make the experiments stronger and more convincing.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper297/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Official_Review", "cdate": 1542234493769, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper297/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335692889, "tmdate": 1552335692889, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper297/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HklxaibB2m", "original": null, "number": 2, "cdate": 1540852664400, "ddate": null, "tcdate": 1540852664400, "tmdate": 1541534113914, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Official_Review", "content": {"title": "Review", "review": "The paper propses to use a combination of Direct Feedback Allignment (DFA) and BackPropagation (BP) to improve upon standard back propagation.\nTo understand what is done, consider the following: Feedback Alignment is +- equal to back propagation when using random but fixed weights in the backwards pass. Direct feedback alignment uses random backprojections directly to the layer of interest. \nThe advantage of DFA is that It bypasses the normal computational graph. The advantage of this is that if compute is infinite, all of these updates can be computed in parallel instead of pipelining them as is done in standard BP.\n\nIn the current paper, the use of DFA for dense layers and BP for conv layers which is named CDFA is proposed.\nIn addition the paper also proposes a binarized version of BDFA to limit memory consumption and communication. It is claimed that the proposed techniques improve upon standard back propagation.\n\nOverall, the paper is easy to understand, but I lean towards rejecting this paper because I am not convinced by the experimental evidence. As outlined below, the key issue is that the baseline appears to be weak. Additionally, the main limitation of the proposed approach can only benefit a very limited set of architectures. \n\nPositive points:\n---------------------\nThe authors did an excellent job of introducing BP, FA and DFA in the paper. This makes the core concepts and ideas accessable without having to delve through prior work.\n\n\nThe own contributions and the key idea is easy to understand. \n\nLimitations and possible improvements\n-------------------------------------------------------\nA core limitation is that recent networks do not have a combination of dense layers and convolutional layers. In many cases the networks are fully convolutional, this limits the applicability of the proposed combination of DFA and BP. The use of additional networks would benefit the paper. Currently only VGG 16 on Cifar 10 is used. Also, the data augmentation strategy is not discussed. Of course, it would be nice if additional datasets could be included as well, but this of course depends on the computational resources the authors have available. \n\n\nThe key issue to me is that performance improvements for CIFAR are reported, but I fear that the baseline accuracy for VGG16 might be a bit low. If I memory serves me well, it should be able to achieve around 90% at least on CIFAR 10 using VGG style networks. I did a quick search and found http://torch.ch/blog/2015/07/30/cifar.html corroborating this but I did not verify this directly. \n\n\nRelated to the previous point, since this is an empirical paper, describing the hyper-parameter optimizations and final settings in detail  can convince the reader that the study is exectued correctly. Much of the information is missing now.\n\n\nSimilarly, I have trouble understanding section 4.1 and section 4.2 since I do not know the exact details of the experiments. This can be fixed easily however.\n\n\nProvide complexity estimates of the potential speedup or provide actual timing information. (Although this might not be that meaningful without much additional work given that gpu kernels are often heavily optimized).\n\n\nLast year there was a submission to ICLR about fixing the final output layer and only learning the convolutional layers. If we consider that random projections work remarkably well and can be considered approximations of kernels, it could be interesting to add a baseline where the fully connected layers are fixed and only the convolutional layers are trained. The error signal can be propagated using standard BP, FA or DFA methods but it would shed light on whether learning in the higher layers is actually needed or BP in the conv layers is sufficient.\n\nMinor possible improvements\n------------------------------------------\nFinally, I would strongly suggest that the authors perform some additional proofreading. There are quite a few strange formulations and spelling mistakes. That being said, it did not prevent me from understanding the manuscript so this remark DID NOT factor into my judgement.\n\nIn addition to remark above, I would suggest removing the second paragraph from the introduction. It feels out of place to me, and the vanishing gradient effects are not discussed in the remainder of the manuscript.\n\n\nThe list of possible optimizers before the selection for SGD+Momentum is not needed. Simply stating that SGD with momentum is used should be sufficient. \n\n\n\u201cTraining from scratch\u201d instead of \u201cTraining from the scratch\u201d\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper297/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Official_Review", "cdate": 1542234493769, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper297/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335692889, "tmdate": 1552335692889, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper297/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJxID8GNh7", "original": null, "number": 1, "cdate": 1540789853840, "ddate": null, "tcdate": 1540789853840, "tmdate": 1541534113666, "tddate": null, "forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper297/Official_Review", "content": {"title": "The method needs more insight or novelty, and the results have room to improve", "review": "This manuscript extends the direct feedback alignment (DFA) approach to convolutional neural networks (CNN) by (1) only applying DFA to FC layers with backpropagation (BP) in place for convolutional layers (2) using binary numbers for feedback matrix.\n\nOriginality wise, I think (1) is a very straightforward extension to the original DFA approach by just applying DFA to places where it works. It still does not solve the ineffectiveness of DFA on convolutional layers. And there is no much insight obtained. (2) is interesting in that a binary matrix is sufficient to get good performance empirically. This would indeed save memory bandwidth and storage. This falls into the category of quantization or binarization, which is not super novel in the area of model compression. \n\nThe experimental results show that the proposed approach is better than BP based on accuracy. However, these results might be called into question because the shown accuracies on CIFAR10 and CIFAR100 are not state-of-the-art results. For example, the top 1 accuracy of CIFAR10 in this paper 81.11%. But with proper tuning, a CNN should be able to get more than 90% accuracy. See this page for more details.\nhttp://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\nTherefore, though the claimed accuracy of the proposed method is 89%, it is still not the state-of-the-art result and it seems to be lack of tuning for the BP approach to perform similar level of accuracy. The same conclusion applies to CIFAR100. In fact, from figure 4, the training accuracy gets 100% while the testing accuracy is around 40% for BP, which seems to be overfitting. With these results, it is hard to judge the significance of the manuscript.\n\nMinor typos:\nIn Equation 1, the letter i is overloaded.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper297/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Convolutional Neural Network Training with Direct Feedback Alignment", "abstract": "There were many algorithms to substitute the back-propagation (BP) in the deep neural network (DNN) training. However, they could not become popular because their training accuracy and the computational efficiency were worse than BP. One of them was direct feedback alignment (DFA), but it showed low training performance especially for the convolutional neural network (CNN). In this paper, we overcome the limitation of the DFA algorithm by combining with the conventional BP during the CNN training. To improve the training stability, we also suggest the feedback weight initialization method by analyzing the patterns of the fixed random matrices in the DFA. Finally, we propose the new training algorithm, binary direct feedback alignment (BDFA) to minimize the computational cost while maintaining the training accuracy compared with the DFA. In our experiments, we use the CIFAR-10 and CIFAR-100 dataset to simulate the CNN learning from the scratch and apply the BDFA to the online learning based object tracking application to examine the training in the small dataset environment. Our proposed algorithms show better performance than conventional BP in both two different training tasks especially when the dataset is small.", "keywords": ["Direct Feedback Alignment", "Convolutional Neural Network", "DNN Training"], "authorids": ["hdh4797@kaist.ac.kr", "hjyoo@kaist.ac.kr"], "authors": ["Donghyeon Han", "Hoi-jun Yoo"], "pdf": "/pdf/5dcc9a62d816f75f56b6dde2609bb197a2c5487b.pdf", "paperhash": "han|efficient_convolutional_neural_network_training_with_direct_feedback_alignment", "_bibtex": "@misc{\nhan2019efficient,\ntitle={Efficient Convolutional Neural Network Training with Direct Feedback Alignment},\nauthor={Donghyeon Han and Hoi-jun Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=BkGiPoC5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper297/Official_Review", "cdate": 1542234493769, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkGiPoC5FX", "replyto": "BkGiPoC5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper297/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335692889, "tmdate": 1552335692889, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper297/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}