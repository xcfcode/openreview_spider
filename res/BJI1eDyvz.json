{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124437488, "tcdate": 1518460894341, "number": 201, "cdate": 1518460894341, "id": "BJI1eDyvz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJI1eDyvz", "signatures": ["~Thomas_Anderson_Keller1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1521816698472, "tcdate": 1521816698472, "number": 2, "cdate": 1521816698472, "id": "HJftVqMqf", "invitation": "ICLR.cc/2018/Workshop/-/Paper201/Official_Comment", "forum": "BJI1eDyvz", "replyto": "HyCFsD-5z", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"title": "acceptance decision", "comment": "Dear T. Anderson Keller,\n\nOur decisions (in either direction) don't indicate agreement with every point in every review. With hundreds of papers it was not possible to debate and correct every detail with the reviewers. As you have done, authors are free to set the record straight on any details they disagree with.\n\nHowever, two of the program chairs did read a large number of borderline papers -- and yours was read and considered. As is always the case with a space-limited conference, we will have left out papers that will turn out to be important. However, we have done the best that we could, and the decisions are final.\n\nBest wishes,\nTara, Oriol, Marc'Arelio, and Iain."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222447642, "id": "ICLR.cc/2018/Workshop/-/Paper201/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJI1eDyvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper201/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper201/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper201/Reviewers", "ICLR.cc/2018/Workshop/Paper201/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222447642}}}, {"tddate": null, "ddate": null, "tmdate": 1521740492636, "tcdate": 1521740492636, "number": 3, "cdate": 1521740492636, "id": "BJr09vZ5M", "invitation": "ICLR.cc/2018/Workshop/-/Paper201/Public_Comment", "forum": "BJI1eDyvz", "replyto": "B1uSp7gtf", "signatures": ["~Thomas_Anderson_Keller1"], "readers": ["everyone"], "writers": ["~Thomas_Anderson_Keller1"], "content": {"title": "Reviewer 3 Response", "comment": "Dear Reviewer 3,\n\nThank you for taking the time to review our work.\n\nWe believe there is a misunderstanding related to our description of the experimental setup since we have exhaustively described the setup in the paper. We indeed did a systematic grid search to optimize hyperparameters, to which the entire Appendix is dedicated. Additionally, as mentioned in the paper, with the intended release of the code, our results will be transparent and reproducible, significantly mitigating any uncertainty.\n\nWe believe our results, despite simplicity due to usage of toy examples, are categorical in that they demonstrated a task which two existing models utterly failed to learn, but can be almost perfectly solved when the two mechanisms were combined.  This has never been reported before.  \n\nRegards"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625016, "id": "ICLR.cc/2018/Workshop/-/Paper201/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper201/Reviewers"], "reply": {"replyto": null, "forum": "BJI1eDyvz", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625016}}}, {"tddate": null, "ddate": null, "tmdate": 1521740381202, "tcdate": 1521740381202, "number": 2, "cdate": 1521740381202, "id": "rkBDcPWcM", "invitation": "ICLR.cc/2018/Workshop/-/Paper201/Public_Comment", "forum": "BJI1eDyvz", "replyto": "Bkx3TiWFG", "signatures": ["~Thomas_Anderson_Keller1"], "readers": ["everyone"], "writers": ["~Thomas_Anderson_Keller1"], "content": {"title": "Reviewer 2 Response", "comment": "Dear Reviewer 2,\n\nThank you for your thoughtful review.\n\nWe are working on a demonstration of solutions to real-world problems using FW-LSTM. However, we believe our results using toy examples in this short paper are still compelling and useful as a starting point for others to begin experimenting with this new architecture. Specifically we think the novelty of the proposed model is undervalued when combined with the modified associative retrieval task which demonstrates, and provides insights into, its advantages.\n\nOur results in fact included direct comparison against LSTM and FW-RNN, and showed that either mechanism alone failed to learn a task while the combination of the two succeeded. Our goal is to demonstrate this synergy conclusively and concisely in this short paper, and thus we did not compare and contrast against other memory mechanisms like DNCs. Additionally, the LSTM results were included in Table 1, but validation accuracy curves were omitted from Figure 2 due to their significantly lower final validation error. These can easily be added for completeness if desired.\n\nWe believe our work is novel in the sense that this is by far the first attempt to combine gated RNN cells with memory augmentations of RNNs, which are two approaches to enhancing RNNs that, to this day, have only been used in separation. Most importantly, our results demonstrated a task that either of the approaches alone failed to learn, but could be successfully solved by their combination.\n\nRegards"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625016, "id": "ICLR.cc/2018/Workshop/-/Paper201/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper201/Reviewers"], "reply": {"replyto": null, "forum": "BJI1eDyvz", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625016}}}, {"tddate": null, "ddate": null, "tmdate": 1521740043483, "tcdate": 1521740043483, "number": 1, "cdate": 1521740043483, "id": "rkQGYvZqz", "invitation": "ICLR.cc/2018/Workshop/-/Paper201/Public_Comment", "forum": "BJI1eDyvz", "replyto": "ByfnV0-tf", "signatures": ["~Thomas_Anderson_Keller1"], "readers": ["everyone"], "writers": ["~Thomas_Anderson_Keller1"], "content": {"title": "Reviewer 1 Response", "comment": "Dear Reviewer 1,\n\nThank you for your review. We agree with your understanding of the paper and appreciate the time spent to understand our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625016, "id": "ICLR.cc/2018/Workshop/-/Paper201/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper201/Reviewers"], "reply": {"replyto": null, "forum": "BJI1eDyvz", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625016}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582823974, "tcdate": 1520610624270, "number": 1, "cdate": 1520610624270, "id": "B1uSp7gtf", "invitation": "ICLR.cc/2018/Workshop/-/Paper201/Official_Review", "forum": "BJI1eDyvz", "replyto": "BJI1eDyvz", "signatures": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer3"], "content": {"title": "Applying \"Fast Weight to Attend to the Recent Past\" to LSTMs", "rating": "5: Marginally below acceptance threshold", "review": "This submission explores whether the formulation of attention proposed in \"Using Fast Weights to Attend to the Recent Past\" paper is applicable to LSTM. The associative memory bit is applied to the proposed input (often denoted by \"j\") before it's gated (\"i\") and added to the gated state (\"f*c_{t-1}\").\n\nThe experiments are done on a simple associative retrieval task and a new, harder variant of the same. The results indicate that the fast weight LSTM improves on both the plain LSTM and the fast weight RNN. It is unclear how trustworthy these results are since the experimental setup (especially hyperparameter tuning) is not described in much detail.\n\nThe proposed model is simple, the experiments are a good start, but even for a workshop paper, I feel more convincing ones are necessary.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582823793, "id": "ICLR.cc/2018/Workshop/-/Paper201/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper201/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper201/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper201/AnonReviewer1"], "reply": {"forum": "BJI1eDyvz", "replyto": "BJI1eDyvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582823793}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582699891, "tcdate": 1520709032313, "number": 2, "cdate": 1520709032313, "id": "Bkx3TiWFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper201/Official_Review", "forum": "BJI1eDyvz", "replyto": "BJI1eDyvz", "signatures": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer2"], "content": {"title": "Incorporating Fast Weights into the LSTMs", "rating": "5: Marginally below acceptance threshold", "review": "Summary: This paper proposes to incorporate fast weights to the LSTMs. In a way the proposed method resembles WeiNet's construction by Zhang&Zho which is in turn inspired by the Ba et al's work on fast weights. The authors test their model on associative retrieval tasks and they show better results than FW-RNN and LN-LSTM. Overall, the paper is well-written.\n\nCriticism: \n- The results are quite encouraging, however it would have been much more convincing if they have shown some results on non-toyish tasks. It is difficult to say if their results would hold on more realistic tasks.\n- The comparisons against other memory models such as DNCs or LSTMS are missing.\n- Concept is interesting, but the novelty of the model in this paper is not that big. This would not be an issue if they had results on a real world task.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582823793, "id": "ICLR.cc/2018/Workshop/-/Paper201/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper201/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper201/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper201/AnonReviewer1"], "reply": {"forum": "BJI1eDyvz", "replyto": "BJI1eDyvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582823793}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582685237, "tcdate": 1520719018450, "number": 3, "cdate": 1520719018450, "id": "ByfnV0-tf", "invitation": "ICLR.cc/2018/Workshop/-/Paper201/Official_Review", "forum": "BJI1eDyvz", "replyto": "BJI1eDyvz", "signatures": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer1"], "content": {"title": "Study on using fast weights on LSTMs", "rating": "6: Marginally above acceptance threshold", "review": "The authors study the application of fast weights to LSTM cells. Fast weights have been shown to improve the memory capacity and time scale of RNNs. In this study the authors study whether fast weights can help gated RNNs like LSTM cells. The conclusion of this work is that fast weights is also beneficial to LSTMs and results in much faster training and lower test error.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582823793, "id": "ICLR.cc/2018/Workshop/-/Paper201/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper201/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper201/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper201/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper201/AnonReviewer1"], "reply": {"forum": "BJI1eDyvz", "replyto": "BJI1eDyvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper201/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582823793}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573582649, "tcdate": 1521573582649, "number": 169, "cdate": 1521573582310, "id": "BJvA0A0Yz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJI1eDyvz", "replyto": "BJI1eDyvz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Weight Long Short-Term Memory", "abstract": "Associative memory using fast weights is a short-term memory mechanism that substantially  improves  the  memory  capacity  and  time  scale  of  recurrent  neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs.  In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories.  We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.", "paperhash": "keller|fast_weight_long_shortterm_memory", "keywords": ["Fast Weights", "LSTM", "Long Short-Term Memory", "Associative Memory", "Recurrent", "RNN"], "_bibtex": "@misc{\n  keller2018fast,\n  title={Fast Weight Long Short-Term Memory},\n  author={T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJI1eDyvz}\n}", "authorids": ["andy.a.keller@intel.com", "sharath.nittur.sridhar@intel.com", "xin3.wang@intel.com"], "authors": ["T. Anderson Keller", "Sharath Nittur Sridhar", "Xin Wang"], "TL;DR": "We show that LSTM with fast weight associative memory trains much faster and achieves lower test error in associative retrieval tasks than previously reported fast weights RNNs.", "pdf": "/pdf/66d5ab22b286c409cc74fe2d585236a2af843896.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 9}