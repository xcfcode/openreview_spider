{"notes": [{"id": "S1lvn0NtwH", "original": "ryeFvYFuvr", "number": 1360, "cdate": 1569439407163, "ddate": null, "tcdate": 1569439407163, "tmdate": 1577168279805, "tddate": null, "forum": "S1lvn0NtwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "VgHcd0kOv", "original": null, "number": 1, "cdate": 1576798721442, "ddate": null, "tcdate": 1576798721442, "tmdate": 1576800915149, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents an understudied bias known to exist in the learning patterns of children, but not present in trained NN models.  This bias is the mutual exclusivity bias: if the child already knows the word for an object, they can recognize that the object is likely not the referent when a new word is introduced.  So that is, the names of objects are mutually exclusive. \n\nThe authors and reviewers had a healthy discussion. In particular, Reviewer 3 would have liked to have seen a new algorithm or model proposed, as well as an analysis of when ME would help or hurt.  I hope these ideas can be incorporated into a future submission of this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712235, "tmdate": 1576800261582, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Decision"}}}, {"id": "BklyWIdlcS", "original": null, "number": 3, "cdate": 1572009463011, "ddate": null, "tcdate": 1572009463011, "tmdate": 1574385207516, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks. \n\nIn general, the whole paper tries to tell a very interesting, and good story. The paper is very well organized and written. However, I have the following concerns.\n\n1, the ME problem is quite similar to the concept ontology, e.g. , a \u201cDalmatian,\u201d a \u201cdog\u201d, or a \u201cmammal\u201d.  So what\u2019s the key difference? Hierarchical learners can avoid this problem.\n\n2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning. The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian. It\u2019s probably a bit unfair or misleading to claim neural networks suffering from ME bias. \n\n3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments. Please give more explanations.\n\n4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues?\n\n5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN, the ME bias will be solved. Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning. \n\n6, the experimental design of Sec. 4.2 is also a bit unfair. It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this  task. Of course, this common NNs can not address it.\n\n----\nI read the rebuttal. the authors clarified and answered the questions. I would like to raise the score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575838715488, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Reviewers"], "noninvitees": [], "tcdate": 1570237738510, "tmdate": 1575838715501, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Review"}}}, {"id": "S1xAWnjRFH", "original": null, "number": 2, "cdate": 1571892230243, "ddate": null, "tcdate": 1571892230243, "tmdate": 1574258395341, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "Summary:\n\nThis paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another. Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation. Authors argue that ME bias could help the model to handle new classes and rare events better.\n\nMy comments:\n\nI very much enjoyed reading this paper. I support accepting this paper. It highlights one of the missing inductive biases in ML and proposes it as a challenge. As the authors also agree, ME bias is missing not just in DNNs. It is the issue of MLE. It would be good to have some non-NN results too. I see this is a challenge for MLE than DNNs.\n\n1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?\n2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.\n3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain.\n4. Are the authors willing to release the code and data to reproduce the results?\n\nMinor comments:\n\n\n1. Page 3: second para, line 4: \u201cour aim is to study\u201d\n2. Page 5: last line: estimate for -> estimated for\n3. Section 4.2: 3rd line: \u201cthe class for the from\u201d\n\n=====================================================\n\nAfter rebuttal: I have read the authors' response and  I stand by my decision.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575838715488, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Reviewers"], "noninvitees": [], "tcdate": 1570237738510, "tmdate": 1575838715501, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Review"}}}, {"id": "SkedBT5ntB", "original": null, "number": 1, "cdate": 1571757375527, "ddate": null, "tcdate": 1571757375527, "tmdate": 1573814565372, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "*** Increased to weak accept after discussion of merits of ME bias was improved in the paper *** \n\nThis paper investigates whether neural networks exhibit a \u2018mutual exclusivity (ME) bias\u2019, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data. The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning. While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias. The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example.\n\nComments / questions:\n* The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples. Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.\n* For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples?\n    * If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically. It\u2019s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes). So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes? Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.\n    * The authors say that \u201cME can be generalized from applying to 'novel versus familiar\u2019 stimuli to instead handling \u2018rare versus frequent\u2019 stimuli\u201d and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above. It\u2019s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.\n* It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2. When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones? From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes? Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not.\n\nMinor comments /questions not affecting review:\n* Is the acronym ME pronounced like the word \u201cme\u201d or is it spelled out \u201cM-E\u201d? If the latter, then all cases of \u201ca ME bias\u201d should be corrected to \u201can ME bias\u201d.\n* Section 4.2 line 3: \u201csample the class [for the] from a power law distribution\"", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575838715488, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Reviewers"], "noninvitees": [], "tcdate": 1570237738510, "tmdate": 1575838715501, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Review"}}}, {"id": "Hygl1Z-hoB", "original": null, "number": 11, "cdate": 1573814488503, "ddate": null, "tcdate": 1573814488503, "tmdate": 1573814488503, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "BklvRdsjsr", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "Improved discussion and qualification of merits of ME bias - increasing to weak accept", "comment": "Though I still think the paper would ideally benefit from some more in-depth and specific analysis of when ME bias is desirable (and I hope to see in future work!), I think that it is strengthened by the changes made to the discussion described above and I am willing to increase my score to weak accept. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}, {"id": "BklvRdsjsr", "original": null, "number": 10, "cdate": 1573791950876, "ddate": null, "tcdate": 1573791950876, "tmdate": 1573791950876, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "H1ew95gooS", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "A discussion on the utility of ME", "comment": "Thanks for continuing the discussion of our work. Your point is well received and we agree that the ME bias is not appropriate for every use case. Moreover, even if it is optimal given the inherent tradeoffs in any inductive bias, it\u2019s bias that is going to get some cases right and other cases wrong. As you rightly point out, the issue is complex and warrants more nuance in the discussion, and in response we uploaded another revision that expands the discussion in this way (see Section 5). We summarize the main points here too. As we say, the ME bias may not be helpful in every case, but it\u2019s equally clear that the status quo is sub-optimal: models shouldn\u2019t have a strong anti-ME bias regardless of what the task and dataset demands. So how do we proceed? An ideal model would decide for itself how strongly to use ME based on the task demands. For instance, in our synthetic example, an ideal learner would discover the one-to-one correspondence and use this as a meta-strategy (through a perfect ME bias). If the dataset has more many-to-one correspondences, it would adopt another meta-strategy. This meta-strategy could even change depending on the stage of learning. There are promising results in this direction: Santoro et al. (2016) and Lake (2019) [see references in paper] do not build a ME strategy into the model, but rather observe one as an emergent consequence of meta learning. We see potential in this direction but there are other means of tackling the challenge too. We hope that by introducing this challenge, our paper will stimulate debate and ultimately progress in addressing it. Our discussion is now more appropriately nuanced, and we thank you again for your feedback on this point.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}, {"id": "H1ew95gooS", "original": null, "number": 7, "cdate": 1573747342782, "ddate": null, "tcdate": 1573747342782, "tmdate": 1573747342782, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "B1xHDTm9iS", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "Pros and cons of introducing an ME bias should be discussed in more depth", "comment": "Thank you for your response. I appreciate that it is difficult to develop an algorithm that corrects the ME bias since it\u2019s not trivial to decide whether an input is from a new class or not, but given that such an algorithm is not put forth, I don\u2019t think enough justification is provided that directly inducing this bias will help performance. It seems to me that there are circumstances where it\u2019s not clear that inducing an ME bias is a good idea and, while I understand that in some cases it may be useful, it may be harmful to it an explicit objective - for this reason, I think there should be more of a discussion of when it is useful, when it is not, and how it can go wrong.\nIn the 5th bullet point of my review, I tried to make the point that it\u2019s not obvious that an ME bias is strategically smart for a neural network that is uncertain whether a new input belongs to a new class or not, especially when the total number of classes is large. For example, if the network has seen lots of black bears and gets a picture of a brown bear (a class its never seen before), it might be a better bet to make a strong guess that it\u2019s another black bear rather than make a scattered prediction across the (potentially numerous) previously unseen classes. For a similar reason, as mentioned in bullet point 3, an ME bias early on in training might not be very useful since there are so many unseen classes. As mentioned in my review, it would be interesting to see how the network distributes the class probabilities of new inputs for this reason. In general, there seems to be a tradeoff between better classification of new classes and potential misclassification of old classes that would be introduced by inducing an ME bias that could be affected by factors such as the similarity of groups of classes, number of classes, stage of training and the loss function. While there is a line in section 4.1 mentioning that the \u201canswer to whether or not ME holds is not absolute\u201d, citing polysemy and synonymy as examples, I think it\u2019s important that the pros and cons are fleshed out more, given that the paper advocates for introduction of ME bias into training."}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}, {"id": "rklpjoQ9oH", "original": null, "number": 4, "cdate": 1573694372768, "ddate": null, "tcdate": 1573694372768, "tmdate": 1573694983925, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "S1xAWnjRFH", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "Specific Response to R1", "comment": "Thank you for your supportive review. \nWe answer the specific queries below and have also added them to the revised version of the paper. \n1.         We found that the entropy regularizer produces an ME score that stays constant across training, at the cost of the model being less confident about predictions made for seen classes. We added details regarding this condition to the manuscript.\n2.         The base rate is the probability of observing a new word in the target at that particular point in training. We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word. Thus, the base rate at time t in training is defined as:\n$$P(\\text{new in target at t}) = \\frac{ \\text{# of unseen sentences in target with new words}} {\\text{# of unseen sentences}}$$\n3.         In Section 4.2, we use \u201cnew\u201d to refer to the set of all the unseen classes at a particular timepoint t. For the classifier, P(N|t) is calculated by adding the probabilities the model assigns to all the \u201cnew\u201d classes when iterating through the remaining corpus (similar to Equation 1 in our paper). For the dataset, we compute P(N|t) by sampling all unseen images in the corpus and compute the proportion from \u201cnew\u201d classes given their ground truth labels.\n4.         We will release our code and data with the publication of the paper. Most of our experiments are easy to replicate as they use standard datasets, models, loss functions and optimizers. We sincerely hope that our challenge and these resources will stimulate progress in this area.\n\nPlease also see above where we write a general response to all reviews."}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}, {"id": "HkeodF7qir", "original": null, "number": 1, "cdate": 1573693811503, "ddate": null, "tcdate": 1573693811503, "tmdate": 1573694958322, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "SkedBT5ntB", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "Thank you and please see above", "comment": "We thank you for your constructive feedback. We reply to all reviews in a general response above."}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}, {"id": "rklsz575oB", "original": null, "number": 3, "cdate": 1573693971476, "ddate": null, "tcdate": 1573693971476, "tmdate": 1573694901960, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "BklyWIdlcS", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "Thank you and please see general response above", "comment": "Thanks for your feedback. We reply to all reviewers jointly in our comments above."}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}, {"id": "rklKFaQ9jr", "original": null, "number": 6, "cdate": 1573694849230, "ddate": null, "tcdate": 1573694849230, "tmdate": 1573694849230, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "B1xHDTm9iS", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "General Response (2/2)", "comment": "Our paper is unique for introducing a challenge that does not yet have a solution, but we see this as the best way forward for stimulating research in this important area. There is a wide gap between the centrality of ME in human language development and the strong anti-ME effects we found in standard DNNs, and MLE-based approaches more generally. We see our work as the beginning of a larger effort to integrate ME and inductive biases from cognitive science into modern AI models, and we thank you for considering our paper in your further discussions."}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}, {"id": "B1xHDTm9iS", "original": null, "number": 5, "cdate": 1573694812982, "ddate": null, "tcdate": 1573694812982, "tmdate": 1573694812982, "tddate": null, "forum": "S1lvn0NtwH", "replyto": "S1lvn0NtwH", "invitation": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment", "content": {"title": "General Response (1/2)", "comment": "We thank each of the reviewers for their thoughtful feedback on our work. We uploaded a revised paper that incorporates your suggestions, and we describe the changes in our response below. To echo R1, our paper \u201chighlights one of the missing inductive biases in ML and proposes it as a challenge.\u201d We demonstrate that popular deep neural network (DNNs) architectures do not show the mutual exclusivity (ME) bias that children use to help learn new words; in fact, they have an anti-ME bias that is completely backwards. We show how this anti-ME bias is poorly aligned with machine translation and object recognition tasks on common datasets, especially in more realistic lifelong learning settings. Our paper proposes a challenge to develop DNN architectures that can use the ME bias, like children, to support better zero-shot inferences and rapid learning.\n\nAs requested by R2, we clarify how the synthetic experiments (Section 3) implement the classic ME paradigm from Ellen Markman. As shown in Fig. 1 of our paper, children tend to select a novel object over a familiar object when asked to \u201cShow me the dax\u201d (Markman and Wachtel, 1988). Translating this into a synthetic experiment, input units denote words and output units denote objects. We ask the neural network to \u201cShow me the dax\u201d by activating the \u201cdax\u201d input unit and asking it to select amongst possible referents. The network produces a probability distribution a set of candidate referents, but it can make relative (two object) comparisons by isolating the scores of two candidates referents. To quantify the overall propensity toward ME, our \u201cME score\u201d measures the relative probability of choosing any of the novel objects (as opposed to the familiar objects), which directly translates to higher scores on Markman\u2019s forced choice task. We revised the task description in Section 3.1 to make these links clearer.\n\nR2 offers suggestions for how to get models to exhibit the ME bias, and we appreciate your ideas here. We have been thorough in our explorations and have tried some of these, without success, which is why we see ME as such an interesting challenge! We have revised the paper to provide more details related to your suggestions, which we summarize here. R2 suggests that the anti-ME bias may arise from \u201cthe small number of training instances,\u201d but we have verified that DNNs do not learn the ME regularity no matter how much data is presented, either in one-to-one synthetic mappings (Section 3) or on real datasets (Section 4). We clarify this in Section 3.1. The suggestion to use pre-trained embeddings (word2vec) is interesting but not pertinent to the ME paradigm which always tests novel words which are out of sample (\u201cdax\u201d, \u201czup,\u201d \u201cfep\u201d, etc.), corresponding to novel concepts in a lifelong learning setting. Additionally, R2 suggests that adding hierarchy through a concept ontology or a hierarchical Bayesian learner might help induce the bias. This is an interesting idea that we can explore in future work, and we added it to the discussion section, but it\u2019s not obvious to us that adding superordinate level categories like \u201canimal\u201d that have multiple referents would help map novel names to novel referents. Finally, with regards to the unbalanced classes in Section 4 (as mentioned by R2), an intelligent learner should be capable of modeling an open world that is inherently unbalanced, like children and adults do, with the possibility of encountering a new class at any point.\n\nR3\u2019s main critique is that it is hard to quantify the improvement ME would provide, in part because we do not \u201cprovide an empirical analysis of an algorithm that uses ME reasoning to improve learning.\u201d Simply put, we can\u2019t provide such an analysis because no such algorithm exists (yet)! This is why we are challenging the community to work in this direction, and we see our experiments as a demonstration that there will be applications on real tasks (Section 4). There is a clear misalignment between the inductive biases of standard neural nets and the statistical structure of common tasks especially when interpreted as lifelong learning. Recently, there have been some demonstrations of using ME to improve performance on specific tasks [Santoro et al. (2016), Lake, Linzen and Baroni (2019), Cohn-Gordon (2019), Lake (2019); see paper for bibliography], but none of these approaches represent a general solution. We see a general solution as an advance that will improve both zero-shot predictions and the speed of learning after just a few examples of a new input, by mitigating the strong bias to familiar responses.\n \nFinally, we thank R1 for their positive feedback on our work. R1 asked for elaboration on some of the technical details of how some of the quantities are computed. We respond to your comment directly with answers to the questions and make corresponding updates to the main text. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1360/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mutual Exclusivity as a Challenge for Deep Neural Networks", "authors": ["Kanishk Gandhi", "Brenden Lake"], "authorids": ["kanishk.gandhi@nyu.edu", "brenden@nyu.edu"], "keywords": ["Cognitive Science", "Deep Learning", "Word Learning", "Lifelong Learning"], "TL;DR": "Children use the mutual exclusivity (ME) bias to learn new words, while standard neural nets show the opposite bias, hindering learning in naturalistic scenarios such as lifelong learning.", "abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.", "pdf": "/pdf/f98c7ad382028ec61b7aae7f7e1a2fcca6fb90fd.pdf", "paperhash": "gandhi|mutual_exclusivity_as_a_challenge_for_deep_neural_networks", "original_pdf": "/attachment/19562d6bad4b783f0bd73e2a5c1bff09461d51ee.pdf", "_bibtex": "@misc{\ngandhi2020mutual,\ntitle={Mutual Exclusivity as a Challenge for Deep Neural Networks},\nauthor={Kanishk Gandhi and Brenden Lake},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lvn0NtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lvn0NtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1360/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1360/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1360/Authors|ICLR.cc/2020/Conference/Paper1360/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157175, "tmdate": 1576860535545, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1360/Authors", "ICLR.cc/2020/Conference/Paper1360/Reviewers", "ICLR.cc/2020/Conference/Paper1360/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1360/-/Official_Comment"}}}], "count": 13}