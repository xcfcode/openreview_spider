{"notes": [{"id": "SyeBqsRctm", "original": "BJgsETi9t7", "number": 530, "cdate": 1538087820814, "ddate": null, "tcdate": 1538087820814, "tmdate": 1545355377248, "tddate": null, "forum": "SyeBqsRctm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning", "abstract": " In this paper, we introduce a novel method, called step-wise sensitivity analysis, which makes three contributions towards increasing the interpretability of Deep Neural Networks (DNNs). First, we are the first to suggest a methodology that aggregates results across input stimuli to gain model-centric results. Second, we linearly approximate the neuron activation and propose to use the outlier weights to identify distributed code. Third, our method constructs a dependency graph of the relevant neurons across the network to gain fine-grained understanding of the nature and interactions of DNN's internal features. The dependency graph illustrates shared subgraphs that generalise across 10 classes and can be clustered into semantically related groups. This is the first step towards building decision trees as an interpretation of learned representations.", "keywords": ["Interpretability", "Interpretable Deep Learning", "XAI", "dependency graph", "sensitivity analysis", "outlier detection", "instance-specific", "model-centric"], "authorids": ["botty.dimanov@cl.cam.ac.uk", "mateja.jamnik@cl.cam.ac.uk"], "authors": ["Botty Dimanov", "Mateja Jamnik"], "TL;DR": "We find dependency graphs between learned representations as a first step towards building decision trees to interpret the representation manifold.", "pdf": "/pdf/ba387065b9e05da99fe9e68338a91c0bdea44438.pdf", "paperhash": "dimanov|stepwise_sensitivity_analysis_identifying_partially_distributed_representations_for_interpretable_deep_learning", "_bibtex": "@misc{\ndimanov2019stepwise,\ntitle={Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning},\nauthor={Botty Dimanov and Mateja Jamnik},\nyear={2019},\nurl={https://openreview.net/forum?id=SyeBqsRctm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HklJDWR-g4", "original": null, "number": 1, "cdate": 1544835414856, "ddate": null, "tcdate": 1544835414856, "tmdate": 1545354531680, "tddate": null, "forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper530/Meta_Review", "content": {"metareview": "This work proposes a modification of gradient based saliency map methods that measure the importance of all nodes at each layer. The reviewers found the novelty is rather marginal and that the evaluation is not up to par (since it's mostly qualitative). The reviewers are in strong agreement that this work does not pass the bar for acceptance.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper530/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper530/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning", "abstract": " In this paper, we introduce a novel method, called step-wise sensitivity analysis, which makes three contributions towards increasing the interpretability of Deep Neural Networks (DNNs). First, we are the first to suggest a methodology that aggregates results across input stimuli to gain model-centric results. Second, we linearly approximate the neuron activation and propose to use the outlier weights to identify distributed code. Third, our method constructs a dependency graph of the relevant neurons across the network to gain fine-grained understanding of the nature and interactions of DNN's internal features. The dependency graph illustrates shared subgraphs that generalise across 10 classes and can be clustered into semantically related groups. This is the first step towards building decision trees as an interpretation of learned representations.", "keywords": ["Interpretability", "Interpretable Deep Learning", "XAI", "dependency graph", "sensitivity analysis", "outlier detection", "instance-specific", "model-centric"], "authorids": ["botty.dimanov@cl.cam.ac.uk", "mateja.jamnik@cl.cam.ac.uk"], "authors": ["Botty Dimanov", "Mateja Jamnik"], "TL;DR": "We find dependency graphs between learned representations as a first step towards building decision trees to interpret the representation manifold.", "pdf": "/pdf/ba387065b9e05da99fe9e68338a91c0bdea44438.pdf", "paperhash": "dimanov|stepwise_sensitivity_analysis_identifying_partially_distributed_representations_for_interpretable_deep_learning", "_bibtex": "@misc{\ndimanov2019stepwise,\ntitle={Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning},\nauthor={Botty Dimanov and Mateja Jamnik},\nyear={2019},\nurl={https://openreview.net/forum?id=SyeBqsRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper530/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353183341, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper530/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper530/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper530/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353183341}}}, {"id": "HJe2emInR7", "original": null, "number": 2, "cdate": 1543426803785, "ddate": null, "tcdate": 1543426803785, "tmdate": 1543426803785, "tddate": null, "forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper530/Official_Comment", "content": {"title": "General Rebuttal", "comment": "We thank the reviewers for their thoughtful comments. \n\nThe main concern of the reviewers is whether the magnitude of gradients can be used to determine neuron relevance and they suggested to illustrate the validity of this approach through a toy example. \n\nFirst, the novelty of our work is the new approach of generating dependency graphs to explaining neural networks, and we present our first algorithms for computing such graphs. There could be others, which we will explore in future work. We thank Reviewer 1 for bring up PatternNet(Kindermans et al. 2017), which addresses the limitation of gradient-based approaches that in the context of heatmap visualisation the gradients answer the question which pixels makes this classification more or less like bird for example, not which pixels make this classification a bird.  Gradients indicate which neurons should change the least, so that there would be the greatest change in the output (Samek 2017). Since we are interested in the global understanding of the network and not in why in particular this image was classified as a bird, we direct our attention to the neurons that mostly influence the activation of an adjacent neuron.  \n\nIn the future it would be interesting to compare the quality of dependency graphs produced through sensitivity analysis and PatternNet. However, we believe that the magnitude of the gradients approach is suitable for the current iteration of the paper for two reasons.\nFirst, it is important to clarify that we are talking about the relative magnitudes of the gradients at every neuron, not the absolute. In that case regardless of how much we scale the weights, for every neuron we will have a relative ranking between neurons indicating the sensitivity of the upper layer neuron (target neuron) to its lower layer connections. The ranking function can be improved. The Taylor expansion gives a linear approximation of the target neuron\u2019s activation, the weights of which we use as a proxy for the relevance. We utilise the relative values of these weights separately for every neuron. Hence, a strongly connected upper-layer neuron cannot dominate the relevance ranking of other upper layer neurons.\n\nSecond, in a toy example that will appear in the revised version, we compare the averaged (across target neurons) relevance ranking that is computed through activations, gradients and weights. Our experiments demonstrate that on a global level removing the neurons with the strongest absolute weights results in the greatest performance degradation. However, such relevance ranking corresponds solely to the overall performance of the network in contrast to a local performance for a specific class. Due to the fact that the aim of this paper is to extract dependency graphs local to different classes we utilise the gradients strategy, for which on the global level our experiment confirms the supposition that it outperforms the activations strategy.\n\nOur dependency graph evaluation on the large scale VGG16, which computes the performance difference between a forward pass through the proposed dependency graphs and a random dependency graph, would take us longer to complete due to lack of computational resources. Do the reviewers believe it is an integral part of the evaluation section?\n\nAdditional minor corrections:\n\nWe thank Reviewer 3 for bringing up Yosinski et al. (2015). Although there have been many papers [e.g. Zintgraf (2017); Zhou (2015)], which explore multiple instance-specific results to gain a model-centric understanding, they undertake a manual investigation of the instance-specific results. In contrast, we leverage the output of instance-specific results within our algorithm to produce a model-centric explanation. \n\nWe thank Reviewer 2 for pointing out the visualisation approach. We will make it explicit that we leverage guided back-propagation for the visualisations, and we are planing to compare the effect of using the guided-backpropagation definition to compute the relative relevance ranking; however, we have not mentioned guided-backpropagation within the method description because our method is inherently different \u2014  in contrast to guided-backpropagation, we do not necessarily exclude units with negative forward activations.\n\nWe will remove the speculative claim about the German Shepard from the evaluation. \n\nIt will be made explicit that logits are used at the softmax layer.\n\nReferences:\nSamek, Wojciech, et al. \"Evaluating the visualization of what a deep neural network has learned.\" IEEE transactions on neural networks and learning systems 28.11 (2017): 2660-2673.\nKindermans, Pieter-Jan, et al. \"PatternNet and PatternLRP\u2013improving the interpretability of neural networks.\" stat 1050 (2017): 16.\nZhou, Bolei, et al. \"Object detectors emerge in deep scene cnns.\" (2015).\nZintgraf, Luisa M., et al. \"Visualizing deep neural network decisions: Prediction difference analysis.\" arXiv preprint arXiv:1702.04595 (2017)."}, "signatures": ["ICLR.cc/2019/Conference/Paper530/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper530/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper530/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning", "abstract": " In this paper, we introduce a novel method, called step-wise sensitivity analysis, which makes three contributions towards increasing the interpretability of Deep Neural Networks (DNNs). First, we are the first to suggest a methodology that aggregates results across input stimuli to gain model-centric results. Second, we linearly approximate the neuron activation and propose to use the outlier weights to identify distributed code. Third, our method constructs a dependency graph of the relevant neurons across the network to gain fine-grained understanding of the nature and interactions of DNN's internal features. The dependency graph illustrates shared subgraphs that generalise across 10 classes and can be clustered into semantically related groups. This is the first step towards building decision trees as an interpretation of learned representations.", "keywords": ["Interpretability", "Interpretable Deep Learning", "XAI", "dependency graph", "sensitivity analysis", "outlier detection", "instance-specific", "model-centric"], "authorids": ["botty.dimanov@cl.cam.ac.uk", "mateja.jamnik@cl.cam.ac.uk"], "authors": ["Botty Dimanov", "Mateja Jamnik"], "TL;DR": "We find dependency graphs between learned representations as a first step towards building decision trees to interpret the representation manifold.", "pdf": "/pdf/ba387065b9e05da99fe9e68338a91c0bdea44438.pdf", "paperhash": "dimanov|stepwise_sensitivity_analysis_identifying_partially_distributed_representations_for_interpretable_deep_learning", "_bibtex": "@misc{\ndimanov2019stepwise,\ntitle={Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning},\nauthor={Botty Dimanov and Mateja Jamnik},\nyear={2019},\nurl={https://openreview.net/forum?id=SyeBqsRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper530/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623846, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyeBqsRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper530/Authors", "ICLR.cc/2019/Conference/Paper530/Reviewers", "ICLR.cc/2019/Conference/Paper530/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper530/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper530/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper530/Authors|ICLR.cc/2019/Conference/Paper530/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper530/Reviewers", "ICLR.cc/2019/Conference/Paper530/Authors", "ICLR.cc/2019/Conference/Paper530/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623846}}}, {"id": "rkxd0SW7R7", "original": null, "number": 1, "cdate": 1542817232093, "ddate": null, "tcdate": 1542817232093, "tmdate": 1542817232093, "tddate": null, "forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper530/Official_Comment", "content": {"title": "Thank you for the constructive suggestion. A revision of the paper will follow shortly.", "comment": "Dear Reviewers,\n\nOur sincerest gratitude for your fruitful comments and thoughtful suggestions!\nYou have considerably helped us to improve our work.\n\nWe have been conducting experiments to address the two main concerns, namely 1) the mathematical grounding of SSA and 2) a stronger evaluation section that contains a simple toy example and demonstrates the utility of the dependency graphs.\n\nA revision of the paper will follow shortly, where we will also include discussions w.r.t  Kindermans et al. (2017) & (Yosinski et al. 2015). "}, "signatures": ["ICLR.cc/2019/Conference/Paper530/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper530/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper530/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning", "abstract": " In this paper, we introduce a novel method, called step-wise sensitivity analysis, which makes three contributions towards increasing the interpretability of Deep Neural Networks (DNNs). First, we are the first to suggest a methodology that aggregates results across input stimuli to gain model-centric results. Second, we linearly approximate the neuron activation and propose to use the outlier weights to identify distributed code. Third, our method constructs a dependency graph of the relevant neurons across the network to gain fine-grained understanding of the nature and interactions of DNN's internal features. The dependency graph illustrates shared subgraphs that generalise across 10 classes and can be clustered into semantically related groups. This is the first step towards building decision trees as an interpretation of learned representations.", "keywords": ["Interpretability", "Interpretable Deep Learning", "XAI", "dependency graph", "sensitivity analysis", "outlier detection", "instance-specific", "model-centric"], "authorids": ["botty.dimanov@cl.cam.ac.uk", "mateja.jamnik@cl.cam.ac.uk"], "authors": ["Botty Dimanov", "Mateja Jamnik"], "TL;DR": "We find dependency graphs between learned representations as a first step towards building decision trees to interpret the representation manifold.", "pdf": "/pdf/ba387065b9e05da99fe9e68338a91c0bdea44438.pdf", "paperhash": "dimanov|stepwise_sensitivity_analysis_identifying_partially_distributed_representations_for_interpretable_deep_learning", "_bibtex": "@misc{\ndimanov2019stepwise,\ntitle={Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning},\nauthor={Botty Dimanov and Mateja Jamnik},\nyear={2019},\nurl={https://openreview.net/forum?id=SyeBqsRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper530/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623846, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyeBqsRctm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper530/Authors", "ICLR.cc/2019/Conference/Paper530/Reviewers", "ICLR.cc/2019/Conference/Paper530/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper530/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper530/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper530/Authors|ICLR.cc/2019/Conference/Paper530/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper530/Reviewers", "ICLR.cc/2019/Conference/Paper530/Authors", "ICLR.cc/2019/Conference/Paper530/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623846}}}, {"id": "S1e_LoqbT7", "original": null, "number": 3, "cdate": 1541675855959, "ddate": null, "tcdate": 1541675855959, "tmdate": 1541675855959, "tddate": null, "forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper530/Official_Review", "content": {"title": "Replacement for an original reviewer", "review": "The paper proposes a modification of the saliency map/gradient approach to explain neural networks.\n\n# Method summary\n\nThe approach is as follows:\nFor each layer, the gradient w.r.t. it's input layer is computed for multiple images concurrently.\nThen for conv layers, the activations are averaged per feature map (over space).\nAs a result, for both fully connected and convolutional layers there is a 3D feature map.\nFrom these at most b positive outliers are selected to be propagated further. \nWhat is a bit strange is that in the results section, guided backpropagation is mentioned and clearly used in the visualizations but not mentioned in the technical description.\n\n# Recommendation\n\nThe current evaluation is definitely not sufficient for acceptance. \nThe evaluation is done in a purely qualitative matter (even in section 4.1 Quantitive justification of outliers as relevant neurons). The results appear to be interesting but there is no effort done to confirm that the neurons considered to be relevant are truly relevant. On top of that, it is also evaluated only on a single network and no theoretical justification is provided.\n\n# Discussion w.r.t. the evaluation\n\nTo improve section 4.1,  the authors could for example drop out the most important neurons and re-evaluate the model to see whether the selected neurons have a larger impact than randomly selected neurons. Since the network is trained with dropout, it should be somewhat robust to this. This would not be a definitive test, but it would be more convincing than the current evaluation. Furthermore high values do not imply importance. \n\nIt might be possible that I misunderstood the experiment in Figure 2. So please correct me if this is the case in the reasoning below. \nIn figure 2, FC2 is analyzed. This is the second to last layer. So I assume that only the back-propagation from logits (I make this assumption since this is what is done commonly and it is not specified in the paper) to FC2 was used. Since we start at the same output neuron for a single class, all visualisations will use the same weight vector that is propagated back. The only difference between images comes from which Relu's were active but the amount if variability is probably small since the images were selected to be classified with high confidence. Hence, the outliers originate from a large weight to a specific neuron. \n\nThe interpretation in the second paragraph of section 4.2.1 is not scientific at all. I looked at the German Shepherd images and there are no teeth visible. But again, this is a claim that can be falsified easily. Compare the results when german Shepherds with teeth visible are used and when they are not. The same holds for the hypothesis of the degree of danger w.r.t. the separation. \n\nFinally, there is no proof that the approach works better than using the magnitude of neuron activations themselves, which would be an interesting baseline. \n\nAdditional remarks\n---------------------------\n\nThe following is an odd formulation since it takes a 3D tensor out of a 5D one and mixes these in the explanation:\n\"... the result of equation for is a 5D relevance tensor $\\omega^l_{n,i,..} \\in R^{H\\times W\\times K} .....\"\n\nThe quality of the figures is particularly poor. \n- Figure 1 b did not help me to understand the concept.\n- Figure 2 The text on the figure is unreadable. \n- Figure 4a is not readable when printed. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper530/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning", "abstract": " In this paper, we introduce a novel method, called step-wise sensitivity analysis, which makes three contributions towards increasing the interpretability of Deep Neural Networks (DNNs). First, we are the first to suggest a methodology that aggregates results across input stimuli to gain model-centric results. Second, we linearly approximate the neuron activation and propose to use the outlier weights to identify distributed code. Third, our method constructs a dependency graph of the relevant neurons across the network to gain fine-grained understanding of the nature and interactions of DNN's internal features. The dependency graph illustrates shared subgraphs that generalise across 10 classes and can be clustered into semantically related groups. This is the first step towards building decision trees as an interpretation of learned representations.", "keywords": ["Interpretability", "Interpretable Deep Learning", "XAI", "dependency graph", "sensitivity analysis", "outlier detection", "instance-specific", "model-centric"], "authorids": ["botty.dimanov@cl.cam.ac.uk", "mateja.jamnik@cl.cam.ac.uk"], "authors": ["Botty Dimanov", "Mateja Jamnik"], "TL;DR": "We find dependency graphs between learned representations as a first step towards building decision trees to interpret the representation manifold.", "pdf": "/pdf/ba387065b9e05da99fe9e68338a91c0bdea44438.pdf", "paperhash": "dimanov|stepwise_sensitivity_analysis_identifying_partially_distributed_representations_for_interpretable_deep_learning", "_bibtex": "@misc{\ndimanov2019stepwise,\ntitle={Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning},\nauthor={Botty Dimanov and Mateja Jamnik},\nyear={2019},\nurl={https://openreview.net/forum?id=SyeBqsRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper530/Official_Review", "cdate": 1542234440348, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper530/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335745244, "tmdate": 1552335745244, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper530/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlqCanjnX", "original": null, "number": 2, "cdate": 1541291474286, "ddate": null, "tcdate": 1541291474286, "tmdate": 1541533915918, "tddate": null, "forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper530/Official_Review", "content": {"title": "Ok but not good enough", "review": "Summary: \nThis paper introduces step-wise sensitivity analysis (SSA), which is a modification of saliency maps (Baehrens et al. 2010, Simonyan et al. 2013) to a per-layer implementation. Instead of only measuring the importance of input nodes (e.g. pixels) to the classification, SSA measures the importance of all nodes at each layer. This allows for a way to find the important sub-nodes for each node in the tree given a particular sample. It is then straightforward to aggregate results across different input samples and output a dependency graph for nodes.\n\nNovelty:\nThe technical contribution is a very simple extension of Simonyan et al. 2013. The main novelty lies within the created dependency graph from the node importance weights, but the usefulness of such graph is unclear. In addition, the claim that this is the first method that aggregates results of an instance-specific method to gain model-centric results is a stretch considering other works have found important nodes or filters for a specific class by aggregating across instance-specific samples (Yosinski et al. 2015).\n\nEvaluation: \nThe idea of producing an interpretable dependency graph for nodes is interesting, and the possible conclusions from such graphs seem promising. However, most of the interesting possible conclusions seem to be put off for future work. I don\u2019t believe the experiments are sufficient to show the significance of SSA. The main hypothesis is that dependency graphs allow for a way to interpret the model across samples, but it doesn\u2019t show any conclusive results about the data or models that wasn\u2019t previously known. The results are mostly speculative, such as the fact that German shepherd and great white shark nodes are clustered together, possibly due to the fact that both of these classes share a PDR encoding sharp teeth, but that is never actually demonstrated.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper530/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning", "abstract": " In this paper, we introduce a novel method, called step-wise sensitivity analysis, which makes three contributions towards increasing the interpretability of Deep Neural Networks (DNNs). First, we are the first to suggest a methodology that aggregates results across input stimuli to gain model-centric results. Second, we linearly approximate the neuron activation and propose to use the outlier weights to identify distributed code. Third, our method constructs a dependency graph of the relevant neurons across the network to gain fine-grained understanding of the nature and interactions of DNN's internal features. The dependency graph illustrates shared subgraphs that generalise across 10 classes and can be clustered into semantically related groups. This is the first step towards building decision trees as an interpretation of learned representations.", "keywords": ["Interpretability", "Interpretable Deep Learning", "XAI", "dependency graph", "sensitivity analysis", "outlier detection", "instance-specific", "model-centric"], "authorids": ["botty.dimanov@cl.cam.ac.uk", "mateja.jamnik@cl.cam.ac.uk"], "authors": ["Botty Dimanov", "Mateja Jamnik"], "TL;DR": "We find dependency graphs between learned representations as a first step towards building decision trees to interpret the representation manifold.", "pdf": "/pdf/ba387065b9e05da99fe9e68338a91c0bdea44438.pdf", "paperhash": "dimanov|stepwise_sensitivity_analysis_identifying_partially_distributed_representations_for_interpretable_deep_learning", "_bibtex": "@misc{\ndimanov2019stepwise,\ntitle={Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning},\nauthor={Botty Dimanov and Mateja Jamnik},\nyear={2019},\nurl={https://openreview.net/forum?id=SyeBqsRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper530/Official_Review", "cdate": 1542234440348, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper530/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335745244, "tmdate": 1552335745244, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper530/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hygkyo2t27", "original": null, "number": 1, "cdate": 1541159638675, "ddate": null, "tcdate": 1541159638675, "tmdate": 1541533915714, "tddate": null, "forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "invitation": "ICLR.cc/2019/Conference/-/Paper530/Official_Review", "content": {"title": "Interesting idea, but serious concerns about whether it produces meaningful results", "review": "Summary:\nThe paper introduces a new approach for interpreting deep neural networks called step-wise sensitivity analysis. The approach is conceptually quite simple and involves some interesting ideas, but I have some serious concerns whether the output produced by this method carries any meaning at all. If the authors were able to refute my concerns detailed below, I would raise my score substantially.\n\n\nStrengths:\n+ Potentially interesting heuristic to identify groups of feature channels in DNNs that encode image features in a distributed way\n\n\nWeaknesses:\n- Using the magnitude of the gradient in intermediate layers of ReLU networks is not indicative of importance\n- No verification of the method on a simple toy example\n\n\nDetails:\n\n\nMain issue: Magnitude of the gradient as a measure of importance.\n\nI have trouble with the use of the gradient to identify \"outliers,\" which are deemed important. Comparing the magnitude of activations across features does not make sense in a convnet with ReLUs, because the scale of activations in each feature map is arbitrary and meaningless. Consider a feature map h^l[i,x,y,f] (l=layer, i=images, x/y=pixels, f=feature channels), convolution kernels w^l[x,y,k,f] (k=input channels, f=output channels) and biases b^l[f]:\n\nh^l[i,:,:,f] = ReLU(b^l[f] + \\sum_k h^(l-1)[i,:,:,k] * w^l[:,:,k,f])\n\nAssume, without loss of generality, the feature map h^l[:,:,:,f] has mean zero and unit variance, computed over all images (i) in the training set and all pixels (x,y). Let's multiply all \"incoming\" convolution kernels w^l[:,:,:,f] and biases b^l[f] by 10. As a result, this feature map will now have a variance of 100 (over images and pixels). Additionally, let's divide all \"outgoing\" kernels w^(l+1)[:,:,f,:] by 10.\n\nSimple linear algebra suffices to verify that the next layer's features h^(l+1) -- and therefore the entire network output -- are unaffected by this manipulation. However, the gradient of all units in this feature map is 10x as high as that of the original network. Of course the gradient in layer l-1 will be unaltered once we backpropagate through w^l, but because of the authors' selection of \"outlier\" units, their graph will look vastly different.\n\nIn other words, it is unclear to me how any method based on gradients should be able to meaningfully assign \"importance\" to entire feature maps. One could potentially start with the assumption of equal importance when averaged over all images in the dataset and normalize the activations. For instance, ReLU networks with batch norm and without post-normalization scaling would satisfy this assumption. However, for VGG-16 studied here, this is not the case.\n\nOn a related note, the authors' observation in Fig. 4b that the same features are both strongly positive and strongly negative outliers for the same class suggests that this feature simply has a higher variance than the others in the same layer and is therefore picked most of the time. Similarly, the fact that vastly different classes such as shark and German Sheppard share the same subgraphs speaks to the same potential issue.\n\n\nSecondary issue: No verification of the method on simple, understandable toy example.\n\nAs shown by Kindermans et al. [1], gradient-based attribution methods fail to produce the correct result even for the simplest possible linear examples. The authors do not seem to be aware of this work (at least it's not cited), so I suggest they have a look and discuss the implications w.r.t. their own work. In addition, I think the authors should demonstrate on a simple, controlled (e.g. linear) toy example that their method works as expected before jumping to a deep neural network. I suppose the issue discussed above will also surface in purely linear multi-layer networks, where the intermediate layers (and their gradients) can be rescaled arbitrarily without changing the network's function.\n\n\nReferences:\n[1] Kindermans P-J, Sch\u00fctt KT, Alber M, M\u00fcller K-R, Erhan D, Kim B, D\u00e4hne S (2017) Learning how to explain neural networks: PatternNet and PatternAttribution. arXiv:170505598. Available at: http://arxiv.org/abs/1705.05598", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper530/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning", "abstract": " In this paper, we introduce a novel method, called step-wise sensitivity analysis, which makes three contributions towards increasing the interpretability of Deep Neural Networks (DNNs). First, we are the first to suggest a methodology that aggregates results across input stimuli to gain model-centric results. Second, we linearly approximate the neuron activation and propose to use the outlier weights to identify distributed code. Third, our method constructs a dependency graph of the relevant neurons across the network to gain fine-grained understanding of the nature and interactions of DNN's internal features. The dependency graph illustrates shared subgraphs that generalise across 10 classes and can be clustered into semantically related groups. This is the first step towards building decision trees as an interpretation of learned representations.", "keywords": ["Interpretability", "Interpretable Deep Learning", "XAI", "dependency graph", "sensitivity analysis", "outlier detection", "instance-specific", "model-centric"], "authorids": ["botty.dimanov@cl.cam.ac.uk", "mateja.jamnik@cl.cam.ac.uk"], "authors": ["Botty Dimanov", "Mateja Jamnik"], "TL;DR": "We find dependency graphs between learned representations as a first step towards building decision trees to interpret the representation manifold.", "pdf": "/pdf/ba387065b9e05da99fe9e68338a91c0bdea44438.pdf", "paperhash": "dimanov|stepwise_sensitivity_analysis_identifying_partially_distributed_representations_for_interpretable_deep_learning", "_bibtex": "@misc{\ndimanov2019stepwise,\ntitle={Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning},\nauthor={Botty Dimanov and Mateja Jamnik},\nyear={2019},\nurl={https://openreview.net/forum?id=SyeBqsRctm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper530/Official_Review", "cdate": 1542234440348, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyeBqsRctm", "replyto": "SyeBqsRctm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper530/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335745244, "tmdate": 1552335745244, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper530/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 7}