{"notes": [{"id": "r1eOnh4YPB", "original": "rkea4RrfPH", "number": 194, "cdate": 1569438895685, "ddate": null, "tcdate": 1569438895685, "tmdate": 1577168280194, "tddate": null, "forum": "r1eOnh4YPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["youkaichao@gmail.com", "mingsheng@tsinghua.edu.cn", "jimwang@tsinghua.edu.cn", "jordan@cs.berkeley.edu"], "title": "How Does Learning Rate Decay Help Modern Neural Networks?", "authors": ["Kaichao You", "Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "pdf": "/pdf/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "TL;DR": "We provide another novel explanation of learning rate decay: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.", "abstract": "Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization. Common beliefs in how lrDecay works come from the optimization analysis of (Stochastic) Gradient Descent: 1) an initially large learning rate accelerates training or helps the network escape spurious local minima; 2) decaying the learning rate helps the network converge to a local minimum and avoid oscillation. Despite the popularity of these common beliefs, experiments suggest that they are insufficient in explaining the general effectiveness of lrDecay in training modern neural networks that are deep, wide, and nonconvex. We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns. The proposed explanation is validated on a carefully-constructed dataset with tractable pattern complexity. And its implication, that additional patterns learned in later stages of lrDecay are more complex and thus less transferable, is justified in real-world datasets. We believe that this alternative explanation will shed light into the design of better training strategies for modern neural networks.", "keywords": ["Learning rate decay", "Optimization", "Explainability", "Deep learning", "Transfer learning"], "paperhash": "you|how_does_learning_rate_decay_help_modern_neural_networks", "original_pdf": "/attachment/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "_bibtex": "@misc{\nyou2020how,\ntitle={How Does Learning Rate Decay Help Modern Neural Networks?},\nauthor={Kaichao You and Mingsheng Long and Jianmin Wang and Michael I. Jordan},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eOnh4YPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "VIOBObL07F", "original": null, "number": 1, "cdate": 1576798689907, "ddate": null, "tcdate": 1576798689907, "tmdate": 1576800945252, "tddate": null, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "invitation": "ICLR.cc/2020/Conference/Paper194/-/Decision", "content": {"decision": "Reject", "comment": "This paper seeks to understand the effect of learning rate decay in neural net training. This is an important question in the field and this paper also proposes to show why previous explanations were not correct. However, the reviewers found that the paper did not explain the experimental setup enough to be reproducible. Furthermore, there are significant problems with the novelty of the work due to its overlap with works such as (Nakiran et al., 2019), (Li et al. 2019) or (Jastrz\u0119bski et al. 2017).", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youkaichao@gmail.com", "mingsheng@tsinghua.edu.cn", "jimwang@tsinghua.edu.cn", "jordan@cs.berkeley.edu"], "title": "How Does Learning Rate Decay Help Modern Neural Networks?", "authors": ["Kaichao You", "Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "pdf": "/pdf/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "TL;DR": "We provide another novel explanation of learning rate decay: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.", "abstract": "Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization. Common beliefs in how lrDecay works come from the optimization analysis of (Stochastic) Gradient Descent: 1) an initially large learning rate accelerates training or helps the network escape spurious local minima; 2) decaying the learning rate helps the network converge to a local minimum and avoid oscillation. Despite the popularity of these common beliefs, experiments suggest that they are insufficient in explaining the general effectiveness of lrDecay in training modern neural networks that are deep, wide, and nonconvex. We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns. The proposed explanation is validated on a carefully-constructed dataset with tractable pattern complexity. And its implication, that additional patterns learned in later stages of lrDecay are more complex and thus less transferable, is justified in real-world datasets. We believe that this alternative explanation will shed light into the design of better training strategies for modern neural networks.", "keywords": ["Learning rate decay", "Optimization", "Explainability", "Deep learning", "Transfer learning"], "paperhash": "you|how_does_learning_rate_decay_help_modern_neural_networks", "original_pdf": "/attachment/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "_bibtex": "@misc{\nyou2020how,\ntitle={How Does Learning Rate Decay Help Modern Neural Networks?},\nauthor={Kaichao You and Mingsheng Long and Jianmin Wang and Michael I. Jordan},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eOnh4YPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730186, "tmdate": 1576800282929, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper194/-/Decision"}}}, {"id": "rJxKrLM3KS", "original": null, "number": 1, "cdate": 1571722817497, "ddate": null, "tcdate": 1571722817497, "tmdate": 1572972626882, "tddate": null, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "invitation": "ICLR.cc/2020/Conference/Paper194/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper investigates the role of learning rate decay in neural network training. While there are prevalent ideas of how/why learning rate decay help both optimization and generalization of neural networks, this work proposes interpretation based on pattern complexity. The mechanism the paper proposes is that initial learning rate helps ignore noise in the beginning and decayed learning rate help to learn complex patterns. \n\nThe question the paper tackles is a very important question in understanding deep learning that requires careful study. As the learning rate schedule benefits neural network models beyond specific domain, this question has high significance and potential impact.\n\nThe authors propose a view from a pattern complexity. I think this view point is well motivated in the light of Li et al. (2019). The artificial task constructed by authors reveals that with different phases of learning rates, different types of complexity are being learnt. Also transfer learning tasks also reveals that one might want to use models before dropping the learning rate for purpose of transfer learning.\n\nWhile the question studied is of high importance, I am not confident that the methods presented justifies the claims in the paper. Especially authors analysis on how previous understanding of learning rate decay is flawed doesn\u2019t seem fully supported. With current submission I slightly lean towards rejecting. \n\nOne reason is that overall the details of experiments are not specific enough that I personally wouldn\u2019t feel comfortable reproducing the results. For example, in Section 4, was data augmentation or weight decay used? What was the mini-batch size used for SGD experiments? Which learning rate is used for  large learning rate and to which learning rate was it decayed to? I think even one is using full batch GD, the performance shown in Figure 4 which is just above 60% test accuracy is quite low for models like WideResNet.\n\nAlso I do not agree with the claim that SGD explanation leads to training curves as in Figure 6. The plot indicates large learning rate / lower learning rate have about the same probability of reaching the 'global minima\u2019. However as described in section 3.2, the role of lowering the learning rate should be increasing the probability of global minima. To me experiments shown in Figure 7, supports both explanations from Lecun et al (1991) Kleinberg et al. (2018) described in Table1. \n\nFew extra comments: \n\nOne question I have regarding transferability analysis. Kornblith et al. (2019) showed that better Imagenet models transfers better. However section 6 shows that  transferability is higher for stage 2 which has higher error. How do you reconcile this discrepancy? \n\nI don't quite comprehend the motivation for putting two complexity patterns in disjoint channels. Especially the comment \u201cThis mimics the intuition of patterns as the eye pattern and the nose pattern have different locations in an image of human face.\u201d is unclear. Could you elaborate?\n\nFigure 15 seems incomplete. Should there be arrows for decay points? Please also include details of those experiments too. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper194/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper194/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youkaichao@gmail.com", "mingsheng@tsinghua.edu.cn", "jimwang@tsinghua.edu.cn", "jordan@cs.berkeley.edu"], "title": "How Does Learning Rate Decay Help Modern Neural Networks?", "authors": ["Kaichao You", "Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "pdf": "/pdf/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "TL;DR": "We provide another novel explanation of learning rate decay: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.", "abstract": "Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization. Common beliefs in how lrDecay works come from the optimization analysis of (Stochastic) Gradient Descent: 1) an initially large learning rate accelerates training or helps the network escape spurious local minima; 2) decaying the learning rate helps the network converge to a local minimum and avoid oscillation. Despite the popularity of these common beliefs, experiments suggest that they are insufficient in explaining the general effectiveness of lrDecay in training modern neural networks that are deep, wide, and nonconvex. We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns. The proposed explanation is validated on a carefully-constructed dataset with tractable pattern complexity. And its implication, that additional patterns learned in later stages of lrDecay are more complex and thus less transferable, is justified in real-world datasets. We believe that this alternative explanation will shed light into the design of better training strategies for modern neural networks.", "keywords": ["Learning rate decay", "Optimization", "Explainability", "Deep learning", "Transfer learning"], "paperhash": "you|how_does_learning_rate_decay_help_modern_neural_networks", "original_pdf": "/attachment/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "_bibtex": "@misc{\nyou2020how,\ntitle={How Does Learning Rate Decay Help Modern Neural Networks?},\nauthor={Kaichao You and Mingsheng Long and Jianmin Wang and Michael I. Jordan},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eOnh4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575566238177, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper194/Reviewers"], "noninvitees": [], "tcdate": 1570237755668, "tmdate": 1575566238191, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper194/-/Official_Review"}}}, {"id": "SygcfD6htS", "original": null, "number": 2, "cdate": 1571768082072, "ddate": null, "tcdate": 1571768082072, "tmdate": 1572972626849, "tddate": null, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "invitation": "ICLR.cc/2020/Conference/Paper194/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contributions:\nThis paper investigates the use of learning rate decay in deep neural networks.  The main contribution is an empirical analysis trying to understand lr decay. Authors claim that the high-learning rate phase provides regularization and prevents the network to fit/memorize noisy data initially to focus on simple patterns. \n\nAuthors design a set of experiments showing that lr decay allows SGD to first fit \u2018simple patterns\u2019 instead of more complex/noisy one. They also show that the model transferability decreases through training.\n\nIn addition to those experiments, authors provide experimental results which aims at contradicting common beliefs on lr decay.\n\nNovelty/Significance:\nMy main concern is about the novelty/significance of the paper.  Similar argument has already been made in (Nakiran et al., 2019) or (Li et al. 2019), which are cited in the paper, and other works such as \u201cOn the Spectral Bias of Neural Networks\u201d. In addition, prior to this work, the paper \u201cThree factors Influencing Minima in SGD\u201d empirically showed that the noise in SGD (which is controlled by the learning rate) prevents memorization (see Figure 6 in their paper). Although the latter work did not focus the lr decay, it is not clear to me how the main argument of the current paper differs from it. \n\nAdditional comments:\n-\tIt is unclear to me why the model the model before and after decay should have the same training performance in section 4.2. This would assume a specific geometry of the loss function (i.e. that the loss surface is not significantly narrower so that you can reach it with a high-learning rate), but authors do not provide evidence of it.\n-\tIt would be nice to provide more details about the experimental settings. What is the base learning rate? Did you try different learning rate? Do you use with SGD-momentum, weight-decay?\n-\tHow to combine the current explanation of lr decay with the observation that learning rate warm-up has been a successful way to train neural network in the large batch setting?\n-\tThe transferability experiment confounds two factors: the number of iteration steps   and the learning rates. Does training a model with low learning rate, but with a number a step equal to high learning rate lead to lower transferability?\n-\tIt would be nice to validate the hypothesis on more datasets/models.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper194/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper194/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youkaichao@gmail.com", "mingsheng@tsinghua.edu.cn", "jimwang@tsinghua.edu.cn", "jordan@cs.berkeley.edu"], "title": "How Does Learning Rate Decay Help Modern Neural Networks?", "authors": ["Kaichao You", "Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "pdf": "/pdf/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "TL;DR": "We provide another novel explanation of learning rate decay: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.", "abstract": "Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization. Common beliefs in how lrDecay works come from the optimization analysis of (Stochastic) Gradient Descent: 1) an initially large learning rate accelerates training or helps the network escape spurious local minima; 2) decaying the learning rate helps the network converge to a local minimum and avoid oscillation. Despite the popularity of these common beliefs, experiments suggest that they are insufficient in explaining the general effectiveness of lrDecay in training modern neural networks that are deep, wide, and nonconvex. We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns. The proposed explanation is validated on a carefully-constructed dataset with tractable pattern complexity. And its implication, that additional patterns learned in later stages of lrDecay are more complex and thus less transferable, is justified in real-world datasets. We believe that this alternative explanation will shed light into the design of better training strategies for modern neural networks.", "keywords": ["Learning rate decay", "Optimization", "Explainability", "Deep learning", "Transfer learning"], "paperhash": "you|how_does_learning_rate_decay_help_modern_neural_networks", "original_pdf": "/attachment/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "_bibtex": "@misc{\nyou2020how,\ntitle={How Does Learning Rate Decay Help Modern Neural Networks?},\nauthor={Kaichao You and Mingsheng Long and Jianmin Wang and Michael I. Jordan},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eOnh4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575566238177, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper194/Reviewers"], "noninvitees": [], "tcdate": 1570237755668, "tmdate": 1575566238191, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper194/-/Official_Review"}}}, {"id": "S1lvGJSE9H", "original": null, "number": 3, "cdate": 1572257550549, "ddate": null, "tcdate": 1572257550549, "tmdate": 1572972626807, "tddate": null, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "invitation": "ICLR.cc/2020/Conference/Paper194/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: This paper investigates the way decaying the learning rate helps the training of neural networks. First the paper discusses about other existing hypothesis such as the \u201cGradient Descent Hypothesis\u201d by Lecun et al 1991 and SGD explanation by Kleinberg et al 2018. Then the paper tries to find contradicting examples against those two hypothesis with experiments. Then they propose their explanation which suggests that initially fitting noisy data and then decaying it helps it to learn more complex data. Then the paper tries to experimentally explain why the other explanations fail and theirs is better.\n\nThis paper is very badly written. The authors should definitely rethink about the organization of the paper. The first pages is mostly about the background material with figures taken from other papers. The terminology that is being used in this paper is very vague. They used the term complex patterns in the paper, but don\u2019t even explain it until page 6. When the paper explains it, still the notion itself how to compute it in a tractable way is a bit vague for neural networks.\n\nThis paper proposes all those different explanations of how SGD works just as if they are completely orthogonal. However, for example learning both the proposed explanation in this paper and the fact that learning rate decay improves stability can both be true. \n\nThe experimental arguments are quite vague in this paper. The authors should give more details about their experimental setup, for example what is the starting learning rate, ending learning rate, details of scheduling, what type of distributed training method (sync or async SGD?) and etc\u2026 is missing. Without those details it is very difficult to interpret the experimental conclusions in for example section 4.1. In Figure 5, what does the converge interval mean? In Figure 7 and 4 all the curves are so similar, why so?\nThe arguments that learning rate decaying helps models to learn complex patterns and without decay the models can not learn those is a very strong claim and not very well supported in this paper.\n\nThe details of how the PS10 dataset is constructed is somewhat vague. In terms of arguments that decaying learning rates help the neural networks to learn simpler examples first and the harder ones have already been done in other papers such as Li et al 2019 which is also cited in this paper. It is not completely clear what this paper contributes over those other existing papers :)\n\n\nQuestions:\nHow does the cyclic learning rates come into the picture. Can one explain why cyclic learning rates works with the hypothesis that this paper processes?\nHow would different types of annealing methods come into the picture: e.g. linearly decaying the learning rate, exponential decay and etc\u2026\nWhy are there two columns of \u201csupported\u201d in Table 1? What is the difference between them.\nHow does the adaptive learning rate algorithms such as Adam come into the picture with the learning rate decay hypothesis?"}, "signatures": ["ICLR.cc/2020/Conference/Paper194/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper194/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youkaichao@gmail.com", "mingsheng@tsinghua.edu.cn", "jimwang@tsinghua.edu.cn", "jordan@cs.berkeley.edu"], "title": "How Does Learning Rate Decay Help Modern Neural Networks?", "authors": ["Kaichao You", "Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "pdf": "/pdf/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "TL;DR": "We provide another novel explanation of learning rate decay: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.", "abstract": "Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization. Common beliefs in how lrDecay works come from the optimization analysis of (Stochastic) Gradient Descent: 1) an initially large learning rate accelerates training or helps the network escape spurious local minima; 2) decaying the learning rate helps the network converge to a local minimum and avoid oscillation. Despite the popularity of these common beliefs, experiments suggest that they are insufficient in explaining the general effectiveness of lrDecay in training modern neural networks that are deep, wide, and nonconvex. We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns. The proposed explanation is validated on a carefully-constructed dataset with tractable pattern complexity. And its implication, that additional patterns learned in later stages of lrDecay are more complex and thus less transferable, is justified in real-world datasets. We believe that this alternative explanation will shed light into the design of better training strategies for modern neural networks.", "keywords": ["Learning rate decay", "Optimization", "Explainability", "Deep learning", "Transfer learning"], "paperhash": "you|how_does_learning_rate_decay_help_modern_neural_networks", "original_pdf": "/attachment/ff5f9ae02c0b504e1b0d791f86a5f0729e10fdc8.pdf", "_bibtex": "@misc{\nyou2020how,\ntitle={How Does Learning Rate Decay Help Modern Neural Networks?},\nauthor={Kaichao You and Mingsheng Long and Jianmin Wang and Michael I. Jordan},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eOnh4YPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eOnh4YPB", "replyto": "r1eOnh4YPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575566238177, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper194/Reviewers"], "noninvitees": [], "tcdate": 1570237755668, "tmdate": 1575566238191, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper194/-/Official_Review"}}}], "count": 5}