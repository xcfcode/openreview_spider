{"notes": [{"id": "H1x3SnAcYQ", "original": "Bye5qIacK7", "number": 1579, "cdate": 1538088003971, "ddate": null, "tcdate": 1538088003971, "tmdate": 1545355387969, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJxBDIyblE", "original": null, "number": 1, "cdate": 1544775260727, "ddate": null, "tcdate": 1544775260727, "tmdate": 1545354522548, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Meta_Review", "content": {"metareview": "This paper extends the DiCE estimator with a better control variate baseline for variance reduction. \nThe reviewers all think the paper is fairly clear and well written. However, as the reviews and discussion indicates,  there are several critical issues, including lack of explanation of the choice of baseline, the lack more realistic experiments and a few misleading assertions.  We encourage the authors to rewrite the paper to address these criticism. We believe this work will make a successful submission with proper modification in the future. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Good work but some critical issues need to be addressed "}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1579/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352785469, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1579/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1579/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352785469}}}, {"id": "BylyjkynC7", "original": null, "number": 8, "cdate": 1543397270704, "ddate": null, "tcdate": 1543397270704, "tmdate": 1543397270704, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "ryglNsz9p7", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "content": {"title": "Thank you for the relevant references and the review. We disagree on relevance (as you can imagine). ", "comment": "\u201cI would \"reverse engineer\" from the exact derivatives and figure out the corresponding DiCE formula.\u201d\nThere are two separate but related challenges: First of all, you need to formulate the correct baseline for the 2nd order derivatives. In particular we wanted to make sure that this baseline can be constructed using the standard state-value function. \nSecondly, this baseline needs to be constructed via a combination of DiCE operators, such that it can be included in the original objective. In other words, it needs to leave the evaluation of both, the original objective and the first order gradient, unchanged, but then also generate the correct terms for the 2nd order variance reduction when differentiated twice. This is non-trivial.\n\n\u201cb_w,.. Is this choice optimal for the second order control variate\u201d:\nThe main application of the DiCE formalism is within the context of Reinforcement Learning. In Reinforcement Learning, b_w is simply the state value-function, V(s). While this is not an optimal baseline, it is the best *practical* baseline based upon applications. It is used by state-of-the-art algorithms such as PPO (https://arxiv.org/abs/1707.06347), A3C (https://arxiv.org/pdf/1602.01783.pdf) and others.\nOur 2nd order baseline is the extension of this \u2018good enough\u2019 baseline to higher order terms. As such it is not an optimal baseline, but a \u2018good enough\u2019 and easy to implement one. \n\n\u201cdesign choice of b_w is not rigorously explained,\u201d:\nOur focus is on having a variance reduction baseline which keeps the estimator unbiased. As such the terms need to be of the form b_w as described in the paper. We\u2019ll clarify this further as appropriate. We respectfully disagree that MAML is a more complex task. LOLA has the same properties in terms of differentiating through the learning step of an agent. One major difference here is the continuous vs discrete action space.\n\n\u201cwhy using marginal distribution is valid when nodes in W are not independent.\u201d:\nThe nodes w are indeed independent when conditioned on their causes. Intuitively, you can think of this as the actions being sampled iid once you condition on the states. \n\n\u201cCite [1], [2]\u201d:\nMany thanks, we\u2019ll update the paper to include these references! \n\n\"correlation coefficient\":\n-Yes. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626716, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1x3SnAcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1579/Authors|ICLR.cc/2019/Conference/Paper1579/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626716}}}, {"id": "Byx4BJJ3RQ", "original": null, "number": 7, "cdate": 1543397179879, "ddate": null, "tcdate": 1543397179879, "tmdate": 1543397179879, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "HJl83sY967", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "content": {"title": "Thank you for an encouraging and insightful review", "comment": "Thank you for an encouraging and insightful review. We address specific points below.\n\n\u201cMore explanation/intuition\u201d:\nWe will add further intuition regarding the construction of the 2nd order baseline. The basic derivation is currently provided in the appendix on the bottom of page 11: When the DiCE objective is differentiated twice, the resulting terms can be rewritten as a double summation over nodes in the graph, with the inner sum containing an R_v (ie. sum of downstream costs). Importantly, the first order baseline does not provide a variance-reduction term for the R_v in term A^2. To do so we need a term that is the same as the A^2 term but contains -b_w instead of R_v, after double differentiation. That\u2019s how we constructed the 2nd order baseline. We fully agree that this part should be explained more clearly in the paper and will do so in the next version.\n\n\u201cshow that the reduction in variance is isolated to the second term\u201d:\nIn fact, we already show this in the paper. In Figure 3, we compare the performance of the original DiCE objective (including the first order baseline) with the DiCE objective including the 1st and 2nd order baselines. We will make this point more clear in the text.\n\n\u201c solve new, more difficult problems\u201d:\nThis is a great idea which we hope to address in future work. We also believe that making this method broadly available will encourage other research groups to use the tool to solve new problems.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626716, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1x3SnAcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1579/Authors|ICLR.cc/2019/Conference/Paper1579/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626716}}}, {"id": "SJgVaACi0X", "original": null, "number": 6, "cdate": 1543397052099, "ddate": null, "tcdate": 1543397052099, "tmdate": 1543397052099, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "SJxbt-3sjX", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "content": {"title": "A detailed review that partially misses the point due to a miss understanding of the purpose of the paper. ", "comment": "First of all, we would like to thank you for taking the time to review the paper and for taking the time to reply to our comments. This is appreciated, especially at a busy time of the year like this. \n\n@1) This is fair: While from a practitioner\u2019s perspective \u2018c = 1\u2019, from an educational point we agree that mentioning the optimal \u2018c\u2019 is valuable and we\u2019ll include this in the next revision of the paper. We had omitted this since it seemed irrelevant from a practical point of view, but you are right that it is useful background.\n\n@2)-5): All of these points indicate a miss-understanding of the paper\u2019s main point, which we will emphasise more in a revision:\nBy \u201cbetter baseline\u201d we mean literally \u201cbetter than current DiCE baseline\u201d. So the point is not a comparison between action-dependent and state-dependent baselines, but simply making a \u2018better baseline\u2019 easily available. \nWe agree that investigating action-dependent baselines is a fascinating research area. However, that\u2019s also the main reason why we do not focus on them in our work: The point of this paper is to make methods that have been proven to work (and are commonly being used), more easily available to practitioners. \nThis fits in nicely with the narrative of DiCE overall: The main point of DiCE is to facilitate the development and deployment of methods that require higher order gradients. Note that higher order gradients here are not the subject of the research, but merely a required tool. This process should be pursued in parallel to the development, investigation, and analysis of different higher order estimators and variance reduction techniques.\nWe also strongly disagree with the statement that the baseline should be tested on novel settings to increase the novelty of the paper: Reproducibility is absolutely vital for scientific progress, especially for the development of tools (such as DiCE and its baselines).\n\nSo I think what it comes down to is this: Are higher order gradient estimators well enough developed so that they can be used for practical applications as a tool rather than having to be the subject of research itself? Our belief is yes and this submission constitutes an important step in that direction. Reducing the sample-requirements by a factor of 100x should not be just brushed aside, even if it's on a 'toy' problem. \n\nFor future work we do agree that it would be great to extend our formalism for the baseline to include the option of having action-dependent baselines and others."}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626716, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1x3SnAcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1579/Authors|ICLR.cc/2019/Conference/Paper1579/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626716}}}, {"id": "SJxbt-3sjX", "original": null, "number": 1, "cdate": 1540239736537, "ddate": null, "tcdate": 1540239736537, "tmdate": 1543187829424, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "content": {"title": "Nicely written paper contributes useful trick; novelty too low for conference track, some correctness issues present", "review": "Overview: \nThis nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. I assess the novelty and scale of the current contribution as too low for publication at ICLR. Also, the paper includes a few incorrect assertions regarding the control variate framework as well as action-dependent baselines in reinforcement learning. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. I do not recommend publication at this time.\n\nPros:\nThe paper is well written modulo the issues discussed below. It strikes me as a valuable workshop contribution once the errors are addressed, but it lacks enough novelty for the main conference track.\n\nIssues:\n\n* (p.5) \"R_w and b_w are positively correlated by design, as they should be for variance reduction of the first order gradients.\"\n\nThis statement is not true in general. Intuitively, a control variate reduces variance because when a single estimate of an expectation of a function diverges from its true value according to some delta, then, with high probability, some function strongly correlated with that function will also diverge with a similar delta. Such a delta might be positive or negative, so long as the error may be appropriately modeled as drawn from some symmetric distribution (i.e. is Gaussian).\n\nControl variates are often estimated with an optimal scaling constant that depends on the covariance of the original function and its control variate. Due to the dependence on the covariance, the scaling constant flips sign as appropriate in order reduce variance for any delta. For more information, see the chapter on variance reduction and subsection on control variates in Sheldon Ross's textbook \"Simulation.\"\n\nThe fact that a control variate appears to work despite this is not surprising. Biased and suboptimal unbiased gradient estimators have been shown to work well for reasons not fully explored in the literature yet. See, for example, Tucker et al.'s \"Mirage of Action-Dependent Baselines\", https://arxiv.org/abs/1802.10031.\n\nSince the authors claim on page 6 that the baseline is positively correlated by design, this misunderstanding of the control variate framework appears to be baked into the baseline itself. I recommend the authors look into adaptively estimating an optimal scale for the baseline using a rolling estimator of the covariance and variance to fix this issue. See the Ross book cited above for full derivation of this optimal scale.\n\n* The second error is a mischaracterization of the use and utility of action-dependent baselines for RL problems, on page 6: \"We choose the baseline ... to be a function of state ... it must be independent of the action ....\" and \"it is essential to exclude the current action ... because the baselines ... must be independent of the action ... to remain unbiased.\" In the past year, a slew of papers have presented techniques for the use of action-dependent baselines, with mixed results (see the Mirage paper just cited), including two of the papers the authors cited.\n\nCons\n* Much of paper revises the DiCE estimator results, arguing for and explaining again those results rather than referring to them as a citation. \n* I assess the novelty of proposed contribution as too low for publication. The baseline is an extension of the same method used in the original paper, and does not generalize past the second order gradient, making the promising formalism of the DiCE estimator as infinitely differentiable still unrealizable in practice.\n* The experiments are practically identical to the DiCE estimator paper, also reducing the novelty and contribution of the paper.\n\n*EDIT: \nI thank the authors for a careful point-by-point comparison of our disagreements on this paper so that we may continue the discussion. However, none of the points I identified were addressed, and so I maintain my original score and urge against publication. In their rebuttal, the authors have defended errors and misrepresentations in the original submission, and so I provide a detailed response to each of the numbered issues below:\n\n(1) I acknowledge that it is common to set c=1 in experiments. This is not the same as the misstatements I cited, verbatim, in the paper that suggest this is required for variance reduction. My aim in identifying these mistakes is not to shame the authors (they appear to simply be typos) but simply to ensure that future work in this area begins with a correct understanding of the theory. I request again that the authors revise the cited lines that incorrectly state the reliance of a control variate on positive correlation. It is not enough to state that \"everyone knows\" what is meant when the actual claim is misleading.\n\n(2) Without more empirical investigation, the authors' new claim that a strictly state-value-function baseline is a strength rather than a weakness cannot be evaluated. This may be the case, and I would welcome some set of experiments that establish this empirical claim by comparing against state-action-dependent baselines. The authors appear to believe that state-action-dependent baselines are never effective in reducing variance, and this is perhaps the central error in the paper that should be addressed. See response (3). Were the authors to fix this, they would necessarily compare against state-action-dependent baselines, which would be of great value for the community at large in settling this open issue.\n\n(3) Action-dependent baselines have not been shown to be ineffective. I wish to strongly emphasize that this is not the conclusion of the Mirage paper, and the claim repeated in the authors' response (3) has not been validated empirically or analytically, and does not represent the state of variance reduction in reinforcement learning as of this note. I repeat a few key arguments from the Mirage paper in an attempt to dispel the authors' repeated misinterpretation of the paper.\n\nThe variance of the policy gradient estimator, subject to a baseline \"phi,\" is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper. This decomposition identifies a non-zero contribution from \"phi(a,s)\", the (adaptive or non-adaptive) baseline. The Mirage paper analyzes under what conditions such a contribution is expected to be non-negligible. Quoting from the paper:\n\"We expect this to be the case when single actions have a large effect on the overall discounted\nreturn (e.g., in a Cliffworld domain, where a single action could cause the agent to fall of the cliff and suffer a large negative reward).\"\nPlease see Sec. 3, \"Policy Gradient Variance Decomposition\" of the Mirage paper for further details.\nThe Mirage paper does indeed cast reasonable doubt on subsets of a few papers' experiments, and shows that the strong claim, mistakenly made by these papers, that state-action-dependence is always required for an adaptive control variate to reduce variance over state dependence, is not true. \n\nIt should be clear from the discussion of the paper to this point that this does _not_ imply the even stronger claim in \"A Better Second Order Baseline\" that action dependence is never effective and should no longer be considered as a means to reduce variance from a practitioner's point of view. Such a misinterpretation should not be legitimized through publication, as it will muddy the waters in future research. I again urge the authors to remove this mistake from the paper.\n\n(4) I acknowledge the efforts of the authors to ensure that adequate background is provided for readers. This is a thorny issue, and it is difficult to balance in any work. Since this material represents a sizeable chunk of the paper and is nearly identical to existing published work, it leads me to lower the score for novelty of contribution simply by that fact. Perhaps the authors could have considered placing the extensive background materials in the appendix and instead summarizing them briefly in the body of the paper, leaving more room for discussion and experimental validation beyond the synthetic cases already studied in the DiCE paper.\n\n(5), (6) In my review I provided specific, objective criteria by which I have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the DiCE paper. As I noted in response (4) above, this reduces space for further analysis and experimentation.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "cdate": 1542234199325, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977029, "tmdate": 1552335977029, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJl83sY967", "original": null, "number": 4, "cdate": 1542261678358, "ddate": null, "tcdate": 1542261678358, "tmdate": 1542261678358, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "content": {"title": "Interesting paper, could push it further", "review": "This paper extends the \"infinitely differentiable Monte Carlo gradient estimator\" (or DiCE) with a better control variate baseline for reducing the variance of the second order gradient estimates.\n\nThe paper is fairly clear and well written, and shows significant improvements on the tasks used in the DiCE paper.\n\nI think the paper would be a much stronger submission with the following improvements:\n\n- More explanation/intuition for how the authors came up with their new baseline (eq. (8)). As the paper currently reads, it feels as if it comes out of nowhere.\n- Some analysis of the variance of the two terms in the second derivative in eq. (11). In particular, it would be nice to show the variance of the two terms separately (for both DiCE and this paper), to show that the reduction in variance is isolated to the second term (I get that this must be the case, given the math, but would be nice to see some verification of this). Also I do not have good intuition for which of these two terms dominates the variance. \n- I appreciate that the authors tested their estimator on the same tasks as in the DiCE paper, which makes it easy to compare them. However, I think the paper would have much more impact if the authors could demonstrate that their estimator allows them to solve new, more difficult problems. Some of these potential applications are discussed in the introduction, it would be nice if the authors could demonstrate improvements in those domains.\n\nAs is, the paper is still a nice contribution.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "cdate": 1542234199325, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977029, "tmdate": 1552335977029, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryglNsz9p7", "original": null, "number": 3, "cdate": 1542232872415, "ddate": null, "tcdate": 1542232872415, "tmdate": 1542232872415, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "content": {"title": "A paper on an important topic, but the contribution is not very significant", "review": "Thank you for an interesting read.\n\nThis paper extends the recently published DiCE estimator for gradients of SCGs and proposed a control variate method for the second order gradient. The paper is well written. Experiments are a bit too toy, but the authors did show significant improvements over DiCE with no control variate.\n\nGiven that control variates are widely used in deep RL and Monte Carlo VI, the paper can be interesting to many people. I haven't read the DiCE paper, but my impression is that DiCE found a way to conveniently implement the REINFORCE rules applied infinite times. So if I were to derive a baseline control variate for the second or higher order derivatives, I would \"reverse engineer\" from the exact derivatives and figure out the corresponding DiCE formula. Therefore I would say the proposed idea is new, although fairly straightforward for people who knows REINFORCE and baseline methods.\n\nFor me, the biggest issue of the paper is the lack of explanation on the choice of the baseline. Why using the same baseline b_w for both control variates? Is this choice optimal for the second order control variate, even when b_w is selected to be optimal for the first order control variate? The paper has no explanation on this issue, and if the answer is no, then it's important to find out an (approximately) optimal baseline for this second order control variate. \n\nAlso the evaluation seems quite toy. As the design choice of b_w is not rigorously explained, I am not sure the better performance of the variance-reduced derivatives generalises to more complicated tasks such as MAML for few-shot learning.\n\nMinor:\n1. In DiCE, given a set of stochastic nodes W, why did you use marginal distributions p(w, \\theta) for a node w in W, instead of the joint distribution p(W, \\theta)? I agree that there's no need to use p(S, \\theta) that includes all stochastic nodes, but I can't see why using marginal distribution is valid when nodes in W are not independent.\n\n2. For the choice of b_w discussed below eq (4), you probably need to cite [1][2].\n\n3. In your experiments, what does \"correlation coefficient\" mean? Normalised dot product?\n\n[1] Mnih and Rezende (2016). Variational inference for Monte Carlo objectives. ICML 2016.\n[2] Titsias and L\u00e1zaro-Gredilla (2015). Local Expectation Gradients for Black Box Variational Inference. NIPS 2015.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "cdate": 1542234199325, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977029, "tmdate": 1552335977029, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eK5IRF6X", "original": null, "number": 2, "cdate": 1542215313279, "ddate": null, "tcdate": 1542215313279, "tmdate": 1542215313279, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "SJxbt-3sjX", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "content": {"title": "We strongly disagree with the main points raised, particularly the misconception around the optimal scaling constant \u2018c\u2019 and action-dependent baselines.", "comment": "Thank you for your feedback. However, we strongly disagree with the main points of criticism, particularly the misconception around the optimal scaling constant \u2018c\u2019 and action-dependent baselines. Please see our detailed response to the individual points of the review below.\n\n1) Re \u201cpositive vs negative correlation\u201d: Of course covariates can be positively or negatively correlated. The fact that we say it should be positively correlated does not indicate a misunderstanding but merely reflects the fact that in our case c is set to 1 (see below why directly evaluating the optimal c is not practical in realistic settings).\n\n2) Re \u201coptimal scaling constant\u201d: While it is well known that the optimal variance reduction depends on the covariance between the control variate and the estimator, this optimal factor is rarely used in practice for reinforcement learning due to the computational costs of doing one gradient estimate per entry in a batch. What is used in practice across the board for Deep RL (eg. A3C, PPO, IMPALA, etc) is the value-function based variance reduction, which we are enabling for higher order gradients through the DiCE formalism. The fact that our method only depends on the commonly used state-value-function for the baseline computation is a strength, not a weakness.\n\n3) Re \u201cindependent of the action\u201d: We will clarify this issue in the paper but the review misrepresents the facts on this point. Yes, there is a way to account for the bias introduced by an action-dependent baseline and in some cases this bias can be removed exactly. However, this is another method (like the optimal scaling factor mentioned above) that has not been shown to work in practice. In fact the very paper cited by the reviewer (the \u2018mirage of action dependent baselines\u2019), concludes that from a practitioner's point of view there currently is no reason to consider action dependent baselines. Our submission extends the utility of value functions to provide action-independent variance reduction for higher order gradients.\n\n4) Re \u201crevises DiCE formalism\u201d: Thank you for this suggestion. Prior to submission, we carefully considered how our contribution and decided that in order for the paper to be self-contained, Stochastic Computation Graphs as well as the DiCE formalism should be explained clearly in the Background section. To highlight the delta to prior work, we clearly separated out the revision of Stochastic Computation Graphs and the DiCE formalism (Section 2) from the Method (Section 3). \n\n5) Re \u201cnovelty too low.. does not generalize past the second order gradient\u201d: This is obviously a subjective claim. However, note that second order gradients are a key use-case in meta-learning and multi-agent RL and as such the new baseline has the potential to unlock a large number of applications and is of key importance to the community (in fact we are aware of one other research group that has already started experimentation with this baseline). Also, just as the 1st order variance reduction term contributes to a lower 2nd order variance, our new 2nd order baseline also acts as a variance reduction for higher order gradient estimators, although we did not quantify the impact experimentally (since 2nd order is the most relevant for current research). \n\n6) Re \u201cexperiments are identical\u201d: This is also a strength, not a weakness: for the sake of reproducibility between the original DiCE and the new baseline, it is crucial to use the same setting. Also, note that his paper is about proposing a new tool, rather than demonstrating full solutions to novel applications. Experimental results are provided as proof-of-principle and are not the main point of the paper. Clearly, the experimental results support the utility of the new baseline compared to a previously published result.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626716, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1x3SnAcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1579/Authors|ICLR.cc/2019/Conference/Paper1579/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626716}}}, {"id": "Hyl7UE0Ya7", "original": null, "number": 1, "cdate": 1542214731084, "ddate": null, "tcdate": 1542214731084, "tmdate": 1542214731084, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "HJeQrTXahX", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "content": {"title": "Thank you for the positive review, we\u2019re excited about applying this tool to larger problems in future work.", "comment": "Many thanks for the review. While we agree that more experimental validation would have value, this paper is primarily proposing a novel method, which is validated both through proof-of-principle experiments, but, also, and more importantly, theoretically. Furthermore, since we uploaded the paper to OpenReview, another research group has already started experimenting with the new baseline."}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626716, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1x3SnAcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1579/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1579/Authors|ICLR.cc/2019/Conference/Paper1579/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers", "ICLR.cc/2019/Conference/Paper1579/Authors", "ICLR.cc/2019/Conference/Paper1579/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626716}}}, {"id": "HJeQrTXahX", "original": null, "number": 2, "cdate": 1541385531137, "ddate": null, "tcdate": 1541385531137, "tmdate": 1541533017028, "tddate": null, "forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "content": {"title": "An important direction motivated by recent need for second-order gradient estimation, but need to verify its advantages more thoroughly", "review": "In this paper, the author proposed a better control variate formula for second-order Monte Carlo gradient estimators, based on a special version of DiCE (Foerster et al, 2018).  The motivation and the main method is easy to follow and the paper is well written.  The author followed the same experiments setting as DiCE, numerically verifying the advantages of the newly proposed baseline, which can estimate the Hession accurately. \n\nThe work is essentially important due to the need for second-order gradient estimation for meta-learning (Finn et al., 2017) and multi-agent reinforcement learnings.  However, the advantage of the proposed method is not verified thoroughly. The only real application demonstrated in the paper, can be achieved the same performance as the second-order baseline using a simple trick.  Since this work only focuses on second-order gradient estimations, I think it would be better to verify its advantages in various scenarios such as meta-learning or sparse reward RL  as the author suggested in the paper.\n\nFinn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML 2017.\nFoerster, Jakob, et al. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" ICML 2018.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1579/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs", "abstract": "Motivated by the need for higher order gradients in multi-agent reinforcement learning and meta-learning, this paper studies the construction of baselines for second order Monte Carlo gradient estimators in order to reduce the sample variance. Following the construction of a stochastic computation graph (SCG), the Infinitely Differentiable Monte-Carlo Estimator (DiCE) can generate correct estimates of arbitrary order gradients through differentiation. However, a baseline term that serves as a control variate for reducing variance is currently provided only for first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient. We provide theoretical analysis and numerical evaluations of our baseline term, which demonstrate that it can dramatically reduce the variance of second order gradient estimators produced by DiCE. This computational tool can be easily used to estimate second order gradients with unprecedented efficiency wherever automatic differentiation is utilised, and has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.", "keywords": ["Reinforcement learning", "meta-learning", "higher order derivatives", "gradient estimation", "stochastic computation graphs"], "authorids": ["jingkai.mao@gmail.com", "jakobfoerster@gmail.com", "tim.rocktaeschel@gmail.com", "gregory.farquhar@cs.ox.ac.uk", "alshedivat@cs.cmu.edu", "shimon.whitesone@cs.ox.ac.uk"], "authors": ["Jingkai Mao", "Jakob Foerster", "Tim Rockt\u00e4schel", "Gregory Farquhar", "Maruan Al-Shedivat", "Shimon Whiteson"], "TL;DR": "We extend the DiCE formalism of higher order gradient estimation with a new baseline for variance reduction of second order derivatives, improving sample efficiency by two orders of magnitude. ", "pdf": "/pdf/932da89ced7b4d18614d2a9e13fad9659305985e.pdf", "paperhash": "mao|a_better_baseline_for_second_order_gradient_estimation_in_stochastic_computation_graphs", "_bibtex": "@misc{\nmao2019a,\ntitle={A Better Baseline for Second Order Gradient Estimation in Stochastic Computation Graphs},\nauthor={Jingkai Mao and Jakob Foerster and Tim Rockt\u00e4schel and Gregory Farquhar and Maruan Al-Shedivat and Shimon Whiteson},\nyear={2019},\nurl={https://openreview.net/forum?id=H1x3SnAcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1579/Official_Review", "cdate": 1542234199325, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1x3SnAcYQ", "replyto": "H1x3SnAcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1579/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977029, "tmdate": 1552335977029, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1579/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}