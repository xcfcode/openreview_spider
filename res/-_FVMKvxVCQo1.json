{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392769320000, "tcdate": 1392769320000, "number": 2, "id": "65bxfeIlx7fo1", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "-_FVMKvxVCQo1", "replyto": "llsYh8vU1Lhd5", "signatures": ["Balazs Kegl"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'The main issue I found is how this algorithm scales: boosted trees are used for large scale problems, and addressing this issue is critical for significance. The authors only report large scale experiments in very vague terms, without any comparisons or training times (the authors do not report any numerical results and do not give reference. I do not know what to make for instance of the claim that it won the recent Interspeech challenge).'\r\n\r\nThere is a clickable hyperlink in the paper pointing to the site (Emotion Sub-Challenge):\r\n\r\nhttp://emotion-research.net/sigs/speech-sig/is13-compare\r\n\r\nThe Yahoo results are in the proceedings of Chapelle et al., but we also have a full MLJ paper on it (I added the reference).\r\n\r\n'The cost per iteration appears linear in the number of features, examples and dimensions, which is standard, but what about the number of iterations? My experience is that discrete Adaboost is quite slow on larger datasets compared to real-value prediction or gradient-based boosting.'\r\n\r\nAll number of iterations are reported in the supplementary:\r\n\r\nhttps://www.lri.fr/~kegl/research/multiboostResults.pdf\r\n\r\nHere is a table of wallclock times on the small sets (10x10 CV with 12 different number of inner nodes tried = 1200 jobs).\r\n\r\nname\ttype\tmaxT\tk\tn\td\tK\tnumCPU\ttTree\ttProduct\r\nbreast\tNUMERIC\t1000\t10\t106\t10\t6\tstrat3\t0.1h\t0.1h\r\niris\tNUMERIC\t1000\t10\t150\t4\t3\tmac3\t0.2h\t0.2h\r\nwine\tNUMERIC\t1000\t10\t173\t13\t3\tmac3\t0.2h\t0.2h\r\nbalance\tNUMERIC\t1000\t10\t625\t4\t3\tstrat8\t0.2h\t0.1h\r\nblood\tNUMERIC\t1000\t10\t748\t5\t2\tstrat3\t0.4h\t0.2h\r\necoli\tNUMERIC\t1000\t10\t336\t8\t8\tmac3\t0.7h\t0.7h\r\nsaccmem\tNUMERIC\t1000\t10\t254\t48\t4\tmac3\t1.7h\t1.9h\r\npima\tNUMERIC\t1000\t10\t768\t8\t2\tstrat3\t1.8h\t1.5h\r\nwdbc\tNUMERIC\t1000\t10\t569\t32\t2\tstrat3\t2.0h\t1.9h\r\nsonar\tNUMERIC\t1000\t10\t208\t60\t2\tmac3\t1.3h\t2.3h\r\n\r\nAccelerating training is possible using tricks borrowed from random forests: subsample the data points (stochastic boosting) or the features (called LazyBoost, usually used with large feature spaces, such as Haar filters on images or text classification). Sometimes these randomization steps even improve generalization; we are in the process of evaluating these effects on large-scale experiments.\r\n\r\n- 'Authors should better explain the weak learning condition.'\r\n\r\nI usually get the remark from boosting experts that these are known and trivial results and that I should not repeat them. I'm actually happy to comply with this request. I added a new subsection on the algorithmic convergence of AdaBoost.MH. It's slightly longish and tutorialish, but I give ample warning to the reader that it is not novel, and that it is not necessary to read it for understanding the novelty of the paper. \r\n\r\n- In section 3, there is a single alpha, so the subscript j in \u201calpha_j\u201d must be a typo. \r\n\r\nI deleted it (although note that the base classifier does return an alpha for each cut).\r\n\r\n- Section 3: choice of font for h_j should be consistent. \r\n\r\nIt is: h_j is a simple classifier, and Gothic h_j denotes a node classifier (which is recursive). If I missed this logic somewhere, please point it out where exactly.\r\n\r\n- I do not see how one can get O(nKd log(N)) if the tree is balanced: I assume one still has to run TREEBASE, which calls BASE N times, and BASE is O(nKd)??? \r\n\r\nIt does but the number of points in each node is smaller than n. More precisely, each tree level partitions the data set, and since stump algorithm is linear, each _level_ costs O(nkd).\r\n\r\n- Section 2.2: the case against Adaboost.M1 used in a multiclass setting that would lead to an error rate of (K-1)/K sounds strange. I thought one would use K separate Adaboost.M1 in a 1-vs-other setting?\r\n\r\nThat would be a possibility, but it is not what Adaboost.M1 is doing: it uses single-label but multi-class trees with multi-class error < 50%.\r\n\r\nNote that I uploaded the new version temporarily here:\r\n\r\nhttps://www.lri.fr/~kegl/research/revised1.pdf\r\n\r\nIt will appear on arxiv once I figure out the latex bug which blocks the compilation."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The return of AdaBoost.MH: multi-class Hamming trees", "decision": "submitted, no decision", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "pdf": "https://arxiv.org/abs/1312.6086", "paperhash": "k\u00e9gl|the_return_of_adaboostmh_multiclass_hamming_trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "authorids": ["balazs.kegl@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392769320000, "tcdate": 1392769320000, "number": 1, "id": "J3zMNF85uKJRd", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "-_FVMKvxVCQo1", "replyto": "llsYh8vU1Lhd5", "signatures": ["Balazs Kegl"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'The main issue I found is how this algorithm scales: boosted trees are used for large scale problems, and addressing this issue is critical for significance. The authors only report large scale experiments in very vague terms, without any comparisons or training times (the authors do not report any numerical results and do not give reference. I do not know what to make for instance of the claim that it won the recent Interspeech challenge).'\r\n\r\nThere is a clickable hyperlink in the paper pointing to the site (Emotion Sub-Challenge):\r\n\r\nhttp://emotion-research.net/sigs/speech-sig/is13-compare\r\n\r\nThe Yahoo results are in the proceedings of Chapelle et al., but we also have a full MLJ paper on it (I added the reference).\r\n\r\n'The cost per iteration appears linear in the number of features, examples and dimensions, which is standard, but what about the number of iterations? My experience is that discrete Adaboost is quite slow on larger datasets compared to real-value prediction or gradient-based boosting.'\r\n\r\nAll number of iterations are reported in the supplementary:\r\n\r\nhttps://www.lri.fr/~kegl/research/multiboostResults.pdf\r\n\r\nHere is a table of wallclock times on the small sets (10x10 CV with 12 different number of inner nodes tried = 1200 jobs).\r\n\r\nname\ttype\tmaxT\tk\tn\td\tK\tnumCPU\ttTree\ttProduct\r\nbreast\tNUMERIC\t1000\t10\t106\t10\t6\tstrat3\t0.1h\t0.1h\r\niris\tNUMERIC\t1000\t10\t150\t4\t3\tmac3\t0.2h\t0.2h\r\nwine\tNUMERIC\t1000\t10\t173\t13\t3\tmac3\t0.2h\t0.2h\r\nbalance\tNUMERIC\t1000\t10\t625\t4\t3\tstrat8\t0.2h\t0.1h\r\nblood\tNUMERIC\t1000\t10\t748\t5\t2\tstrat3\t0.4h\t0.2h\r\necoli\tNUMERIC\t1000\t10\t336\t8\t8\tmac3\t0.7h\t0.7h\r\nsaccmem\tNUMERIC\t1000\t10\t254\t48\t4\tmac3\t1.7h\t1.9h\r\npima\tNUMERIC\t1000\t10\t768\t8\t2\tstrat3\t1.8h\t1.5h\r\nwdbc\tNUMERIC\t1000\t10\t569\t32\t2\tstrat3\t2.0h\t1.9h\r\nsonar\tNUMERIC\t1000\t10\t208\t60\t2\tmac3\t1.3h\t2.3h\r\n\r\nAccelerating training is possible using tricks borrowed from random forests: subsample the data points (stochastic boosting) or the features (called LazyBoost, usually used with large feature spaces, such as Haar filters on images or text classification). Sometimes these randomization steps even improve generalization; we are in the process of evaluating these effects on large-scale experiments.\r\n\r\n- 'Authors should better explain the weak learning condition.'\r\n\r\nI usually get the remark from boosting experts that these are known and trivial results and that I should not repeat them. I'm actually happy to comply with this request. I added a new subsection on the algorithmic convergence of AdaBoost.MH. It's slightly longish and tutorialish, but I give ample warning to the reader that it is not novel, and that it is not necessary to read it for understanding the novelty of the paper. \r\n\r\n- In section 3, there is a single alpha, so the subscript j in \u201calpha_j\u201d must be a typo. \r\n\r\nI deleted it (although note that the base classifier does return an alpha for each cut).\r\n\r\n- Section 3: choice of font for h_j should be consistent. \r\n\r\nIt is: h_j is a simple classifier, and Gothic h_j denotes a node classifier (which is recursive). If I missed this logic somewhere, please point it out where exactly.\r\n\r\n- I do not see how one can get O(nKd log(N)) if the tree is balanced: I assume one still has to run TREEBASE, which calls BASE N times, and BASE is O(nKd)??? \r\n\r\nIt does but the number of points in each node is smaller than n. More precisely, each tree level partitions the data set, and since stump algorithm is linear, each _level_ costs O(nkd).\r\n\r\n- Section 2.2: the case against Adaboost.M1 used in a multiclass setting that would lead to an error rate of (K-1)/K sounds strange. I thought one would use K separate Adaboost.M1 in a 1-vs-other setting?\r\n\r\nThat would be a possibility, but it is not what Adaboost.M1 is doing: it uses single-label but multi-class trees with multi-class error < 50%.\r\n\r\nNote that I uploaded the new version temporarily here:\r\n\r\nhttps://www.lri.fr/~kegl/research/revised1.pdf\r\n\r\nIt will appear on arxiv once I figure out the latex bug which blocks the compilation."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The return of AdaBoost.MH: multi-class Hamming trees", "decision": "submitted, no decision", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "pdf": "https://arxiv.org/abs/1312.6086", "paperhash": "k\u00e9gl|the_return_of_adaboostmh_multiclass_hamming_trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "authorids": ["balazs.kegl@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392769020000, "tcdate": 1392769020000, "number": 1, "id": "7AnJAci6tB71g", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "-_FVMKvxVCQo1", "replyto": "ly7Ep3gRItHdk", "signatures": ["Balazs Kegl"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'Also there should be a description of the alternative multi-class Adaboost methods used in the experimental comparison, in order to appreciate the originality of this new proposition.'\r\n\r\nFull description is out of the scope of this paper. The main difference is that they all use multi-class (but single-label) trees."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The return of AdaBoost.MH: multi-class Hamming trees", "decision": "submitted, no decision", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "pdf": "https://arxiv.org/abs/1312.6086", "paperhash": "k\u00e9gl|the_return_of_adaboostmh_multiclass_hamming_trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "authorids": ["balazs.kegl@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392031020000, "tcdate": 1392031020000, "number": 3, "id": "TYV3arI6LeT82", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "-_FVMKvxVCQo1", "replyto": "-_FVMKvxVCQo1", "signatures": ["anonymous reviewer 0c59"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The return of AdaBoost.MH: multi-class Hamming trees", "review": "The author presents a way to train vector-valued decision trees (Hamming trees) in the context of multi-class AdaBoost.MH, as one way of not doing K binary one-vs-all classifications. The idea is a clever factorization that allows for efficient modeling and predictions.  The paper seems to be performing very thorough validation, comparisons and hyper-parameter selection, which is obviously a good thing.\r\n\r\nUnfortunately, I cannot speak with any authority as to whether the algorithmic contributions of this paper are substantial or not. While the results do seem to support the idea that the proposed method, AdaBoost.MH + Hamming trees, works as well or better than SVMs/other AdaBoost methods, I cannot evaluate in earnestness whether this is a very novel contribution or incremental, since recent developments in boosting are not exactly my area of expertise."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The return of AdaBoost.MH: multi-class Hamming trees", "decision": "submitted, no decision", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "pdf": "https://arxiv.org/abs/1312.6086", "paperhash": "k\u00e9gl|the_return_of_adaboostmh_multiclass_hamming_trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "authorids": ["balazs.kegl@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391935860000, "tcdate": 1391935860000, "number": 2, "id": "llsYh8vU1Lhd5", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "-_FVMKvxVCQo1", "replyto": "-_FVMKvxVCQo1", "signatures": ["anonymous reviewer 3cb5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The return of AdaBoost.MH: multi-class Hamming trees", "review": "In the last 10 years, boosted trees have been considered as the best performing algorithm for large scale data with numerical features (though this may change with deep learning). However, why they perform so well has never been fully understood, especially as the boosting and the tree parts of the algorithm tend to be optimized separately. This is all the more true in the multi-class setting, where boosted tree implementations are full of ad-hoc patches.\r\n\r\nThis paper offers a model where the construction of the tree is part of the boosting algorithm in a full multi-class setting, and I assume this is novel. It matches the performance of SVMs on small tasks where they were traditionally superior to boosting, and the performance of the best previous multi-class booting implementation. This is a well written paper of high technical quality with extensive and rigorous experiments, and effective descriptions of the algorithm implementations.\r\n\r\nThe main issue I found is how this algorithm scales: boosted trees are used for large scale problems, and addressing this issue is critical for significance. The authors only report large scale experiments in very vague terms, without any comparisons or training times (the authors do not report any numerical results and do not give reference. I do not know what to make for instance of the claim that it won the recent Interspeech challenge). The cost per iteration appears linear in the number of features, examples and dimensions, which is standard, but what about the number of iterations? My experience is that discrete Adaboost is quite slow on larger datasets compared to real-value prediction or gradient-based boosting.\r\n\r\nThis paper packs a lot of technical contributions in 9 pages, to the point that the authors had to take a few short cuts that impact clarity, but I would only recommend to cut comments that are not understandable by non-expert in boosting, or expand them (for instance all the references to the weak learning condition, which is never explained).\r\n\r\nI noticed there was not a single boosting paper at ICLR\u201913, while this is probably still the most widely used class of machine learning algorithms. Through the greedy selection of weak classifier, boosting offers a way to learn representation that is different from deep learning, but highly practical for many problems.\r\nThe same author has submitted another paper that targets specifically the choice of the features for boosting (correlation-base construction of neighborhood and edge features), however, I find this paper the most worthy to be accepted.\r\n\r\nDetailed comments:\r\n-\tAuthors should better explain the weak learning condition.\r\n-\tIn section 3, there is a single alpha, so the subscript j in \u201calpha_j\u201d must be a typo.\r\n-\tSection 3: choice of font for h_j should be consistent.\r\n-\tI do not see how one can get O(nKd log(N)) if the tree is balanced: I assume one still has to run TREEBASE, which calls BASE N times, and BASE is O(nKd)???\r\n-\tSection 2.2: the case against Adaboost.M1 used in a multiclass setting that would lead to an error rate of (K-1)/K sounds strange. I thought one would use K separate Adaboost.M1 in a 1-vs-other setting?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The return of AdaBoost.MH: multi-class Hamming trees", "decision": "submitted, no decision", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "pdf": "https://arxiv.org/abs/1312.6086", "paperhash": "k\u00e9gl|the_return_of_adaboostmh_multiclass_hamming_trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "authorids": ["balazs.kegl@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391695080000, "tcdate": 1391695080000, "number": 1, "id": "ly7Ep3gRItHdk", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "-_FVMKvxVCQo1", "replyto": "-_FVMKvxVCQo1", "signatures": ["anonymous reviewer 98f6"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of The return of AdaBoost.MH: multi-class Hamming trees", "review": "The paper describes an Adaboost algorithm, with tree based learners, adapted to the multi-class setting. The claim is that direct multi-class algorithms cannot be used to grow tree base learners in Adaboost. The author proposes a factorization of the classifier into a product of 3 terms, one performing a binary separation of the input space, a second one projecting this binary value onto the  classes and the third being a confidence term. With this formulation, base tree learners can be grown using the binary separator corresponding tot the first term. Experiments are described on a series of UCI data sets and reach good results compared to a series of baselines.\r\nThe idea of factorizing the multiclass classifier is interesting and seems to lead to good results \u2013 even if the experiments have been performed on rather small size problems. The paper is relatively clear, but could be improved. In many places, there are shortcuts which will probably be hard to follow for non-specialists of multi-class Adaboost. Also there should be a description of the alternative multi-class Adaboost methods used in the experimental comparison, in order to appreciate the originality of this new proposition.\r\nOverall, I think that this is a good paper. On the other hand, I am not sure that it is adapted to ICLR. It is not concerned with representation learning and would probably be better suited to a more general ML conference."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "The return of AdaBoost.MH: multi-class Hamming trees", "decision": "submitted, no decision", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "pdf": "https://arxiv.org/abs/1312.6086", "paperhash": "k\u00e9gl|the_return_of_adaboostmh_multiclass_hamming_trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "authorids": ["balazs.kegl@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1388112180000, "tcdate": 1388112180000, "number": 60, "id": "-_FVMKvxVCQo1", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "-_FVMKvxVCQo1", "signatures": ["balazs.kegl@gmail.com"], "readers": ["everyone"], "content": {"title": "The return of AdaBoost.MH: multi-class Hamming trees", "decision": "submitted, no decision", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "pdf": "https://arxiv.org/abs/1312.6086", "paperhash": "k\u00e9gl|the_return_of_adaboostmh_multiclass_hamming_trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "authorids": ["balazs.kegl@gmail.com"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}