{"notes": [{"id": "r1xPh2VtPB", "original": "H1lWngBGwH", "number": 192, "cdate": 1569438894830, "ddate": null, "tcdate": 1569438894830, "tmdate": 1583912046698, "tddate": null, "forum": "r1xPh2VtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "SVQN: Sequential Variational Soft Q-Learning Networks", "authors": ["Shiyu Huang", "Hang Su", "Jun Zhu", "Ting Chen"], "authorids": ["huangsy1314@163.com", "suhangss@mail.tsinghua.edu.cn", "dcszj@tsinghua.edu.cn", "tingchen@tsinghua.edu.cn"], "keywords": ["reinforcement learning", "POMDP", "variational inference", "generative model"], "TL;DR": "SVQNs  formalizes the inference of hidden states and maximum entropy reinforcement learning under a unified graphical model and optimizes the two modules jointly.", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "pdf": "/pdf/8a721a545dcca1345b0a53dfdede82fce8787479.pdf", "paperhash": "huang|svqn_sequential_variational_soft_qlearning_networks", "_bibtex": "@inproceedings{\nHuang2020SVQN:,\ntitle={SVQN: Sequential Variational Soft Q-Learning Networks},\nauthor={Shiyu Huang and Hang Su and Jun Zhu and Ting Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xPh2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8008ad85d04121d3d6904ec5ee8509532cee3fca.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "YLchM1IawG", "original": null, "number": 1, "cdate": 1576798689846, "ddate": null, "tcdate": 1576798689846, "tmdate": 1576800945309, "tddate": null, "forum": "r1xPh2VtPB", "replyto": "r1xPh2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper192/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposes a novel model-free solution to POMDPs, which proposes a unified graphical model for hidden state inference and max entropy RL. The method is principled and provides good empirical results on a set of experiments that relatively comprehensive. I would have liked to see more POMDP tasks instead of Atari, but the results are good. Overall this is good work.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SVQN: Sequential Variational Soft Q-Learning Networks", "authors": ["Shiyu Huang", "Hang Su", "Jun Zhu", "Ting Chen"], "authorids": ["huangsy1314@163.com", "suhangss@mail.tsinghua.edu.cn", "dcszj@tsinghua.edu.cn", "tingchen@tsinghua.edu.cn"], "keywords": ["reinforcement learning", "POMDP", "variational inference", "generative model"], "TL;DR": "SVQNs  formalizes the inference of hidden states and maximum entropy reinforcement learning under a unified graphical model and optimizes the two modules jointly.", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "pdf": "/pdf/8a721a545dcca1345b0a53dfdede82fce8787479.pdf", "paperhash": "huang|svqn_sequential_variational_soft_qlearning_networks", "_bibtex": "@inproceedings{\nHuang2020SVQN:,\ntitle={SVQN: Sequential Variational Soft Q-Learning Networks},\nauthor={Shiyu Huang and Hang Su and Jun Zhu and Ting Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xPh2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8008ad85d04121d3d6904ec5ee8509532cee3fca.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1xPh2VtPB", "replyto": "r1xPh2VtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730198, "tmdate": 1576800282943, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper192/-/Decision"}}}, {"id": "HJgfkkxDsS", "original": null, "number": 3, "cdate": 1573482202331, "ddate": null, "tcdate": 1573482202331, "tmdate": 1573482276186, "tddate": null, "forum": "r1xPh2VtPB", "replyto": "HJgdqX8HKB", "invitation": "ICLR.cc/2020/Conference/Paper192/-/Official_Comment", "content": {"title": "Response to AnonReviewer #3", "comment": "We thank Reviwer #3 for the valuable comments and the appreciation of our novel contributions. "}, "signatures": ["ICLR.cc/2020/Conference/Paper192/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper192/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SVQN: Sequential Variational Soft Q-Learning Networks", "authors": ["Shiyu Huang", "Hang Su", "Jun Zhu", "Ting Chen"], "authorids": ["huangsy1314@163.com", "suhangss@mail.tsinghua.edu.cn", "dcszj@tsinghua.edu.cn", "tingchen@tsinghua.edu.cn"], "keywords": ["reinforcement learning", "POMDP", "variational inference", "generative model"], "TL;DR": "SVQNs  formalizes the inference of hidden states and maximum entropy reinforcement learning under a unified graphical model and optimizes the two modules jointly.", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "pdf": "/pdf/8a721a545dcca1345b0a53dfdede82fce8787479.pdf", "paperhash": "huang|svqn_sequential_variational_soft_qlearning_networks", "_bibtex": "@inproceedings{\nHuang2020SVQN:,\ntitle={SVQN: Sequential Variational Soft Q-Learning Networks},\nauthor={Shiyu Huang and Hang Su and Jun Zhu and Ting Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xPh2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8008ad85d04121d3d6904ec5ee8509532cee3fca.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xPh2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper192/Authors", "ICLR.cc/2020/Conference/Paper192/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper192/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper192/Reviewers", "ICLR.cc/2020/Conference/Paper192/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper192/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper192/Authors|ICLR.cc/2020/Conference/Paper192/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175003, "tmdate": 1576860551946, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper192/Authors", "ICLR.cc/2020/Conference/Paper192/Reviewers", "ICLR.cc/2020/Conference/Paper192/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper192/-/Official_Comment"}}}, {"id": "HyxupCkPsH", "original": null, "number": 2, "cdate": 1573482175640, "ddate": null, "tcdate": 1573482175640, "tmdate": 1573482261146, "tddate": null, "forum": "r1xPh2VtPB", "replyto": "SJls05-ptS", "invitation": "ICLR.cc/2020/Conference/Paper192/-/Official_Comment", "content": {"title": "Response to AnonReviewer #1", "comment": "We thank the reviewer for the valuable comments. However, we humbly disagree on the primary concern that our paper is an \"obvious extension\" of DRQNs (Hausknecht & Stone, 2015). \n\nFirstly, it is not just an \"obvious extension\" of DRQNs. As stated in the third paragraph of Section 1, both DRQNs and its improved version of Action-specific Deep Recurrent QNetwork (ADRQN) (Zhu et al., 2018) fail to utilize the Markov property of the state in POMDPs, because they just represent the state as latent variables of neural networks. To solve this problem, our algorithm starts from the graphical model representation of POMDPs, which is very intuitive and generic, and then we derive the ELBO and finally lead to the design of the neural network. In the experiments, we show that our method outperforms both DRQN and ADRQN by a large margin on several challenging tasks. And our ablation study also shows that our method is more robust to the disturbance of the observation. Overall, as agreed by Reviewer #3, we provide a novel solution to POMDPs, which is better than DRQNs and ADRQN in terms of both theoretical formulation and empirical results.  \n\nSecondly, it is not just \"replace DQN with Soft Q-learning\". The reason why we use soft Q-learning (i.e., Maximum Entropy Reinforcement Learning) has been explained in Section 3.2.  As we want to solve the POMDPs under a unified graphical model, we derive the ELBO of POMDPs and design generative models to handle the inference of the hidden states. Moreover, we apply additional approximate functions to tackle the conditional prior problem (as stated in Section 4.2). Finally, to train the generative models and the planing algorithm jointly, we design a recurrent neural network to reduce the computation complexity. We also explore two different recurrent models in our context: GRU and LSTM, and both of them outperform the baseline methods on various tasks. Fig. 4(a) also shows that our jointly training process is more effient than other baselines. Table 1 shows that our algorithm is more powerful than the naive soft q-learning algorithm (DSQN). To summarize, we have done much work to deal with the challenges in POMDPs and improve the performance of our algorithm.\n\nWe hope our answers can address your concerns."}, "signatures": ["ICLR.cc/2020/Conference/Paper192/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper192/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SVQN: Sequential Variational Soft Q-Learning Networks", "authors": ["Shiyu Huang", "Hang Su", "Jun Zhu", "Ting Chen"], "authorids": ["huangsy1314@163.com", "suhangss@mail.tsinghua.edu.cn", "dcszj@tsinghua.edu.cn", "tingchen@tsinghua.edu.cn"], "keywords": ["reinforcement learning", "POMDP", "variational inference", "generative model"], "TL;DR": "SVQNs  formalizes the inference of hidden states and maximum entropy reinforcement learning under a unified graphical model and optimizes the two modules jointly.", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "pdf": "/pdf/8a721a545dcca1345b0a53dfdede82fce8787479.pdf", "paperhash": "huang|svqn_sequential_variational_soft_qlearning_networks", "_bibtex": "@inproceedings{\nHuang2020SVQN:,\ntitle={SVQN: Sequential Variational Soft Q-Learning Networks},\nauthor={Shiyu Huang and Hang Su and Jun Zhu and Ting Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xPh2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8008ad85d04121d3d6904ec5ee8509532cee3fca.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xPh2VtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper192/Authors", "ICLR.cc/2020/Conference/Paper192/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper192/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper192/Reviewers", "ICLR.cc/2020/Conference/Paper192/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper192/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper192/Authors|ICLR.cc/2020/Conference/Paper192/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175003, "tmdate": 1576860551946, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper192/Authors", "ICLR.cc/2020/Conference/Paper192/Reviewers", "ICLR.cc/2020/Conference/Paper192/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper192/-/Official_Comment"}}}, {"id": "HJgdqX8HKB", "original": null, "number": 1, "cdate": 1571279760480, "ddate": null, "tcdate": 1571279760480, "tmdate": 1572972627081, "tddate": null, "forum": "r1xPh2VtPB", "replyto": "r1xPh2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper192/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a new sequential model-free Q-learning methodology for POMDPs that relies on variational autoencoders to represent the hidden state. The approach is generic, well-motivated and has  clear applicability in the presence of partial observability. The idea is to create a joint model for optimizing the hidden-state inference and planning jointly. For that reason variational inference is used to optimize the ELBO objective in this particular setting. All this is combined with a recurrent architecture that makes the whole process feasible and efficient.\n\nThe work is novel and it comes with the theoretical derivation of a variational lower bound for POMDPs in general. This intuition is exploited to create a VAE based recurrent architecture. One motivation comes from maximal entropy reinforcement learning (MERL), but which has the ad hoc objective of maximizing the policy entropy. On the other hand SVQN optimizes both a variational approximation of the policy and that of the hidden state. Here the rest terms of the ELBO objective can be approximated generatively and some of them are conditioned on the previous state which calls for a recurrent architecture. The other parts are modeled by a VAE.\n\nThe paper also explores two different recurrent models in this context: GRU and LSTM are both evaluated. Besides the nice theoretical derivation the paper presents compelling evidence by comparing this approach to competing approaches on four games of the flickering ATARI benchmark and outperforming the baselines significantly. Also both the GRU and LSTM version outperforms the baseline methods on various tasks of the VIZDoom benchmark as well.\n\nIn general, I find that this well written paper presents a significant progress in modelling POMDPS in a model-free manner with nice theoretical justification and compelling empirical evidence.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper192/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper192/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SVQN: Sequential Variational Soft Q-Learning Networks", "authors": ["Shiyu Huang", "Hang Su", "Jun Zhu", "Ting Chen"], "authorids": ["huangsy1314@163.com", "suhangss@mail.tsinghua.edu.cn", "dcszj@tsinghua.edu.cn", "tingchen@tsinghua.edu.cn"], "keywords": ["reinforcement learning", "POMDP", "variational inference", "generative model"], "TL;DR": "SVQNs  formalizes the inference of hidden states and maximum entropy reinforcement learning under a unified graphical model and optimizes the two modules jointly.", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "pdf": "/pdf/8a721a545dcca1345b0a53dfdede82fce8787479.pdf", "paperhash": "huang|svqn_sequential_variational_soft_qlearning_networks", "_bibtex": "@inproceedings{\nHuang2020SVQN:,\ntitle={SVQN: Sequential Variational Soft Q-Learning Networks},\nauthor={Shiyu Huang and Hang Su and Jun Zhu and Ting Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xPh2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8008ad85d04121d3d6904ec5ee8509532cee3fca.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xPh2VtPB", "replyto": "r1xPh2VtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper192/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper192/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576387092570, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper192/Reviewers"], "noninvitees": [], "tcdate": 1570237755696, "tmdate": 1576387092583, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper192/-/Official_Review"}}}, {"id": "SJls05-ptS", "original": null, "number": 2, "cdate": 1571785426743, "ddate": null, "tcdate": 1571785426743, "tmdate": 1572972627047, "tddate": null, "forum": "r1xPh2VtPB", "replyto": "r1xPh2VtPB", "invitation": "ICLR.cc/2020/Conference/Paper192/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes SVQN, an algorithm for POMDPs based on the soft Q-learning framework which uses recurrent neural networks to capture historical information for the latent state inference. In order to obtain this formulation, the author first derive the variational bound for POMDPs and then present a practical algorithm.\n\nThe key idea of the paper is to replace DQN with Soft Q-learning that already demonstrated better performance on a variety of tasks. This seems to be an obvious extension of DRQNs (Hausknecht & Stone, 2015) even though it did not appear in the literature.\n\nThe authors evaluate the final algorithm on a set of ALE and DoomViz tasks. The algorithm outperforms the previous methods, in particular, DRQNs. The set of tasks and prior methods is adequate.\n\nOverall, the contribution of the paper is not significant enough to be accepted to ICLR.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper192/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper192/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SVQN: Sequential Variational Soft Q-Learning Networks", "authors": ["Shiyu Huang", "Hang Su", "Jun Zhu", "Ting Chen"], "authorids": ["huangsy1314@163.com", "suhangss@mail.tsinghua.edu.cn", "dcszj@tsinghua.edu.cn", "tingchen@tsinghua.edu.cn"], "keywords": ["reinforcement learning", "POMDP", "variational inference", "generative model"], "TL;DR": "SVQNs  formalizes the inference of hidden states and maximum entropy reinforcement learning under a unified graphical model and optimizes the two modules jointly.", "abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "pdf": "/pdf/8a721a545dcca1345b0a53dfdede82fce8787479.pdf", "paperhash": "huang|svqn_sequential_variational_soft_qlearning_networks", "_bibtex": "@inproceedings{\nHuang2020SVQN:,\ntitle={SVQN: Sequential Variational Soft Q-Learning Networks},\nauthor={Shiyu Huang and Hang Su and Jun Zhu and Ting Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xPh2VtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8008ad85d04121d3d6904ec5ee8509532cee3fca.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xPh2VtPB", "replyto": "r1xPh2VtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper192/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper192/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576387092570, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper192/Reviewers"], "noninvitees": [], "tcdate": 1570237755696, "tmdate": 1576387092583, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper192/-/Official_Review"}}}], "count": 6}