{"notes": [{"id": "H1gyFYbjiV", "original": null, "number": 8, "cdate": 1556973942761, "ddate": null, "tcdate": 1556973942761, "tmdate": 1556973942761, "tddate": null, "forum": "r1eiqi09K7", "replyto": "HygEnIClNE", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "Based on my and my colleague's experience, we found R-Adam easy to work with. Specifically, we tried to replicate https://papers.nips.cc/paper/7780-hyperbolic-neural-networks with changed optimizer (repo: [1]). Probably due to carefully tuned R-SGD in this work we were not able to reproduce the results with SGD. We used batch_size=1000 not to wait for convergence forever and that might affect SGD and be the reason for our failure to reproduce the results here.  On the contrary, increased batch_size does not affect R-Adam (as expected) and we get nice convergence with large batches. Empirically, we found that optimization is more smooth with smaller momentum betas (due to large batch_size?), we used (0.7, 0.9). Finally, we obtained similarly looking plots that compare Euclidean network to Hyperbolic one using Adam. \n\n[1] https://github.com/ferrine/hyrnn This repo needs to be polished a lot, we plan to move it into https://github.com/geoopt/ once we have a bit more free time to work on it.\n", "title": "Adam is fine"}, "signatures": ["~Maxim_Vadimovich_Kochurov1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Maxim_Vadimovich_Kochurov1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}, {"id": "r1eiqi09K7", "original": "rJea-bh5KX", "number": 561, "cdate": 1538087826568, "ddate": null, "tcdate": 1538087826568, "tmdate": 1550456264638, "tddate": null, "forum": "r1eiqi09K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HygEnIClNE", "original": null, "number": 10, "cdate": 1548965548150, "ddate": null, "tcdate": 1548965548150, "tmdate": 1548965548150, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1xCwSC7-V", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "content": {"title": "Thanks a lot for sharing!", "comment": "Very happy to hear! \n\nIf you get new empirical insights on what is best to use, or applications where you found it useful, please feel free to post it on this thread :)\n\nWill keep an eye on your work!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613386, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1eiqi09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper561/Authors|ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613386}}}, {"id": "r1xCwSC7-V", "original": null, "number": 7, "cdate": 1546016102339, "ddate": null, "tcdate": 1546016102339, "tmdate": 1546016102339, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "Hi! Thank you for this inspiring work! As soon as I saw the paper submitted I tried to implement it just for fun and use it for some areas in my research. It turned into a python package for pytorch. I've implemented Stiefel manifolds, Adam, Amsgrad and SGD so far due to limited time. A friend of mine helped me with adding some experimental samplers on Riemannian manifolds. Feel free to check it out on github/pip\n\nhttps://github.com/ferrine/geoopt\nhttps://pypi.org/project/geoopt/", "title": "Unofficial implementation:)"}, "signatures": ["~Maxim_Vadimovich_Kochurov1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Maxim_Vadimovich_Kochurov1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}, {"id": "SkxkMjiyeN", "original": null, "number": 1, "cdate": 1544694534620, "ddate": null, "tcdate": 1544694534620, "tmdate": 1545354506722, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Meta_Review", "content": {"metareview": "Dear authors,\n\nAll reviewers agreed that your work sheds new light on a popular class of algorithms and should thus be presented at ICLR.\n\nPlease make sure to implement all their comments in the final version.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "An interesting analysis and algorithm"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper561/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353172564, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper561/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353172564}}}, {"id": "SyebSCK30X", "original": null, "number": 9, "cdate": 1543441976549, "ddate": null, "tcdate": 1543441976549, "tmdate": 1543441976549, "tddate": null, "forum": "r1eiqi09K7", "replyto": "SkxSY3VnAX", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "content": {"title": "Many thanks for your comment", "comment": "Our bounds indeed hold for any isometry phi. \nEven with phi(m) = -m. \nWe understand your confusion.\n\nFirst, please note that if phi(m) = -m, although the past gradients in the exponential moving average would be alternated, i.e. multiplied by some (-1)^t, the current gradient is not modified by phi. \n\nThat being said, what you are pointing out (\u201con the choice of phi\u201d, bottom of page 6) shows that in the Euclidean case, if you would take phi to be a random orthogonal transformation, even a different one at each step, the bounds still hold. \n\nIsn\u2019t this puzzling?\n\nOur Riemannian generalization sheds light on this part of the convergence proof of AmsGrad. Indeed, this can be easily seen from our proof, Eq.(18) and Eq.(30), that one only needs to know the \u201csize\u201d of the update \u201ctransported\u201d by phi, and not its direction. \n\nIt does not tell that the bounds of Reddi et al. are wrong, it just means that the current bounds that the community has on Adam-like algorithms are \u201ctoo loose\u201d. \n\nMore precisely, although the currently known regret bounds guarantee convergence, they do not exploit acceleration coming from momentum. This suggests that much better bounds could potentially be obtained.\n\nWe hope that this work will motivate further research in better understanding Adam-like algorithms. \n\nThank you for your interest! \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613386, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1eiqi09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper561/Authors|ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613386}}}, {"id": "SkxSY3VnAX", "original": null, "number": 6, "cdate": 1543421052906, "ddate": null, "tcdate": 1543421052906, "tmdate": 1543421052906, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "You claim your regret bound holds for any isometry phi. In the simple euclidean case that means that if for example you use phi(m) = -m your regret bound should still hold even though the optimization should fail. I even tried a simple script just to be sure and it indeed doesn't improve (just sanity check not claiming to check thoroughly). The gradient norm also do not increase so it is unlikely the bound holds in a vacuous way. \nThis might indicate a issue with the proof. ", "title": "Issue with any isometry"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}, {"id": "r1x3Gsjl0m", "original": null, "number": 5, "cdate": 1542662932141, "ddate": null, "tcdate": 1542662932141, "tmdate": 1542662932141, "tddate": null, "forum": "r1eiqi09K7", "replyto": "HygZGC2Csm", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "content": {"title": "Thank you for reviewing our work", "comment": "Thank you for your detailed feedback. We have updated our paper according to your suggestions.\n\n\nWe reply below to each of your remarks, more specifically.\n\n*) Typo corrected.\n*) Retraction: indeed, choosing the retraction R_x(v)=x+v requires having immersed the manifold into an ambient Euclidean space: note that we only say that the retraction is \u201cmost often chosen as\u201d such, not that this choice is always a valid one. We mention it here because it is the one we used in our experiments. \n*) T is the number of iterations, [T] denotes the set of integers from 1 to T. We use same notations as in [1]: Each f_t is the objective function of the parameters to be optimized, evaluated at the batch taken at time t. For instance, when training a neural network, one could alternatively write f_t(x) = \\sum_{y\\in S_t} L(x,y), where x is the set of parameters of the model, L is the loss, each y is an input to the network, and S_t the (mini)batch taken at time t. \n*) yes, these are coordinate-wise operations. We did not write explicitly the loop over i to not influence the reader into implementing this algorithm with a loop over i. In most languages, such as python or C++, coordinate-wise operations such as adding vectors are highly optimized in the standard library. One could rewrite the algorithm without the \u201ci\u201d, with coordinate wise operations on vectors. \n*) We added a footnote clarifying the domain of the loss function in Section 5. \n*) Tables 2 & 3 -> figures 2 & 3: Thank you, we corrected this typo. \n*) Other potential applications include any optimization-based graph or word embedding method on a manifold. Note that the product-structure assumption is natural, since if one needs to embed n nodes into a manifold, the parameter space is a product of n manifolds. \nFollowing your suggestions, we have added a few references [2,3,4] as suggestions for further experiments, at the beginning of the experiment section.\n\n\n[1] On the convergence of Adam and beyond, Reddi et al., ICLR 2018\n[2] Representation trade-offs for hyperbolic embeddings, De Sa et al., ICML 2018\n[3] Hyperbolic entailment cones for learning hierarchical embeddings, Ganea et al., ICML 2018\n[4] Learning continuous hierarchies in the Lorentz model of hyperbolic geometry, Nickel & Kiela, ICML 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613386, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1eiqi09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper561/Authors|ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613386}}}, {"id": "B1gnp5olCX", "original": null, "number": 4, "cdate": 1542662851870, "ddate": null, "tcdate": 1542662851870, "tmdate": 1542662881746, "tddate": null, "forum": "r1eiqi09K7", "replyto": "SylzCu-S3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "content": {"title": "Thank you for reviewing our work", "comment": "Thank you for your interest and professionalism. We reply below to each of your concerns.\n\nProduct structure: \n\n(i) The product structure is natural for any optimization-based graph or word embedding method: if one wants to embed n nodes into a manifold M, then the parameter space is M^n. In particular, this would apply also if M is a PSD manifold.\n\n(ii) We noticed recently that other very recent approaches propose to also embed each point into a product of spaces, arguing that it allows the embeddings to benefit from the metric properties of each space [1,2].\n\n(iii) Our proof arguments would not hold without this product structure. This is easier to see from the convergence proof of Euclidean AMSgrad [3], appendix D, Eq.(18), where the last equality exploits the Euclidean coordinate system to expand the squared norms. This is not possible on a general Riemannian manifold. However, with a product structure, one can expand squared distances in the product manifold, as the sum of squared distances in each manifold of the product. \n\n\nPi operator: \n\n-The presence of the projection operator is mostly useful for the convergence proof, to guarantee that the learning trajectory in parameter space is bounded, hence the presence of D_\\infty in the bounds. \n-Note that this operator is also required to obtain theoretical bounds for Euclidean AMSgrad/Adam, even though it is often omitted in practice. \n-Note that in [4, section 3], it is assumed to be given, as a \u201cprojection oracle\u201d. Also note that for many applications of interest, such as computing Karcher means on PSD manifolds (as done in [4, section 4]), a projection operator is not used nor needed for convergence, since the trajectory is trivially bounded (formally, this amounts to choosing a trivial projection into a ball containing the trajectory). \n-In the Poincar\u00e9 ball, if X is a ball centered at the origin (as in our experiments), then the projection is naturally given by the parametrization in the Euclidean ambient space.\n\n\nFr\u00e9chet means: \n\n-This is an interesting suggestion that we will keep in mind for future work. \n\n\nRelated work: \n\n-Thank you for pointing us to this relevant reference. We have added it to the related work section.\n\n\nOther remarks: \n\nIn Eq.(5), \\hat{v} is defined from v, which is defined as for Adam, defined just above. We have added a footnote explaining this.\nAdded a reference for the claim line 3, page 4, about gradient and Hessian.\nAt the bottom of page 4, \\alpha is not required to go to 0: we consider here an (R)SGD update of fixed size \\alpha. \nIf by [.] you refer to gyr[. , .], this square bracket comes from the notation of the gyro-operator, for which we provide a reference. We use it in our experiments to efficiently compute parallel transport in the Poincar\u00e9 ball.\nThe dimension we use in our experiments is 5, as suggested in [5]. Added to experiments section. \n\n\n[1] Learning mixed curvature representations in product spaces,\nhttps://openreview.net/forum?id=HJxeWnCcF7\n[2] Poincar\u00e9 Glove: hyperbolic word embeddings, \nhttps://openreview.net/forum?id=Ske5r3AqK7\n[3] On the convergence of Adam and beyond, Reddi et al., ICLR 2018\nhttps://openreview.net/forum?id=ryQu7f-RZ\n[4] First order methods for geodesically convex optimization, Zhang & Sra, JMLR 2016 \nproceedings.mlr.press/v49/zhang16b.pdf\n[5] Poincar\u00e9 embeddings for learning hierarchical representations, Nickel & Kiela, NIPS 2017\nhttps://arxiv.org/abs/1705.08039\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613386, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1eiqi09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper561/Authors|ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613386}}}, {"id": "rJglV5jxCQ", "original": null, "number": 3, "cdate": 1542662696483, "ddate": null, "tcdate": 1542662696483, "tmdate": 1542662714268, "tddate": null, "forum": "r1eiqi09K7", "replyto": "HkeQ6djp2X", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "content": {"title": "Thank you for reviewing our work", "comment": "Thank you for reviewing our work. Even though RSGD is indeed slightly easier to use, we will make our code available to facilitate the use of our algorithms."}, "signatures": ["ICLR.cc/2019/Conference/Paper561/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613386, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1eiqi09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper561/Authors|ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613386}}}, {"id": "rkx1x9oeAm", "original": null, "number": 2, "cdate": 1542662631248, "ddate": null, "tcdate": 1542662631248, "tmdate": 1542662631248, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "content": {"title": "Many thanks for reviewing our paper.", "comment": "We would like to thank all three reviewers for their work and their interest in our work.\n\nWe have incorporated the modifications suggested by reviewers and are open to further comments.\n\nPlease find more detailed responses below each review.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613386, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1eiqi09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper561/Authors|ICLR.cc/2019/Conference/Paper561/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613386}}}, {"id": "HkeQ6djp2X", "original": null, "number": 3, "cdate": 1541417146718, "ddate": null, "tcdate": 1541417146718, "tmdate": 1541533888949, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Review", "content": {"title": "Riemannian Adam/Amsgrad on product manifolds with convergence guarantee but not supported well by experiments.", "review": "The paper extends Euclidean optimization methods, Adam/Amsgrad, to the Riemannian setting, and provides theoretical convergence analysis which includes the Euclidean versions as a special case. To avoid breaking the sparsity, coordinate-wise updates are performed on product manifolds. \n\nThe empirical performance seems not very good, compared to RSGD which is easier to use.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Review", "cdate": 1542234433294, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335752303, "tmdate": 1552335752303, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SylzCu-S3Q", "original": null, "number": 2, "cdate": 1540851914353, "ddate": null, "tcdate": 1540851914353, "tmdate": 1541533888747, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Review", "content": {"title": "Riemannian ADAM", "review": "I have enjoyed reading this paper. The paper is accessible in most cases and provides a novel optimization technique. Having said this, I have a few concerns here,\n\n\n- I am not sure why the notion of product manifolds is required in developing the technique. To me, all the arguments follow without that. Even if the authors are only interested in manifolds that can be constructed in a product manner (say R^n from R),  the development can be done without explicitly going along that path. Nevertheless I may have missed something so please elaborate why product manifolds. I have to add that in many cases, the underlying Riemannian geometry cannot be derived as a product  space. For example, the SPD manifold cannot be constructed as a product space of lower dimensional geometries. \n\n- I have a feeling that finding the operator \\Pi in many interesting cases is not easy. Given the dependency of the developments on this operator, I am wondering if the method can be used to address problems on other manifolds such as SPD, Grassmannian or Stiefel. Please provide the form of this operator for the aforementioned manifolds and comment on how the method can be used if such an operator is not at our disposal.\n\n- While I appreciate the experiments done in the paper,  common tests (e.g., Frechet means) are not presented in the paper (see my comment below as well).  \n\n- last but not least, the authors missed the work of   Roy et. al., \"Geometry Aware Constrained Optimization Techniques for Deep Learning\", CVPR'18 where RSGC with momentum and Riemannian version of RMSProp are developed. This reference should be considered and compared.\n\n\nAside from the above, please\n\n- define v and \\hat{v} for Eq.(5) \n\n- provide a reference for the claim at l3-p4 (claim about the gradient and Hessian)\n\n- maybe you want to mention that \\alpha -> 0 for |g_t^i| at the bottom of p4\n\n- what does [.] mean in the last step of the algorithm presented in p7\n\n- what is the dimensionality of the Hn in the experiments\n \n\n\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Review", "cdate": 1542234433294, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335752303, "tmdate": 1552335752303, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HygZGC2Csm", "original": null, "number": 1, "cdate": 1540439561512, "ddate": null, "tcdate": 1540439561512, "tmdate": 1541533888539, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Official_Review", "content": {"title": "This paper is well-writen except a few flaws (see below). The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.", "review": "This paper presents Riemannian versions of adaptive optimization methods, including ADAGRAD, ADAM, AMSGRAD and ADAMNC. There are no natural coordinates on a manifold. Therefore, the authors resort to product of manifolds and view each manifold component as a coordinate. Convergence analyses for those methods are given. The the theoretical results and their Euclidean versions coincide. An experiment of embedding a tree-like graph into a Poincare model is used to show the performance of the Riemannian versions of the four methods.\n\nThis paper is well-written except a few flaws (see below). I do not have time to read the proofs carefully. The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.\n\nRemarks:\n*) P1, line 2: it particular -> in particular.\n*) P3, line 9: Is R_x(v) = x + v most often chosen? A manifold is generally nonlinear. A simple addition would not give a point in the manifold.\n*) P5, in Assumptions and notations paragraph: what are T and [T]? Is T the number of total iterations or the number of functions in the function family. The subscript of the function f_t seems to be an index of the functions. But its notation is also related to the number of iterations, see (8) and the algorithms in Figure 1.\n*) P5, Figure 1: does a loop for the index $i$ missing?\n*) Section 5: it would be clearer if the objective function is written as L:(D^n)^m \\to R: \\theta-> , where m is the number of nodes. Otherwise, it is not obvious to see the domain. \n*) P7, last paragraph: Tables 2 and 3 -> Figures 2 and 3.\n*) Besides the application in the experiments, it would be nice if more applications, at least references, are added.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper561/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Official_Review", "cdate": 1542234433294, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper561/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335752303, "tmdate": 1552335752303, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper561/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkx-VlBpFX", "original": null, "number": 5, "cdate": 1538244648560, "ddate": null, "tcdate": 1538244648560, "tmdate": 1538257401690, "tddate": null, "forum": "r1eiqi09K7", "replyto": "ryxWLeM6F7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "Thanks for your response. \n\nFor 1. and 2., I just think if you claim that the convergence result with retraction is not too different, the statement is more general since exp map is also a retraction, see https://arxiv.org/abs/1802.09128. I agree that for those manifolds exp map is easy to compute, retraction is an approximation of exp map and for general manifolds it is easier. If the convergence guarantee is different for retraction and exp map, it may be also worth pointing out and comparing.\n\nFor 3., the product structure is not used for sphere, etc., so it's just unusual to me. It helps reader if you stress it, and recall some basics or intuitions, such as Sec 3.2 (d(x,y))^2 = \\sum d(x^i,y^i)^2. Maybe you can add a few line proof in appendix or just quote some literature.\n\nFor 4. is there a closed form solution for parallel translation for Stiefel manifold (see ex 8.1.2 Optimization Algorithms on Matrix Manifolds by Absil et al)? I'm not so sure about the isometry stuff, maybe you can explain more, and discuss how such \"inexact\" parallel translation interacts with geodesic convexity, Lipschitz constants and so on. But anyways, you have a concrete example where the algorithm works, so the statement are no doubt legit.\n\nAnd I see your point for defining retraction in another reply.\n\nThanks again for your response! And my apology it's a quick comment so I did not go through every detail.\n\n ", "title": "Thanks for your response"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}, {"id": "ryxWLeM6F7", "original": null, "number": 3, "cdate": 1538232392830, "ddate": null, "tcdate": 1538232392830, "tmdate": 1538245758710, "tddate": null, "forum": "r1eiqi09K7", "replyto": "S1lynLJot7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "Hi, thank you for your interest! \n\n1. Although the exponential map is not always known in closed-form, your remark also applies to any Riemannian optimization algorithm, even to Riemannian SGD. Note however that its formula is known for spherical and hyperbolic spaces, Stiefel and Grassmann manifolds, as well as for a variety of matrix Lie groups, which correspond nowadays to the main application cases of Riemannian optimization. In cases where it is not, a retraction mapping can be used instead, but the choice of your retraction will affect the trajectory. Although comparing the use of various retractions could constitute interesting future work, this was not our theoretical focus.\n\n2. Could you please share with us why you think it would be better to stick with the retraction mapping in general? Besides the fact that using the exp map is more mathematically principled, note that [1] also obtained significantly better empirical results by using fully Riemannian methods in the Lorentz model of hyperbolic geometry. In practice, what is best seems to depend on the manifold, the model and the task.\n\n3. As explained in the last paragraph of our introduction, the product structure is relevant to any method aiming to embed a set of points in a Riemannian manifold M. This concerns in particular all optimization-based graph and word embedding methods. If k is the size of the set of vertices or words to embed, then the parameter space is M^k. Moreover, note that one could also choose M itself as a product of manifolds, as was done for instance in [2,3,4].\n\n4. Concerning the closed-form formula of parallel-transport, although it is also given in the above mentioned cases, it can be seen from our proofs that our convergence theorems still hold if it is replaced by any isometry between the corresponding tangent spaces. We will add this as an interesting remark, thank you for pointing it out! Also notice that in our experiments section, we provide the formula for parallel-transport in the Poincare ball, using gyro operations.\n\n\n[1] Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry, Nickel & Kiela, ICML 2018\n[2] https://openreview.net/forum?id=HJxeWnCcF7\n[3] https://openreview.net/forum?id=Ske5r3AqK7\n[4] https://openreview.net/forum?id=r1xRW3A9YX\n", "title": "Thanks for you interest!"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}, {"id": "ryxFFxGaYX", "original": null, "number": 4, "cdate": 1538232448707, "ddate": null, "tcdate": 1538232448707, "tmdate": 1538232477823, "tddate": null, "forum": "r1eiqi09K7", "replyto": "HylTxaesFm", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "Concerning the use of a projection operation, note that we include it both in our algorithms and convergence proofs. As soon as you have an extrinsic representation of your manifold in an ambient Euclidean space (which we do in our experimental setup), the + operation is well defined, and whether you include the projection in the retraction or apply it on top is just a matter of notations. See [5] for similar definitions/notations as ours. \n\n[5] Poincare embeddings for learning hierarchical representations, Nickel & Kiela, NIPS 2017\n\n", "title": "A matter of notations."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}, {"id": "S1lynLJot7", "original": null, "number": 1, "cdate": 1538090662534, "ddate": null, "tcdate": 1538090662534, "tmdate": 1538197174869, "tddate": null, "forum": "r1eiqi09K7", "replyto": "r1eiqi09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "A quick comment. Seems you use exponential map and parallel transport in your algorithm, and I guess it's okay to compute exponential map and parallel transport for some simple manifold. But is it usually possible for cases where people are interested? What I often see is retraction instead of exponential map, and sometimes parallel transport has no close form solution. It seems also unusual to assume cartesian product exists. \n\nAs you said, you probably can replace exp map with retraction, so it's better to stick to it, say in algorithm figure 1 and your proof.\n\nFor example section, people have some manifolds in mind, like sphere, orthogonal group/stiefel, grassmann, etc. so maybe it helps to address one of them.", "title": "Can you compute exponential map and parallel transport in general?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}, {"id": "HylTxaesFm", "original": null, "number": 2, "cdate": 1538096372619, "ddate": null, "tcdate": 1538096372619, "tmdate": 1538170832606, "tddate": null, "forum": "r1eiqi09K7", "replyto": "S1lynLJot7", "invitation": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "content": {"comment": "And for retraction, you cannot do R_x(v) = x+v since it maps to manifold. Maybe a projection.", "title": "Retraction on page 3"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper561/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Adaptive Optimization Methods", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam, Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.", "keywords": ["Riemannian optimization", "adaptive", "hyperbolic", "curvature", "manifold", "adam", "amsgrad", "adagrad", "rsgd", "convergence"], "authorids": ["gary.becigneul@inf.ethz.ch", "octavian.ganea@inf.ethz.ch"], "authors": ["Gary Becigneul", "Octavian-Eugen Ganea"], "TL;DR": "Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. ", "pdf": "/pdf/6d98513691f23d0d74536f9f198165d5e207aab1.pdf", "paperhash": "becigneul|riemannian_adaptive_optimization_methods", "_bibtex": "@inproceedings{\nbecigneul2018riemannian,\ntitle={Riemannian Adaptive Optimization Methods},\nauthor={Gary Becigneul and Octavian-Eugen Ganea},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=r1eiqi09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper561/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311807545, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1eiqi09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper561/Authors", "ICLR.cc/2019/Conference/Paper561/Reviewers", "ICLR.cc/2019/Conference/Paper561/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311807545}}}], "count": 19}