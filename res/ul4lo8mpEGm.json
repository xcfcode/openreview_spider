{"notes": [{"id": "ul4lo8mpEGm", "original": "zcTKGmG78GT", "number": 31, "cdate": 1615310254590, "ddate": null, "tcdate": 1615310254590, "tmdate": 1615313024315, "tddate": null, "forum": "ul4lo8mpEGm", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Generalizable Representations for Reinforcement Learning", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper31/Authors"], "authors": ["Anonymous"], "keywords": ["Reinforcement Learning", "Robotics", "Representation Learning", "Imitation learning"], "TL;DR": "RRL: Resnet as a representation for Reinforcement Learning -- a straightforward yet effective approach that can learn complex behaviors directly from proprioceptive inputs.", "abstract": "The ability to autonomously learn behaviors via direct interactions in uninstrumented environments can lead to generalist robots capable of enhancing productivity or providing care in unstructured settings like homes. Such uninstrumented settings warrant operations only using the robot\u2019s proprioceptive sensor such as onboard cameras, joint encoders, etc which can be challenging for policy learning owing to the high dimensionality and partial observability issues. While these issues can be circumvented by leveraging compressed represenations, such representations acquired via self-supervised techniques are often brittle to task variations and donot generalize across tasks. We propose RRL: Resnet as a representation for Reinforcement Learning \u2013 a straightforward yet effective approach that can learn complex behaviors directly from proprioceptive inputs. RRL fuses features extracted from pre-trained Resnet into the standard reinforcement learning pipeline and deliver results comparable to learning directly from the state. In a simulated dexterous manipulation benchmark, where the state of the art methods fails to make significant progress, RRL delivers contact rich behaviors. Its effectiveness in learning behaviors directly from visual inputs with performance and sample efficiency matching learning directly from the state, even in complex high dimensional domains, is far from obvious.", "pdf": "/pdf/3034ec0df3674beae7d530002f6d81bfeef5456c.pdf", "paperhash": "anonymous|generalizable_representations_for_reinforcement_learning", "_bibtex": "@inproceedings{\nanonymous2021generalizable,\ntitle={Generalizable Representations for Reinforcement Learning},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=ul4lo8mpEGm},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}