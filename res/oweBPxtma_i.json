{"notes": [{"id": "oweBPxtma_i", "original": "UiurBqRlzGa", "number": 2719, "cdate": 1601308301399, "ddate": null, "tcdate": 1601308301399, "tmdate": 1614985779924, "tddate": null, "forum": "oweBPxtma_i", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IAvkmDzbpOY", "original": null, "number": 1, "cdate": 1610040350146, "ddate": null, "tcdate": 1610040350146, "tmdate": 1610473939132, "tddate": null, "forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "invitation": "ICLR.cc/2021/Conference/Paper2719/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "In this paper, the authors work on improving the interpretability of CNNs following distillation methods. The paper is written in such a convoluted way and with many changes in the notation that makes it hard to understand what they are proposing and what for. This is a major impediment for the paper going forward. But also, the reviewers point out some clear aspects of the paper and the interpretability that it provides for CNNs, for example, the entropy analysis and its variability or referring to the discrimination part of the CNN or mentioning an explainable alternative to CNNs, while still relying on the convolutional filters. The authors need to dedicate more time to explain this paper carefully before it can be properly reviewed and published at any major conference."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "tags": [], "invitation": {"reply": {"forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040350126, "tmdate": 1610473939113, "id": "ICLR.cc/2021/Conference/Paper2719/-/Decision"}}}, {"id": "KS26dwlMGTL", "original": null, "number": 3, "cdate": 1606017032530, "ddate": null, "tcdate": 1606017032530, "tmdate": 1606018001626, "tddate": null, "forum": "oweBPxtma_i", "replyto": "BAVYJ09zazC", "invitation": "ICLR.cc/2021/Conference/Paper2719/-/Official_Comment", "content": {"title": "A self-explanatory method for the black problem on discrimination part of CNN", "comment": "Thank you very much for your comments. We will revise the manuscript based on your suggestions.  Some responses to the questions have been shown in the following.\n\n(1)\tI found this paper very difficult to follow as it has many grammatical and syntactic errors.\n\nResponse:Thank you for your suggestions. We will carefully revise these errors in the latest edition.\n\n(2)\tIf I understand the paper correctly, which I am not sure I do, I believe the authors propose a method for extracting an interpretable model which replaces the fully connected layers of a Convolutional Image Network.\n\nResponse:The purpose of extracting logical network is not to replace the full connection layer of CNN, but to improve the interpretable performance of full connection layer. In the absence of ideal interpretable model, a new self-interpretation method is proposed in this paper. Firstly, the interpretable model is extracted from the full connection layer of CNN, and then the interpretative distance between the two is shrunk by game to improve the interpretable performance of the full connection layer, and to keep its well generalization performance.\n\n(3)\tMy primary issue with the paper is that it attempts to provide an 'explainable alternative' to a CNN but this explainable model still relies on the features extracted from the convolutional section of a CNN. The paper does not put forward a convincing argument to justify the focus on the fully connected layers. It is interesting to extract an interpretable model from a fully connected network, but if this is the goal of the paper, then the authors should focus on datasets in which a fully connected network outperforms standard explainable models such as logistic regression but interpretability would still be desirable, such as the MIMIC medical dataset.\n\nResponse:In this paper, though we extract the interpretable model according to all feature maps, it does not depend on these feature maps but the physical structure of the fully connected layers. The definition of DCLM in paper have provided evidence to show its disjunctive normal forms are related to fully connected layers but not feature maps.\n\n(4)\tThe paper would also be significantly improved if more realistic datasets would be explored. The only datasets used are variants of MNIST, in which good performance can be achieved with traditional explainable models.\n\nResponse:Many thanks. We will revise the manuscript based on your suggestion. Some new data sets will be added for evaluating the proposed method. Moreover, we think that the most important contribution in our paper does not lie in designing the interpretability model but providing a self-interpretation method on CNN\u2019s fully connected layer to improve its interpretable performance. Besides, the experiment in this paper also shows that our method is very excellent."}, "signatures": ["ICLR.cc/2021/Conference/Paper2719/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs", "ICLR.cc/2021/Conference/Paper2719/Reviewers", "ICLR.cc/2021/Conference/Paper2719/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oweBPxtma_i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2719/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2719/Authors|ICLR.cc/2021/Conference/Paper2719/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845191, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2719/-/Official_Comment"}}}, {"id": "mUOeYCmUCd", "original": null, "number": 4, "cdate": 1606017845700, "ddate": null, "tcdate": 1606017845700, "tmdate": 1606017845700, "tddate": null, "forum": "oweBPxtma_i", "replyto": "qZb6h-deJy", "invitation": "ICLR.cc/2021/Conference/Paper2719/-/Official_Comment", "content": {"title": "A self-explanatory method for the black problem on discrimination part of CNN", "comment": "Thanks for the thoughtful reviews and constructive suggestions. We will revise the manuscript based on your suggestions.  Some responses to the questions have been shown in the following.\n\n(1)\tWhile the motivation is easy to understand, notation is continuously introduced in the paper without clear description and definitions.\n\nResponse: In the paper, some notations are not described and defined in the first appearance. In its\u2019 new version, we will make revisions for the negligence.\n\n(2)\tWhat is the iterative optimization procedure described in Page 5? Highly unclear how the CNN model is actually updated in Algorithm 1 to do the minimization of Eq (14).\n\nResponse: Thanks for your suggestions. The pseudocode of the algorithm 1 in the first version of the paper was too simple. In the new version the complete pseudocode is provided.\n\n(3)\tCan the authors elaborate why the accuracy gap for more challenging datasets like FashionMNIST is much more than simpler datasets for the DCLM model? I also suggest authors to include other datasets instead of focusing on MNIST only.\n\nResponse: We think that it is chiefly because some challenging datasets can make input neurons capture more features and cause more intricate logic relationships between the activated state and the inactivated state of input neurons. In this situation, the logical relationship obtained by our game maybe not a fully complete one. But the main contribution of this paper is to provide a self-explanatory learning paradigm, which can real-time obtain the inherent causality of the full connection part of CNN without any prior knowledge. More significantly, our work improves its interpretable performance based on ensuring its generalization performance does not obviously decline. Further studies will further optimize the method.\n\n(4)\tThe entropy analysis is interesting but I would've liked to see more insight into why entropy behavior across different datasets is so variable.\n\nResponse: Information entropy calculates the diversity of logical relations obtained by game from full connection part. However, in the initial epoch, the diversities of logical relations obtained by different training sample sets are different, so its initial information entropy values are also different, leading to different information-entropy curves. However, no matter what kind of structure of fully connected parts, this method can always ensure that the entropy tends to a smaller region, which shows that the method can ensure the diversity of logical relations tend to be stable. At the same time, through the change of entropy, we can perceive the change law of the inherent logical relationship of the fully connected parts. We will carefully revise this paper to ensure that the latest edition will improve its readability and express these contents more clearly.\n\n(5)\tWay too many typos in the paper. Please proof-read and correct, I only highlighted a few.\n\nResponse: Thanks for your comments, we will scrutinize this paper and correct all mistakes.\n\n(6)\tClarify what \"discrimination part\" of CNN means, very early on in the introduction and abstract.\n\nResponse: we will add the meaning of \"discrimination part\" in the introduction and abstract.\n\n(7)\tDid the authors do a qualitative analysis of where exactly the approximate DCLM has lower accuracy compared to the CNN model and why?\n\nResponse: We think that this is mainly attributed to that data bias and noisy data in training data set make it difficult for DCLM to approximate to the optimal fully connected part of CNN, so the generalization performance of DCLM is worse than CNN. In its\u2019 new version, we will express these contents more clearly.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2719/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs", "ICLR.cc/2021/Conference/Paper2719/Reviewers", "ICLR.cc/2021/Conference/Paper2719/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oweBPxtma_i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2719/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2719/Authors|ICLR.cc/2021/Conference/Paper2719/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845191, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2719/-/Official_Comment"}}}, {"id": "SIkoZwyq8U", "original": null, "number": 2, "cdate": 1606016160590, "ddate": null, "tcdate": 1606016160590, "tmdate": 1606016160590, "tddate": null, "forum": "oweBPxtma_i", "replyto": "uLSEy5LJyT5", "invitation": "ICLR.cc/2021/Conference/Paper2719/-/Official_Comment", "content": {"title": "A self-explanatory method for the black problem on discrimination part of CNN", "comment": "Thank you very much for your comments and constructive suggestions. Some responses to the questions have been shown in the following.\n\n(1)\tThe definition of the term \"discrimination part of CNN\" is described at the beginning of Sec3, although the term is frequently used in Sec1. I could not understand what the \"discrimination part of CNN\" means when I first read the paper:\n\nResponse:Thank you for your suggestions. We will correct this flaw in subsequent new edition.\n\n(2)\tThe paper does not describe why interpreting \"discrimination part of CNN\" is important. Thus, the paper failed to motivate the problem. Although the authors cited some related studies, such as [Wan+20], the importance of the problem need to be described in the paper:\n\nResponse:The former part of CNN can be understood as the process of feature extraction. We can obtain the physical interpretation of every neuron through visualization method. However, the fully connected layers cannot be interpreted by visualization methods. Due to the uninterpretable nature of the fully connected layers, it is impossible to know its inherent causality which can conclude the reason for its prediction results. Moreover, it is also impossible to monitor the evolution of the diversity of logical relations implied in the network in the training process in real time, so as to provide the basis for further improving the training process in the future. In the new version, the introduction part will be revised for the flaw.\n\n(3)\tAlgorithm1 contains undefined notations.\n\nResponse:The pseudocode of the algorithm 1 in the first version of the paper was too simple. In the new version the complete pseudocode will be provided.\n\n(4)\tThe authors merely compared the accuracies of DCLM and its approximation performance. There is no demonstration nor discussion that DCLM can resolve the problem of interpretability of the \"discrimination part of CNN\", which should be the primal goal of this study.\n\nResponse:First of all, the DCLM is an interpretable model, which includes many disjunctive normal forms. These normal forms can be easily expressed as understandable implications. When it keeps approximating and finally reach consistent convergence with the discriminating parts of CNN, the interpretable model can explain the discriminating part of CNN. This point has been proved from both experimental and theoretical perspectives. Finally, we will carefully revise this paper to clarify this point.\n\n(5)\tOverall, I think the paper needs major rewriting so that the main message of the paper to be clear: what the problem is; why it is important; how the authors solved the problem; and how the authors confirmed the problem is solved.\n\nResponse:Thank you for your suggestions. We will carefully revise this paper to ensure that the latest edition will improve its readability and express the four points more clearly.\n\n(6)\tIn addition to the readability, I also found some technical errors\n\nResponse:Thank you for your reminders. We will carefully revise these equations.\n\n(7)\tThe use of decision tree for approximating the \"discrimination part of CNN\" is considered by [Wan+20]. This paper proposes using a logical formula instead of the decision tree. The idea seems to be straightforward, and the innovation made in this paper is marginal.\n\nResponse:The innovation of this paper is not only the establishment of interpretable model DCLM, but also the proposal of self-interpretation method, which solves the problem of how to improve the interpretable performance of discrimination part of CNN when there is no prior knowledge and the optimal interpretable model. At the same time, because of the lightweight structure of the interpretable model, it can ensure that the logical relationship in the fully connected layers can be known in time in each epoch of the training process. It is easy to find the change law of this logical relationship to better guide the training process. This can be verified by the experimental results in Fig. 3,4,5,6.\n\n(8)\tI could not find anything positive about this paper.\n\nResponse: We think that the following points are worth reading in this paper: \n1. Aiming at the uninterpretable problem of the discrimination part of CNN, this paper proposes a self-interpretation method, which does not need any prior knowledge; \n2. In order to realize this self-interpretation method, a lightweight interpretable model and interpretation distance are proposed to measure the interpretability performance \n3. This paper analyzes the black box problem, deduces the necessary and sufficient conditions for this problem, and points out the reasons why the problem cannot be solved at present.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2719/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs", "ICLR.cc/2021/Conference/Paper2719/Reviewers", "ICLR.cc/2021/Conference/Paper2719/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oweBPxtma_i", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2719/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2719/Authors|ICLR.cc/2021/Conference/Paper2719/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845191, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2719/-/Official_Comment"}}}, {"id": "uLSEy5LJyT5", "original": null, "number": 1, "cdate": 1603679458955, "ddate": null, "tcdate": 1603679458955, "tmdate": 1605024147405, "tddate": null, "forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "invitation": "ICLR.cc/2021/Conference/Paper2719/-/Official_Review", "content": {"title": "The paper is very hard to read.", "review": "### Paper Summary\nThe authors proposed Deep Cognitive Learning Model (DCLM) as a surrogate of \"discrimination part of CNN\" which is the fully connected layers after the convolution layers. The authors claimed that DCLM can explain the causal relationship between the features and the output result of the discrimination part of CNN.\n\n### Quality & Clarity\nI found the paper very hard to read. I tried my best to understand the paper. The major contributions of this paper are as follows.\n1. The authors proposed to use DCLM to approximates the fully connected layers (FC), i.e. $\\mathrm{FC}(x) \\approx \\mathrm{DCLM}(x)$ for any $x$. Because DCLM is in a logical formula, we can interpret the \"discrimination part of CNN\" approximately by interpreting DCLM.\n2. The authors further proposed to use DCML to \"regularize\" FC. That is, train CNN so that its FC to be close to DCLM.\nThe first contribution is described in Sec3, while the second contribution is described in Sec4 and Sec5.\n\nBelow, I list the points that makes the paper hard to read.\n* The definition of the term \"discrimination part of CNN\" is described at the beginning of Sec3, although the term is frequently used in Sec1. I could not understand what the \"discrimination part of CNN\" means when I first read the paper. This problem is fatal because the readers cannot understand the problem the authors want to solve in this study.\n* The paper does not describe why interpreting \"discrimination part of CNN\" is important. Thus, the paper failed to motivate the problem. Although the authors cited some related studies, such as [Wan+20], the importance of the problem need to be described in the paper. This problem is fatal because the readers cannot understand why the problem is important.\n* Algorithm1 contains undefined notations. For example, the inputs and the outputs of $\\mathrm{CN}$ and $LN$ are not defined. There are no descriptions what $FMs$ and $f_{nn}$ are. Moreover, the second $\\mathrm{CN}$ in Algorithm1 outputs $FCMs$ in addition to $FMs$ and $f_{nn}$. This problem is fatal because the readers cannot understand how the proposed method operates.\n* The authors merely compared the accuracies of DCLM and its approximation performance. There is no demonstration nor discussion that DCLM can resolve the problem of interpretability of the \"discrimination part of CNN\", which should be the primal goal of this study. This problem is fatal because the readers cannot understand whether the problem considered in this study is solved.\n\nOverall, I think the paper needs major rewriting so that the main message of the paper to be clear: what the problem is; why it is important; how the authors solved the problem; and how the authors confirmed the problem is solved.\n\nIn addition to the readability, I also found some technical errors.\n* In Eq.(5), the partition function $\\log \\Xi$ should appear because it is a function of $y_{dclm}$ and $a$. The authors somehow ignored it.\n* In Eq.(13) and Eq.(14), the partition functions $\\log \\Xi_1$ and $\\log \\Xi_2$ should appear because they are functions of $w$. The authors somehow ignored them.\nThere should be some justifications why one can ignore the partition functions. Or, the experimental results need to be updated based on the objective functions with the partition functions.\n\n### Originality & Significance\nThe use of decision tree for approximating the \"discrimination part of CNN\" is considered by [Wan+20]. This paper proposes using a logical formula instead of the decision tree. The idea seems to be straightforward, and the innovation made in this paper is marginal.\n\n### Pros & Cons\n[Pros]\n* I could not find anything positive about this paper.\n\n[Cons]\n* The paper is very hard to read. The major rewriting is needed.\n* There are some technical errors.\n* The improvement over [Wan+20] seems to be marginal.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2719/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2719/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090084, "tmdate": 1606915758400, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2719/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2719/-/Official_Review"}}}, {"id": "BAVYJ09zazC", "original": null, "number": 2, "cdate": 1603768118226, "ddate": null, "tcdate": 1603768118226, "tmdate": 1605024147343, "tddate": null, "forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "invitation": "ICLR.cc/2021/Conference/Paper2719/-/Official_Review", "content": {"title": "Significant issues in the language severally limit this work, though there are content issues as well.", "review": "I found this paper very difficult to follow as it has many grammatical and syntactic errors. I believe it needs a significant amount of editing in order for the paper to be published in english. In particular careful attention should be made to omitted particles and pluralization.  This alone is a barrier to publication. \n\nSetting aside the grammatical errors, I believe there are some issues in the content of the paper that would need to be addressed as well in order to make this paper ready for publication. \n\nIf I understand the paper correctly, which I am not sure I do, I believe the authors propose a method for extracting an interpretable model which replaces the fully connected layers of a Convolutional Image Network. They present results on toy datasets, MNIST, Emnist, and FashionMNIST.   \n\nMy primary issue with the paper is that it attempts to provide an 'explainable alternative' to a CNN but this explainable model still relies on the features extracted from the convolutional section of a CNN. The paper does not put forward a convincing argument to justify the focus on the fully connected layers. It is interesting to extract an interpretable model from a fully connected network, but if this is the goal of the paper, then the authors should focus on datasets in which a fully connected network outperforms standard explainable models such as logistic regression but interpretability would still be desirable, such as the MIMIC medical dataset. \n\nThe paper would also be significantly improved if more realistic datasets would be explored. The only datasets used are variants of MNIST, in which good performance can be achieved with traditional explainable models.\n\n", "rating": "3: Clear rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2719/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2719/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090084, "tmdate": 1606915758400, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2719/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2719/-/Official_Review"}}}, {"id": "qZb6h-deJy", "original": null, "number": 3, "cdate": 1603841859977, "ddate": null, "tcdate": 1603841859977, "tmdate": 1605024147272, "tddate": null, "forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "invitation": "ICLR.cc/2021/Conference/Paper2719/-/Official_Review", "content": {"title": "This paper proposes an interesting framework for training interpretable CNNs, similar to distillation methods. While I found the idea and the framework very compelling, the structure of the paper makes it challenging to evaluate the details of the procedure. Please see my concerns below.", "review": "This paper proposes an interesting framework for training interpretable CNNs, similar to distillation methods. The authors propose a probabilistic model to approximate CNN predictions (specifically the discriminatory part i.e. fully connected network, and a procedure for training CNN+ DCLM as a game. Results show interesting performance over benchmark datasets in comparison to existing distillation baselines. \n\nMajor concerns:\n1. While the motivation is easy to understand, notation is continuously introduced in the paper without clear description and definitions. This makes it very challenging to parse the technical correctness of the paper. \n\n2. What is the iterative optimization procedure described in Page 5? Highly unclear how the CNN model is actually updated in Algorithm 1 to do the minimization of Eq (14)\n\n3. Can the authors elaborate why the accuracy gap for more challenging datasets like FashionMNIST is much more than simpler datasets for the DCLM model? I also suggest authors to include other datasets instead of focusing on MNIST only.\n\n4. The entropy analysis is interesting but I would've liked to see more insight into why entropy behavior across different datasets is so variable.\n\n5. Way too many typos in the paper. Please proof-read and correct, I only highlighted a few:\n 1. Update_CNN_Gradient in Algorithm 1\n 2. Information Entropy in Figure 5 caption\n 3. Clarify what \"discrimination part\" of CNN means, very early on in the introduction and abstract.\n\n6. Did the authors do a qualitative analysis of where exactly the approximate DCLM has lower accuracy compared to the CNN model and why?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2719/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2719/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A self-explanatory method for the black box problem on discrimination part of CNN", "authorids": ["~Jinwei_Zhao1", "852333436@qq.com", "axmaiqiu@foxmail.com", "guoxie@xaut.edu.cn", "wwang@xaut.edu.cn", "heixinhong@xaut.edu.cn", "~Deyu_Meng1"], "authors": ["Jinwei Zhao", "Qizhou Wang", "Wanli Qiu", "Guo Xie", "Wei Wang", "Xinhong Hei", "Deyu Meng"], "keywords": ["Convolution neural network", "Interpretability performance", "Markov random field"], "abstract": "Recently, for finding inherent causality implied in CNN, the black box problem of its discrimination part, which is composed of all fully connected layers of the CNN, has been studied by different scientific communities. Many methods were proposed, which can extract various interpretable models from the optimal discrimination part based on inputs and outputs of the part for finding the inherent causality implied in the part. However, the inherent causality cannot readily be found. We think that the problem could be solved by shrinking an interpretable distance which can evaluate the degree for the discrimination part to be easily explained by an interpretable model. This paper proposes a lightweight interpretable model, Deep Cognitive Learning Model(DCLM). And then, a game method between the DCLM and the discrimination part is implemented for shrinking the interpretation distance. Finally, the proposed self-explanatory method was evaluated by some contrastive experiments with certain baseline methods on some standard image processing benchmarks. These experiments indicate that the proposed method can effectively find the inherent causality implied in the discrimination part of the CNN without largely reducing its generalization performance. Moreover, the generalization performance of the DCLM also can be improved.", "one-sentence_summary": "For finding the inherent causality implied in the discrimination part of CNN without largely reducing its generalization performance, a self-explanatory method is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|a_selfexplanatory_method_for_the_black_box_problem_on_discrimination_part_of_cnn", "pdf": "/pdf/a0d2917e45eb09012a908b5fb840b5a23ca5c221.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zK-q9So0vd", "_bibtex": "@misc{\nzhao2021a,\ntitle={A self-explanatory method for the black box problem on discrimination part of {\\{}CNN{\\}}},\nauthor={Jinwei Zhao and Qizhou Wang and Wanli Qiu and Guo Xie and Wei Wang and Xinhong Hei and Deyu Meng},\nyear={2021},\nurl={https://openreview.net/forum?id=oweBPxtma_i}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oweBPxtma_i", "replyto": "oweBPxtma_i", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2719/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090084, "tmdate": 1606915758400, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2719/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2719/-/Official_Review"}}}], "count": 8}