{"notes": [{"id": "SkVhlh09tX", "original": "BkxV9H65tm", "number": 1115, "cdate": 1538087924174, "ddate": null, "tcdate": 1538087924174, "tmdate": 1550714655411, "tddate": null, "forum": "SkVhlh09tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 23, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bkxa89b2G4", "original": null, "number": 12, "cdate": 1547602516905, "ddate": null, "tcdate": 1547602516905, "tmdate": 1547602516905, "tddate": null, "forum": "SkVhlh09tX", "replyto": "H1gWfxVozE", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Other components besides the convolutions and self-attentions are also important", "comment": "One of our contributions is to better understand the importance of self-attention which is often perceived as the most important design choice in the architecture of Vaswani et al. \nTable 3 of our paper shows that self-attention alone only accounts for a small portion of the improvement of Vaswani et al. over previous work, e.g., row 6 in Table 3 \"CNN Depthwise + Increasing kernel\" uses the depthwise convolution of Kaiser et al. (2017) and it is only 0.5 BLEU behind our reimplementation of Vaswani et al.\nTherefore, modeling choices other than self-attention contribute a very large fraction of the improvement of Vaswani et al. over other work. In early experiments, we found that FFN blocks between the self-attention module in the Transformer are very important. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "H1gWfxVozE", "original": null, "number": 10, "cdate": 1547546632659, "ddate": null, "tcdate": 1547546632659, "tmdate": 1547546632659, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "I'm not sure if I have fully-understand your great work.\nIn my opinion, the difference between you and (Kaiser et al., 2017) is the softmax-normalized and share weights over the channel dimension. And you use these two mechanism not only reduce the number of parameter, but increase the result greatly(from 26.1 to 28.9). \nSo can you explain why these two mechanism is so useful?\nThanks", "title": "Problem about lightconv"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "SJg8yJrbG4", "original": null, "number": 11, "cdate": 1546895069752, "ddate": null, "tcdate": 1546895069752, "tmdate": 1546993110380, "tddate": null, "forum": "SkVhlh09tX", "replyto": "H1xwAVoPb4", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Re: Problems on the implementation of LightConv", "comment": "1. Thank you for pointing out this typo.  It should reduce the number of parameters by a factor of \u201cd/H\u201d rather than \u201cH\u201d, so 112 is still the correct number.\n\n2. The matrix would be a band matrix, i.e. entries outside of the kernel are zeros. As you noticed, this is similar to the matrix multiplication in the self-attention, which has O(n^2) time complexity. However, we observe that when the sequence is short (< 1000), this implementation is practically faster.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "H1xwAVoPb4", "original": null, "number": 8, "cdate": 1546265806752, "ddate": null, "tcdate": 1546265806752, "tmdate": 1546265893809, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "It's a good paper and easy to catch, and I got 2 problems confused me when reading it. Forgive me if I misunderstand the paper.\n\n1. In the weight sharing of LightConv in Section 3, as far as I understand, the description \"We tie the parameters of every subsequent number of d/H channels, which reduces the number of parameters by a factor of H.\" should led to 448 (d/H x  k) weights, instead of 112 stated, which I think is a typo.\n\n2. As for the implementation in the same section, the operation \"batch matrix multiplication\" confused me a lot. As LightConv is a conv operator, we can implement it by (1) applying image_to_coloum to input, (2) copying the kernels and (3) taking matrix multiplication. These procedures are taken by many DL platforms, like Caffe, to implement the conv operator. But the paper states that only reshape and transpose are applied to input before \"batch matrix multiplication\", which seems an aggregation over all position (same sprit as self-attention) when taking batch matrix multiplication, conflicting to the paper's claim.\n\n\nLooking forward to your code!", "title": "Problems on the implementation of LightConv"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "ByeyU5YM-E", "original": null, "number": 10, "cdate": 1545931335015, "ddate": null, "tcdate": 1545931335015, "tmdate": 1545931335015, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SyexV29PJN", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Thank you for pointing out these interesting CNN papers!", "comment": "There are indeed some similarities to their work, but there are also significant differences, including:\n1) Their methods focus on using the information from one sequence to generate the convolution filter that operates on the other sequence, while we focus on using one sequence as both the source and the target like self-attentions. Admittedly, Gong et al. use intra-sentence convolutional interactions; however, their ablation study is limited to removing them instead of replacing them with self-attentions.\n2) They use a filter generator network to predict the kernel, while DynamicConv requires only a simple linear projection.\n3) Their models use the same convolution filter at each time step, while our DynamicConv uses different filters at each time step. This is possible due to LightConv (depthwise + weight sharing) which significantly reduces the number of parameters.\n4) Their filter generator network uses the information from the whole sequence to generate a convolutional filter, while we only use the information at the current time step.\n5) Our filters are softmax-normalized, while theirs are not.\n\nWe consider their work as orthogonal to our methods. Future work may try to apply LightConv and DynamicConv to their models in order to achieve even better performance!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "S1l8GOC-k4", "original": null, "number": 1, "cdate": 1543788557871, "ddate": null, "tcdate": 1543788557871, "tmdate": 1545354516276, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Meta_Review", "content": {"metareview": "Very solid work, recognized by all reviewers as worthy of acceptance. Additional readers also commented and there is interest in the open source implementation that the authors promise to provide.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Oral)", "title": "Accept"}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1115/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352960952, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352960952}}}, {"id": "SyexV29PJN", "original": null, "number": 7, "cdate": 1544166440381, "ddate": null, "tcdate": 1544166440381, "tmdate": 1544166440381, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "I found your work very interesting, but there are some recent works that are closely related to your work, which take a sentence as input and generate convolutional kernels that are further applied on the sentence, but with a different granularity. I think those works are definitely worth comparing to.\n\nmissing references:\nLearning Context-Sensitive Convolutional Filters for Text Processing (Shen et al.)\nConvolutional Interaction Network for Natural Language Inference (Gong et al.)", "title": "Can you explain the difference between your work with more closely related works such as Convolutional Net (Gong et al.) and Context-Sensitive Convolution (Shen et al.)"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "H1glXkLRhm", "original": null, "number": 2, "cdate": 1541459735530, "ddate": null, "tcdate": 1541459735530, "tmdate": 1543877868503, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Review", "content": {"title": "Interesting work, strong results, good paper", "review": "Overall, this is a really good paper.\nThe authors propose an alternative to content based similarity for NL applications as compared to self-attention models by proposing the parameter and sequence length efficient Lightweight and Dynamic Convolutions.\nThe authors show, over various NL tasks like Translation, LM and Abstractive summarisation, the comparison of self attention models with Lightweight and Dynamic convolution layer.\nThe weight sharing was particularly interesting and can be seen as applying different heads for the same kernel. \n\nThe experimental results give strong evidence for these alternatives proposed by the authors.\nThe lightweight and dynamic convolution layers, both perform similar or better than the self-attention layer in all the tasks.\nThe WMT EnFr result is much better than all the other models, establishing a new state of the art.\n\nQuestion for the authors:\n1. Is the weight sharing within the kernel mostly for reducing computation?\nIf so, did you trying varying H size and measure how much that affects performance? What is surprising is that, in the ablation table the weight sharing increases the BLEU score by 0.1. \n2. Did you run any experiments where the kernel size covers the whole sentence?\n3. Since the number of parameters only change linearly wrt sequence length, did you try running this on datasets that have really long sequences to show the effectiveness of this approach further?\n4. How important was softmax normalization for training?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Review", "cdate": 1542234302706, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875920, "tmdate": 1552335875920, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgRkkYAAQ", "original": null, "number": 6, "cdate": 1543569126036, "ddate": null, "tcdate": 1543569126036, "tmdate": 1543569126036, "tddate": null, "forum": "SkVhlh09tX", "replyto": "Bkx6Dmk6Tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "please note bytenet can also be used for language model, i.e., using  only decoder. So it is very important to compare with it, which is also one type of lightweight cnn", "title": "hi"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "BkxEwCe6pQ", "original": null, "number": 8, "cdate": 1542422108097, "ddate": null, "tcdate": 1542422108097, "tmdate": 1542422108097, "tddate": null, "forum": "SkVhlh09tX", "replyto": "Byg-bBXZaX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "code", "comment": "We are planning to share the code later."}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "BJxVasJp6m", "original": null, "number": 7, "cdate": 1542417339939, "ddate": null, "tcdate": 1542417339939, "tmdate": 1542417339939, "tddate": null, "forum": "SkVhlh09tX", "replyto": "Byx0nlaKnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for your comments. We improved the description of the adaptive softmax hyperparameters ('band' terminology) in the updated version of the paper. We hope this is clearer now. \n\nWe refer to different subsets of the vocabulary as 'bands'. The most frequent words are denoted as \"head band\", and so on."}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "B1lA_ikT6m", "original": null, "number": 6, "cdate": 1542417269568, "ddate": null, "tcdate": 1542417269568, "tmdate": 1542417269568, "tddate": null, "forum": "SkVhlh09tX", "replyto": "H1glXkLRhm", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for your fruitful comments.\nQ1: For DynamicConv, weight sharing reduces both computation and memory footprint, while for LightConv, it only reduces memory footprint.  Yes, we did try using large H sizes; however, the performance degrades and the memory footprint increases dramatically which prohibits us from using a large batch size. As a consequence, training becomes much slower.  For your information, DynamicConv with H=64 gets BLEU score 26.8 \u00b1 0.1 on newstest2013 compared to 26.9 \u00b1 0.2 with H=16 in Table 3. \n\nQ2: We conducted an additional experiment based on your suggestion. We set the encoder kernel size to 237 and the decoder kernel size to 267 at each layer to cover the whole sequence. The BLEU score drops slightly to 26.7 \u00b1 0.1. This is a small difference and we expect that slightly tuned hyperparameters would close the gap.\n\nQ3: In section 6.4, we show experiments for document summarization (CNN/DailyMail) where the input sequence is capped at 400 words and the output sequence is 57 words on average with some examples having summaries of up to 478 words. Our results show that the model performs very well in this setting.\n\nQ4: We found it very important as training diverged without softmax-normalization (see Note in Table 3) for DynamicConv. We added a comparison of softmax-normalization to various alternatives to Appendix A of the updated paper.\nFurthermore, we are able to train the model without softmax-normalization with more aggressive gradient clipping, a lower learning rate (reducing it by a factor of 5) and more updates (increasing it by 5 times), but this slowed down training dramatically. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "r1gUWo16Tm", "original": null, "number": 5, "cdate": 1542417150229, "ddate": null, "tcdate": 1542417150229, "tmdate": 1542417150229, "tddate": null, "forum": "SkVhlh09tX", "replyto": "H1gJoj8C3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for your fruitful comments.\nQ: Comparison to \"Depthwise Separable Convolutions for Neural Machine Translation\" from Kaiser et al. (ICLR 2018).\nSuper-separable convolutions modify the pointwise operation (introducing groups) that follows depthwise convolutions, while we modify the latter (which focuses on aggregating the temporal information). We did try to introduce groups to the linear layers of the feed-forward block (that follows light/dynamic convolutions) but to reach the same accuracy, we had to increase the number of parameters of the model to a similar level as with the dense linear, at which point the network became slower.\n\nQ: \u201cI'd also like to see train time numbers\u201d\nHere are training times:\nSelf-attention, 17.5h (with or without limited window)\nDynamicConv, 16.9h\nLightConv, 16.5h\n\nOur current implementation of dynamic convolutions is actually quite inefficient. We put the various convolution kernels in a sparse tensor that has only non-zero entries for the diagonal entry, thus using a lot of space. We expect a dedicated CUDA kernel to be more efficient. We are investigating such a kernel.\n\nNote that batching is much more efficient during training which smooths out some of the speed advantages we see at test time. During inference batching is by far not as efficient due to repeated invocation of the decoder at every time step.\n\nQ: \u201cOther papers (particularly a series of three papers from Tao Shen at University of Technology Sydney and Tianyi Zhou at UW) have proposed architectures that are similarly intermediate between self-attention and (in their case 1x1) convolution\u201d\nWe will discuss our work in the light of Shen & Zhou (2017, 2018) and also reference Ott et al. (2018) wrt fairseq speed.\n\nQ: \u201cYou should have tried a language modeling dataset with longer-term dependencies\u201d\nIn section 6.4, we show experiments for CNN/DailyMail document summarization which entails long input and output sequences. The input sequence is capped at 400 words and the output sequence is 57 words on average with some examples having summaries of up to 478 words. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "Bkx6Dmk6Tm", "original": null, "number": 4, "cdate": 1542415205126, "ddate": null, "tcdate": 1542415205126, "tmdate": 1542415227804, "tddate": null, "forum": "SkVhlh09tX", "replyto": "rygh1-1upQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Re: explain the advantages over bytenet ", "comment": "We do compare to a CNN baseline (non-separable convolutions), see Table 3 \"CNN (k=3)\". However, our model does use source-target attention which is not the case for ByteNet. Finally, our model performs better on newstest2014 of WMT English-German translation at 29.7 BLEU vs. 23.75 BLEU for ByteNet.\n\nAnd yes, we will release the code."}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "SygyHzyaTX", "original": null, "number": 3, "cdate": 1542414903484, "ddate": null, "tcdate": 1542414903484, "tmdate": 1542414910227, "tddate": null, "forum": "SkVhlh09tX", "replyto": "S1xedHZ_pX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "CUDA kernel & code", "comment": "We are currently investigating a dedicated CUDA kernel and we will make the code available."}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "BJlW-fk667", "original": null, "number": 2, "cdate": 1542414840807, "ddate": null, "tcdate": 1542414840807, "tmdate": 1542414840807, "tddate": null, "forum": "SkVhlh09tX", "replyto": "BklnHDDcT7", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Open sourcing the code", "comment": "Yes, we will share the code at a later stage!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}, {"id": "BklnHDDcT7", "original": null, "number": 5, "cdate": 1542252356203, "ddate": null, "tcdate": 1542252356203, "tmdate": 1542252356203, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "I found this paper is very interesting, would you like to share the source code, which is very helpful for fully understanding it", "title": "Dear authors,"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "S1xedHZ_pX", "original": null, "number": 4, "cdate": 1542096231811, "ddate": null, "tcdate": 1542096231811, "tmdate": 1542096256757, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "1 what do you mean by saying \u201cWe expect a dedicated CUDA kernel to be much more efficient.\u201d\n\nYou mean the efficiency. advantage in current CUDA is not obvious??\n\nis it possible to expect a new CUDA kernel specifically designed for your model\n\n2 code is not available\n\nCode and pre-trained models available at http://anonymized", "title": "can you explain this"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "rygh1-1upQ", "original": null, "number": 3, "cdate": 1542086884155, "ddate": null, "tcdate": 1542086884155, "tmdate": 1542094651320, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "Hi,\n\nI have a question. You claim that your lightweight cnn can has fewer parameters and linear time. I think it is very necessary to compare with a well-know CNN sequence baseline, i.e. bytenet. it is also a pure con sequence model and shows very good performance in language modeling and translation. Have you compare with it?? Better accuracy or higher efficiency??\n\nDo you plan to you share your code? I am quite interested.", "title": "Hi can you explain the advantages over bytenet"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "Byg-bBXZaX", "original": null, "number": 2, "cdate": 1541645560534, "ddate": null, "tcdate": 1541645560534, "tmdate": 1541645560534, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "content": {"comment": "Hi, the Code link is not available!", "title": "Hi, the Code link is not available!"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311675094, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkVhlh09tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311675094}}}, {"id": "H1gJoj8C3X", "original": null, "number": 3, "cdate": 1541462934632, "ddate": null, "tcdate": 1541462934632, "tmdate": 1541533409417, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Review", "content": {"title": "Major advance in sequence-to-sequence architectures", "review": "The authors present lightweight convolutions and dynamic convolutions, two significant advances over existing depthwise convolution sequence models, and demonstrate very strong results on machine translation, language modeling, and summarization. Their results go even further than those of the Transformer paper in countering the conventional wisdom that recurrence (or another way of directly modeling long-distance dependencies) is crucial for sequence-to-sequence tasks. Some things that I noticed:\n\n- While you do cite \"Depthwise Separable Convolutions for Neural Machine Translation\" from Kaiser et al. (ICLR 2018), there are some missed opportunities to compare more directly to that paper (e.g., by comparing to their super-separable convolutions). Kaiser et al. somewhat slipped under the community's radar after the same group released the Transformer on arXiv a week later, but it is in some ways a more direct inspiration for your work than the Transformer paper itself.\n\n- I'd like to see more analysis of the local self-attention ablation. It's fantastic to see such a well-executed ablation study, especially one that includes this important comparison, but I'd like to understand more about the advantages and drawbacks of local self-attention compared to dynamic convolutions. (For instance, dynamic convolutions are somewhat faster at inference time in your results, but I'm unsure if this is contingent on implementation choices or if it's inherent to the architecture.)\n\n- From a systems and implementation perspective, it would be great to see some algorithm-level comparisons of parallelism and critical path length between dynamic convolutions and self-attention. My gut feeling is that dynamic convolutions significantly more amenable to parallelization on certain kinds of hardware, especially at train time, but that the caching that's possible in self-attention inference might make the approaches more comparable in terms of critical path latency at inference time; this doesn't necessarily line up with your results so far though.\n\n- You mostly focus on inference time, but you're not always as clear about that as you could be; I'd also like to see train time numbers. Fairseq is incredibly fast on both sides (perhaps instead of just saying \"highly optimized\" you can point to a paper or blog post?)\n\n- The nomenclature in this space makes me sad (not your fault). Other papers (particularly a series of three papers from Tao Shen at University of Technology Sydney and Tianyi Zhou at UW) have proposed architectures that are similarly intermediate between self-attention and (in their case 1x1) convolution, but have decided to call them variants of self-attention. I could easily imagine a world where one of these groups proposed exactly your approach but called it \"Dynamic Local Self-Attention,\" or even a world where they've already done so but we can't find it among the zillions of self-attention variants proposed in the past year. Not sure if there's anything anyone can do about that, but perhaps it would be helpful to briefly cite/compare to some of the Shen/Zhou work.\n\n- I think you should have tried a language modeling dataset with longer-term dependencies, like WikiText-103. Especially if the results were slightly weaker than Transformer, that would help place dynamic convolutions in the architecture trade-off space.\n\nThat last one is probably my most significant concern, and one that should be fairly easy to address. But it's already a great paper.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Review", "cdate": 1542234302706, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875920, "tmdate": 1552335875920, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Byx0nlaKnX", "original": null, "number": 1, "cdate": 1541161141698, "ddate": null, "tcdate": 1541161141698, "tmdate": 1541533408970, "tddate": null, "forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Review", "content": {"title": "well-written, surprising and promising results", "review": "The paper proposes a convolutional alternative to self-attention. To achieve this, the number of parameters of a typical convolution operation is first reduced by using a depth-wise approach (i.e. convolving only within each channel), and then further reduced by tying parameters across layers in a round-robin fashion. A softmax is applied to the filter weights, so that the operation computes weighted sums of its (local) input (LightConv).\n\nBecause the number of parameters is dramatically reduced now, they can be replaced by the output of an input-dependent linear layer (DynamicConv), which gives the resulting operation a \"local attention\" flavour. The weights depend only on the current position, as opposed to the attention weights in self-attention which depend on all positions. This implies that the operation is linear in the number of positions as opposed to quadratic, which is a significant advantage in terms of scaling and computation time.\n\nIn the paper, several NLP benchmarks (machine translation, language modeling) that were previously used to demonstrate the efficacy of self-attention models are tackled with models using LightConv and DynamicConv instead, and they are shown to be competitive across the board (with the number of model parameters kept approximately the same).\n\nThis paper is well-written and easy to follow. The proposed approach is explained and motivated well. The experiments are thorough and the results are convincing. I especially appreciated the ablation experiment for which results are shown in Table 3, which provides some useful insights beyond the main point of the paper. The fact that a linear time approach can match the performance of self-attention based models is a very promising and somewhat surprising result.\n\nIn section 5.3, I did not understand what \"head band, next band, last band\" refers to. I assume this is described in the anonymous paper that is cited, so I suppose this is an artifact of blind review. Still, even with the reference unmasked it might be useful to add some context here.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Review", "cdate": 1542234302706, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkVhlh09tX", "replyto": "SkVhlh09tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875920, "tmdate": 1552335875920, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkgbd38ChQ", "original": null, "number": 1, "cdate": 1541463145124, "ddate": null, "tcdate": 1541463145124, "tmdate": 1541463145124, "tddate": null, "forum": "SkVhlh09tX", "replyto": "Byx0nlaKnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "content": {"title": "Where \"head band\" comes from", "comment": "The \"head band, next band, last band\" terminology is from https://openreview.net/forum?id=ByxZX20qFQ, which is presumably the cited anonymous paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper1115/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1115/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pay Less Attention with Lightweight and Dynamic Convolutions", "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "keywords": ["Deep learning", "sequence to sequence learning", "convolutional neural networks", "generative models"], "authorids": ["fw245@cornell.edu", "angelfan@fb.com", "alexei.b@gmail.com", "yann@dauphin.io", "michael.auli@gmail.com"], "authors": ["Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli"], "TL;DR": "Dynamic lightweight convolutions are competitive to self-attention on language tasks.", "pdf": "/pdf/bc01ded1a059dfb3a08003d867a2147f5f3dbceb.pdf", "paperhash": "wu|pay_less_attention_with_lightweight_and_dynamic_convolutions", "_bibtex": "@inproceedings{\nwu2018pay,\ntitle={Pay Less Attention with Lightweight and Dynamic Convolutions},\nauthor={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkVhlh09tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1115/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618678, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkVhlh09tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1115/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1115/Authors|ICLR.cc/2019/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1115/Reviewers", "ICLR.cc/2019/Conference/Paper1115/Authors", "ICLR.cc/2019/Conference/Paper1115/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618678}}}], "count": 24}