{"notes": [{"id": "Syl1pGI9wN", "original": "S1xF0w5wwE", "number": 2, "cdate": 1552732854588, "ddate": null, "tcdate": 1552732854588, "tmdate": 1562082914308, "tddate": null, "forum": "Syl1pGI9wN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "content": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "authorids": ["bwkevintan@gmail.com", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "eric.xing@petuum.com"], "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "pdf": "/pdf/81d5d9ba2a559a383d5c1752e96c74ea9c43ba6f.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "cdate": 1552732853551, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*", "values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1552732853551, "tmdate": 1554911328507, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}, "tauthor": "OpenReview.net"}, {"id": "SyxP_aLdFE", "original": null, "number": 1, "cdate": 1554701678969, "ddate": null, "tcdate": 1554701678969, "tmdate": 1554910464850, "tddate": null, "forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept", "comment": "This paper builds interesting connections of multiple methods (MLE, RL, RAML, etc) used to train model for sequence generation. All reviewers recommend acceptance. "}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "authorids": ["bwkevintan@gmail.com", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "eric.xing@petuum.com"], "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "pdf": "/pdf/81d5d9ba2a559a383d5c1752e96c74ea9c43ba6f.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Decision", "cdate": 1554496489022, "reply": {"forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554496489022, "tmdate": 1554910460935, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "HkxkY2T8KE", "original": null, "number": 5, "cdate": 1554599031126, "ddate": null, "tcdate": 1554599031126, "tmdate": 1554910458926, "tddate": null, "forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "content": {"title": "Good work, but not that natrual", "review": "The authors try to unify different algorithms for sequence generation and present a generalized entropy regularized policy optimization formulation. They show that divergent algorithms can be reformulated as special instances of the framework,\nwith the only difference being the configurations of reward function and a couple of hyperparameters.\n\nThe analysis and proposed framework are interesting. \n\nI have several technical questions. \n\n1. One cercen about the work is that the formulation in Eq. (1) is not natrual and it is not reasonable intuitively. Which one is the final model, q or \\theta?  According to previous description, \\theta is the model (or model parameters). However, for MLE, when \\alpha->0, \\theta will not make any impact to the reward defiend in Eq. (1), and only q determines the reward. That is, optimizing L(q,\\theta) will only update q but not \\theta. \n\n2. \"Generally, a larger exploration space would lead to a harder training problem.\" I don't get this point.  \"common rewards (e.g., BLEU) used in policy optimization are more smooth than the \u000e-reward, and permit exploration in a broader space.\"  If BLEU is more smooth, why leads to a harder training problem? ", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "authorids": ["bwkevintan@gmail.com", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "eric.xing@petuum.com"], "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "pdf": "/pdf/81d5d9ba2a559a383d5c1752e96c74ea9c43ba6f.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "cdate": 1553778229244, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229244, "tmdate": 1554911862038, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "r1gDTrGIKE", "original": null, "number": 4, "cdate": 1554552255354, "ddate": null, "tcdate": 1554552255354, "tmdate": 1554910457808, "tddate": null, "forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "content": {"title": "Nice contribution of unifying multiple common objectives in a single framework leading to algorithmic insight", "review": "This paper takes a number of widely-used algorithms for sequence generation, including maximum-likelihood, RAML, SPG and data noising and shows that they can all be viewed as optimizing a member of a family of objective functions, which is defined through three hyper-parameters that control parts of the objective - the reward, a maximum entropy term, and a KL term between a variational distribution and the model. \n\nThe authors show the exact values of the hyper-parameters that lead to these various objectives and also shed light onto how these hyper-parameters correspond to a trade-off between the exploration and difficulty of learning, where more exploration results in a more difficult learning problem. \n\nBecause now we have a family of objectives, the authors now naturally propose an algorithm that anneals the values of the hyper-parameters using a curriculum where simple learning without exploration happens at the beginning and more exploration is added later on to avoid local minima.\n\nI found the analysis clear and interesting, the insights on the relation between the algorithms to be informative and the final simple algorithmic contribution to be natural and worthwhile. Overall, a nice paper that I think definitely fits well in this workshop.\n\nIt is worth noting that a similar but different attempt has been made in the context of sequence generation when there is no gold sequence given at training time: See Misra et al. 2018\nhttp://www.cs.cornell.edu/~dkm/papers/mchy-emnlp.2018.pdf", "rating": "5: Top 15% of accepted papers, strong accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "authorids": ["bwkevintan@gmail.com", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "eric.xing@petuum.com"], "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "pdf": "/pdf/81d5d9ba2a559a383d5c1752e96c74ea9c43ba6f.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "cdate": 1553778229244, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229244, "tmdate": 1554911862038, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "HkxAInwBFV", "original": null, "number": 3, "cdate": 1554508885863, "ddate": null, "tcdate": 1554508885863, "tmdate": 1554910456692, "tddate": null, "forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "content": {"title": "A nice review of recent approaches (RAML, SPG, data noising)", "review": "The authors unify several recent frameworks for training sequence generation models and claim that their general framework is principled and provides novel interpretations of previous algorithms. They propose an interpolated model which they evaluate on several sequence generation task and show improved performance.\n\nIt was unclear why the unified model was principled (this was not addressed in the paper), and I did not find that it provided novel interpretations beyond the existing literature. The unified objective amounts to adding previously used terms together with weights. The analysis of the objective is mostly qualitative and descriptive, rather than analytical. The paper could benefit from clearly defining terms like \"regular shaped rewards\", \"smoothness\" for discrete distributions, and \"exploration area\", etc.\n\nThe authors briefly mention learning to search approaches, but given their recent strong performance (e.g., Sabour et al. ICLR 2019), this is an important comparison that is missing.\n\nFor a workshop submission, the review of previous methods and empirical results are sufficiently interesting for acceptance.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "authorids": ["bwkevintan@gmail.com", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "eric.xing@petuum.com"], "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "pdf": "/pdf/81d5d9ba2a559a383d5c1752e96c74ea9c43ba6f.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "cdate": 1553778229244, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229244, "tmdate": 1554911862038, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "B1gGnx_NK4", "original": null, "number": 2, "cdate": 1554444458289, "ddate": null, "tcdate": 1554444458289, "tmdate": 1554910455535, "tddate": null, "forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "content": {"title": "A useful unified formulation for several standard learning algorithms", "review": "The paper presents a unified objective function for optimizing sequence generation models. The unified formulation includes several standard learning algorithms as special cases. These standard algorithms include training by MLE, RAML, SPG, data noising, and sequential sampling. \n\nThe framework is based on policy optimization combined with entropy based regularization. The ground truth sequence y* is perturbed according to a probability distribution q(.|x) resulting in a perturbed target sequence y (i.e. y~q(.|x)). The goal is to maximize the expected reward under the new perturbed distribution of labels q. A KL-divergence penalty is also included to prevent deviation from the model distribution parameterized by theta. The objective function is regularized by imposing a maximum entropy assumption on q. The optimization objective is solved in an expectation maximization fashion. Different choices for the reward function and penalties for the KL-divergence and entropy terms yield standard learning algorithms ranging from MLE to SPG. \n\nFinally, a new algorithm is proposed that dynamically interpolates between the different learning algorithms. The algorithm is evaluated in a very limited experimental setting for machine translation and text summarization.\n\nStrength:\n========\n\nThe unified formal connection is useful in understanding the difference between the learning algorithms in terms of how different target sequences are rewarded, which sequences are explored, and how the policy is regularized. \n\nWeakness: \n=========\n\nThe experimental setting for evaluating the new proposed algorithm is very limited. The dataset used for the machine translation experiment is relatively small. This is a major concern as the exposure bias problem can be mitigated by using larger datasets. \n\n\nClarity:\n======\n\nThe paper is mostly clear. However, in section 3.1, it wasn\u2019t clear at the beginning what the distribution q represents. The future reference in the \u201cmore details below\u201d was not helpful. In general proving more intuition about the objection function in equation (1) would be helpful and make the presentation more clear.\n\n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "authorids": ["bwkevintan@gmail.com", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "eric.xing@petuum.com"], "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "pdf": "/pdf/81d5d9ba2a559a383d5c1752e96c74ea9c43ba6f.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "cdate": 1553778229244, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229244, "tmdate": 1554911862038, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "SkgJtsr4YN", "original": null, "number": 1, "cdate": 1554434934804, "ddate": null, "tcdate": 1554434934804, "tmdate": 1554910455025, "tddate": null, "forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "content": {"title": "A generalized entropy regularized policy formulation encodes a variety of approaches to seq2seq learning, from maximum likelihood estimation to reinforcement learning implemented using the RAML, SPG, and data noising methods and leads to improved results on two tasks: machine translation and summarization.", "review": "This is a nice submission. The authors show how a generalized entropy regularized policy formulation encodes a variety of approaches to seq2seq learning, from maximum likelihood estimation to reinforcement learning implemented using the RAML, SPG, and data noising methods. Besides the theoretical insight, the authors show how by implementing an easy-to-hard paradigm that resembles curriculum learning leads to improvements on two tasks: machine translation and summarization.\n\nThe paper is likely to generate good discussions, especially with respect to other approaches to sequence learning that are not yet encompassed by the proposed framework.\n", "rating": "5: Top 15% of accepted papers, strong accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "authorids": ["bwkevintan@gmail.com", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "eric.xing@petuum.com"], "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "pdf": "/pdf/81d5d9ba2a559a383d5c1752e96c74ea9c43ba6f.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper2/Official_Review", "cdate": 1553778229244, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "Syl1pGI9wN", "replyto": "Syl1pGI9wN", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper2/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778229244, "tmdate": 1554911862038, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper2/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}], "count": 7}