{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028551760, "tcdate": 1490028551760, "number": 1, "id": "HJgfOKpjl", "invitation": "ICLR.cc/2017/workshop/-/paper8/acceptance", "forum": "S1AtgaPug", "replyto": "S1AtgaPug", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028552311, "id": "ICLR.cc/2017/workshop/-/paper8/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1AtgaPug", "replyto": "S1AtgaPug", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028552311}}}, {"tddate": null, "tmdate": 1489698766503, "tcdate": 1489698766503, "number": 1, "id": "B18AkFOix", "invitation": "ICLR.cc/2017/workshop/-/paper8/official/comment", "forum": "S1AtgaPug", "replyto": "BkPoOGvog", "signatures": ["ICLR.cc/2017/workshop/paper8/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper8/AnonReviewer2"], "content": {"title": "Updated score", "comment": "The updated paper fixes some of the concerns raised and I have updated my score.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486504070371, "tcdate": 1486504070371, "id": "ICLR.cc/2017/workshop/-/paper8/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "S1AtgaPug", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper8/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper8/reviewers", "ICLR.cc/2017/workshop/paper8/areachairs"], "cdate": 1486504070371}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489698607593, "tcdate": 1489356848536, "number": 2, "id": "HkuEuBQig", "invitation": "ICLR.cc/2017/workshop/-/paper8/official/review", "forum": "S1AtgaPug", "replyto": "S1AtgaPug", "signatures": ["ICLR.cc/2017/workshop/paper8/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper8/AnonReviewer2"], "content": {"title": "A useful idea to emphasize, but not a significant contribution", "rating": "6: Marginally above acceptance threshold", "review": "Paper summary -\nSeveral weight initialization schemes for deep neural networks aim to maintain\nthe variance of each layer's output to be one, in order to prevent an\nexponential blow-up or decay with increasing depth.  This paper describes a way\nto adjust the scale of the weight initialization to take into account the fact\nthat dropout contributes some extra variance. The proposed solution is to\nmultiply the variance of the initialization distribution by the keep-probability\np. The paper also proposes that Batch Normalization parameters should be\nre-estimated with dropout turned off, as opposed to using the moving average\nones inferred during training (where dropout is turned on).\n\nPros:\n- The paper shows some improvements over a strong DenseNet baseline using the proposed\n  adjustment.\n- The paper highlights a simple adjustment that becomes important when using\n  dropout in all (or a large number of) layers of a very deep network.\n\nCons:\n- The paper is poorly written. In particular, Section 2.1 uses notation without\n  introducing it before. This makes it hard to follow the arguments being made\nin this paper. Appendix A.1, which has the main derivations, is not referenced\nanywhere in the text of the paper. Also there are two Appendix As.\n\n- The idea seems very incremental. It is a little surprising if this is not\n  already being used to initialize weight scales. It might be that because\ndropout is typically only used for the last layer in most applications, this\nadjustment has not been emphasized before. However, it is easy to see that this will\nbecome more important when dropout is used throughout a very deep network.\n\n- Regarding re-estimating the Batch norm parameters, the original paper from\n  Ioffe and Szegedy does recommend re-estimating the parameters.  Turning\ndropout off for this is the natural thing to do since the idea is to prepare the\nnetwork to be run at test time.  So, it is not immediately clear what additional\ninsight is being presented in this paper.\n\n- Several details are missing. For example, the other initializations being\n  compared to (Xavier, He) should be clearly described. It is also not mentioned\nin which layers is dropout being done in the models used for the experiments.\n\n- \"To meet these different goals, we can initialize our weights by adding these\n  variances.\" - It is not clear how the goals are met by adding the variances.\nPlease explain.\n\nOverall, while the paper emphasizes an important adjustment which should be kept in\nmind when using dropout across the depth of a deep network, it is a somewhat\nobvious thing to do and not significantly interesting as a work of research.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489356849275, "id": "ICLR.cc/2017/workshop/-/paper8/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper8/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper8/AnonReviewer1", "ICLR.cc/2017/workshop/paper8/AnonReviewer2"], "reply": {"forum": "S1AtgaPug", "replyto": "S1AtgaPug", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper8/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489356849275}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489606916207, "tcdate": 1489606815169, "number": 3, "id": "BkPoOGvog", "invitation": "ICLR.cc/2017/workshop/-/paper8/public/comment", "forum": "S1AtgaPug", "replyto": "S1AtgaPug", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Paper Update", "comment": "We have updated the paper to define a few symbols in the main text, thanks to the reviewers' comments. I am sorry for the oversight and hope that the paper is reconsidered in view of this change.\n\nThis paper is a compressed conference submission to ICLR, which under a heavier review process received 7,5,6 initial reviews, and the area chair described it as a \"borderline paper\" for conference. For this workshop track, our reviewers have starkly lower estimates, and we hope that the fixes to previously confusing notation and clarifications make this paper acceptable in their eyes."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486504070369, "tcdate": 1486504070369, "id": "ICLR.cc/2017/workshop/-/paper8/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper8/reviewers"], "reply": {"forum": "S1AtgaPug", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486504070369}}}, {"tddate": null, "tmdate": 1489606713571, "tcdate": 1489606713571, "number": 2, "id": "SJGSdGPil", "invitation": "ICLR.cc/2017/workshop/-/paper8/public/comment", "forum": "S1AtgaPug", "replyto": "r1oaoKese", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Response", "comment": "Thank you for your analysis of our paper.\n\n> The proposed method is fairly incremental.\n\nLeCun initialization is where one multiplies the weights matrix by 1/sqrt(n) to stabilize variance.\nAs you know, He initialization noted that for ReLUs we should instead multiply by 1/sqrt(0.5*n).\nIn our view, He initialization was not fairly incremental, given its widespread use.\nHe et al. showed that a simple corrective scalar can mean the difference between convergence and divergence.\nWe generalize their idea to arbitrary nonlinearities like the ELU and show that dropout needs an adjustment too,\nall while preserving the simplicity of simple initializations.\nMoreover, we beat state of the art by a 1.25% accuracy difference with no architecture changes\nwith a simple and highly general technique.\n\n> Formatting\n\nI agree entirely. This was an oversight and a byproduct of compressing a conference submission into three pages.\nThe symbols which were previously undefined in this shortened version are now defined in the main text. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486504070369, "tcdate": 1486504070369, "id": "ICLR.cc/2017/workshop/-/paper8/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper8/reviewers"], "reply": {"forum": "S1AtgaPug", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486504070369}}}, {"tddate": null, "tmdate": 1489606640759, "tcdate": 1489606640759, "number": 1, "id": "SyYeOfvsx", "invitation": "ICLR.cc/2017/workshop/-/paper8/public/comment", "forum": "S1AtgaPug", "replyto": "HkuEuBQig", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Response", "comment": "Thank you for your analysis of our paper.\n\n> Section 2.1 uses notation without introducing it before.\nI agree entirely. This was an oversight and a byproduct of compressing a conference submission into three pages.\nThe symbols which were previously undefined in this shortened version are now defined in the main text. Thank you!\n\n> The idea seems very incremental\n\nLeCun initialization is where one multiplies the weights matrix by 1/sqrt(n) to stabilize variance.\nAs you know, He initialization noted that for ReLUs we should instead multiply by 1/sqrt(0.5*n).\nIn our view, He initialization was not fairly incremental, given its widespread use.\nHe et al. showed that a simple corrective scalar can mean the difference between convergence and divergence.\nWe generalize their idea to arbitrary nonlinearities like the ELU and show that dropout needs an adjustment too,\nall while preserving the simplicity of simple initializations.\nMoreover, we beat state of the art by a 1.25% accuracy difference with no architecture changes\nwith a simple and highly general technique.\n\n> It is a little surprising if this is not already being used to initialize weight scales\n\nWe have not found anyone proposing how to generally adjust for an arbitrary nonlinearity or\nrecommending that they adjust for dropout variance in weight initialization of batch normalization estimates.\nIn the conference review cycle, no one voiced that this was already used in practice, and the same idea is true for the following comment.\n\n> Ioffe and Szegedy does recommend re-estimating the parameters.\n\nForemost, we found that re-estimating the mean and variance was overall somewhat harmful in early experiments, so we only re-estimated the variance.\nSecondly, their recommendation does not specify to turn off dropout.\nThird, although batch normalization is prominent, subsequent re-estimation is not prominent because it is thought either pointless, cumbersome, harmful or all three.\nConsequently, Ioffe and Szegedy may recommend to re-estimate parameters, but few think to do so (e.g., DenseNet's creators).\nIf someone thinks to re-estimate parameters, they must to think to turn off dropout too, and to make re-estimation beneficial, they must also discover not to re-estimate the mean parameters. \n\n> It is also not mentioned in which layers is dropout being done in the models used for the experiments.\n\nWe note in the body of the main paper that this is a VGGNet and we also note that the interested reader can find the dropout rates,\nrandom hyperparameter search procedure, and more in the appendix.\n\n> It is not clear how the goals are met by adding the variances.\n\nGlorot et al. adjust for variances by averaging variances instead of adding variances, which is less mathematically natural to us. We found that adding worked better, and if we add the variances, Xavier initialization is a special case of our initialization if we used a Uniform distribution and used a ReLU nonlinearity while not using dropout. Similarly, the He initialization is a special case of our initialization if we use the ReLU and do not train with dropout and omit backpropagation's variance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486504070369, "tcdate": 1486504070369, "id": "ICLR.cc/2017/workshop/-/paper8/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper8/reviewers"], "reply": {"forum": "S1AtgaPug", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486504070369}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1489606079052, "tcdate": 1486504069624, "number": 8, "id": "S1AtgaPug", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1AtgaPug", "original": "r1BJLw9ex", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "content": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1485031066416, "tcdate": 1478288861357, "number": 383, "id": "r1BJLw9ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1BJLw9ex", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "content": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/aed672a70c2793d881f5a2d0b526f73b60103d40.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489177538775, "tcdate": 1489177538775, "number": 1, "id": "r1oaoKese", "invitation": "ICLR.cc/2017/workshop/-/paper8/official/review", "forum": "S1AtgaPug", "replyto": "S1AtgaPug", "signatures": ["ICLR.cc/2017/workshop/paper8/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper8/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The authors propose a correction for the statistics of BatchNormalization under dropout for the test phase. The proposed method is fairly incremental. While the results indicate some improvement, they do not seem too encouraging.\n\nAlthough this is a workshop paper, I do not think that this is an excuse for leaving out crucial details in the main text. The main text should be self-contained\u2013but the main method is impossible to understand without referencing to the appendix: it is presented as a formula containing various undefined quantities. \n\n\n ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjusting for Dropout Variance in Batch Normalization and Weight Initialization", "abstract": "We show how to adjust for the variance introduced by dropout with corrections to weight initialization and Batch Normalization, yielding higher accuracy. Though dropout can preserve the expected input to a neuron between train and test, the variance of the input differs. We thus propose a new weight initialization by correcting for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Since Batch Normalization trained with dropout estimates the variance of a layer's incoming distribution with some inputs dropped, the variance also differs between train and test. After training a network with Batch Normalization and dropout, we simply update Batch Normalization's variance moving averages with dropout off and obtain state of the art on CIFAR-10 and CIFAR-100 without data augmentation.", "pdf": "/pdf/4543cc44f901c0c390dc2e09ca8ffcc8b1f37a50.pdf", "TL;DR": "Batch Norm Incorrectly Estimates Variance When Dropout Is On", "paperhash": "hendrycks|adjusting_for_dropout_variance_in_batch_normalization_and_weight_initialization", "keywords": ["Deep learning"], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489356849275, "id": "ICLR.cc/2017/workshop/-/paper8/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper8/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper8/AnonReviewer1", "ICLR.cc/2017/workshop/paper8/AnonReviewer2"], "reply": {"forum": "S1AtgaPug", "replyto": "S1AtgaPug", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper8/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper8/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489356849275}}}], "count": 8}