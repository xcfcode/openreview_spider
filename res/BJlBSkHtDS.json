{"notes": [{"id": "BJlBSkHtDS", "original": "SyxF6ba_wr", "number": 1690, "cdate": 1569439549068, "ddate": null, "tcdate": 1569439549068, "tmdate": 1583912051723, "tddate": null, "forum": "BJlBSkHtDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "WDBg901Ci5", "original": null, "number": 1, "cdate": 1576798729944, "ddate": null, "tcdate": 1576798729944, "tmdate": 1576800906564, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposed a new learnable activation function called Pad\u00e9 Activation Unit (PAU) based on parameterization of rational function. All the reviewers agree that the method is soundly motivated, the empirical results are strong to suggest that this would be a good addition to the literature. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714598, "tmdate": 1576800264336, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Decision"}}}, {"id": "SkxMQumTKB", "original": null, "number": 3, "cdate": 1571792922386, "ddate": null, "tcdate": 1571792922386, "tmdate": 1574445989015, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The authors introduce an activation function based on learnable Pad\u00e9 approximations. The numerator and denominator of the learnable activation function are polynomials of m and n, respectively. The authors name them Pad\u00e9 activation units (PAUs). The authors also propose a randomized a version of these functions that add noise to the coefficients of the polynomials in order to regularize the network. The authors show, at best, marginal improvements over a variety of baselines including MNIST, fashion MNIST, CIFAR10, and Imagenet. The authors also show that pruning neurons with PAU units results in slightly better accuracy that pruning neurons with ReLU units.\n\nThe improvements over baselines shown were marginal and I do not think they warrant publication at this conference. The accuracy improvements were no more impressive than other learned activation functions which the authors perhaps did not see, such as SReLUs (Deep Learning with S-Shaped Rectified Linear Activation Units) and APLs (Learning Activation Functions to Improve Deep Neural Networks).\n\n** After author response **\nChanging from reject to weak accept\nThe authors have included new experiments that compare to a wider range of learned activation functions. While not ground breaking, it shows that it is competitive with state-of-the-art learned activation functions and could have something to offer.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576446235768, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Reviewers"], "noninvitees": [], "tcdate": 1570237733711, "tmdate": 1576446235787, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Review"}}}, {"id": "HJgsBTOLKr", "original": null, "number": 1, "cdate": 1571355971310, "ddate": null, "tcdate": 1571355971310, "tmdate": 1573764503173, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #2", "review": "This work proposes an activation function that contain parameters to be learned through training. The idea is to give the learning algorithm more \"freedom\" to choose a good activation function, and hopefully better performance can be achieved.\n\nThe paper is well written, and the experiment results look reasonable. However, there are several key issues.\n\n1) as the authors stated, a \"good\" activation function should maintain the universal approximation property of the neural network. This seems not discussed for the PADE activation function.  Does (1) satisfy the conditions (i)-(v) listed in table I? Is there a rigorous proof? Table I seems to claim that the PADE based neural network satisfies (i), but there is no formal proof.\n\n2)  In order to avoid poles, the activation function used in this work is (2). How well can (2) approximate (1)? What is the potential loss? Perhaps there should be more discussion on this - preferably some theoretical supports.\n\nOverall, the reviewer feels that this paper starts with an interesting idea, but the developments on the theoretical side is a bit thin.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576446235768, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Reviewers"], "noninvitees": [], "tcdate": 1570237733711, "tmdate": 1576446235787, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Review"}}}, {"id": "HkgclbXsjH", "original": null, "number": 8, "cdate": 1573757169627, "ddate": null, "tcdate": 1573757169627, "tmdate": 1573757169627, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "BJemW-L5ir", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment", "content": {"title": "Updates", "comment": "Thanks for your comments. \nFor (iii), we have introduced the Safe PAUs to avoid poles, which was an initial difficulty we faced when training even after few epochs on very simple networks. To prove some guarantees regarding exploding values, we could introduce a form of Lipschitz regularization that combined with BatchNorm could give us some initial assumptions. \nRegarding a proof against vanishing gradients (ii), this could be connected with a relaxed version of the Safe PAUs where the denominator is allowed to be < 1, as mentioned in the paper, this could potentially allow for gradient amplification. \nFor the moment, we show empirically that Safe PAU is stable and doesn\u2019t suffer from vanishing or exploding gradients more than any of the other activation functions that we compared to.\n\nMore importantly, we have updated the paper and included the proof for (i), we also added the calculation for the number of parameters (iv) as $\\phi=L*(m+n)$, which for our experiments is $\\phi=10*L$ where $L$ is the number of activation layers. The exact number of parameters for the experiments are in the Appendix, and they are orders of magnitude less than the remaining parameters of the network.\n\nWe thank you for the discussion and motivation to make the paper stronger.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlBSkHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1690/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1690/Authors|ICLR.cc/2020/Conference/Paper1690/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152305, "tmdate": 1576860560933, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment"}}}, {"id": "rklNJvljoS", "original": null, "number": 7, "cdate": 1573746396153, "ddate": null, "tcdate": 1573746396153, "tmdate": 1573746396153, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "BkxfeQYBor", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment", "content": {"title": "Further Experiments", "comment": "We have a small update: Here are the remaining experiments which will finish before the Deadline for Author Comments and Responses.\n\n              \t     CIFAR10_Densenet\nAPL*                 $94.45\\pm0.23$\nSReLU\t\t  $94.77\\pm0.24$  \nRPAU                $95.27\\pm0.10$\n\n\n              Imagenet_MobileNetV2\n\t\t\tAcc@1\t  Acc@5\nSReLU\t      $70.62$   \t $89.59$\nSwish            $71.24$     $89.95$     \nPAU\t      $71.35$\t $89.85$\n\nSReLU outperforms most of the activation functions. However, both Swish and PAU outperform SReLU.\n\nThe following table shows the summary of our experiments:\n\n\t\t\t\t\t    | ReLU | ReLU6 | LReLU | RReLU | ELU | CELU | PReLU | Swish | Maxout |Mixture| APL | SReLU |\n\nPAU/RPAU >= Baseline   | 33       | 34        | 33        | 32 \t| 39    | 39 \t | 38 \t|  41       | 9 \t         | 20 \t | 32    | 33\t   |\nPAU/RPAU < Baseline     |   8       |   7        |   8        |   9         |   2    |  2        |  3 \t|    1       | 6 \t         |  0 \t |   7    |   8 \t   |\n \nAgain, we believe the experiments show that PAUs are indeed competitive and have a place among the learnable activation functions."}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlBSkHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1690/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1690/Authors|ICLR.cc/2020/Conference/Paper1690/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152305, "tmdate": 1576860560933, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment"}}}, {"id": "BJemW-L5ir", "original": null, "number": 6, "cdate": 1573703931332, "ddate": null, "tcdate": 1573703931332, "tmdate": 1573703931332, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "rkesLmKBoH", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment", "content": {"title": "Thanks for the reply.", "comment": "Thanks for the reply. I think if the universal approximation property of PAU can be rigorously shown, this paper can be much stronger. I do not mind you using Kidger et al 2019 to show it. Showing that Kidger et al covers (i)-(iv) may be straightforward, but the readers may hope to see it in a clear way.\n\nIt is still unclear to me if the   \"safe PAU\" can retain the universal approximation property of PAU, since this is almost equally important. Without discussion on this point, the theoretical side still feels not significant enough."}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlBSkHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1690/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1690/Authors|ICLR.cc/2020/Conference/Paper1690/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152305, "tmdate": 1576860560933, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment"}}}, {"id": "BJeUaB5YiH", "original": null, "number": 5, "cdate": 1573655997627, "ddate": null, "tcdate": 1573655997627, "tmdate": 1573655997627, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "rkg9m7YHiS", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment", "content": {"title": "I think it's a good paper.", "comment": "Thanks for the response. I am not convinced by your argument about safe PAUs above, but I like the additional experiments performed in response to R3. I'm keeping my scores and am recommending acceptance."}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlBSkHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1690/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1690/Authors|ICLR.cc/2020/Conference/Paper1690/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152305, "tmdate": 1576860560933, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment"}}}, {"id": "rkesLmKBoH", "original": null, "number": 4, "cdate": 1573389138739, "ddate": null, "tcdate": 1573389138739, "tmdate": 1573389138739, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "HJgsBTOLKr", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment", "content": {"title": "Safe PAU Universal Approximator", "comment": "We thank you for taking the time to read our paper, and the comments.\n\nIt is true that we do not provide a formal proof for the safe PAU, as also mentioned by reviewer #1. However, PAU matches the assumption  of Kidger et al.  [1] proof. More precisely. Kidger et al. show that under certain size constraints, networks using non-affine continuous functions, with a continuous nonzero derivative at some point are also universal approximators. We will include this in the camera-ready version.\n\nMoreover, (i)-(v) are covered by PAU. For (i) we refer to the universal approximator discussion above as well as in the paper. Since PAU is seemingless integrated with the differentiable learning stack, standard methods for avoiding vanishing gradients can be used, hence, (ii) is covered. To cover (iii), we introduce safe PAUs and refer also to [1]. (iv) is covered since we only have a small overhead of parameters. Moreover, due to Telgarsky (2017) and  other recent results on ResNets one can actually expect to require less parameters, but this is future work. Finally (v) is covered as demonstrated by our empirical results. We will put these arguments into the camera-ready version. Thanks for pointing us to this. \n\n[1] Kidger, Patrick, and Terry Lyons. \"Universal Approximation with Deep Narrow Networks.\" arXiv preprint arXiv:1905.08539 (2019).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlBSkHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1690/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1690/Authors|ICLR.cc/2020/Conference/Paper1690/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152305, "tmdate": 1576860560933, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment"}}}, {"id": "rkg9m7YHiS", "original": null, "number": 3, "cdate": 1573389090079, "ddate": null, "tcdate": 1573389090079, "tmdate": 1573389090079, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "SkgoV1AnFH", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment", "content": {"title": "Safe PAUs", "comment": "We thank you for the time and comments. \n\nIndeed, the evaluation of the experiments is quite expensive and this is one of the challenges we are facing. Although we intend to test PAUs on other tasks/architectures, we consider the comparisons to the baselines we have (including the experiments proposed by reviewer #3) as a good introduction for PAUs into the community.\n\nYou are right in that we do not have a proof for the safe version of PAUs presented in the paper, but we are equally interested in this topic, too. Consequently, we now tested another \u201csafe\u201d version of the form P(X)/(eps + |Q(X)|). This version can be proven to be a universal approximator via similar arguments as the general PAU. However, empirically this version turned out to be very unstable. Unfortunately, the existence and form of safe PAUs, does not necessarily tell us about the stability and optimization characteristics. Fortunately, there is an indirect way around this and we redirect you to our reply to reviewer #2 for a further discussion. \n\nThank you once again for your review.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlBSkHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1690/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1690/Authors|ICLR.cc/2020/Conference/Paper1690/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152305, "tmdate": 1576860560933, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment"}}}, {"id": "BkxfeQYBor", "original": null, "number": 2, "cdate": 1573389034372, "ddate": null, "tcdate": 1573389034372, "tmdate": 1573389034372, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "SkxMQumTKB", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment", "content": {"title": "Further Experiments", "comment": "We thank you for the time and comments.\n\nWe have a different, rather very positive perspective. We are proposing an activation function that helps practitioners to avoid the search for activation functions as done in [1], and replaces this by learning.  PAU can match the performance of or sometimes outperform even the best baselines, in some cases up to 2% better than common activation functions. For instance we are  boosting the performance of MobileNetV2, a CVPR 2018 state-of-the-art approach. Moreover, in contrast to previous work, PAU directly paves the way to provably robust deep learning (Croce et al., 2019). Nevertheless, we fully agree with the reviewer that the paper would be even stronger by comparing to more learnable activation functions such as SReLUs and APLs, so we implemented and ran some more experiments: \n\n\n                  MNIST_VGG        MNIST_LeNet\nSReLUs    $99.15 \\pm0.03$    $99.13 \\pm0.14$\nAPLs        $99.18 \\pm0.10$    $99.35 \\pm0.11$ \nPAU        $99.30 \\pm0.05$    $99.21 \\pm0.04$ \n\n                  FMNIST_VGG        FMNIST_LeNet\nSReLUs    $89.65 \\pm0.42$    $89.83 \\pm0.30$\nAPLs        $91.41 \\pm0.48$    $89.72 \\pm0.30$ \nPAU        $91.25 \\pm0.18$    $90.30 \\pm0.15$ \n\n\n                  CIFAR10_VGG        CIFAR10_MVNet      CIFAR10_RNet\nSReLUs    $92.66 \\pm0.27$    $94.03 \\pm0.11$    $95.24 \\pm0.13$\nAPLs        $91.63 \\pm0.13$    $93.62 \\pm0.64$     $94.12 \\pm0.36$\nRPAU        $92.50 \\pm0.09$    $94.82 \\pm0.21$     $95.34 \\pm0.13$ \n    \n\nAgain, of the 7 new experiments, PAU is better than APL in 5 of them, and better than SReLU in 6 of them. More experiments on DenseNet and ImageNet, are running, and we expect to have them before the rebuttal deadline is over. Hence, PAU\u2019s perspective on robust deep learning via rationalization gets even more interesting. Thanks for pushing us to run more experiments. We believe the experiments show that PAUs are indeed competitive and have a place among the learnable activation functions.\n\nWe will keep you posted.\n\n[1] P. Ramachandran, B. Zoph, and Q. V. Le. Searching for activation functions. In Proceedings of the Workshop Track of the 6th International Conference on Learning Representations (ICLR), 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlBSkHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1690/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1690/Authors|ICLR.cc/2020/Conference/Paper1690/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152305, "tmdate": 1576860560933, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Authors", "ICLR.cc/2020/Conference/Paper1690/Reviewers", "ICLR.cc/2020/Conference/Paper1690/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Comment"}}}, {"id": "SkgoV1AnFH", "original": null, "number": 2, "cdate": 1571770162764, "ddate": null, "tcdate": 1571770162764, "tmdate": 1572972435853, "tddate": null, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "invitation": "ICLR.cc/2020/Conference/Paper1690/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper introduces a novel parametric activation function, called the Pade Activation Unit (PAU), for use in general deep neural networks. Pade is a rational function, which is a ratio of two polynomials, and which can very well approximate any of the usually used activation functions while having only a few parameters that can be learned from data. Moreover, the authors identify five properties that an activation function should have, and either prove or empirically show that PAUs satisfy all of them, unlike some of the baselines. Additionally, since Pade approximation can have poles and be unstable, this work introduces safe PAUs, where the polynomial in the denominator is constrained to attain values greater than or equal to one. Since one of the suggested properties is that a function using a given activation function be a universal function approximator, the authors provide a sketch of a proof that PAUs do allow that. This proof applies only to the unsafe version of the PAU, and it is unclear whether it extends to the safe PAU---an issue that is not mentioned by the authors.\nFurthermore, the authors propose a stochastic version of PAU with noise injected into parameters, which allows regularization. The empirical evaluation is quite extensive, and the PAU is compared against nine baselines on five different architectures (LeNet, VGG, DenseNet, ResNet, MobileNet) on four different datasets (MNIST, Fashion MNIST, CIfar10, ImageNet) for the classification task. The evaluation confirms that PAUs can match the performance of or sometimes outperform even the best baselines while the attained learning curves show that PAUs also lead to faster convergence of trained models. Finally, the authors demonstrate that (and provide intuition why) using PAUs allow for high-performing pruned models.\n\nI recommend ACCEPTing this paper as it is well written, extensively evaluated, and provides performance improvements or at least matches the performance of the best baseline across several datasets and model architectures.\n\nMy only two suggestions for improvement are a) make the universal approximation proof tighter by making sure that it extends to the safe PAU version, and b) evaluate the proposed activation function on tasks other than just classification."}, "signatures": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1690/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["molina@cs.tu-darmstadt.de", "schramowski@cs.tu-darmstadt.de", "kersting@cs.tu-darmstadt.de"], "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks", "authors": ["Alejandro Molina", "Patrick Schramowski", "Kristian Kersting"], "pdf": "/pdf/7c3c245f99116c181b7fcca470559c4c2456ce8d.pdf", "TL;DR": "We introduce PAU, a new learnable activation function for neural networks. They free the network designers from the activation selection process and increase the test prediction accuracy.", "abstract": "The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Pad\u00e9 Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.", "code": "https://github.com/ml-research/pau", "keywords": [], "paperhash": "molina|pad\u00e9_activation_units_endtoend_learning_of_flexible_activation_functions_in_deep_networks", "_bibtex": "@inproceedings{\nMolina2020Pad\u00e9,\ntitle={Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},\nauthor={Alejandro Molina and Patrick Schramowski and Kristian Kersting},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlBSkHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/701024c7e94bc10fdc914c56ca3953c9cd05f232.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlBSkHtDS", "replyto": "BJlBSkHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1690/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576446235768, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1690/Reviewers"], "noninvitees": [], "tcdate": 1570237733711, "tmdate": 1576446235787, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1690/-/Official_Review"}}}], "count": 12}