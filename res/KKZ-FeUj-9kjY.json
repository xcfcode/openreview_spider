{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362193620000, "tcdate": 1362193620000, "number": 2, "id": "dWSK4E1RkeWRi", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KKZ-FeUj-9kjY", "replyto": "KKZ-FeUj-9kjY", "signatures": ["anonymous reviewer e6d4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "review": "Since the last version of the paper (v2) is incomplete my following comments are mainly based on the first version.\r\n\r\nThis paper proposes using $l_{1,2}$ regularization (for both non-overlapping and overlapping groups) upon the activation possibilities of hidden units in RBMs. Then DBNs pretrained by the resulting mixed Norm RBMs are applied the task of digit recognition.    \r\n\r\nMy main concern is the mistakes in equation 16 (and 17), the core of this paper. The sign of the term of $lambda$ should be minus. There is also a missing $P(h_j=1|x^l)$ in that term. Since these mistakes could explain why the results are worse than the baseline and why the bigger non-overlapping groups (which can make the regularization term smaller) are preferred very well, I do not think they are merely typos."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "abstract": "Deep Belief Networks (DBN) have been successfully applied on popular machine learning tasks. Specifically, when applied on hand-written digit recognition, DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the data representation achieved by the DBN and maximize their descriptive power, recent advances have focused on inducing sparse constraints at each layer of the DBN. In this paper we present a theoretical approach for sparse constraints in the DBN using the mixed norm for both non-overlapping and overlapping groups. We explore how these constraints affect the classification accuracy for digit recognition in three different datasets (MNIST, USPS, RIMES) and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage.", "pdf": "https://arxiv.org/abs/1301.3533", "paperhash": "halkias|sparse_penalty_in_deep_belief_networks_using_the_mixed_norm_constraint", "keywords": [], "conflicts": [], "authors": ["Xanadu Halkias", "S\u00e9bastien PARIS", "Herve Glotin"], "authorids": ["xch1@caa.columbia.edu", "sebastien.paris@lsis.org", "glotin.univ-tln.fr"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362153000000, "tcdate": 1362153000000, "number": 1, "id": "ttT0L-IGxpbuw", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KKZ-FeUj-9kjY", "replyto": "KKZ-FeUj-9kjY", "signatures": ["anonymous reviewer 0136"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "review": "The paper proposes a mixed norm penalty for regularizing RBMs and DBNs. The work extends previous work on sparse RBMs and DBNs and extends the work of Luo et al. (2011) on sparse group RBMs (and DBMs) to deep belief nets. The method is tested on several datasets and no significant improvement is reported compared to the original DBN.\r\n\r\nThe paper has limited novelty as the proposed mixed-norm has already been investigated in details by Luo et al. (2011) on a RBM and DBM. Also, the original contribution is not properly referenced as it appears only in the references section but not in the text.\r\n\r\nIn the caption of Figure 1, it is said that hidden units will overrepresent vs. underrepresent the data. It is unclear what is exactly meant. Can this be quantified? Is this overrepresentation/underrepresentation problem intrinsic to the investigated mixed norms or is it more a question of choosing the right hyperparameters? The authors use a fixed regularization parameter for all investigated variants of the DBN. Could that be the reason for under/overrepresentation?\r\n\r\nThe authors are choosing three datasets that are all isolated handwritten digits recognition datasets. There are other problems such as handwritten characters (e.g. Chinese), Caltech 101 silhouettes, that also have binary representation and would be worth considering in order to assess the generality of the proposed method. Also, if the authors are targeting the handwriting recognition application, more realistic and challenging scenarios could be considered (e.g. non-isolated characters).\r\n\r\nMinor comments:\r\n\r\n- The last version of the paper (v2) is not properly compiled and the citations are missing.\r\n- The filters shown in Figure 1 should be made bigger.\r\n- In Figure 2 and 4, x and y labels should be made bigger.\r\n- Figure 2 is discussed in the caption of Figure 1."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "abstract": "Deep Belief Networks (DBN) have been successfully applied on popular machine learning tasks. Specifically, when applied on hand-written digit recognition, DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the data representation achieved by the DBN and maximize their descriptive power, recent advances have focused on inducing sparse constraints at each layer of the DBN. In this paper we present a theoretical approach for sparse constraints in the DBN using the mixed norm for both non-overlapping and overlapping groups. We explore how these constraints affect the classification accuracy for digit recognition in three different datasets (MNIST, USPS, RIMES) and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage.", "pdf": "https://arxiv.org/abs/1301.3533", "paperhash": "halkias|sparse_penalty_in_deep_belief_networks_using_the_mixed_norm_constraint", "keywords": [], "conflicts": [], "authors": ["Xanadu Halkias", "S\u00e9bastien PARIS", "Herve Glotin"], "authorids": ["xch1@caa.columbia.edu", "sebastien.paris@lsis.org", "glotin.univ-tln.fr"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362144480000, "tcdate": 1362144480000, "number": 3, "id": "ijgMjq-uMOiYw", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KKZ-FeUj-9kjY", "replyto": "KKZ-FeUj-9kjY", "signatures": ["anonymous reviewer 61fc"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "review": "In this paper the authors propose a method to make the hidden units of RBM group sparse. The key idea is to add a penalty term to the negative log-likelihood loss penalizing the L2/L1 norm over the activations of the RBM. The authors demonstrate their method on three digit classification tasks. These experiments show similar accuracy to the baseline model but faster convergence.\r\n\r\nThere is a vast literature on sparse coding and group sparse coding and several references are missing.\r\nAmong the works that use group sparse coding but not RBMs there are:\r\nA. Hyvarinen and U. Koster. Complex cell pooling and the statistics of natural images. Network, 18(2):81\u2013100, 2007 \r\nK. Kavukcuoglu, M. Ranzato, R. Fergus, Y. LeCun. 'Learning Invariant Features through Topographic Filter Maps'. Proc. of Computer Vision and Pattern Recognition Conference (CVPR 2009), Miami, 2009\r\n\r\nwhile these are some works related to RBM where sparse features are grouped in a similar way to group sparse coding methods:\r\nS. Osindero, M. Welling, and G. E. Hinton. Topographic product\r\nmodels applied to natural scene statistics. Neural Comp., 18:\r\n344\u2013381, 2006.\r\nM. Ranzato, A. Krizhevsky, G. Hinton, 'Factored 3-Way Restricted Boltzmann Machines for Modeling Natural Images'. Proc. of the 13-th International Conference on Artificial Intelligence and Statistics (AISTATS 2010), Italy, 2010\r\n\r\nOverall, the novelty of the proposed method is limited. It would be sufficient if the method was well motivated and described (see more detailed comments below). The quality of the work is fair since also the empirical validation is pretty weak: comparisons are reported on three small datasets which are very similar to each other, accuracy is on par with baseline methods and only convergence time is better but this finding has not been analyzed enough to make solid conclusions.\r\n\r\nPROS\r\n+ simple method\r\n\r\nCONS\r\n- limited novelty\r\n- the method is not well motivated (see below)\r\n- missing references\r\n- unconvincing empirical validation\r\n- writing needs improvements (see below)\r\n\r\nDetailed comments\r\n-- The major concern is about the proposed method in general. \r\nOn the one hand it makes totally sense to add a sparsity constraint to the negative log likelihood loss. On the other hand, RBM are a probabilistic model and one wonders what this additional term means. If it is interpreted as a prior on the conditional distribution over the hidden units, how is that changing the marginal likelihood, for instance? This takes to the discussion on an alternative approach which is to wrap the group sparsity constraint into the probabilistic model itself and to maximize the likelihood of this. The above references on topographic PoT and cRBM can indeed be interpreted as extensions of RBMs to make hidden units sparse in groups.\r\nA potential problem with the current formulation is that inference of the features does not take into account any sparsity (which is achieved only through learning). Overall after fine-tuning, one may expect little if any sparsity in the hidden units (which may explain why results are so similar to the baseline). \r\nIn light of this, it would have been nice if the authors commented on this way to tackle the problem, advantages and disadvantages of each approach.\r\nMore generally, I found very weak the motivation of this paper. The reason why sparsity and group sparsity is enforced is pretty vague and unconvincing.\r\n-- The empirical validation is very weak. The three datasets are very homogeneous and results are not better than the baseline.\r\nWhy is DBN so much slower? This is the strongest result of the paper in my opinion but it is not clear why that happens. \r\n-- There are lots of imprecise statements. Here a few.\r\nFirst, the title should be changed from 'DBN' to 'RBM'.\r\nAbstract\r\nThe results in the abstract '98.8' may refer to a specific dataset (MNIST?) but does not hold in general.\r\n'optimize the data representation achieved by the DBN \u2026' is vague.\r\n'theoretical approach': I would not call this approach theoretical!\r\nSec. 1\r\n'due to their generative and unsupervised learning framework': needs to be rephrased.\r\n[2, 3]: these references are not appropriate, perhaps [12, 13]?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "abstract": "Deep Belief Networks (DBN) have been successfully applied on popular machine learning tasks. Specifically, when applied on hand-written digit recognition, DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the data representation achieved by the DBN and maximize their descriptive power, recent advances have focused on inducing sparse constraints at each layer of the DBN. In this paper we present a theoretical approach for sparse constraints in the DBN using the mixed norm for both non-overlapping and overlapping groups. We explore how these constraints affect the classification accuracy for digit recognition in three different datasets (MNIST, USPS, RIMES) and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage.", "pdf": "https://arxiv.org/abs/1301.3533", "paperhash": "halkias|sparse_penalty_in_deep_belief_networks_using_the_mixed_norm_constraint", "keywords": [], "conflicts": [], "authors": ["Xanadu Halkias", "S\u00e9bastien PARIS", "Herve Glotin"], "authorids": ["xch1@caa.columbia.edu", "sebastien.paris@lsis.org", "glotin.univ-tln.fr"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358700300000, "tcdate": 1358700300000, "number": 50, "id": "KKZ-FeUj-9kjY", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "KKZ-FeUj-9kjY", "signatures": ["xch1@caa.columbia.edu"], "readers": ["everyone"], "content": {"decision": "reject", "title": "Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint", "abstract": "Deep Belief Networks (DBN) have been successfully applied on popular machine learning tasks. Specifically, when applied on hand-written digit recognition, DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the data representation achieved by the DBN and maximize their descriptive power, recent advances have focused on inducing sparse constraints at each layer of the DBN. In this paper we present a theoretical approach for sparse constraints in the DBN using the mixed norm for both non-overlapping and overlapping groups. We explore how these constraints affect the classification accuracy for digit recognition in three different datasets (MNIST, USPS, RIMES) and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage.", "pdf": "https://arxiv.org/abs/1301.3533", "paperhash": "halkias|sparse_penalty_in_deep_belief_networks_using_the_mixed_norm_constraint", "keywords": [], "conflicts": [], "authors": ["Xanadu Halkias", "S\u00e9bastien PARIS", "Herve Glotin"], "authorids": ["xch1@caa.columbia.edu", "sebastien.paris@lsis.org", "glotin.univ-tln.fr"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 4}