{"notes": [{"id": "r1eVXa4KvH", "original": "rkxx_ZywDH", "number": 446, "cdate": 1569439004183, "ddate": null, "tcdate": 1569439004183, "tmdate": 1577168212264, "tddate": null, "forum": "r1eVXa4KvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2FHgt5pFGV", "original": null, "number": 1, "cdate": 1576798696670, "ddate": null, "tcdate": 1576798696670, "tmdate": 1576800938992, "tddate": null, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "invitation": "ICLR.cc/2020/Conference/Paper446/-/Decision", "content": {"decision": "Reject", "comment": "This paper studies tradeoffs in the design of attention-based architectures. It argues and formally establishes that the expressivity of an attention head is determined by its dimension and that fixing the head dimension, one gains additional expressive power by using more heads.\n\nReviewers were generally positive about the question under study here, but raised important concerns about the significance of the results and the take-home message in the current manuscript. The AC shares these concerns, and recommends rejection, while encouraging the authors to address the concerns raised during this discussion. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723054, "tmdate": 1576800274472, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper446/-/Decision"}}}, {"id": "S1eyy-OVoS", "original": null, "number": 3, "cdate": 1573318871023, "ddate": null, "tcdate": 1573318871023, "tmdate": 1573318871023, "tddate": null, "forum": "r1eVXa4KvH", "replyto": "SJe97j5FKr", "invitation": "ICLR.cc/2020/Conference/Paper446/-/Official_Comment", "content": {"title": "Response to Review #3 ", "comment": "Comment:  \u201c... what is the main contribution of this work\u201d\nResponse:  In this paper we identify and highlight a key capacity bottleneck in multihead attention. We rigorously analyze this by proving capacity bounds of attention as a function of the head size. Based on this analysis, we propose a principled solution of using fixed head size attention layers to alleviate this bottleneck, and experimentally show superior performance (smaller embedding size, better parameter scaling) on 3 standard NLP tasks.\n\nWe clearly state this in the abstract \u201cAs a solution, we propose a new way to set the projection size in attention heads.. ,\u201d. We again state it in the introduction, contributions - point 2: \u201cWe propose a new way to set the head size, and show the proposed fixed head size layers are strictly better than the standard multi-head attention layers...\u201d\n\nComment: \u201c...theorems (seemingly ornamental, or handwavy actually)...\u201d\nResponse: This is an unfair characterization of our work. Theorem 1 shows the relationship between head size and the representation power of a single attention head. Theorem 2 shows that the FixedMultiHead has strictly more representative power than MultiHead layer. We present both the theorems with precise mathematical statements and complete proofs. We are happy to update the paper, if the reviewer noticed an issue with a particular statement, or a particular step in the proof. \n\nComment:  \u201c...the different way refers to explicitly setting each head to 128\u2026\u201d\nResponse: As we mentioned earlier, we clearly state both in the abstract and the introduction, that we train a fixed head size model, as opposed to using the d/h heuristic. We emphasize that our contribution is not just setting the head size parameter, but rather an analysis of the capacity of the multihead attention, and consequently a principled alternative to the current heuristic used to set the head size. Our experiments clearly indicate the advantage of using fixed head size in improving transformer training. We are able to train models with a smaller embedding size (512 vs 1024 for BERT) and with better parameter scaling.\n\nComments:  \u201c...being under-sized can be made up for with multiple heads\u2026\u201d\nResponse: We show in Theorem 2 that this is not the case. MultiHead has strictly lower expressive power than the Fixed MultiHead. Intuitively, increasing the number of heads decreases the head size, and negatively affects the capacity if the head size becomes too small. We establish this both theoretically and empirically.\n\nComments: \u201c...overall shortcomings of the paper...\u201d\nResponse: This is again a wrong characterization of the paper. We ask the reviewer to kindly state their specific issues with the presentation. \n\nWe ask the reviewer to kindly reconsider their score based on the importance of our analysis and our experimental contributions described above.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1eVXa4KvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference/Paper446/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper446/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper446/Reviewers", "ICLR.cc/2020/Conference/Paper446/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper446/Authors|ICLR.cc/2020/Conference/Paper446/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171378, "tmdate": 1576860562173, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference/Paper446/Reviewers", "ICLR.cc/2020/Conference/Paper446/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper446/-/Official_Comment"}}}, {"id": "HklWWx_Vir", "original": null, "number": 2, "cdate": 1573318649122, "ddate": null, "tcdate": 1573318649122, "tmdate": 1573318802835, "tddate": null, "forum": "r1eVXa4KvH", "replyto": "BJxY2KeTtH", "invitation": "ICLR.cc/2020/Conference/Paper446/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for the detailed assessment of our paper. Below we address their concerns, starting with their last question. \n\nComment: \u201c...I'm actually very surprised that this paper is the first trying this.The authors might need justify the technical contribution more.\u201d\nResponse: To reiterate our contribution, In this paper we identify and highlight a key capacity bottleneck in multihead attention. We rigorously analyze this by proving capacity bounds of attention as a function of the head size. Based on this analysis, we propose a principled solution to alleviate this bottleneck, and experimentally show superior performance (smaller embedding size, better parameter scaling) on 3 standard NLP tasks.\n\nThe main technical complexity is in analyzing if the low rank capacity bottleneck due to a decrease in head size is alleviated by the increase in number of heads, in the MultiHead. We show in Theorem 2 that MultiHead indeed suffers in representation power because of the rank constraint, whereas Fixed MultiHead does not. We note that our argument used to prove this result relies on a novel construction and constitutes the main technical contribution of the paper.  We agree that the proposed change to the Transformer training is a relatively simple change, which is actually an advantage of the approach considered.\n\nComment:  \u201cother works \u2026 It would be nice if the authors and discuss this in the revision\u201d\nResponse: We thank the reviewer for sharing these papers. We updated our related works section. [1, 2] study the importance of different heads in an attention layer. They observe that, during inference, many of the heads in each layer can be pruned away with a little effect on the prediction. However, this approach still requires multiple heads during the training.\n\nLow rank/sparse P: We first note that fixing the head size does not disallow low rank P, rather it gives the model capacity to learn arbitrary P as required by the task. Fixing the head size also gives us the ability to choose a smaller embedding size, as we have more degrees of freedom in designing the architecture.\n\n[3-5] impose a sparsity structure on the attention layer during training to improve both interpretability and performance. Fixing the head size will in fact make it easier to learn such sparsity patterns, as a low rank constraint does not allow a head to express all possible sparsity patterns. Combining these techniques can hence potentially enable training of sparse attention models with a smaller embedding size.\n\nComment: \u201cTheorem 2 \u2026 Why is it required that the V matrices for each head have the same product?\u201d\nResponse: In Theorem 2 we show that the standard MultiHead layer has strictly smaller representation power compared to the Fixed MultiHead layer. We prove it by constructing examples of functions that can be represented by the Fixed MultiHead layer, but the standard MultiHead layer fails to represent. We set the product of V matrices from different heads to be the same, only to keep the proof simple and easy to read. We can generalize the construction to different V\u2019s for each head at the cost of more notation in the proof.\n\nComment:  \u201cCan the authors compare the training/inference speed? It probably will be the same as standard transformers...\u201d\nResponse: The inference time, measured for a few of the trained models, shows correlation with the parameter scaling, and is similar to the standard Transformers. We will include the complete statistics in the final version.\n\nComment: \u201cFigure 1: the caption says trying out embedding sizes from 256 to 512\u2026\u201d\nResponse: For training Transformers with the head size heuristic d_p = d/h, we are forced to choose d to be a multiple of h, this limits us to only certain choices of d. For the baseline experiments, in addition to varying the embedding dimension, we also repeat the experiments with 3 different choices of heads [8,16, 32], 4 different choices of width [512, 1024, 2048, 4096] (Fig. 3). We also are limited in our computation budget, as we also present experimental results (Fig. 2) with ablations (Table 2) for the larger/expensive BERT setup.\n\nComment: \u201cIt would be nice to see some NMT experiments.\u201d\nResponse: We evaluated the proposed fixed head size setting on language modeling (LM1B) and BERT experiments, with fine tuning on MNLI and SQuAD datasets. We chose these tasks as they are encoder only models, and allows us to test at different scales. Testing the changes in an encoder-decoder framework, for NMT, is an interesting question, which we plan to pursue in the future.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1eVXa4KvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference/Paper446/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper446/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper446/Reviewers", "ICLR.cc/2020/Conference/Paper446/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper446/Authors|ICLR.cc/2020/Conference/Paper446/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171378, "tmdate": 1576860562173, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference/Paper446/Reviewers", "ICLR.cc/2020/Conference/Paper446/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper446/-/Official_Comment"}}}, {"id": "rylBlJd4jH", "original": null, "number": 1, "cdate": 1573318380922, "ddate": null, "tcdate": 1573318380922, "tmdate": 1573318380922, "tddate": null, "forum": "r1eVXa4KvH", "replyto": "S1xczKie9S", "invitation": "ICLR.cc/2020/Conference/Paper446/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Comment:  \u201cTo decouple the dependency between the head size and the embedding size is not a novel point, ... is not a strict constraint.\u201d \n\nResponse: In this paper we identify and highlight a key capacity bottleneck in multihead attention. We rigorously analyze this by proving capacity bounds of attention as a function of the head size. Based on this analysis, we propose a principled solution to alleviate this bottleneck, and experimentally show superior performance (smaller embedding size, better parameter scaling) on 3 standard NLP tasks.\n\nCalling our contribution as  \u201c...tuning of hyper-parameters\u201d is an unfair characterization that ignores our analysis, and our principled approach for either reducing the embedding size or improving the model performance for the same embedding size by addressing the bottleneck of head size.\n\nWe disagree with the reviewer on this and emphasize that for the training of transformers, this is indeed a novel way to set the head size. All the existing works on transformers such as GPT, BERT, RoBERTa etc., following the initial paper, use this heuristic of setting the head size to d/h, and there has been no work on questioning the optimal way to set the head size. Thus, the quest to improve the model performance via increasing the number of heads has necessitated large embedding sizes of these models, which we show can be reduced by setting the head size appropriately. Our work is the first such paper to analyze how the head size affects the model capacity and show a principled way to set it.  Our ablation studies clearly show the importance of setting the head size the right way, to train Transformers with a smaller embedding size.\n\nThere have been only a few works studying the capacity of transformers, and given their popularity, we believe our work will serve as a good first step towards their better understanding in the future. \n\nWe ask the reviewer to kindly reassess the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper446/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1eVXa4KvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference/Paper446/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper446/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper446/Reviewers", "ICLR.cc/2020/Conference/Paper446/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper446/Authors|ICLR.cc/2020/Conference/Paper446/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171378, "tmdate": 1576860562173, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper446/Authors", "ICLR.cc/2020/Conference/Paper446/Reviewers", "ICLR.cc/2020/Conference/Paper446/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper446/-/Official_Comment"}}}, {"id": "SJe97j5FKr", "original": null, "number": 1, "cdate": 1571560225620, "ddate": null, "tcdate": 1571560225620, "tmdate": 1572972594318, "tddate": null, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "invitation": "ICLR.cc/2020/Conference/Paper446/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a \u201cconcise\u201d version of the well-established Transformer model. The main proposal is to explicitly set the head size in the Transformer model instead of having to divide (share) representation ability amongst heads. \n\nThis paper is poorly written and after an entire 2-page long-winded introduction, the reader is left wondering what is the main contribution of this work. The term \u201cconcise\u201d is also not well-defined and left vague to readers. I re-read this paper multiple times and the only concluding finding I have is that this paper proposes an explicit way of setting the projection dimension regardless of the number of heads. \n\nAfter many mathematical formulations, theorems (seemingly ornamental, or handwavy actually), the final contribution seems to be to set the head size of BERT (size of each head) to 128. This is really trivial. The authors kept teasing a \u201cdifferent\u201d way to do this, but this left the reader completely unsatisfied when the different way refers to explicitly setting each head to 128 and using a smaller model overall. \n\nThe value 128 is derived from a theorem derived by the authors, which suggests that each head should at least be greater or equal than the sequence length (the sequence length here stated by the authors is 128). I\u2019m not very convinced by the argument. While it is intuitive that each head has to be sufficiently large, being under-sized can be made up for with multiple heads. It is also not clear why every X and P must be expressed with transforms W_q and W_k. P here represents the affinity matrix between tokens in a sequence.  It does not make any sense to me to ensure that every variation of P can be expressed because P is literally the pairwise scores between every token in the fully-connected attention graph. \n\nWhile I did not have the luxury of time to parse the Appendix to validate the legitimacy of the proof, I think the overall shortcomings of the paper (highly non-readable, bad presentation and perhaps a fair attempt at masking the lack of contribution) warrants a clear reject from me. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper446/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper446/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575773689339, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper446/Reviewers"], "noninvitees": [], "tcdate": 1570237752012, "tmdate": 1575773689357, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper446/-/Official_Review"}}}, {"id": "BJxY2KeTtH", "original": null, "number": 2, "cdate": 1571781040802, "ddate": null, "tcdate": 1571781040802, "tmdate": 1572972594283, "tddate": null, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "invitation": "ICLR.cc/2020/Conference/Paper446/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work studies the head size <--> head number tradeoff in multihead attention. It argues and formally establishes that (1) the expressivity of an attention head is determined by its dimension and (b) fixing the head dimension, one gains additional expressive power by using more heads. In response to such observations, the paper proposes Fixed Multihead Attention, where the constraint that `head_size * number_of_heads = embedding_size` in standard multihead attention is lifted; and it allows for using more attention heads without making each head smaller. One can control the total amount of parameters by using smaller embedding sizes, making it comparable (in terms of #parameters) to standard multihead attention. Empirical results on language modeling and NLI tasks confirms the arguments. \n\nPros:\n- The arguments on head size and head number tradeoff could be inspiring to future works.\n- A simple approach that proves strong in several NLP tasks.\n\nCons:\n- The theoretical discussion imposes too strong assumptions that might make it less interesting in practice.\n- No NMT experiments.\n- The takeaway seems a bit trivial.\n\nDetails:\n- Theorem 1 presents a rank-based view of each attention head's capacity, which is nice. Yet it is still unclear whether it is the case that the more expressive are the heads the better. For example, several recent works argues for specialized attention heads, i.e., each head has specific \"job,\" which may not require it being very expressive [1, 2]. Further, other works shows that a low-rank P matrix could be beneficial [3, 4, 5], which contradicts the argument in this work. It would be nice if the authors and discuss this in the revision\n\n(To be clear, I do believe this is still an open question, and do not think presenting a different view from previous works hurts the contribution of this work in any way.)\n\n- Theorem 2. I didn't carefully check the proof. Why is it required that the V matrices for each head have the same product. For both Thm.1 and 2, it would be nice to see some discussion on how they translate into the models in practice.\n\n- Can the authors compare the training/inference speed? It probably will be the same as standard transformers, but it would be nice to confirm.\n\n- Figure 1: the caption says trying out embedding sizes from 256 to 512. But it seems that only 4 values are tried. Can the authors comment on this? Also, it is a bit awkward to plot a line chart out of 4 points. Same for Figure 2.\n\n- It would be nice to see some NMT experiments. \n\n- The proposed method is so straightforward that I'm actually very surprised that this paper is the first trying this. The authors might need justify the technical contribution more.\n\n(I'm on the fence for this one, but the system doesn't allow me to. I'm happy to revise the score if the authors can address my concerns.)\n\n\n[1] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. https://arxiv.org/abs/1905.09418\n\n[2] Are Sixteen Heads Really Better than One? https://arxiv.org/abs/1905.10650\n\n[3] Generating Long Sequences with Sparse Transformers. https://arxiv.org/abs/1904.10509\n\n[4] Generating Long Sequences with Sparse Transformers. https://arxiv.org/pdf/1904.10509.pdf.\n\n[5] Adaptively Sparse Transformers. https://arxiv.org/abs/1909.00015."}, "signatures": ["ICLR.cc/2020/Conference/Paper446/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper446/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575773689339, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper446/Reviewers"], "noninvitees": [], "tcdate": 1570237752012, "tmdate": 1575773689357, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper446/-/Official_Review"}}}, {"id": "S1xczKie9S", "original": null, "number": 3, "cdate": 1572022546295, "ddate": null, "tcdate": 1572022546295, "tmdate": 1572972594239, "tddate": null, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "invitation": "ICLR.cc/2020/Conference/Paper446/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work discusses how to set the projection size for each head (head size) in multi-head attention module, especially Transformer. Theorem 1 is interesting, which points out a lower bound for the head size. The proposed method is to decouple the dependency between the head size and the embedding size. The experiments show that the proposed method is able to achieve comparable performance to BERT with fewer training cost.\n\nThe lower bound for the head size is a valuable result. However, the novelty is very limited. To decouple the dependency between the head size and the embedding size is not a novel point. In BERT/Transformer, it is set d_q=d_k=d_v=d, which is not a strict constraint. The only constraint in attention is to have d_q=d_k to allow dot product. Therefore, the proposed method is more like a tuning of hyper-parameters."}, "signatures": ["ICLR.cc/2020/Conference/Paper446/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper446/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Concise Multi-head Attention Models", "authors": ["Srinadh Bhojanapalli", "Chulhee Yun", "Ankit Singh Rawat", "Sashank Reddi", "Sanjiv Kumar"], "authorids": ["bsrinadh@google.com", "chulheey@mit.edu", "ankitsrawat@google.com", "sashank@google.com", "sanjivk@google.com"], "keywords": ["Transformers", "Attention", "Multihead", "expressive power", "embedding size"], "TL;DR": "Fixing the head size of the Transformer models allows one to train them with a smaller embedding size.", "abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.", "pdf": "/pdf/21f779988c315a1e1614050079022c5a4b405a3b.pdf", "paperhash": "bhojanapalli|concise_multihead_attention_models", "original_pdf": "/attachment/f5a0b05b5fb24573decfc0aace6e5c72e7e62a45.pdf", "_bibtex": "@misc{\nbhojanapalli2020concise,\ntitle={Concise Multi-head Attention Models},\nauthor={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},\nyear={2020},\nurl={https://openreview.net/forum?id=r1eVXa4KvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1eVXa4KvH", "replyto": "r1eVXa4KvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper446/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575773689339, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper446/Reviewers"], "noninvitees": [], "tcdate": 1570237752012, "tmdate": 1575773689357, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper446/-/Official_Review"}}}], "count": 8}