{"notes": [{"id": "SJl1o2NFwS", "original": "HJxdO5qxPB", "number": 136, "cdate": 1569438870759, "ddate": null, "tcdate": 1569438870759, "tmdate": 1577168286591, "tddate": null, "forum": "SJl1o2NFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "authors": ["Yiping Lu", "Zhuohan Li", "Di He", "Zhiqing Sun", "Bin Dong", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "authorids": ["yplu@stanford.edu", "zhuohan@berkeley.edu", "di_he@pku.edu.cn", "zhiqings@andrew.cmu.edu", "bindong@math.pku.edu.cn", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "keywords": ["Transformer", "Ordinary Differential Equation", "Multi-Particle Dynamic System", "Natural Language Processing"], "abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized", "pdf": "/pdf/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "paperhash": "lu|understanding_and_improving_transformer_from_a_multiparticle_dynamic_system_point_of_view", "original_pdf": "/attachment/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "_bibtex": "@misc{\nlu2020understanding,\ntitle={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},\nauthor={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl1o2NFwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ggUCBDSikz", "original": null, "number": 1, "cdate": 1576798688386, "ddate": null, "tcdate": 1576798688386, "tmdate": 1576800946706, "tddate": null, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "invitation": "ICLR.cc/2020/Conference/Paper136/-/Decision", "content": {"decision": "Reject", "comment": "In this work, the authors interpret the Transformer as a numerical ODE modelling multi-particle convection. Guided by this connection, the authors take the Transformer that uses a feed forward net over attentions, and create a variant of transformer which instead uses an FFN-attention-FFN layer, thus the name macaron net. The authors present experiments in the GLUE dataset and in two MT datasets, and they overall report improved performance using their variant of Transformer. Thus, the main selling point of the paper is how seeing Transformer under his new light can potentially improve results through the construction of better models. The main criticisms from the authors is that  this story is not entirely convincing because the proposed variant departs a bit from the theory (R1 and comment about the Strang-Marchuk splitting) and the papers does not consider an evaluation of accuracy of Macaron in solving the underlying set of ODEs (comment from R3). As such, I cannot recommend acceptance of this paper -- I believe another set of revisions would increase the impact of this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "authors": ["Yiping Lu", "Zhuohan Li", "Di He", "Zhiqing Sun", "Bin Dong", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "authorids": ["yplu@stanford.edu", "zhuohan@berkeley.edu", "di_he@pku.edu.cn", "zhiqings@andrew.cmu.edu", "bindong@math.pku.edu.cn", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "keywords": ["Transformer", "Ordinary Differential Equation", "Multi-Particle Dynamic System", "Natural Language Processing"], "abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized", "pdf": "/pdf/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "paperhash": "lu|understanding_and_improving_transformer_from_a_multiparticle_dynamic_system_point_of_view", "original_pdf": "/attachment/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "_bibtex": "@misc{\nlu2020understanding,\ntitle={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},\nauthor={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl1o2NFwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730340, "tmdate": 1576800283114, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper136/-/Decision"}}}, {"id": "HJlXYol2jr", "original": null, "number": 1, "cdate": 1573813114853, "ddate": null, "tcdate": 1573813114853, "tmdate": 1573840097041, "tddate": null, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "invitation": "ICLR.cc/2020/Conference/Paper136/-/Official_Comment", "content": {"title": "Author Response", "comment": "We thank all reviewers for the valuable comments.  Since the concerns are shared, we decide to answer all of the questions here.\n\n[Regarding Strang-Marchuk splitting]\n\nFirst, Strang-Marchuk splitting can also be used for  nonautonomous system (right hand side is varying from time, i.e. \\dot x = f(x,t) but not \\dot x = f(x)). In fact the nonautonomous system and autonomous system are nearly the same, for you can write down a nonautonomous system $\\dot x = f(x,t)$ to an equivalent autonomous system  $d [x, R]^T/dt = [f(x,R),1]^T$.  The two FFN blocks have different weights can also be considered as Strang splitting. Moreover, applying the sub-layer twice will make the optimization landscape worse.\n\nSecond, the numerical error is a composition of the splitting scheme error and the integration method. We agree it\u2019s better to replace the Euler's method with a higher-order one and build a higher-order better algorithm. However, such changing is expensive. That\u2019s why we are aiming to have a better splitting, which also helps.\n\n[Regarding the diffusion and convection]\n\nG represents a convection term is due to that the particles are moving in their own ways. In a mean-field viewpoint, the distribution of the particles will move like a transport equation $\\partial \\rho=\\nabla\\cdot(\\rho G)$.\nThe derivation of diffusion PDEs is from a particle system, i.e., limiting the number of particle numbers to infinity(mean-field limit), the evolution of the probability measure will come to a diffusion-like process. We also want to point out that the attention network is not a standard diffusion, it\u2019s more like a nonlocal diffusion as proposed in the following paper. \n\nTao Y, Sun Q, Du Q, et al. Nonlocal neural networks, nonlocal diffusion and nonlocal modeling[C]//Advances in Neural Information Processing Systems. 2018: 496-506."}, "signatures": ["ICLR.cc/2020/Conference/Paper136/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper136/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "authors": ["Yiping Lu", "Zhuohan Li", "Di He", "Zhiqing Sun", "Bin Dong", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "authorids": ["yplu@stanford.edu", "zhuohan@berkeley.edu", "di_he@pku.edu.cn", "zhiqings@andrew.cmu.edu", "bindong@math.pku.edu.cn", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "keywords": ["Transformer", "Ordinary Differential Equation", "Multi-Particle Dynamic System", "Natural Language Processing"], "abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized", "pdf": "/pdf/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "paperhash": "lu|understanding_and_improving_transformer_from_a_multiparticle_dynamic_system_point_of_view", "original_pdf": "/attachment/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "_bibtex": "@misc{\nlu2020understanding,\ntitle={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},\nauthor={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl1o2NFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl1o2NFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper136/Authors", "ICLR.cc/2020/Conference/Paper136/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper136/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper136/Reviewers", "ICLR.cc/2020/Conference/Paper136/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper136/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper136/Authors|ICLR.cc/2020/Conference/Paper136/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175813, "tmdate": 1576860532763, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper136/Authors", "ICLR.cc/2020/Conference/Paper136/Reviewers", "ICLR.cc/2020/Conference/Paper136/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper136/-/Official_Comment"}}}, {"id": "S1eKfLh4KH", "original": null, "number": 2, "cdate": 1571239440893, "ddate": null, "tcdate": 1571239440893, "tmdate": 1572972634327, "tddate": null, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "invitation": "ICLR.cc/2020/Conference/Paper136/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this work, the authors show that the sequence of self-attention and feed-forward layers within a Transformer can be interpreted as an approximate numerical solution to a set of coupled ODEs. Based on this insight, the authors propose to replace the first-order Lie-Trotter splitting scheme by the more accurate, second-order Strang splitting scheme. They then present experimental results that indicate an improved performance of their Macaron Net compared to the Transformer and argue that this is due to the former being a more accurate numerical solution to the underlying set of ODEs.\n\nThe authors highlight an interesting connection between the Transformer architecture and ODEs. In particular, they derive a set of ODEs that is solved numerically by the Transformer and borrow from the body of literature on numerical ODE solvers to improve the architecture. I find that this is a very elegant and promising approach for finding better architectures. \n\nHowever, I also identified two major and a couple of minor shortcomings of the paper that are explained in detail below. Based on these shortcomings, I recommend rejecting the paper but I would be willing to increase the score if these points were addressed in sufficient detail.\n\nMajor points:\n\n1) Replacing the first-order operator splitting scheme by a second-order scheme only guarantees a lower overall truncation error if the split ODEs are solved with sufficiently high accuracy. In particular, the overall accuracy of the numerical solution to the original ODE depends on the accuracy of the operator splitting and the accuracy of the integration scheme used to solve the split ODEs (e.g. Euler\u2019s method). The authors improve the operator splitting, i.e. they make it second-order, but they keep Euler\u2019s method to integrate the individual ODEs. Because of that, they actually do not get rid of the lowest-order error term of the overall scheme and therefore do not obtain a more accurate ODE solver. I think this is a crucial point that invalidates the authors\u2019 claim that the Macaron Net employs a higher-order integration scheme. As far as I am aware, this is not commented on in the paper at all. To address this shortcoming, the authors could replace Euler\u2019s method by a second-order integrator. \n\n2) The experiments considered in this paper are interesting and show competitive performance but, in my opinion, they do not sufficiently support the claim that the Macaron Net yields a more accurate solution to the underlying set of ODEs compared to a Transformer. For a more convincing support of this claim, the authors could consider a toy problem, i.e. a simple set of ODEs with known analytical solution, and actually show that the Macaron Net is more accurate. The accuracy of a numerical ODE solver is commonly assessed by plotting the absolute difference between the exact solution (or a high-resolution numerical approximation to it) and the numerical solution vs the timestep (here \\gamma). I suspect that such an analysis would support my previous comment and show that the proposed new architecture is not more accurate ODE solver than the original one.  \n\nMinor points and questions:\n\ni) Eqs. (17-19) suggest that you apply two different FFN layers (doubling the number of parameters) instead of applying the same FFN layer twice. You comment on this in Sec. 4.1 when you say \u2018..., we set the dimensionality of the inner-layer of the two FFN sub-layers in the Macaron layers to two times of the dimensionality of the hidden states\u2019. It is not clear to me why consistency with Strang splitting requires two different layers rather than applying the same FFN layer twice. Is the reason for having a separate, trainable layer to account for the explicit time dependence of G in Eq. (16)? I think that this is a very important point that should be clarified.\n\nii) I think that this type of system is usually referred to as \u2018dynamical system\u2019 and not \u2018dynamic system\u2019. Please check that and, if applicable, update the title.  \n\niii) The authors say that Eq. (5) is a 'convection-diffusion equation\u2019. As far as I am aware, the diffusion equation is a partial differential equation (PDE). Perhaps there is a different notion 'diffusion equation\u2019 in ODE theory. If that\u2019s the case, could the authors please clarify this point to avoid confusion, e.g. by adding a suitable reference in which this type of ODE is classified as a convection-diffusion equation? \n\niv) In Sec. 2 (2nd paragraph),  you cite Vaswani et al. (2017) but in that work the quantity under the square-root in the denominator of Attention(Q, K, V) is actually d_k, the dimension of the key, and not d_model.\n\nv) Figure 1 is a very vague illustration of the connection to ODEs and provides almost no explanation in the caption. I don\u2019t think there is much value in having this figure there.\n\nvi) There are a couple of mistakes in the paper (grammar and expressions) that should be fixed. For example, \u2018the Euler\u2019s method\u2019 instead of \u2018Euler\u2019s method\u2019, \u2018movement in the space\u2019 instead of \u2018movement in space\u2019, \u2018dynamic system\u2019 instead of \u2018dynamical system\u2019, \u2018project parameter matrices\u2019 instead of \u2018parameter matrices\u2019 or \u2018projections\u2019, \u2018specially\u2019 instead of \u2018specifically\u2019, etc. Please take a look at the relevant sections in the paper and revise them accordingly.\n\nvii) You explain multiple times why the proposed architecture is called a \u2018Macaron Net\u2019 (Abstract, Sec 1, Sec. 3). To avoid repetition, I would only explain it once.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper136/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper136/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "authors": ["Yiping Lu", "Zhuohan Li", "Di He", "Zhiqing Sun", "Bin Dong", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "authorids": ["yplu@stanford.edu", "zhuohan@berkeley.edu", "di_he@pku.edu.cn", "zhiqings@andrew.cmu.edu", "bindong@math.pku.edu.cn", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "keywords": ["Transformer", "Ordinary Differential Equation", "Multi-Particle Dynamic System", "Natural Language Processing"], "abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized", "pdf": "/pdf/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "paperhash": "lu|understanding_and_improving_transformer_from_a_multiparticle_dynamic_system_point_of_view", "original_pdf": "/attachment/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "_bibtex": "@misc{\nlu2020understanding,\ntitle={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},\nauthor={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl1o2NFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575818162028, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper136/Reviewers"], "noninvitees": [], "tcdate": 1570237756556, "tmdate": 1575818162051, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper136/-/Official_Review"}}}, {"id": "SygNWhNWtr", "original": null, "number": 1, "cdate": 1571011579592, "ddate": null, "tcdate": 1571011579592, "tmdate": 1572972634292, "tddate": null, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "invitation": "ICLR.cc/2020/Conference/Paper136/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Contributions: This paper builds an ad-hoc connection between the Transformer and the numerical ODE solver (the Lie-Trotter splitting scheme and the Euler's method) for a convection-diffusion equation in a multi-particle dynamic system. Then, the author(s) developed an ad-hoc Strang-Marchuk splitting style architecture, named Macaron Net. Finally, this paper provides some experiments to verify the performance of the proposed architecture. However, the comparisons with the benchmark results are questionable. I have listed my concerns in the Experiment section.\n\n\nMotivation: This paper developed the Macaron Net based on a locally third-order operator splitting scheme for the convection-diffusion equation. However, there is no theoretical interpretation of why third-order splitting corresponding to better architecture. Theorem 1 in the paper is a known result, and it is irrelevant to the paper, I highly recommend the author to remove it from the main text. I also suggest the author explore more operator splitting schemes and do a systematic comparison between them. Moreover, I think it will be a real contribution if the author can analyze the error between the numerical scheme and architectures.\n\n\nReformulate Transformer Layers ans an ODE solver for Multi-Particle Dynamic System: There is a big gap between Eqns 3, 4 and 5. Why F represents a diffusion term, why G represents a convection term? From a statistical mechanics point of view, this comparison does not make sense. I do not buy this model.\n\n\nRelated Work: There is no related-work section that discusses the related work, and all the referenced papers are generic. For instance, the efforts in developing language models, the application of convection-diffusion equation, and String-Marchuck and other operator splitting schemes in machine learning. The author should better position the paper to exist work.\n\n\nExperiments: This section is extremely questionable. My initial thought after reading the reported results is that the architecture proposed in this paper easily outperforms the existing work. However, after I do a cross-check with the existing work, I found the author did not compare with the best results reported in the benchmark work and hide much information. After simply checking two existing papers, I found that the author ignored the comparison with BERT large. Also, the author ignored the most important result reported by Wu et al. 2019b.  To be fair, the author should perform an apple-to-apple comparison with the existing work and report the uncertainties in their results. Moreover, the author should report the parameters used in all their experiments.\n\n\nI think this heuristic study might be a contribution to ICLR if all my concerns are addressed, and I am willing to raise my rating to accept."}, "signatures": ["ICLR.cc/2020/Conference/Paper136/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper136/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "authors": ["Yiping Lu", "Zhuohan Li", "Di He", "Zhiqing Sun", "Bin Dong", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "authorids": ["yplu@stanford.edu", "zhuohan@berkeley.edu", "di_he@pku.edu.cn", "zhiqings@andrew.cmu.edu", "bindong@math.pku.edu.cn", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "keywords": ["Transformer", "Ordinary Differential Equation", "Multi-Particle Dynamic System", "Natural Language Processing"], "abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized", "pdf": "/pdf/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "paperhash": "lu|understanding_and_improving_transformer_from_a_multiparticle_dynamic_system_point_of_view", "original_pdf": "/attachment/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "_bibtex": "@misc{\nlu2020understanding,\ntitle={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},\nauthor={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl1o2NFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575818162028, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper136/Reviewers"], "noninvitees": [], "tcdate": 1570237756556, "tmdate": 1575818162051, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper136/-/Official_Review"}}}, {"id": "HyexZfMiKB", "original": null, "number": 3, "cdate": 1571656184202, "ddate": null, "tcdate": 1571656184202, "tmdate": 1572972634255, "tddate": null, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "invitation": "ICLR.cc/2020/Conference/Paper136/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper points out a formal analogy between transformers and an ODE modelling multi-particle convection (the feed-forward network) and diffusion (the self-attention head). The paper then adapts the Strang-Marchuk splitting scheme for solving ODEs to construct a slightly different transformer architecture: \u201cFFN of Attention of FFN\u201d, instead of \u201cFFN of Attention\u201d. The new architecture, refered to as a Macaron-Net, yields better performance in a variety of experiments.\n\nPROs\n1. The proposed new architecture is fairly simple.\n2. The experimental results are fairly good.\n\nCONs\n1. Introducing two feedforward layers with *different* parameters W^{down} and W^{up} is a significant deviation from Strang-Marchuk splitting. I expected the two FFNs inside the Macaron to have the same weights. As I understand it, the motivation for the splitting is to improve the numerical performance of the update scheme for the ODE. In contrast, allowing different weights for the FFNs means the \u201cphysical process\u201d is now a lot more \u201cfree\u201d. Is there any \u201cphysical\u201d motivation for the different parameters? (Beyond the fact that it improves performance). How much worse is empirical performance when the parameters are the same?\n\n2. Following on from the above point, the analogy between the multi-particle system and the transformer is quite weak. The equations look similar when you squint the right way. But that\u2019s as far as it goes. Fig 1 is a nice visualization, but it doesn\u2019t provide insight into the dynamics of transformers. What does \u201cParticles move in the space along time (Semantics encoded in stacked neural network layers)\u201d mean? How do the particles connect to the semantics? \n3. The proof of Bobylev & Ohwada\u2019s theorem is included in the paper. Is there any connection between the theorem (or the techniques used in its proof) and transformers? I suspect the answer is no.\n\nSUMMARY\nIn short, the paper (i) proposes two FFN layers instead of one in each block of the transformer and (ii) shows it performs slightly better than before. This is decent, but in my opinion not enough the clear the bar for ICLR.\n\nThe connection to multi-particle ODEs is genuinely interesting. However, it is not sufficiently fleshed out to count as a contribution (yet). It\u2019s possible the authors have discovered something deep. It\u2019s also possible they got lucky with a physically motivated modification of transformers that actually has nothing to do with the dynamics of multi-particle systems. I\u2019m not sure what further experiments would be needed to make the case. But I recommend the authors dig into the equations and the dynamics to see what it really going on under the hood. Just showing improved performance on a few benchmarks is not enough to convince the connection is solid. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper136/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper136/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View", "authors": ["Yiping Lu", "Zhuohan Li", "Di He", "Zhiqing Sun", "Bin Dong", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "authorids": ["yplu@stanford.edu", "zhuohan@berkeley.edu", "di_he@pku.edu.cn", "zhiqings@andrew.cmu.edu", "bindong@math.pku.edu.cn", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "keywords": ["Transformer", "Ordinary Differential Equation", "Multi-Particle Dynamic System", "Natural Language Processing"], "abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized", "pdf": "/pdf/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "paperhash": "lu|understanding_and_improving_transformer_from_a_multiparticle_dynamic_system_point_of_view", "original_pdf": "/attachment/d9320f3ed1c87d959f984d0d50a600863bbc4723.pdf", "_bibtex": "@misc{\nlu2020understanding,\ntitle={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},\nauthor={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl1o2NFwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJl1o2NFwS", "replyto": "SJl1o2NFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper136/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575818162028, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper136/Reviewers"], "noninvitees": [], "tcdate": 1570237756556, "tmdate": 1575818162051, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper136/-/Official_Review"}}}], "count": 6}