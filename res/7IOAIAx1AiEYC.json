{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362529800000, "tcdate": 1362529800000, "number": 4, "id": "_VZcVNP2cvtGj", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "7IOAIAx1AiEYC", "replyto": "7IOAIAx1AiEYC", "signatures": ["Tom Schaul, Yann LeCun"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their constructive comments. We'll try to clarify a few points they bring up:\r\n\r\nParallelization: The batchsize-aware adaptive learning rates (equation 3) are applicable independently of how the minibatches are computed, whether on a multi-core machine, or across multiple machines. They are in fact complementary to the asynchronous updates of Hogwild, in that they remove its need for tuning learning rate ('gamma') and learning rate decay ('beta').\r\n\r\nBbprop: The original version of vSGD (presented in [1]) does indeed require the 'bbprop' algorithm as one of its components to estimate element-wise curvature. One of the main points of this paper, however, is to replace it by a less brittle approach, based on finite-differences (section 5).\r\n\r\nLarge-scale experiments: We conduced a broad range of such experiments in the precursor paper [1] which demonstrated that the performance of the adaptive learning rates does correspond to the best-tuned SGD. Under the assumption that curvature does not change too fast, the original vSGD (using bbprop) and the one presented here (using finite differences) are equivalent, so those results are still valid -- but for more difficult (non-smooth) learning problems the new variant should be much more robust.\r\n\r\nWe'd also like to point out that an open-source implementation is now available at\r\nhttp://github.com/schaul/py-optim/blob/master/PyOptim/algorithms/vsgd.py"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "decision": "conferencePoster-iclr2013-conference", "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.", "pdf": "https://arxiv.org/abs/1301.3764", "paperhash": "schaul|adaptive_learning_rates_and_parallelization_for_stochastic_sparse_nonsmooth_gradients", "authors": ["Tom Schaul", "Yann LeCun"], "keywords": [], "conflicts": [], "authorids": ["schaul@cims.nyu.edu", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362388500000, "tcdate": 1362388500000, "number": 2, "id": "UUYiUZMOiCjl1", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "7IOAIAx1AiEYC", "replyto": "7IOAIAx1AiEYC", "signatures": ["anonymous reviewer 7b8e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "review": "This is a paper that builds up on the adaptive learning rate scheme proposed in [1], for choosing learning rate when optimizing a neural network.\r\n\r\nThe first result (eq. 3) is that of figuring out an optimal learning rate schedule for a given mini-batch size n (a very realistic scenario, when one cannot adapt the size of the mini-batch during training because of computational and architectural constraints).\r\n\r\nThe second interesting result is that of setting the learning rates in those cases where one has sparse gradients (rectified linear units etc) -- this results in an effective rescaling of the rates by the number of non-zero elements in a given minibatch.\r\n\r\nThe third nice result is the observation that in a sparse situation the gradient update directions are mostly orthogonal. Taking this intuition to the logical conclusion, the authors thus induce a re-weighing scheme that essentially encourages the gradient updates to be orthogonal to each other (by weighing them proportionally to 1/number of times they interfere with each other). While the authors claim that this can be computationally expensive generally speaking, for problems of realistic sizes (d is in the tens of millions and n is a few dozen examples), this can be quite interesting.\r\n\r\nThe final interesting result is that of adapting the curvature estimation to the fact that with the advent of rectified linear units we are often faced with optimizing non-smooth loss functions. The authors propose a method that is  based on finite differences (with some robustness improvements) and is vaguely similar to what is done in SGD-QN.\r\n\r\nGenerally this is a very well-written paper that proposes a few sensible and relatively easy to implement ideas for adaptive learning rate schemes. I expect researchers in the field to find these ideas valuable. One disappointing aspect of the paper is the lack of real-world results on things other than simulated (and known) loss functions."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "decision": "conferencePoster-iclr2013-conference", "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.", "pdf": "https://arxiv.org/abs/1301.3764", "paperhash": "schaul|adaptive_learning_rates_and_parallelization_for_stochastic_sparse_nonsmooth_gradients", "authors": ["Tom Schaul", "Yann LeCun"], "keywords": [], "conflicts": [], "authorids": ["schaul@cims.nyu.edu", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362001560000, "tcdate": 1362001560000, "number": 1, "id": "hhgfZq1Yf5hzr", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "7IOAIAx1AiEYC", "replyto": "7IOAIAx1AiEYC", "signatures": ["anonymous reviewer 7318"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "review": "summary:\r\nThe paper proposes a new variant of stochastic gradient descent that is fully automated (no\r\nhyper-parameter to tune) and is robust to various scenarios, including mini-batches,\r\nsparsity, and non-smooth gradients. It relies on an adaptive learning rate that takes\r\ninto account a moving average of the Hessian. The result is a single algorithm that takes about 4x\r\nmemory (with respect to the size of the model) and is easy to implement.\r\nThe algorithm is tested on purely artificial tasks, as a proof of concept.\r\n\r\nreview.\r\n- The paper relies on some previous algorithm (bbprop) that is not provided here and only\r\nexplained briefly on page 5, while first used on page 2. It would have been nice to provide\r\nmore information about it earlier.\r\n\r\n- The 'parallelization trick' using mini-batches is good for a single-machine approach, where\r\none can use multiple cores, but is thus limited by the number of cores. Also, how would\r\nthis 'interfere' with Hogwild type of updates, which also uses efficiently multi-core approaches\r\nfor SGD?\r\n\r\n- Obviously, results on real large datasets would have been welcomed (I do think experiments\r\non artificial datasets are very useful as well, but they may hide the fact that we have not\r\nfully understood the complexity of real datasets)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "decision": "conferencePoster-iclr2013-conference", "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.", "pdf": "https://arxiv.org/abs/1301.3764", "paperhash": "schaul|adaptive_learning_rates_and_parallelization_for_stochastic_sparse_nonsmooth_gradients", "authors": ["Tom Schaul", "Yann LeCun"], "keywords": [], "conflicts": [], "authorids": ["schaul@cims.nyu.edu", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361565480000, "tcdate": 1361565480000, "number": 3, "id": "_5dVjqxuVf560", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "7IOAIAx1AiEYC", "replyto": "7IOAIAx1AiEYC", "signatures": ["anonymous reviewer 0321"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "review": "This is a followup paper for reference [1] which describes a parameter free adaptive method to set learning rates for SGD.  This submission cannot be read without first reading [1].  It expands the work in several directions: the impact of minibatches, the impact of sparsity and gradient orthonormality, and the use of finite difference techniques to approximate curvature. The proposed methods are justified with simple theoretical considerations under simplifying assumptions and with serious empirical studies. I believe that these results are useful.\r\n\r\nOn the other hand, an opportunity has been lost to write a more substantial self-contained paper. As it stands, the submission reads like three incremental contributions stappled together."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "decision": "conferencePoster-iclr2013-conference", "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.", "pdf": "https://arxiv.org/abs/1301.3764", "paperhash": "schaul|adaptive_learning_rates_and_parallelization_for_stochastic_sparse_nonsmooth_gradients", "authors": ["Tom Schaul", "Yann LeCun"], "keywords": [], "conflicts": [], "authorids": ["schaul@cims.nyu.edu", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358404200000, "tcdate": 1358404200000, "number": 13, "id": "7IOAIAx1AiEYC", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "7IOAIAx1AiEYC", "signatures": ["schaul@cims.nyu.edu"], "readers": ["everyone"], "content": {"title": "Adaptive learning rates and parallelization for stochastic, sparse,\r\n    non-smooth gradients", "decision": "conferencePoster-iclr2013-conference", "abstract": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.", "pdf": "https://arxiv.org/abs/1301.3764", "paperhash": "schaul|adaptive_learning_rates_and_parallelization_for_stochastic_sparse_nonsmooth_gradients", "authors": ["Tom Schaul", "Yann LeCun"], "keywords": [], "conflicts": [], "authorids": ["schaul@cims.nyu.edu", "ylecun@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}