{"notes": [{"id": "ZPa2SyGcbwh", "original": "zIsA0P5mEC", "number": 2811, "cdate": 1601308311824, "ddate": null, "tcdate": 1601308311824, "tmdate": 1616035510265, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "z_5FTD4HqCv", "original": null, "number": 2, "cdate": 1612196828062, "ddate": null, "tcdate": 1612196828062, "tmdate": 1612196828062, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "OVSBPuBIM2R", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Comment", "content": {"title": "Response to the comment", "comment": "Dear Pengfei,\n\nThank you very much for your comment on our work. We appreciate the positive feedback. We have read the referred paper [1] and will cite it in our camera-ready version. \n\nWe would like to clarify that the IDN in [1] is very different and indeed much more restrictive than the diminishing noise proposed in our paper. __Thus, the claims in [1] regarding IDN does not apply to our noise model.__  In particular, the IDN in [1] requires the noise to be positively correlated with the confidence of the Bayesian/clean classifier. Whereas in our paper, we only require the noise to be upper bounded in some high confidence region and allow it to be arbitrary everywhere else. See Figure 2 for a demonstration. \n\nBest,\n\nAuthors \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ZPa2SyGcbwh", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649464144, "tmdate": 1610649464144, "id": "ICLR.cc/2021/Conference/Paper2811/-/Comment"}}}, {"id": "OVSBPuBIM2R", "original": null, "number": 1, "cdate": 1610676213099, "ddate": null, "tcdate": 1610676213099, "tmdate": 1610676244019, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Comment", "content": {"title": "A minor issue and a related work that provides rigorous motivations for learning with feature-dependent noise.", "comment": "Thanks for the excellent work on learning with instance(feature)-dependent noise (IDN)! I really appreciate the modeling on the noise and the theoretical guarantee on the performance of progressive label correction. Learning with IDN is challenging while deriving a theoretical guarantee for IDN is even more challenging since the noise is dependent on individual instance, unlike class-conditional noise (CCN) for which the noise can be modeled by class-dependent noise transition probabilities (namely the noise transition matrix).\n\nIn this paper, the author defines a natural feature-dependent noise and derive the theoretical guarantee based on two assumptions: Definition 2, which somewhat guarantees the accuracy on confident samples; Definition 3, which characterizes the imbalance of data distribution. For example, the data distribution could not be too imbalanced such that most samples are close to the decision boundary. This work is an interesting and important contribution!\n\nThere is a minor issue in a claim in Section 4: **this noise will hurt the network\u2019s performance the most**. Strictly speaking, IDN is confusing and is difficult to mitigate, but in terms of test performance, CCN hurts more. Overfitting to random noise degenerates the generalization most, as demonstrated in the work [1] (Section 3.2 and Figure 4). Still, IDN is more challenging to mitigate since it is easier to overfit. Moreover, [1] provides **rigorous motivations for going beyond CCN and studying IDN. Notably, it is shown that the noise in Clothing1M is impossible to be feature *in*dependent, with a probability lower than $10^{-21250}$** (Section 2.2 and Theorem 1).\n\n[1] Beyond Class-Conditional Assumption: A Primary Attempt to Combat Instance-Dependent Label Noise. arxiv.org/abs/2012.05458. (Accepted by AAAI 2021)."}, "signatures": ["~Pengfei_Chen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Pengfei_Chen1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ZPa2SyGcbwh", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649464144, "tmdate": 1610649464144, "id": "ICLR.cc/2021/Conference/Paper2811/-/Comment"}}}, {"id": "x2CwyAaZSzk", "original": null, "number": 1, "cdate": 1610040486097, "ddate": null, "tcdate": 1610040486097, "tmdate": 1610474091484, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "This paper studies the problem of learning from data that have been corrupted by label noise. The authors define a natural data-dependent noise condition, that allows the noise rate to be large close to the decision boundary, and provide a simple iterative method that eventually converges to the Bayes optimal classifier. The method is evaluated on both synthetic a real datasets. There was a consensus among the reviewers that this is an interesting contribution and I propose acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040486084, "tmdate": 1610474091468, "id": "ICLR.cc/2021/Conference/Paper2811/-/Decision"}}}, {"id": "INie94035y", "original": null, "number": 6, "cdate": 1605680732052, "ddate": null, "tcdate": 1605680732052, "tmdate": 1605750686946, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "H0NCv1gozQ", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment", "content": {"title": "Additional real-world datasets are added", "comment": "**Q:**  Ideally, the paper should include another 2-3 real-world domains in this evaluation. At the very least, they could add to the evaluation the CUB-200 dataset from [Yi & Wu, 2019]\n\n**A:**  Thanks for the suggestion. We conduct experiments on three additional real-world datasets, including CUB-200.\n\n(1) Food-101N [1], which is a dataset for food classification. It consists of 310k training images collected from the web. The estimated label purity is 80%. Following [1], the classification accuracy is evaluated on the Food-101 [2] test set, which contains 25k images with curated annotations. We use ResNet-50 pretrained on ImageNet. We train the network for 30 epochs with SGD optimizer. The batch size is 32 and the initial learning rate is 0.005, which is divided by 10 every 10 epochs. We also adopt simple data augmentation procedures, including random horizontal fiip, and resizing the image with a short edge of 256 and then randomly cropping a 224x224 patch from the resized image. We repeat the experiments with 3 random trials and report the mean value and standard deviation. The results are as follows:\n\n|||\n|:---|---:|\n|Standard         | 81.67|\n|CleanNet [1]   | 83.95|\n|PCL (ours)      | 85.28\u00b10.04|\n|||\n\nOur method achieves better performance than CleanNet. Note that we do not use the extra 55k manually cleaned training data, which are utilized by CleanNet for noise detection.\n\n(2) ANIMAL-10N [3], which is a real-world noisy dataset consisting of human-labeled online images for 10 animals with confusing appearance. The estimated label noise rate is 8%. There are 50,000 training and 5,000 testing images. Following [3], we use VGG-19 with batch normalization. The SGD optimizer is employed. Following [3], we train the network for 100 epochs and use an initial learning rate of 0.1, which is divided by 5 at 50% and 75% of the total number of epochs. We repeat the experiments with 3 random trials and report the mean value and standard deviation. The results are as follows:\n\n|||\n|:---|---:|\n|Standard         | 79.4\u00b10.14|\n|SELFIE [3]      | 81.8\u00b10.09|\n|PCL (ours)      | 83.4\u00b10.43|\n|||\n\nWe observe that our method achieves better performance than SELFIE.\n\n(3) Following the suggestion of AnonReviewer1, we conduct experiments on the CUB-200 [4] which is for fine-grained bird classification. The labels in this dataset are considered clean. We use ResNet-50 pretrained on ImageNet. Simple data augmentation procedures are also applied, including horizontal random flip, and resizing the image to 256x256 and then randomly cropping a 224x224 patch from the resized image. We use SGD optimizer and batch size of 16. The initial learning rate is 0.002, which is divided by 10 after 100, 120 and 140 epochs. The total number of training epochs is 160.\n\nThe standard method (using cross entropy loss alone) achieves classification accuracy 79.91\u00b10.04. Our PCL method has accuracy 79.99\u00b10.04. The performance of our method does not drop for a dataset without any noise. \n\n[1] Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for scalable image classifier training with label noise. In CVPR, 2018.\n\n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013 mining discriminative components with random forests. In ECCV, 2014.\n\n[3] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. SELFIE: Refurbishing Unclean Samples for Robust Deep Learning. In ICML, 2019.\n\n[4] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.\n\n[5] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In CVPR, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZPa2SyGcbwh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844233, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment"}}}, {"id": "80tbqmvbhzs", "original": null, "number": 4, "cdate": 1605680279741, "ddate": null, "tcdate": 1605680279741, "tmdate": 1605750536683, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "BvmU2V48tyB", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment", "content": {"title": "Provide more experiments, ablation studies and discussions", "comment": "**Q1:** It is better to show the connection between the polynomial margin diminishing noise and the other noises.\n\n**A:** In Section 2.1, we listed several existing types of noise: uniform and BCN. In most of the feature domain, our PMD noise is very flexible and generalizes these known noise types. The only exception is the region where $\\eta$ is highly confident (almost 0 or almost 1). There our PMD noise requires a nontrivial noise upper bound.\n\n\n**Q2:**  It is necessary to show the details about $\\beta$, which has a direct influence on the results.\n\n**A:** On CIFAR-10, the effect of the step size $\\beta$ on the model performance is shown below (we set $\\exp(\\theta)$ to 0.3):\n\n| $\\beta$ | 0.05 | 0.1| 0.2 | 0.3 |\n|:----:|:----:|:----:|:----:|:----:| \n|Type-I   noise  | 83.58 | 83.04 | 83.28 | 83.31|\n|Type-II  noise  | 80.94 | 81.18 | 80.98 | 80.86|\n|Type-III noise  | 81.91 | 81.75 | 82.13 | 82.39|\n||\n\nWe observe that our method is not sensitive to $\\beta$.\n\n\n**Q3:** It is better to show the results without any noise.\n\n**A:**  Thanks for the suggestion. We applied our method to CIFAR-10 and CIFAR-100 without any label noise. It achieved similar performance as the standard method. The results are as follows:\n\n||||\n|:----:|:----:|:----:|\n|CIFAR-10   | Standard    | 93.29\u00b10.59|\n|CIFAR-10   | PCL (ours) | 93.60\u00b10.02|\n|CIFAR-100 | Standard    | 74.30\u00b10.39|\n|CIFAR-100 | PCL (ours) | 74.25\u00b10.09|\n||\n\n**Q4:** Since the polynomial margin diminishing noise is general, whether the polynomial margin diminishing noise can represent any noise functions theoretically. \n\n**A:** As discussed in Q1, the PMD noise can represent arbitrarily noise function in most of the feature domain. The only exception is the area where $\\eta$ is highly confident (almost 0 or almost 1). \n\n**Q5:** Some details about the experiments are not clear, such as the experimental settings of the compared methods.\n\n**A:** Regarding common hyperparameters, we use the same setting for all methods: batch size = 128, total epoch = 180, and SGD optimizer with initial learning rate = 0.01. Some other model-specific hyperparameters are tuned based on the recommendation of the papers and validation performance.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZPa2SyGcbwh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844233, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment"}}}, {"id": "xImikjL8wJE", "original": null, "number": 7, "cdate": 1605680898192, "ddate": null, "tcdate": 1605680898192, "tmdate": 1605750423529, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "uPoXsoSsvts", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment", "content": {"title": "Literature part is expanded and synthetic experiment is added", "comment": "**Q1:** The authors mention Menon et al. in their \"related works\" section, but dismiss it immediately as \"...it does not recalibrate individual data based on their contexts, and thus are not as effective as other deep-learning-based methods in practice.\" A citation here is needed showing this point, or further experiments.\n\n**A:** Thank you for pointing this out. Let us revise our statement as follows: (Menon et al. 2018) provides an elegant theoretical framework, showing that losses fulfilling certain conditions naturally resist instance-dependent noise. The method can achieve even better theoretical properties (i.e., Bayes-consistency) with stronger assumption on the clean posterior probability $\\eta$. In practice, this method has not been extended to deep neural networks. We empirically compare this method with ours. We follow the experimental setting of Menon et al. [1] and inject different level BCN noise into MNIST data set. We observed that PCL outperforms the Isotron method by Menon et al. [1]. The results are as follows. \n\n| MNIST  | Menon\u2019s Isotron  |  PCL  |\n|:---:|:---:|:---:|\n|30%  | 95.65 \u00b1 0.00 | 97.55 \u00b1 1.07 |\n|40%  | 87.68 \u00b1 0.00 | 91.98 \u00b1 2.66 |\n|45%  | 80.88 \u00b1 0.03 | 86.69 \u00b1 3.48 |\n||||\n\nWe have updated the discussion of this method in the related work section accordingly.\n\n**Q2:** The related works section is very short and is essentially a list of other approaches. A more thorough discussion would help readers.\n\n**A:** We added more discussion of the literature in the related work section.\n\n**Q3:** A selection of baseline methods are chosen for comparison to the proposed approach in the empirical evaluation, but the particular choices aren't discussed in any detail. Some of the methods were mentioned in the related works section, but some are not (why not?), and neither section explains some of the choices.\n\n**A:** We select a collection of recent methods representing different strategies. Co-teaching+ is a data-selection method: it selects the clean data by training two networks simultaneously, and then updates the networks on the selected clean data. GCE and SL aim to combat the label noise with noise-tolerant losses. LRT not only detects the noisy labels but also corrects them. This way more training data are utilized, compared with data-selection methods. We think these methods are reasonably representative of the recent developments in learning with label noise. \n\nThanks for the suggestion regarding the related work. We add discussions of all the selected baselines in the related work section. Also, we will provide more details and discussions on these methods.\n\n[1] Learning from Binary Labels with Instance-Dependent Corruption. Aditya Krishna Menon, Brendan van Rooyen, Nagarajan Natarajan. arXiv preprint arXiv:1605.00751\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZPa2SyGcbwh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844233, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment"}}}, {"id": "lgwRRZV848i", "original": null, "number": 8, "cdate": 1605713454006, "ddate": null, "tcdate": 1605713454006, "tmdate": 1605713454006, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment", "content": {"title": "Thank you for your constructive and helpful comments.", "comment": "We thank reviewers for constructive and helpful comments. We are glad all reviewers are thinking positively about this paper. Per their suggestions, we have added more experimental results on real-world dataset and ablation studies. We have also improved the presentation of our manuscript. Below we will address the concerns of each reviewer one-by-one.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZPa2SyGcbwh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844233, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment"}}}, {"id": "Q8Rrye1XKSn", "original": null, "number": 5, "cdate": 1605680518373, "ddate": null, "tcdate": 1605680518373, "tmdate": 1605680540851, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "lovzCBL7xba", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment", "content": {"title": "We've fixed typos in the paper", "comment": "**Q:** Typos in the paper\n\n**A:** Thank you so much for the positive feedback. We have fixed the typos according to your comments. Please refer to the paper for the update.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZPa2SyGcbwh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844233, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment"}}}, {"id": "Von_rAGLiqu", "original": null, "number": 3, "cdate": 1605679915411, "ddate": null, "tcdate": 1605679915411, "tmdate": 1605679934199, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "xj-UnJM2rLF", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment", "content": {"title": "Thanks for your suggestion", "comment": "**Q:** Please consider referencing/comparing to these papers.\n\n**A:**  Thanks for the suggestion. We have added discussion of these works. Please see the related work section for the update.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZPa2SyGcbwh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2811/Authors|ICLR.cc/2021/Conference/Paper2811/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844233, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Comment"}}}, {"id": "xj-UnJM2rLF", "original": null, "number": 1, "cdate": 1605042953862, "ddate": null, "tcdate": 1605042953862, "tmdate": 1605042953862, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Public_Comment", "content": {"title": "Please consider referencing/comparing to these more recent works", "comment": "I would like to point out that our work (Amid et al. 2019a) extends the Generalized CE loss (Zhang and Sabuncu 2018) by introducing two temperatures t1 and t2 which recovers GCE when t1 = q and t2 = 1. Our more recent work, called the bi-tempered loss (Amid et al. 2019b) extends these methods by introducing a proper (unbiased) generalization of the CE loss and is shown to be extremely effective in reducing the effect of noisy examples. Please consider referencing/comparing to these papers.\n\n(Amid et al. 2019a) Amid et al. \"Two-temperature logistic regression based on the Tsallis divergence.\" In The 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. \n\n(Amid et al. 2019b) Amid et al. \"Robust bi-tempered logistic loss based on Bregman divergences.\" In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n"}, "signatures": ["~Ehsan_Amid1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Ehsan_Amid1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZPa2SyGcbwh", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/Authors", "ICLR.cc/2021/Conference/Paper2811/Reviewers", "ICLR.cc/2021/Conference/Paper2811/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024959668, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Public_Comment"}}}, {"id": "uPoXsoSsvts", "original": null, "number": 1, "cdate": 1603086894735, "ddate": null, "tcdate": 1603086894735, "tmdate": 1605024126378, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review", "content": {"title": "A good practical method and theoretical contribution for learning from feature dependent label noise", "review": "The paper presents a learning method for the scenario of feature dependent label noise. A framework where label noise diminishes away from the decision boundary is established and a relabeling strategy based on this by relabeling highly confident points is proposed.  The method is a straight-forward adaptive method which the authors both theoretically and empirically explore in detail.\n\n# Pros\n\n- The approach is simple and easy to implement on top of existing methods\n- The authors support their simple method with some very nice theoretical results\n- Good empirical evaluation showing competitive performance to other methods\n- The paper is clearly written and easy to read\n- Citations place the work well among existing literature\n\n# Cons\n\n- The authors mention Menon et al. in their \"related works\" section, but dismiss it immediately as \"...it does not recalibrate individual data based on their contexts, and thus are not as effective as other deep-learning -based methods in practice.\" A citation here is needed showing this point, or further experiments.\n- The related works section is very short and is essentially a list of other approaches. A more thorough discussion would help readers.\n- A selection of baseline methods are chosen for comparison to the proposed approach in the empirical evaluation, but the particular choices aren't discussed in any detail. Some of the methods were mentioned in the related works section, but some are not (why not?), and neither section explains some of the choices.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088177, "tmdate": 1606915796413, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2811/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review"}}}, {"id": "H0NCv1gozQ", "original": null, "number": 2, "cdate": 1603677979266, "ddate": null, "tcdate": 1603677979266, "tmdate": 1605024126311, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review", "content": {"title": "The authors introduce an iterative approach to learning from data with noisy labels under realistic circumstances (rather than iid assumptions).", "review": "The work appears to original, and it deals with an important topic (ie, how to deal with noisy labels). The paper is well written and reasonably easy to follow. Figure 1 is extremely helpful in providing an intuitive explanation that helps understanding the rest of the paper.   \n\nEven though the empirical validation is reasonably thorough for a paper that also includes a theoretical analysis of the novel approach, it is here that the authors could improve the paper the most. The main weakness of this section is that even though the results on the synthetic datasets are strong, the ones on the Clothing1M domain are significantly less so, with two competing approaches (PENCIL  & MLTN) within 0.53% - 0.55% from the PCL result. Ideally, the paper should include another 2-3 real-world domains in this evaluation. At the very least, they could add to the evaluation the CUB-200 dataset from [Yi & Wu, 2019], on which PENCIL was evaluated for robustness on less noisy data.  Adding two more synthetic domains such as MNIST and ModelNet40, as done in [Zheng et al, 2020], is another possibility - but far less appealing.   ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088177, "tmdate": 1606915796413, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2811/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review"}}}, {"id": "lovzCBL7xba", "original": null, "number": 3, "cdate": 1603739505093, "ddate": null, "tcdate": 1603739505093, "tmdate": 1605024126248, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review", "content": {"title": "Simple and effective label de-noising algorithm", "review": "**Summary of paper**\n\nThe authors introduce a data-relabeling method that they claim is the first that both allows for data-dependent noise and is theoretically guaranteed to converge to an optimal model.\n\nThe authors introduce a novel family of label noise, called Polynomial Margin Diminishing (PMD), which defines a polynomially-decreasing upper bound on the label noise as the true label probability is above some threshold and as it approaches 1. Below the threshold (when the true label probability is closer to 0.5), the noise is unbounded.\n\nThe authors introduce an algorithm, \"Progressive Label Correction\", which iteratively flips the training labels of examples for which the model's confidence is above a threshold. The threshold decreases over time, so that only the most confident examples are flipped at first, and then the less-confident examples are flipped later.\n\nThe authors prove (Theorem 1) that under the assumption of PMD label noise distribution, as well as Assumption 1 concerning the flexibility of the hypothesis class and the continuity of the true label's conditional distribution, then their Progressive Label Correction algorithm asymptotically approaches a set of corrected labels that match the true (de-noised) labels with high probability.\n\nThe authors experimentally show that their algorithm consistently outperforms 5 alternatives on CIFAR-10 and CIFAR-100 datasets with various types of synthetic noise, both feature-dependent and hybrid feature-depedent/indepdendent. Finally, they show their algorithm outperforms 10 alternatives on a real-world dataset (Clothing1M) with unknown noise.\n\n**Conclusions**\n\nQuality: Overall I like this paper. It's a pretty simple algorithm to implement, and it seems to be quite effective in practice and have nice theoretical properties.\n\nClarity: The paper is structured very well. It is easy to follow the narrative and high-level ideas. There are a few minor typos (see below for some examples). The theory is relatively easy to follow.\n\nOriginality: I'm not familiar with the related work, but this appears to be original/novel from my limited perspective.\n\nSignificance: This algorithm can potentially be used to improve test accuracy on any supervised learning task, so the intended audience is quite large. The improvement on both synthetic and real data seems quite large. It's hard to tell whether the results were cherry-picked at all, but they are impressive.\n\n**Minor comments**\n\nFigre 1 caption has a typo: \"Red dots are the data that remain incorrect. that remain un-corrected and are closer to the\ndecision boundary.\"\n\nIn Section 2.1, \"illustrate the upperbound (red curve)\", but the figure actually shows an orange curve (not red) for the upper bound.\n\nFigure 2 caption, \"closed to 0 or 1\" should be \"close to 0 or 1\". Also, \"a equal probability\" should be \"an equal probability\".", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088177, "tmdate": 1606915796413, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2811/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review"}}}, {"id": "BvmU2V48tyB", "original": null, "number": 4, "cdate": 1603764414568, "ddate": null, "tcdate": 1603764414568, "tmdate": 1605024126187, "tddate": null, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "invitation": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review", "content": {"title": "This paper proposes a progressive label correction algorithm by correcting labels and refine the model iteratively.", "review": "Comments:\nLabel noise is very frequently in many real world applications. However, the noise can be with different distributions. If we build the learning model under a certain distribution, it is difficult to capture the discriminative information. In this paper, without assuming that the noise is a certain distribution, the proposed method can handle the general noise, and it mainly target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. The experimental results show that the proposed method is promising. Meanwhile, the theoretical analysis of the proposed method is well inferred.\n\nStrong points:\n[1] The theoretical foundation of the proposed method is strong. \n[2] The experimental results of the proposed method are promising.\nWeak points:\n[1] Some details about the experiments are not clear, such as the experimental settings of the compared methods.\n[2] It is better to show the connection between the polynomial margin diminishing noise and the other noises.\n\nAccept reason:\n[1] The paper has shown a promising performance than several state-of-the-art methods. The noise assumption is more general than the traditional types. Hence, the paper may provide a novel way to deal with noise labels.\n\nFeedbacks:\n[1] I found that the step size \\beta has an influence on the threshold \\theta, and how to set it. It is necessary to show the details about \\beta, which has directly influence on the results.\n[2] The noise in the experiments is more than 30%. Whether the proposed method is suitable to the high-level noise. It is better to show the results without any noise.\n[3] Since the polynomial margin diminishing noise is general, whether the polynomial margin diminishing noise can represent any noise functions in theoretical.\n[4] The details of all the compared method may need to be provided.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2811/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2811/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "authorids": ["~Yikai_Zhang1", "~Songzhu_Zheng1", "~Pengxiang_Wu1", "~Mayank_Goswami1", "~Chao_Chen1"], "authors": ["Yikai Zhang", "Songzhu Zheng", "Pengxiang Wu", "Mayank Goswami", "Chao Chen"], "keywords": ["Noisy Label", "Deep Learning", "Classification"], "abstract": "Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.", "one-sentence_summary": "We propose a progressive label correction approach for noisy label learning task.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_with_featuredependent_label_noise_a_progressive_approach", "pdf": "/pdf/ce4a88b01e7209978fd5f9f84e8f485b4d2fde91.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning with Feature-Dependent Label Noise: A Progressive Approach},\nauthor={Yikai Zhang and Songzhu Zheng and Pengxiang Wu and Mayank Goswami and Chao Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZPa2SyGcbwh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZPa2SyGcbwh", "replyto": "ZPa2SyGcbwh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2811/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538088177, "tmdate": 1606915796413, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2811/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2811/-/Official_Review"}}}], "count": 15}