{"notes": [{"id": "04ArenGOz3", "original": "x1bCmu55_Fus", "number": 3297, "cdate": 1601308366214, "ddate": null, "tcdate": 1601308366214, "tmdate": 1613928430673, "tddate": null, "forum": "04ArenGOz3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "V_LKT32Z3xV", "original": null, "number": 1, "cdate": 1610040465143, "ddate": null, "tcdate": 1610040465143, "tmdate": 1610474068615, "tddate": null, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper proposes to predict sets using conditional density\nestimates. The conditional densities of the reponse set given the\nobserved features is modeled through an energy based function. The\nenergy function can be specified using tailored neural nets like deep\nsets and is trained trough approximate negative log likelihoods using\nsampling. \n\nThe paper was nice to read and was liked by all the reviewers. The one\nthing that stood out to me was the emphasis on multi-modality. (multi\nappears 51 times).  This could be toned down because little is said\nabout the quality relative to the true p(Y | x) and the focus is\nmainly on the lack of this in existing work."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "04ArenGOz3", "replyto": "04ArenGOz3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040465128, "tmdate": 1610474068600, "id": "ICLR.cc/2021/Conference/Paper3297/-/Decision"}}}, {"id": "Tgwd_d-_Npm", "original": null, "number": 1, "cdate": 1603860213547, "ddate": null, "tcdate": 1603860213547, "tmdate": 1606797164793, "tddate": null, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review", "content": {"title": "A novel framework for set prediction", "review": "Authors propose a new method for formulating set prediction tasks. They propose to use a noisy energy-based model with langevin mcmc + noisy startup as their model. The can approximate the gradient of the likelihood function by computing the enery of ground truth pairs and energy of synthesized pairs where the target is sampled from the model distribution.\n\nA major advantage of this formulation with contrastive GAN style loss (competing synthesized vs gt and improving synthesized) over the previous related work which optimized a distance metric suitable for sets is that it works on a wider range of tasks. Experiments show that most common metrics (hungarian and chamfer distance) both fail in certain tasks where the energy based contrastive approach prevails. Apparently they only need 1 synthetic set to approximate the gradient of partition function which is surprising. Given this, one expects their computation cost and training time to be on par with metric based methods. Unfortunately such analysis is currently missing.\n\nOne of their main contributions is advocating for adding gaussian noise to the first 80% of the steps. They reason it is an effective way of covering multi modal scenarios. They achieve impressive results on anomaly detection tasks which they attribute mainly to their multi modal abilities. It would be interesting to have an ablation on the anomaly detection task with no start up noise in the mcmc algorithm.\n\nAnother question that arises from their stochastic generation is regarding mnist autoencoding and clevr task. It is not clear if they compute the results based on how many samples per example.\n\nPros: The paper is relatively well written. \nThey cover several different tasks in their experimental analysis.\nThe merits of optimizing for a distribution over the sets is explained well, is intuitive and experiments shows that it works.\nThey introduce several novel ideas that can be significant in this line of research later on. \n\ncons: \nlack of computation cost, training time, inference time analysis. It is not clear if this method scales to larger tasks with many more elements in the set. For example they can tackle shapenet point cloud reconstruction as a larger scale dataset.\nTheir datasets are toy scale in general, but given that their method consistently outperforms previous work is not a major shortcoming. \n\nniit: The overloading of variable Z is confusing. In page 2, Z is both the partitioning function and a random noise added to the transition function for smoothing the gradients. \n\n--------------------Post Author Response\nThank you for addressing my concern about complexity and adding Fig. 6. It seems that it is still better than baselines in terms of complexity.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078312, "tmdate": 1606915790745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3297/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review"}}}, {"id": "NSqu2Ip5xx_", "original": null, "number": 3, "cdate": 1603909821109, "ddate": null, "tcdate": 1603909821109, "tmdate": 1606474576854, "tddate": null, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review", "content": {"title": "Conditional density estimation and noisy optimization for set prediction", "review": "## Summary\nThis paper introduces a two step process to learning models for set prediction tasks (such as identifying locations of objects in images, generating polygons or point clouds).\n\nThe first phase learns an energy model of the probability of a given set Y given features x, modeled using deep networks. This allows to use the negative log-likelihood as a loss rather than assignment based losses, which allows to model multiple plausible sets given a specific choice of features. To learn this model, the authors approximate the NLL by sampling from data, and, when necessary, generating synthetic sets Y from the current model iteration.\n\nIn the set prediction phase, the learned energy model is optimized using noisy SGD during the first few iterations in order to potentially sample from multiple different optima.\n\n## Quality\nThis paper is well motivated, and the experiments clearly show the benefits of avoiding assignment-based loss functions. However, some statements and design choices would, in my opinion, have benefited from deeper theoretical or empirical justifications. \n\nMy main question is regarding the use of the noise Z in section 2.2. The noise is motivated by the discovery of multiple optima. Do the authors have any results showing that this is indeed the downstream effect of the noise (e.g., showing a greater diversity in generated polygons)? Noisy SGD is also known to help with non-convex optimization problems, regardless of identifying multiple minima.\n\n## Clarity\nOverall this paper was clear, but several clarifications could be added to improve readability, such as providing the mathematical form of the Hungarian and Chamfer losses, and avoiding the use of $Z$ for both the partition function $Z(x; \\theta)$ and the random variable $\\sim \\mathcal N(0, \\epsilon I)$.\n\nI am also curious to know how the sets are encoded: previous work (e.g., Belanger et al.) looked at tasks where the ground set is finite, and the zero-one encoding of the set is made continuous for the purpose of the optimization, then projected back into discrete space. However, given the chosen tasks in this paper, it seems like the encodings are simply continuous coordinates, with the cardinality being constrained. Is this correct?\n\nFinally, is there a minus sign missing from the derivation at Eq. (4)? \n\n## Originality\nMy understanding is that this paper proposes two key contributions:\n - learning the conditional energy density by sampling and generating synthetic examples\n - using noisy SGD steps for the first $S$ iterations during prediction.\n\nIf my understanding is correct, I would have like to see these two contributions explored in more detail. The learning procedure seems similar to noise contrastive estimation (but with a more subtle definition of \"noisy samples\"). Is this point of view correct? If so, did you explore other noise distributions for the synthetic samples? \n\nAs mentioned above, I would also be curious to know if the noisy SGD during estimation leads to improved diverse mode sampling, as well.\n\n# Significance\nModeling distributions over sets with neural networks is, as the authors point out, a difficult problem due in part to  permutation invariance. The use of the NLL under a distribution over sets as a loss function is an elegant way of avoiding this problem, and generalizes approaches that already assume a more specific energy form. The significance of this work lies in how this NLL is optimized, and how good samples are then drawn from the learned distribution. However, I believe that a more thorough investigation of how the proposed method compares to other methods to learn the energy density or optimize the sampling process would increase this paper's impact.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078312, "tmdate": 1606915790745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3297/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review"}}}, {"id": "4ZSFBXRO5Cu", "original": null, "number": 5, "cdate": 1605874219635, "ddate": null, "tcdate": 1605874219635, "tmdate": 1605874479163, "tddate": null, "forum": "04ArenGOz3", "replyto": "Tgwd_d-_Npm", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment", "content": {"title": "Response for AnonReviewer2", "comment": "We thank AnonReviewer2 for the detailed and constructive feedback. \n\n**Apparently they only need 1 synthetic set to approximate the gradient of partition function which is surprising. Given this, one expects their computation cost and training time to be on par with metric based methods. Unfortunately such analysis is currently missing.**\n\nWe address the computation cost and training time in the first part of section 5.1 of the revised paper:\n\"DESP offers non-trivial computation cost trade-offs, when we compare it to a baseline trained via assignment-based set losses. We identify three main factors that are crucial and specific to our analysis: 1. Number of transition steps $T$, 2. Complexity of the set neural network and 3. Complexity of the loss function. Similar to baselines that form predictions with an inner optimization (Zhang et al. 2019; Belanger et al. 2017), DESP\u2019s training & inference time scale linearly with $T$. Though, in practice DESP requires a larger $T$ to achieve reliable sampling quality, potentially resulting in longer training and inference times.\"\n\n**It would be interesting to have an ablation on the anomaly detection task with no start up noise in the mcmc algorithm.**\n\nIndeed, an ablation over the anomaly detection task offers additional insight into the prediction algorithm. Removing the start up noise has little effect on the performance ($<0.01$ differences). The prediction procedure finds an easy substitute for the missing initial noise in the subsequent noise samples $U^{(1)}, U^{(2)}, \\ldots$.\nWe added figure 6 to the result discussion of the subset anomaly detection experiment in appendix C, accompanied by the following paragraph:\n\"We examine the effect of varying proportions of stochastic steps in the prediction procedure in figure 6. Similar to what we observe in the other experiments, $\\frac{S}{T}=0.8$ offers the best $F_1$ score. For the fully deterministic case, every predicted set is identical to one another, which is reflected in the low recall score at $\\frac{S}{T}=0$.\"\n\n**Another question that arises from their stochastic generation is regarding mnist autoencoding and clevr task. It is not clear if they compute the results based on how many samples per example.**\n\nThe revised paper is updated with the following clarifications:\nIn section 5.3, Point-Cloud Auto-Encoding: \u201cWe evaluate our approach based on a single prediction per example.\u201d\nIn section 5.4, Object Set Prediction on CLEVR: \u201cOur approach is evaluated based on a single prediction per image.\u201d\n\n**lack of computation cost, training time, inference time analysis. It is not clear if this method scales to larger tasks with many more elements in the set. For example they can tackle shapenet point cloud reconstruction as a larger scale dataset.**\n\nWe address the scaling to large set sizes in the second part of section 5.1 of the revised paper:\n\"The complexity of the set neural network is crucial for determining the computation cost on large set sizes $c$. By choosing a set neural network with time and memory complexity in $\\mathcal{O}(c)$, such as DeepSets (Zaheer et al. 2017), DESP can accommodate large set sizes. In comparison to the baselines, DESP avoids the additional computational burden imposed by an assignment-based set loss, which is in $\\mathcal{O}(c^2)$ for the Chamfer loss and in $\\mathcal{O}(c^3)$ for the Hungarian loss.\"\n\n**The overloading of variable $\\mathbf{Z}$ is confusing. In page 2, $\\mathbf{Z}$ is both the partitioning function and a random noise added to the transition function for smoothing the gradients.**\n\nThank you for pointing out the overloading of the variable $\\mathbf{Z}$. We fix this in the updated paper, by introducing the symbol $\\mathbf{U}^{(t)}$ for the noise in equation 5 and 6. The additional superscript $(t)$ clarifies that it is sampled for each step."}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "04ArenGOz3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3297/Authors|ICLR.cc/2021/Conference/Paper3297/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838974, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment"}}}, {"id": "LUUQGAVwzbO", "original": null, "number": 4, "cdate": 1605874176974, "ddate": null, "tcdate": 1605874176974, "tmdate": 1605874404054, "tddate": null, "forum": "04ArenGOz3", "replyto": "bihrmXMqbzO", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment", "content": {"title": "Response for AnonReviewer4", "comment": "We thank AnonReviewer4 for the detailed and actionable feedback.\n\n**Ideally, I would have liked to have seen some theoretical results based on a simplified setup, indicating more rigorously the ability of this approach in provably adapting to ill-conditioned metrics and learning multimodal distributions. A variety of results for Langevin MCMC exist that can be exploited to provide this analysis. In its current form, I think the paper is acceptable for submission and is relevant to the community, but some more theoretical analysis will definitely add value to a portion of readers like myself.**\n\nWe added the following paragraph in section 2.1 to the revised paper:\n\"The proper formulation of the Langevin MCMC algorithm multiplies the gradient in equation 5 by a factor $\\epsilon$ and further requires a Metropolis-Hastings acceptance step (Neal et al. 1993). We forgo both of these components in favor of increased efficiency, but at the cost of forfeiting theoretical guarantees for desirable properties such as not being trapped in a subset of the sampling space, i.e. ergodicity. Discarding all but the last sample $Y^{(T)}$ of each chain constitutes a non typical usage that undermines the usual importance of ergodicity. Notably, this weakens the hard to meet requirement for the sampler to mix between multiple modes in a single MCMC chain, making it sufficient for multiple independently sampled chains to find different local modes. Although the fixed cutoff at $T$ and missing Metropolis-Hastings update result in a biased sampler, previous works have demonstrated the feasibility of training generative models on images with similar Langevin MCMC methods (Xie et al., 2016; 2018; Nijkamp et al., 2020; Du & Mordatch, 2019; Grathwohlet al., 2019).\"\n\n**Some questions/feedback for the authors: I think there may be a negative sign missing from Equation 4.**\n\nThank you for pointing out the missing negative sign, which we fixed in the updated paper.\n\n**Also, it may be useful to further explain why contrasting energy for real and synthesized examples makes sense. I know this is a common setup for structured prediction problems, but I think it is worth paying some lip service in the text.**\n\nIn order to provide a better intuition behind the training objective, we added the following sentence in section 2.1: \n\u201cThe objective is reminiscent of the discriminator\u2019s loss in generative adversarial networks (Goodfellow et al., 2014), where a real sample is contrasted to a sample synthesized by the generator network.\u201d\n\n**Furthermore, I found it somewhat odd that Equation 5 was framed as an instantiation of Langevin MCMC (perhaps it may be more useful to frame it as an instance of SGLD?), but not equation 6 (at least 6a).**\n\nWe suspect that the usage of the term Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011) is inconsistent in the literature. Following the original work (Welling & Teh, 2011), we understand SGLD to refer to the specific case when the gradient is approximated by a stochastic version, e.g. using mini-batches. Our usage of the langevin dynamics does not require stochastic gradients, motivating our preference for the term Langevin MCMC. \nWe added a clarification after equation 6: \n\u201cNote that the partial derivative $\\frac{\\partial}{\\partial \\mathbf{Y}}E_\\mathbf{\\theta}(\\mathbf{x},\\mathbf{Y}^{(t)})$ is not stochastic and can be computed independent of a mini-batch.\u201d\nWe agree with the reviewer that framing 6a as Langevin MCMC as well improves the clarity and it is included in the revision after the equation 6: \n\u201cOne interpretation of the prediction procedure is: 1. Langevin MCMC sample $\\mathbf{Y}^{(S)}$ based on the energy $E_\\mathbf{\\theta}$ and 2. Refine the sample via gradient descent, such that $\\mathbf{Y}^{(T)}$ is a local minimum of $E_\\mathbf{\\theta}$ that is close to $\\mathbf{Y}^{(S)}$.\u201d\n\n**This also introduces a natural follow up of whether doing Hamiltonian Monte Carlo based methods makes any difference in mixing efficiency on the experimental setups.**\n\nPreliminary tests on including momentum during sampling (Hamiltonion Monte Carlo) destabilized training. While it appears plausible that Hamiltonion Monte Carlo should improve sampling efficiency, it is currently unclear if it is desirable from a training perspective. More specifically, it introduces larger changes to the state space, which we speculate may be the cause for destabilizing training. We believe further examination into this topic would be of great interest, but see it as outside the scope of the current paper.\n\n**Finally, Table 1 was somewhat confusing. The caption explains that Chamfer is bad on Polygons, and Hungarian is bad for Digits, but the numbers don't seem to follow that description. Maybe the rows or columns got switched?**\n\nThank you for pointing out the error in Table 1. Indeed, the column names were mixed up and we fixed the table in the updated paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "04ArenGOz3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3297/Authors|ICLR.cc/2021/Conference/Paper3297/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838974, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment"}}}, {"id": "N9NGTg0Ied-", "original": null, "number": 3, "cdate": 1605874133093, "ddate": null, "tcdate": 1605874133093, "tmdate": 1605874367438, "tddate": null, "forum": "04ArenGOz3", "replyto": "NSqu2Ip5xx_", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment", "content": {"title": "Response for AnonReviewer1", "comment": "We thank AnonReviewer1 for the thorough and helpful feedback. \n\n**In the set prediction phase, the learned energy model is optimized using noisy SGD during the first few iterations in order to potentially sample from multiple different optima. [...] My main question is regarding the use of the noise Z in section 2.2. The noise is motivated by the discovery of multiple optima. Do the authors have any results showing that this is indeed the downstream effect of the noise (e.g., showing a greater diversity in generated polygons)?**\n\nWe clarify that during both the first and second phase, we do not require stochastic gradients when sampling or optimizing for $\\mathbf{Y}$. We added in section 2.2: \n\u201cNote that the partial derivative $\\frac{\\partial}{\\partial \\mathbf{Y}}E_\\mathbf{\\theta}(\\mathbf{x},\\mathbf{Y}^{(t)})$ is not stochastic and can be computed independent of a mini-batch. Thus the sole source of randomness lies with the addition of the noise $\\mathbf{U}^{(t)}$, resulting in a prediction procedure that allows for multiple different predictions.\u201d\nFurthermore, we added figure 6 to the result discussion of the subset anomaly detection experiment in appendix C, accompanied by the following paragraph:\n\"We examine the effect of varying proportions of stochastic steps in the prediction procedure in figure 6. Similar to what we observe in the other experiments, $\\frac{S}{T}=0.8$ offers the best $F_1$ score. For the fully deterministic case, every predicted set is identical to one another, which is reflected in the low recall score at $\\frac{S}{T}=0$.\"\n\n**Overall this paper was clear, but several clarifications could be added to improve readability, such as providing the mathematical form of the Hungarian and Chamfer losses, and avoiding the use of Z for both the partition function Z(x;\u03b8) and the random variable \u223cN(0,\u03f5I).**\n\nWe add the suggested formulation of the set losses to section 5, supplemented by further discussion in Appendix A. We fix double usage of $\\mathbf{Z}$, by introducing $\\mathbf{U}^{(t)}$ for the noise in equation 5 and 6. The additional superscript $(t)$ clarifies it is sampled for each step. Thank you.\n\n**I am also curious to know how the sets are encoded: previous work (e.g., Belanger et al.) looked at tasks where the ground set is finite, and the zero-one encoding of the set is made continuous for the purpose of the optimization, then projected back into discrete space. However, given the chosen tasks in this paper, it seems like the encodings are simply continuous coordinates, with the cardinality being constrained. Is this correct?**\n\nIndeed, we adopt the same strategy as Belanger et al. (2017) for discrete variables. We clarify the subset anomaly detection task in appendix C: \n\u201cIn order to apply our framework to the discrete targets, we optimize and sample over a convex relaxation of the target domain: $o_i\\in[-1,+1]$.\u201d\nWe clarify in section 5.2: \n\u201cTo accommodate different cardinalities, we zero-pad all sets to a fixed maximum size, similar to Zhang et al. (2019). By ensuring that all non-padding elements are unequal to the zero vector, padding can simply be filtered out from the predictions by setting a threshold around a small area around zero.\u201c.\nAnd in section 5.3, we add:\n\u201cThe padding scheme consists of zero-padding sets to a fixed maximum set size and adding a presence variable for each element, which indicates if the element is part of the set or not.\u201d and refer back to the clarification in section 5.4.\n\n**Finally, is there a minus sign missing from the derivation at Eq. (4)?**\n\nThank you, fixed.\n\n**My understanding is that this paper proposes two key contributions:**\n**- learning the conditional energy density by sampling and generating synthetic examples**\n**- using noisy SGD steps for the first  iterations during prediction.**\n**If my understanding is correct, I would have like to see these two contributions explored in more detail. The learning procedure seems similar to noise contrastive estimation (but with a more subtle definition of \"noisy samples\"). Is this point of view correct? If so, did you explore other noise distributions for the synthetic samples?**\n\nWe examine the effect of noisy steps during the prediction procedure in figure 2, where the fraction of noisy steps is varied from $0\\%$ to $100\\%$. As noted above, discovering multiple modes is not possible for the fully deterministic baselines. Approximating the gradient of the partition function via sampling has been applied earlier (e.g. in [1]) and we do not consider this constituent of the framework to be a key contribution in itself. Instead we would like to emphasize that the full training & prediction framework constitutes our main contribution. It manages to remove the limitations of assignment-based set loss training, such as the imposed structure, which we exemplify in a diverse range of experiments.\n\n[1] Teh, Y., et al. Energy-based models for sparse overcomplete representations. JMLR, 4:1235\u20131260, 2003."}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "04ArenGOz3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3297/Authors|ICLR.cc/2021/Conference/Paper3297/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838974, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment"}}}, {"id": "TB6N8X3LAwB", "original": null, "number": 2, "cdate": 1605874055699, "ddate": null, "tcdate": 1605874055699, "tmdate": 1605874337418, "tddate": null, "forum": "04ArenGOz3", "replyto": "MRXiw9V6aS", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment", "content": {"title": "Response for AnonReviewer3", "comment": "We thank AnonReviewer3 for the detailed and constructive comments. \n\n**a) all over the text it is claimed that the proposed model can capture and learn multi-modal set densities and I am not sure how this is possible with the current simplified sampling strategy (which use a perturbation of gradient similar to cheap MCMC with SGD) with no theoretical guarantee can capture this complex underlying distribution.**\n\nWe clarify in section 2.1 of the revised paper:\n\u201cThe proper formulation of the Langevin MCMC algorithm multiplies the gradient in equation 5 by a factor $\\epsilon$ and further requires a Metropolis-Hastings acceptance step (Neal et al. 1993). We forgo both of these components in favor of increased efficiency, but at the cost of forfeiting theoretical guarantees for desirable properties such as not being trapped in a subset of the sampling space, i.e. ergodicity. Discarding all but the last sample $Y^{(T)}$ of each chain constitutes a non typical usage that undermines the usual importance of ergodicity. Notably, this weakens the hard to meet requirement for the sampler to mix between multiple modes in a single MCMC chain, making it sufficient for multiple independently sampled chains to find different local modes. Although the fixed cutoff at $T$ and the missing Metropolis-Hastings update result in a biased sampler, previous works have demonstrated the feasibility of training generative models on images with similar Langevin MCMC methods (Xie et al., 2016; 2018; Nijkamp et al., 2020; Du & Mordatch, 2019; Grathwohlet al., 2019).\u201d\n\n**b) It is also inaccurate to claim that multi-modal distribution over sets cannot be learned directly from a set loss (a sentence in introduction). The definition of set loss is unclear in this context, but the losses can be derived by modelling a set distribution (possibly multi-modal) parametrically where the parameters of this distribution can be learned using deep neural network**\n\nWe acknowledge the ambiguity of the set loss expression used during the introduction and extended the revised paper with precise mathematical formulations in section 5, further supplemented by appendix A. Specifically, we consider assignment-based set losses, such as the Chamfer or Hungarian loss used by (Zhang et al., 2019), which are typically applied in set prediction and cannot be used to learn multi-modal set densities with deep neural networks.\n\n**2- diverse but simplified experiments and unclear evaluations: a) The experiments are diverse enough, but some of the setups are very simplified (eg generation of polygons & Digits experiments which considers a perfect input x and two numbers only). These simplifications might not necessarily reflect the superiority of the proposed approach compared to the baselines when tested in a real-world problem.**\n\nThe reviewer is right. We added the following sentences to section 5.2: \n\u201cThis experiment is tailored towards the special case, when there exist multiple plausible target sets and exemplifies both the short-comings of training with assignment-based set losses and the ability of our approach to predict multiple sets. Whether the results in this simplified experiment will also reflect the superiority of the proposed approach on a real-world problem remains to be tested.\u201d\n\n**CLEVER for the task of object detection also seems to be very simple dataset. But even with this, the proposed approach does not seem to be superior compared to DSPN under specific IoU thresholds, why is this the case?**\n\nThe CLEVR object detection experiment does not represent the type of problems that motivate our proposed framework. Our goal is to address stochastic or underdefined set prediction tasks. The experiment serves to validate and compare our method to previous approaches. Notably, we apply the same neural network and hyper-parameters as DSPN. The results show that our method is generally on-par with DSPN, despite no additional fine-tuning towards our proposed training scheme. The fact that our framework does not outperform the baselines at every IoU threshold, does not detract from the main thesis of our work. Namely, addressing tasks with multiple valid targets, which is supported by the other three experiments.\n\n**b) The suggested approach is claimed to generate multiple plausible sets, are the samples derived from diverse modes or it is an importance sampling? I cannot find out these from the samples in the Figures provided in the appendix. how are these samples weighted and which sample is used for the valuations?**\n\nThe revised paper is updated with the following clarifications:\nIn section 5.2, Generation of Polygons & Digits: \u201cEach prediction represents an independently sampled trajectory of transitions described in Equation 6.\u201d\nIn section 5.3, Point-Cloud Auto-Encoding: \u201cWe evaluate our approach based on a single prediction per example.\u201d\nIn section 5.4, Object Set Prediction on CLEVR: \u201cOur approach is evaluated based on a single prediction per image.\u201d"}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "04ArenGOz3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3297/Authors|ICLR.cc/2021/Conference/Paper3297/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838974, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Comment"}}}, {"id": "bihrmXMqbzO", "original": null, "number": 2, "cdate": 1603907573902, "ddate": null, "tcdate": 1603907573902, "tmdate": 1605024026692, "tddate": null, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review", "content": {"title": "Review", "review": "This paper poses set prediction as a conditional density estimation problem and subsequently develops an energy-based model training/inference procedure. The motivation for this approach is twofold: existing approaches which impose structure via loss functions can induce model bias based on the metric chosen, and existing approaches also induce bias in the sense that they cannot learn multimodal distributions of outputs when, for example, we may want to observe and rank various candidate sets for a given input. These motivations clearly frame the development of the methods in this paper, and the experiments do a good job recalling these motivations as the focus for comparison to previous approaches. Examples in multiple domains are shown to support the hypothesis that this paper's approach performs better than existing approaches. Overall, the writing is clear and the paper flows well. I think overall the paper combines techniques from various fields, namely energy-based methods and set prediction, that renders it useful for the community. \n\nIdeally, I would have liked to have seen some theoretical results based on a simplified setup, indicating more rigorously the ability of this approach in provably adapting to ill-conditioned metrics and learning multimodal distributions. A variety of results for Langevin MCMC exist that can be exploited to provide this analysis. In its current form, I think the paper is acceptable for submission and is relevant to the community, but some more theoretical analysis will definitely add value to a portion of readers like myself.\n\nSome questions/feedback for the authors:\nI think there may be a negative sign missing from Equation 4. Also, it may be useful to further explain why contrasting energy for real and synthesized examples makes sense. I know this is a common setup for structured prediction problems, but I think it is worth paying some lip service in the text. Furthermore, I found it somewhat odd that Equation 5 was framed as an instantiation of Langevin MCMC (perhaps it may be more useful to frame it as an instance of SGLD?), but not equation 6 (at least 6a). This also introduces a natural follow up of whether doing Hamiltonian Monte Carlo based methods makes any difference in mixing efficiency on the experimental setups. Finally, Table 1 was somewhat confusing. The caption explains that Chamfer is bad on Polygons, and Hungarian is bad for Digits, but the numbers don't seem to follow that description. Maybe the rows or columns got switched?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078312, "tmdate": 1606915790745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3297/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review"}}}, {"id": "MRXiw9V6aS", "original": null, "number": 4, "cdate": 1603959445422, "ddate": null, "tcdate": 1603959445422, "tmdate": 1605024026520, "tddate": null, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "invitation": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review", "content": {"title": "unsubstantiated claims, simplified experiments and unclear evaluation", "review": "The paper concerns a learning framework to predict set by formulating it as a conditional density estimation problem. The approach relies on deep energy-based models and predicts multiple plausible sets using gradient-guided sampling. The suggested method has been evaluated on a variety of set prediction problems with competitive performance compared to few set prediction baselines. \n\nThe idea of predicting set using energy-based models seems to be interesting. It is also important to predict but more than one plausible set reflecting mode/sample diversity in the underlying set distribution of the data.  \n\nHowever, my major concerns about the submission are:\n\n1- Few unsubstantiated claims:  \na) all over the text it is claimed that the proposed model can capture and learn multi-modal set densities and I am not sure how this is possible with the current simplified sampling strategy (which use a perturbation of gradient similar to cheap MCMC with SGD) with no theoretical guarantee can capture this complex underlying distribution. \nb) It is also inaccurate to claim that multi-modal distribution over sets cannot be learned directly from a set loss (a sentence in introduction). The definition of set loss is unclear in this context, but the losses can be derived by modelling a set distribution (possibly multi-modal) parametrically where the parameters of this distribution can be learned using deep neural network\n\n    \n 2- diverse but simplified experiments and unclear evaluations:\na) The experiments are diverse enough, but some of the setups are very simplified (eg generation of polygons & Digits experiments which considers a perfect input x and two numbers only). These simplifications might not necessarily reflect the superiority of the proposed approach compared to the baselines when tested in a real-world problem.  \nCLEVER for the task of object detection also seems to be very simple dataset. But even with this, the proposed approach does not seem to be superior compared to DSPN under specific IoU thresholds, why is this the case? \n\nb) The suggested approach is claimed to generate multiple plausible sets, are the samples derived from diverse modes or it is an importance sampling? I cannot find out these from the samples in the Figures provided in the appendix.  how are these samples weighted and which sample is used for the valuations? \n\n ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3297/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3297/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Set Prediction without Imposing Structure as Conditional Density Estimation", "authorids": ["~David_W_Zhang1", "gertjan.burghouts@tno.nl", "~Cees_G._M._Snoek1"], "authors": ["David W Zhang", "Gertjan J. Burghouts", "Cees G. M. Snoek"], "keywords": ["set prediction", "energy based models"], "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.", "one-sentence_summary": "A set prediction training and prediction framework that addresses tasks with ambiguous target sets.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|set_prediction_without_imposing_structure_as_conditional_density_estimation", "pdf": "/pdf/04c489674227569994e57717321c907597b1355c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021set,\ntitle={Set Prediction without Imposing Structure as Conditional Density Estimation},\nauthor={David W Zhang and Gertjan J. Burghouts and Cees G. M. Snoek},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=04ArenGOz3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "04ArenGOz3", "replyto": "04ArenGOz3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3297/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078312, "tmdate": 1606915790745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3297/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3297/-/Official_Review"}}}], "count": 10}