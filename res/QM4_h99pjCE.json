{"notes": [{"id": "QM4_h99pjCE", "original": "-rF74YWuX5", "number": 3223, "cdate": 1601308358089, "ddate": null, "tcdate": 1601308358089, "tmdate": 1614985686926, "tddate": null, "forum": "QM4_h99pjCE", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "neEBpcWuzuB", "original": null, "number": 1, "cdate": 1610040464251, "ddate": null, "tcdate": 1610040464251, "tmdate": 1610474067375, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper offers a direction for mult-agent RL that builds on results for actor-critic methods [Zhang, ICML 2018], extending that work to address deterministic policies.  The authors establish convergence under a number of assumptions.   Both on-policy setting and off-policy settings are treated.  The reviewers point out several concerns and the consensus seems to be that, while the direction looks promising, the paper deserves further work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040464237, "tmdate": 1610474067360, "id": "ICLR.cc/2021/Conference/Paper3223/-/Decision"}}}, {"id": "WDe-YuqWFm", "original": null, "number": 3, "cdate": 1604385206162, "ddate": null, "tcdate": 1604385206162, "tmdate": 1606798636575, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review", "content": {"title": "comprehensive but somewhat incremental", "review": "This paper offers a comprehensive theoretical treatment of deterministic policy gradients in a multi-agent setting, working out several key results:\n* existence and explicit formulas for the multi-agent deterministic policy gradient in off and on-policy settings;\n* convergence of stochastic policy gradients to deterministic ones as policy variance converges to zero; \n* convergence of multi-agent deterministic actor-critic algorithms.\n\nThis paper is very well-written and easy to follow, although technically there is not much to expose here, as most of the paper consists of statements of results. I did not check all the math thoroughly, but there does not seem to be any cause for suspicion as far as I can tell. Related work is sufficiently well presented. \n\nI think that the topic of deterministic policy gradient in a multi-agent is sufficiently interesting and having the foundational results worked out in a single reference is a valuable contribution. The paper is incremental in a way that the deterministic results seem to be extensions of known stochastic results along the lines of well-understood techniques. If any unexpected challenges had to be addressed in order to derive these results (e.g. developing new techniques), this has to be made clearer in the paper. \n\nOverall I think this is a solid paper.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079737, "tmdate": 1606915790508, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3223/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review"}}}, {"id": "YsSEepb4_jB", "original": null, "number": 5, "cdate": 1604664397237, "ddate": null, "tcdate": 1604664397237, "tmdate": 1606782354182, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review", "content": {"title": "Meaningful results but not strong enough", "review": "This paper establishes the asymptotic convergence of on- and  off- policy DPG in the multi-agent setting under some assumptions. Overall the paper is well written and easy to follow. Given the practical usefulness of DPG, I think the result in this paper is somehow meaningful to the community.\n\nHowever, the paper has the following issues:\n(1) Since this paper is a theoretical paper, the assumptions are very important to evaluate the signficance of the main theorems. Thus, the assumptions should be moved to the main body of the paper.\n\n(2) The projection is a practical issue that has been criticized by many researchers in the RL community. Although it is necessarily to make this assumption when handling two time-scale algorithm with time-varying MDP, it can be avoid by adopting other updating structures such as nested-loop, mini-batch two time-scale, etc. Or can the author provide an upper for such a projection radius so that the algorithm can at least be implemented in practice?\n\n(3) Although this paper is a theoretical paper, only considering multi-agent bandit in the experiement is not sufficient to verify the theoretical results in this paper. I suggest the author to consider some RL settings to make the experiment results stronger.\n\n(4) The current muti-agent setting in this paper is not that practical, as too many variables are requied to be globally observable.\n\n(5) The update at time step t required the knowledge in step (t+1), which could introduce some pratice issues as this paper focus on online update. Can the author propose some potential idea to fix this issue?\n\n=== After rebuttal ===\n\nSince the technical contribution of this paper is not significant enough, I will keep my score as weakly reject.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079737, "tmdate": 1606915790508, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3223/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review"}}}, {"id": "NUUKscl4GVx", "original": null, "number": 10, "cdate": 1606233140949, "ddate": null, "tcdate": 1606233140949, "tmdate": 1606233140949, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "AY9AWOesNT", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment", "content": {"title": "Thanks for the response.", "comment": "I hereby acknowledge that I have read the author response."}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QM4_h99pjCE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3223/Authors|ICLR.cc/2021/Conference/Paper3223/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839772, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment"}}}, {"id": "brVEQGDS2p4", "original": null, "number": 9, "cdate": 1605881563668, "ddate": null, "tcdate": 1605881563668, "tmdate": 1605881563668, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "WDe-YuqWFm", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We wish to thank Reviewer 2 for the constructive comments and suggestions.\n\nWe would like to address the comment that the work might be incremental. The closest work to ours would be the one of [Zhang ICML,2018], whose convergence proof for the critic step can be adapted step by step to get the convergence of the critic in our setup. However, there is a significant difference on the actor side as the expression of the deterministic gradient differs from the expression of the stochastic gradient, therefore more specific analysis is needed to establish convergence of the policy parameter. Though the overall analysis approach follows classical two-time-scale algorithm convergence analysis, it is significantly technical, and therefore still needs to be carried out with care to be adapted to the specifities of our updates. In addition, we also prove that the deterministic gradient is a limit case of the stochastic gradient in the context of long-run average reward, which is not a straightforward adaptation of [Silver et al. (January 2014a)]'s result for the discounted reward setting."}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QM4_h99pjCE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3223/Authors|ICLR.cc/2021/Conference/Paper3223/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839772, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment"}}}, {"id": "AY9AWOesNT", "original": null, "number": 8, "cdate": 1605881436998, "ddate": null, "tcdate": 1605881436998, "tmdate": 1605881436998, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "CvrYhdPjzdQ", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We wish to thank Reviewer 3 for the constructive comments and suggestions. The paper has been revised and uploaded. The comments of the reviewer and corresponding changes made are summarized here.\n\nWith regards to the clarifications:\n\n(1) As mentioned in the remark following Theorem 5, \u00e3 variables are required if compatible features are used for the linear approximation of the Q-function.\n\n(2) We suppose that the reviewer is referring to Theorem 2 (as Theorem 1 is on-policy). This difference from Silver et. al. (January 2014a) comes from the fact that Silver et al. (January 2014a) considers discounted rewards while we consider long-run average reward as the objective to maximize, leading to the formulation of equation (1) used to establish Theorem 2. Note that both setups have been considered and studied in the literature so the new contributions to the long-run average scenario is of interest to the community - deriving deterministic gradient expressions, establishing convergence of actor-critic algorithms and showing that deterministic gradient is a limit case of the stochastic gradient (which had only been shown previously under the discounted rewards objective framework). \n\nWith regards to the limited significance:\n\n(1) We have not managed to conduct further experiments yet, but if we manage to do so before the rebuttal deadline, we will post again.\n\n(2) The closest work to ours would be the one of [Zhang ICML,2018], whose convergence proof for the critic step can be adapted straightforwardly step by step to get the convergence of the critic in our deterministic policy case. The difference appears on the actor side as the expression of the deterministic gradient differs from the expression of the stochastic gradient, therefore specific analysis is needed to establish convergence of the policy parameter. Though the overall analysis is a classical but technical two-time-scale algorithm convergence anlaysis, it still requires to be carried out with care to be adapted to the specificities of our update rules. Besides, the proof that the deterministic gradient is a limit case of the stochastic gradient in the context of long-run average reward is not a mere adaptation of [Silver et al. (January 2014a)]'s result obtained for the discounted rewards objective. Looking at the proofs reveal that we are not following the same paths which, e.g.  in our proof, we focus mostly on showing the convergence of stationary distributions and of their gradients (Lemma 2), without considering value functions Q or V, while in  contrary to [Silver et al. (January 2014a)]'s proof establishing convergence of the gradients of Q functions.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QM4_h99pjCE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3223/Authors|ICLR.cc/2021/Conference/Paper3223/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839772, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment"}}}, {"id": "OnQz4SQLXmR", "original": null, "number": 7, "cdate": 1605881348035, "ddate": null, "tcdate": 1605881348035, "tmdate": 1605881348035, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "gzk0qib8dVN", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We wish to thank Reviewer 1 for the constructive comments and suggestions. The paper has been revised and uploaded. The comments of the reviewer and corresponding changes made are summarized here.\n\nWith regards to the cons,\n\n(1) We have not managed to conduct further experiments yet, but if we manage to do so before the rebuttal deadline, we will post again.\n\n(2) There is significant differences from [Zhang, ICML 2018] as the latter handles discrete action space while our paper handles continuous action space. Although their convergence proof for the critic step can be adapted step by step to get the convergence of the critic in our setup, there is a significant difference on the actor side as the expression of the deterministic gradient differs from the expression of the stochastic gradient, therefore more specific analysis is needed to establish convergence of the policy parameter. Though the overall analysis approach follows classical two-time-scale algorithm convergence analysis, it is significantly technical, and therefore still needs to be carried out with care to be adapted to the specifities of our updates. In addition, we also prove that the deterministic gradient is a limit case of the stochastic gradient in the context of long-run average reward, which is not a straightforward adaptation of [Silver et al. (January 2014a)]'s result for the discounted reward setting.\n\n(3) We apologise for the typo. This is the reference to the ICML paper and we has been fixed in the revision. We have also properly introduced and updated the step variable t in the algorithms.\n\n(4) We apologise for the confusion. These are the same quantities.\n\n(5) The full list of assumptions were originally provided in the Appendix. In the revision, these have been moved from the Appendix to the main body. These are technical assumptions on the parameters of the algorithm and do not impose conditions on the problem setting. The assumptions on the problem setting are described in Section 2, notably the regularity assumptions on the Markov chain, and these are standard assumptions in the literature. \n\nWith regards to the questions,\n\n(1) Yes Figure 2 was a reproduction of Figure 1 for completeness of the Appendix.\n\n(2) The details of the experiment are provided in the Appendix.\n\n(3) The results are not comparable with [Zhang, ICML 2018] as the latter is for discrete action space, while our work is for continuous action space.\n\n(4) Unfortunately, we do not completely understand the concern on drawing an action. At the initial step, the state and policy parameters have been initialised, so an action can be obtained. In each iteration, an action is drawn as part of the typical exploration of the RL algorithm."}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QM4_h99pjCE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3223/Authors|ICLR.cc/2021/Conference/Paper3223/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839772, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment"}}}, {"id": "Hgtp3BjN6RK", "original": null, "number": 6, "cdate": 1605881241806, "ddate": null, "tcdate": 1605881241806, "tmdate": 1605881241806, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "hxfuHrv3hG", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We wish to thank Reviewer 4 for the constructive comments and suggestions. \n\nThe closest work to ours would be the one of [Zhang ICML,2018], whose convergence proof for the critic step can be adapted step by step to get the convergence of the critic in our setup. However, there is a significant difference on the actor side as the expression of the deterministic gradient differs from the expression of the stochastic gradient, therefore more specific analysis is needed to establish convergence of the policy parameter. Though the overall analysis approach follows classical two-time-scale algorithm convergence analysis, it is significantly technical, and therefore still needs to be carried out with care to be adapted to the specifities of our updates. In addition, we also prove that the deterministic gradient is a limit case of the stochastic gradient in the context of long-run average reward, which is not a straightforward adaptation of [Silver et al. (January 2014a)]'s result for the discounted reward setting."}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QM4_h99pjCE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3223/Authors|ICLR.cc/2021/Conference/Paper3223/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839772, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment"}}}, {"id": "imV41rIck7", "original": null, "number": 5, "cdate": 1605881161477, "ddate": null, "tcdate": 1605881161477, "tmdate": 1605881161477, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "YsSEepb4_jB", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment", "content": {"title": "Response to Reviewer 5", "comment": "We wish to thank Reviewer 5 for the constructive comments and suggestions. The paper has been revised and uploaded. The comments of the reviewer and corresponding changes made are summarized here.\n\n(1) The assumptions have been moved to the main body. \n\n(2) The projection step has been often used to show convergence in several papers. Some even directly assume boundedness of the update leading to easier convergence analysis - notably in [Bhatnagar et al., Automatica (2009)] where they mention that in practice they do not do the projection but observe empirically that the updates remain bounded and converge.\n\n(3) We have not managed to conduct further experiments yet, but if we manage to do so before the rebuttal deadline, we will post again.\n\n(4) While the exact algorithm might indeed require global observability, nevertheless, the algorithm provides intuition for designing algorithms with partial observability, for example in settings where the influence between agents decays with some intuitive measure of distance in the communication graph.\n\n(5) Unfortunately, we do not understand this comment that the update at step t requires knowledge in step t+1. In each time step, the actions are taken, and then the update is performed based on the actions that are taken. Moreover, in practice, it is also common to accumulate RL updates into batches, or use target networks that are updated more slowly. While we do not provide convergence guarantees under such practical heuristic modifications, these tricks are typically observed to lead to performance and stability improvements. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QM4_h99pjCE", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3223/Authors|ICLR.cc/2021/Conference/Paper3223/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839772, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Comment"}}}, {"id": "CvrYhdPjzdQ", "original": null, "number": 1, "cdate": 1603778360523, "ddate": null, "tcdate": 1603778360523, "tmdate": 1605078851405, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review", "content": {"title": "Detailed theory, however experiments lack coverage", "review": "Clarity: The paper is well written. The following require some clarifications:\n1) $\\tilde{a}$ variables are defined by never seem to be used in Algorithm 1, why?\n\n2) In the  off-policy setting (Theorem 1) letting $\\mathcal{N}=1$ does not recover the previous result by Silver et al. (January 2014a). This is perhaps due to the fact that eq(1) of this paper has $\\bar{R}$ in its objective and eq(14) of Silver et al. (January 2014a) has $Q^{\\text{target-policy}}$ instead. It would be great if the authors clarify why the two objectives are different. If my understanding is correct, in the off-policy setting, we are still interested in the maximising the reward with respect to target policy, and the restriction is that the samples are from the behaviour policy.  \n\n\nQuality:\n1) Strength: The theoretical results are stated clearly and detailed proofs have been provided. But for the off-policy case, other results seem to be correct.\n2) Weakness: The experiments are on a toy domain. The experiments demonstrate that the proposed algorithms indeed converge as expected. However, the experiments do not add any further insights such as a) what happens when we change the communication matrix $C$? b) What happens when the reward structure varies a lot across the agents?  c) what happens when different agents start from different parts of the state space. In short, the experiments have to be carefully thought out to cover the entire range so as to emphasise the multi-agent flavour. As it stands, the current version of the paper spends too little space (only about 4 lines) has been allocated for the experiments. \n\n\nNovelty:\nThe deterministic policy gradient result is new. The off-policy case seems to have some issue (see above). While the result in itself is new, it is not clear whether it follows trivially from prior results. It would be great if the authors can comment on the this.\n\n\nSignificance:\nThe significance of the current version is limited due to, \n1) the lack of experiments, \n2) the possibility that the new results could be obtained as trivial extension of prior work (Zhang ICML,2018 + Silver et al. (January 2014a)). (I am not saying that this is true, authors can clarify)\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079737, "tmdate": 1606915790508, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3223/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review"}}}, {"id": "gzk0qib8dVN", "original": null, "number": 2, "cdate": 1603797670566, "ddate": null, "tcdate": 1603797670566, "tmdate": 1605024043144, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review", "content": {"title": "This paper provides a valuable idea and a promising direction in MARL, but the current version has several problems that need to be fixed. ", "review": "This paper extends the results for actor-critic with stochastic policies of [Zhang, ICML 2018] to deterministic policies and offers the proof of convergence under some specific assumptions. The authors consider both the on-policy setting and the off-policy setting and offers some convincing derivation. It provides a valuable idea and a promising direction in MARL, but the current version has several problems that need to be fixed. Specifically, some parts of equations, algorithms, and expressions are ambiguous and unintelligible. Besides, problems with the format in the formula and citations also exist, which degrade the paper\u2019s quality and clarity.\n\nPros:\n1.\tThe motivation that considers both off-policy and on-policy is interesting and attractable. This work might provide a promising way to address the problem of inefficiency of exploration in MARL. \n2.\tThe convergence of MARL is challenging, and this paper gives a valuable attempt.\n\nCons:\n1.\tThe experiment is insufficient and doubtful, although this paper is convincing in theory, the experiments are not convincing at all. The convergence curves are quite nice, but the authors should compare the results with some other state-of-art algorithms to increase its credibility. \n2.\tThe differences with [Zhang, ICML 2018] are ambiguous. Some definitions and formulas and even algorithms are almost the same as [Zhang, ICML 2018]. \n3.\tThere are lots of format problems. For example, \u201cKaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multiagent reinforcement learning with networked agents. 80:5872\u20135881, 10\u201315 Jul 2018.\u201d In it, I cannot find where this paper is published. Some symbols are used without explanation, e.g., t in Algorithms 1 and 2.\n4.\tThe organization of this paper needs to be polished. In the background section, the authors firstly introduce the optimization problem of maximizing the average reward with the objective $J(\\pi_\\theta)$ on page 2. However, they use $J(\\theta)$ when defining the Q_\\theta function instead of $J(\\pi_\\theta)$ just 3 lines below. This simplification is not noted until Section 3. So the authors should re-organize the paper to make the content more readable.\n5.\tThis paper raises some assumptions when discussing, but neither does it offer a list of these assumptions, nor does it analyze the rationality and influence of them.\n\nQuestions:\n1.\tWhat is the difference between Figure 1 and Figure 2? \n2.\tWhat is the detail of the experiment? \n3.\tHow well do the results compared with [Zhang, ICML 2018]?\n4.\tIn Algorithm 1, why can you draw an action firstly?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079737, "tmdate": 1606915790508, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3223/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review"}}}, {"id": "hxfuHrv3hG", "original": null, "number": 4, "cdate": 1604537294792, "ddate": null, "tcdate": 1604537294792, "tmdate": 1605024043013, "tddate": null, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "invitation": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review", "content": {"title": "This paper proposes two algorithms for learning deterministic policy in multi-agent RL problem. The authors extend the multi-agent policy gradient for stochastic policies to deterministic policies. Decentralized algorithms are proposed to learn the Q/averaged reward in on/off-policy setting.", "review": "This paper proposes a solution to learning deterministic policy in the multi-agent RL setting, where local rewards are private to local agents.  Both on-policy learning and off-policy learning are considered in the paper. \nTo do so, the author first extends the policy gradient theorem in MARL for stochastic policy to deterministic policy.\nSpecifically, in on-policy learning, only global Q-function and local action are needed to compute local policy gradient; in off-policy setting, only averaged reward function and local action are needed.\nThe author then propose a decentralized algorithm for learning the global Q-function/averaged reward function. Two-timescale technique is adopted to show the convergence of the proposed algorithms.\n\nStrengths: \n(1) the extension of policy gradient to deterministic policy in MARL (locally observable reward) is interesting and important.\n(2) the decentralized algorithm itself, although similar to prior work in Zhang et al (2018), provide needed guarantee to the algorithm for convergence.\n(3) the paper is well written and easy to follow.\n\nCons:\nThe proposed decentralized algorithm closely mimics the one in the prior work, in addition to its analysis framework.  I hope the authors can provide more details on how the analysis of the algorithm differentiates itself from previous work in different aspects. I am happy to increase my rating if the author can address this issue in their response.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3223/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3223/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decentralized Deterministic Multi-Agent Reinforcement Learning", "authorids": ["antoine.grosnit@polytechnique.edu", "desmond.cai@gmail.com", "~Laura_Wynter1"], "authors": ["Antoine Grosnit", "Desmond Cai", "Laura Wynter"], "keywords": ["multiagent reinforcement learning", "MARL", "decentralized actor-critic algorithm"], "abstract": "Recent work in multi-agent reinforcement learning (MARL) by [Zhang, ICML12018] provided the first decentralized actor-critic algorithm to offer convergence guarantees. In that work, policies are stochastic and are defined on finite action spaces. We extend those results to develop a provably-convergent decentralized actor-critic algorithm for learning deterministic policies on continuous action spaces. Deterministic policies are important in many real-world settings. To handle the lack of exploration inherent in deterministic policies we provide results for the off-policy setting as well as the on-policy setting. We provide the main ingredients needed for this problem: the expression of a local deterministic policy gradient, a decentralized deterministic actor-critic algorithm, and convergence guarantees when the value functions are approximated linearly. This work enables decentralized MARL in high-dimensional action spaces and paves the way for more widespread application of MARL.", "one-sentence_summary": "We provide a provably-convergent decentralized actor-critic algorithm for learning deterministic reinforcement learning policies on continuous action spaces.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "grosnit|decentralized_deterministic_multiagent_reinforcement_learning", "supplementary_material": "/attachment/a385acc35917e709f44b29b2c66d17bf00685688.zip", "pdf": "/pdf/dcaee4b93ff5a38c3d6871819ef8f9589288a27f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nwzC9dWvDl", "_bibtex": "@misc{\ngrosnit2021decentralized,\ntitle={Decentralized Deterministic Multi-Agent Reinforcement Learning},\nauthor={Antoine Grosnit and Desmond Cai and Laura Wynter},\nyear={2021},\nurl={https://openreview.net/forum?id=QM4_h99pjCE}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QM4_h99pjCE", "replyto": "QM4_h99pjCE", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3223/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538079737, "tmdate": 1606915790508, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3223/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3223/-/Official_Review"}}}], "count": 13}