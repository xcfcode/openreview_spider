{"notes": [{"id": "jLoC4ez43PZ", "original": "HEEeV8xn3t", "number": 318, "cdate": 1601308043213, "ddate": null, "tcdate": 1601308043213, "tmdate": 1615872434601, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OFthhSsFrma", "original": null, "number": 1, "cdate": 1610040503852, "ddate": null, "tcdate": 1610040503852, "tmdate": 1610474110922, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes a simple extension to BERT-like pre-training for source code models, which allows incorporation of data flow information. This is a new way of incorporating code structural information into models, and it appears practical and effective. Reviewers are all in favor of accepting the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040503839, "tmdate": 1610474110906, "id": "ICLR.cc/2021/Conference/Paper318/-/Decision"}}}, {"id": "O-8pm3aJHqe", "original": null, "number": 4, "cdate": 1605497500871, "ddate": null, "tcdate": 1605497500871, "tmdate": 1605697661118, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "EtePPHYyFEI", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment", "content": {"title": "Respond to AnonReviewer2", "comment": "###   Respond to comments:\n\n1. We use the byte-pair encoding (BPE) method [1] to tokenize variable names, e.g. \u201cx1\u201d and \u201cmax_value\u201d will be tokenized to [\u2018\u0120x\u2019,\u20191\u2019] and [\u2018\u0120max\u2019, \u2018_\u2019, \u2018value\u2019] where \u2018\u0120\u2019 is the special token to represent the beginning sub-token of variable names.\n\n   [1] Rico Sennrich, Barry Haddow, and Alexandra Brich. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistic.\n\n2.\tThe attention matrix $\\rm{softmax}(\\frac{Q_iK_i^T}{\\sqrt{d_k}}+M)$ is learnable regarding parameters $Q_i$, $K_i$ in the equation 1. $M$ is a pre-defined matrix and keeps 0 or $-\\infty$ throughout training to mask attention between unconnected node pairs.\n\n3.\tDataflow considered in this work ***represents dependency relation between variables***, which is different from the traditional dataflow that covers more semantic information in compiler analysis. We will further clarify the difference in the paper. \n\n4.\tYes, we mean less complex. Thanks for your suggestion. We will use a more suitable word like \u201cless complex\u201d.\n\n5.\tThanks for your suggestion, we have added an example with more detailed description for the node alignment task. Please kindly find the Figure 3 and detailed description in the latest draft.\n\n6. We discussed experiment settings and hyperparameter in Appendix A to D. \n\n   For code clone detection, code translation, and code refinement tasks, the results presented in the paper is to average over five runs using different random seeds. For code search task, we use the same pipeline released by CodeBERT and keep the same random seed for fair comparison. \n\n   In the rebuttal phase, we further conduct t-test for four downstream tasks. For code search, the results show that the improvements of GraphCodeBERT are significant with $p$<0.01 on 5 programming languages. For code translation and other two tasks, GraphCodeBERT significantly outperforms other models with $p$<0.05 and $p$<0.01, respectively. Please kindly find the t-test results on Table 1 to 4 of the updated draft."}, "signatures": ["ICLR.cc/2021/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jLoC4ez43PZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper318/Authors|ICLR.cc/2021/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872259, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment"}}}, {"id": "oSpWBGN1sa", "original": null, "number": 2, "cdate": 1605497299719, "ddate": null, "tcdate": 1605497299719, "tmdate": 1605697144195, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "HQq2rKwSkFs", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment", "content": {"title": "Respond to AnonReviewer1", "comment": "### Respond to weakness:\n\n1. By truncating the input sequence, we compared GraphCodeBERT with other Transformer models in the setting of same computing cost. From Figure 4, we can see that GraphCodeBERT performs better than other models when we keep the same length of input sequence e.g. 128. The results demonstrate that adding data flow inputs still benefits although truncating the input sequence.\n\n   In the rebuttal phase, we conduct statistical analysis on the input, including the average length of the code, the length of the linearized data flow, the average length of linearized AST. All numbers are calculated on the CodeSearchNet dataset after tokenization. The statistics show that the average length of linearized data flow is about 14%~22% of code input and three times less than AST. We will add these statistics in the final version. Thanks for your suggestion.\n\n   |                                        | Ruby | Python | Java | Php  | Go   | Javascript |\n   | -------------------------------------- | :--- | :----- | :--- | :--- | :--- | :--------- |\n   | Average length of code                 | 116  | 156    | 131  | 152  | 120  | 149        |\n   | Average length of linearized data flow | 22   | 35     | 25   | 21   | 18   | 32         |\n   | Average length of linearized AST       | 64   | 104    | 75   | 94   | 76   | 94         |\n\n2. We have added case study and error analysis for each downstream task. Please kindly find them in Appendix F and G of the latest draft. "}, "signatures": ["ICLR.cc/2021/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jLoC4ez43PZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper318/Authors|ICLR.cc/2021/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872259, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment"}}}, {"id": "L2cC6JOSgvW", "original": null, "number": 6, "cdate": 1605500896887, "ddate": null, "tcdate": 1605500896887, "tmdate": 1605502439332, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment", "content": {"title": "General Response: Summary of Updated Version", "comment": "Thank all reviewers for your suggestions and comments. We have submitted an updated version of the paper based on these comments and included additional experiments. \n\n1 ) We have added an example with more detailed description for node alignment task in Figure 3 suggested by Reviewer 2.\n\n2 ) We have conducted significance test on four downstream tasks in Table 1 to 4 according to Reviewer 2's and Reviewer 4's comments.\n\n3 ) We have added case study and error analysis for each downstream tasks in Appendix F and G suggested by Reviewer 1 and Reviewer 3.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jLoC4ez43PZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper318/Authors|ICLR.cc/2021/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872259, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment"}}}, {"id": "1dOf2lr8ueO", "original": null, "number": 5, "cdate": 1605497531183, "ddate": null, "tcdate": 1605497531183, "tmdate": 1605497980480, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "2KLdo-CCc1p", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment", "content": {"title": "Respond to AnonReviewer4", "comment": "### Respond to weakness:\n1.\tAST contains attribute information of code tokens like function name and operator, dependency information between code tokens like data flow and so on. In this paper, we mainly leverage dependency relation between variables to demonstrate that semantic-level code structure is useful for pre-training. It is promising to consider function name, condition control and operator as well. We leave them in the future works.  \n\n2. We add t-test on four tasks in the latest draft. For code search, the results show that the improvements of GraphCodeBERT are significant with $p$<0.01 on 5 programming languages. For code translation and other two tasks, GraphCodeBERT significantly outperforms other models with $p$<0.05 and $p$<0.01, respectively. Please kindly find the t-test results on Table 1 to 4 from the updated draft.\n\n   Although data flow and the two newly introduced objectives are not directly relevant to natural language comment, data flow improves the code understanding ability of the model, which helps GraphCodeBERT do better semantic matching between source code and queries, as shown in Figure 4. \n\n3. Thanks for your suggestion about the BLEU metric. We will consider not reporting BLEU in the final version.\n\n### Respond to  questions:\n1.\tAST Pre-order Traversal regards AST as a sequence $S$ by linearizing all AST nodes using pre-order traversal algorithm.  We then concatenate the comment $W$ and linearized AST $S$ as the sequence input $X=([CLS], W, [SEP], S, [SEP])$ and add an extra vocabulary for non-terminal nodes. We take $X$ as input to pre-train the model using MLM objective and fine-tune the model on downstream task.\n\n2. AST Subtree Masking regards AST as a tree. The set of non-terminal symbols of AST is denoted as $T$.  We then concatenate the comment $W$, source code $C$, and the set of non-terminal symbols $T$ as the sequence input $X=([CLS], W, [SEP], C, [SEP], T, [SEP])$ and add an extra vocabulary for non-terminal nodes. Following [1], we introduce tree-based embeddings and a subtree masking function. Tree-based embeddings indicate the position of non-terminal symbols of $T$ in AST.  Subtree masking function is to allow each non-terminal node to attend to only its own subtree descendants in Transformer. Applying tree-based embeddings and subtree masking function could introduce tree structure information of AST into Transformer. We take the sequence $X$ as the input to pre-train the model using MLM objective and then fine-tune on downstream tasks.\n\n   [1] Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, and Richard Socher. Tree-structured attention with hierarchical accumulation. International Conference on Learning Representations, 2020.\n\n### Respond to minor:\nThanks for your suggestion and we will consider using two different notations to represent parameters in self-attention (Eq (1)) and the code comment in the final version. "}, "signatures": ["ICLR.cc/2021/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jLoC4ez43PZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper318/Authors|ICLR.cc/2021/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872259, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment"}}}, {"id": "ekQ5hQEgDPs", "original": null, "number": 3, "cdate": 1605497405941, "ddate": null, "tcdate": 1605497405941, "tmdate": 1605497697262, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "bj8YqWK2Wf2", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment", "content": {"title": "Respond to AnonReviewer3", "comment": "### Respond to reject reasons:\n1.\tWe did not compare to GPT-C or C-BERT because they are not publicly available. We will contact authors of related works to conduct fair and comprehensive comparisons.\n2.\tSince CodeBERT is pre-trained on 2.4M functions corpus, for fair comparison, we use the same pre-training dataset (2.4M functions) for GraphCodeBERT, which is mentioned in Appendix A. \n\n### Respond to questions & suggestions:\n1.\tThe motivation of utilizing natural language descriptions for pre-training is to support more code-related tasks involving natural language such as code search.  In the experiments of CodeBERT paper, a model pre-trained on only code without using natural language descriptions performs worse than a model pre-trained on text-code pairs. We retain their setting in this work.  \n\n2.\tThe average number of nodes and the ratio of edge connection are 27 and 8%, respectively. For each example, about 5 nodes will be sampled and 135 node pairs will be used to calculate the loss.  \n\n3.\tWe have added case study and error analysis for each downstream task. Please kindly find them in Appendix F and G of the latest draft.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper318/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jLoC4ez43PZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper318/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper318/Authors|ICLR.cc/2021/Conference/Paper318/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872259, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Comment"}}}, {"id": "2KLdo-CCc1p", "original": null, "number": 1, "cdate": 1603653091096, "ddate": null, "tcdate": 1603653091096, "tmdate": 1605024715413, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Review", "content": {"title": "AnonReviewer4", "review": "This paper proposes GraphCodeBERT as a Transformer-based pretrained model for programming language that incorporates data flow information in the graph representation of variables in the code. The data flow graph encodes the structure of variables based on \u201cwhere-the-value-comes-from\u201d from the AST parse. The pretrained model is jointly trained on the code, the natural language comment of the code, and the data flow graph of the code. In addition to the Masked Language Modeling objective, two new pretraining objectives are proposed including predicting the edge of the data flow graph and predicting the alignment of variables between data flow graph and code. The graph-guided masked attention is used such that the attention can only occur if two variables have an edge in the data flow graph or there is an alignment between data flow graph and code. The experiments show that GraphCodeBERT can deliver improvements on Natural Language Code Search, Code Clone Detection, Code Translation, and Code Refinement.\n\nStrengths\n\n- The approach is well motivated, and I totally agree that we should consider the code structure for a pretrained code language model.\n\n- There are consistent, though small, improvements on all four tasks.\n\n- The case study and ablation study are very helpful to understand why GraphCodeSearch is better than other models with better representation of code semantics on the Code Search task.\n\nWeakness\n\n- This paper uses the data flow graph and claims this is better than AST because AST has an \"unnecessarily deep hierarchy\". However, as the data flow graph is extracted from AST, data flow contains only partial information of AST, especially the information of \u201cwhere-the-value-comes-from\u201d, and throws out other useful information such as function name, condition control, operator. It is not clear to me if this design choice is a good one. \n\n- The experiments should have a comprehensive comparison using AST and the data flow graph to answer the questions such as: What is the performance of using AST instead of the data flow graph? What information in AST is useful for which tasks? In addition to \u201cwhere-the-value-comes-from\u201d information, what other information shall we incorporate into a pretrained language model for code? Right now, there is only analysis on the code search task and there is not enough discussion about AST approaches.\n\n- The improvement of GraphCodeBERT over previous SOTA, especially CodeBERT, is marginal. There is no significance test in the results. On Code Clone Detection, the improvement is 0.6% F1. On code translation, the improvement is less than 1% for accuracy. The most improvement is on 5.1 Natural Language Code Search, but the data flow graph and the two newly introduced objectives are not relevant to natural language comment.\n\n- I don't think BLEU is a meaningful metric for code translation and code refinement, and it can be misleading. For example, in Table 4, BLEU is 90+ while the accuracy is less than 10%. The author can consider not reporting BLEU.\n\nQuestions\n-  Can you please give more details about how to use AST Preorder Traversal and AST Subtree Masking for code search? I am wondering if this is a good usage of AST on this task. Thanks!\n\nMinor\n- Notation W is used to denote both parameters in self-attention (Eq (1)) and the code comment. Consider using two different notations.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper318/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145747, "tmdate": 1606915801187, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper318/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Review"}}}, {"id": "EtePPHYyFEI", "original": null, "number": 2, "cdate": 1603813999395, "ddate": null, "tcdate": 1603813999395, "tmdate": 1605024715324, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Review", "content": {"title": "Simple extension over CodeBERT with small performance improvement for a series of tasks", "review": "This paper extends CodeBERT to include elements of dataflow in\naddition to the comments and sequence of code tokens. Also, the\npretraining includes two tasks that are structure-aware: predicting\ndataflow edges and aligning dataflow nodes to code. The paper is well\nwritten, well motivated and the empirical evaluation is quite\nthorough. The performance compared to CodeBERT is around 1-2%\nadditional accuracy for a series of diverse tasks.\n\nDetailed comments:\n- In general the paper is well written. There were a few parts that\n  required further clarification for me. For example, it is not clear\n  how the variable names are represented textually (e.g. x1 - is it\n  really x1 or a different way of expressing this in the sequence of\n  tokens that is sent to the model). The architecture seems to me to\n  be pretty identical to BERT with the exception of the attention\n  matrix for the dataflow portion. Is this matrix kept constant\n  throughout training? Not clear to me if the dataflow graph is a\n  traditional dataflow (as in the one used in compiler analysis). It\n  appears to me that it's a slightly simplified version based on AST\n  and some idea of data flow. I would contrast and compare with\n  \"traditional\" dataflow graphs (that use SSA - single static\n  assignment form - for example). Not exactly clear what you mean by\n  \"neat\". I would find a different word. Neat sounds like \"pretty\" in\n  this context; I don't find it scientific. Do you mean less complex?\n  The Noe alignment task was not clear to me at all. Perhaps an\n  example would be helpful showing the actual task and what is\n  provided as input (I'm guessing the same thing as usual) and how the\n  prediction looks like.\n\n- I'm not sure there is a discussion on the experimental methodology\n  and hyperparameter tuning. Are the results presented some average\n  over several runs? If they are single runs, the difference in\n  accuracy for most task is within noise, I would think. Also, I\n  particularly find the third decimal a bit much for \"stretching\"\n  result improvements. I think the idea is simple, it gives some\n  limited performance improvement over CodeBERT but the model\n  architecture is overall similar so it is probably worth it", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper318/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145747, "tmdate": 1606915801187, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper318/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Review"}}}, {"id": "bj8YqWK2Wf2", "original": null, "number": 3, "cdate": 1603865802469, "ddate": null, "tcdate": 1603865802469, "tmdate": 1605024715239, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Review", "content": {"title": "Review of Graph Code BERT", "review": "\nOverview:\nThe authors present Graph Code BERT, the first language model that leverages data flow to learn code representation. They use three objective functions: Masked Language Modeling, Edge Prediction, and Node Alignment. They claim their structure-aware pre-training can help improving performance on code-related downstream tasks, including code search, clone detection, code translation, and code refinement.\n\n\nReasons to accept:\n* The paper is well-written and easy-to-follow.\n\n* It is the first pre-trained model that leverages the data flow structure of code to learn code representation.\n\n* They propose two pre-training tasks for learning representation from source code and data flow. Edge Prediction is more interesting in my opinion.\n\n* They show improvement on four downstream tasks, even though some of them are marginal in my opinion compared to CodeBERT.\n\nReasons to reject:\n* This work is not the first work to conduct language model pretraining for code understanding applications. I will expect more comprehensive comparisons with related work. For example, Svyatkovskiy et al. (2020) propose GPT-C, which is actually a transformer architecture that can also use for both generation and classification tasks. Missing C-BERT (Buratti et al.) as well.\n\n* The 2.4M functions used for \"pre-training\" is not very persuasive. I think it is more reasonable to crawl existing code repositories for Pre-training. I do not see a particular reason because pre-training without natural language descriptions still makes sense.\n\nQuestions & Suggestions:\n* Do you find natural language descriptions useful in your experiments? Why if we discard it from pre-training?\n\n* What is the ratio of edge connection? 10%? 20%? The reason I am asking this is that when you do the randomly sampling 20% of nodes Vs in the data flow, how many node pairs you used to calculate the loss? Is the positive-negative ratio balanced? Please give more details about the training objectives.\n\n* Please give more prediction examples or qualitative analysis for each downstream tasks in the Appendix.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper318/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145747, "tmdate": 1606915801187, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper318/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Review"}}}, {"id": "HQq2rKwSkFs", "original": null, "number": 4, "cdate": 1604304579377, "ddate": null, "tcdate": 1604304579377, "tmdate": 1605024715172, "tddate": null, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "invitation": "ICLR.cc/2021/Conference/Paper318/-/Official_Review", "content": {"title": "Solid work on introducing structure aware tasks and data flow representation to pre-training on code", "review": "This work address the pretraining over code and text. It proposes to leverage data flow as additional inputs, and add two structure aware pre-training tasks besides the masked token prediction task. The pretrained model is evaluated on four different tasks and outperforms the CodeBERT baselines as well as other pretrained models. Further analysis confirmed the benefits from the additional tasks and data flow input. \n\nStrength:\n\n1. The use of data flow as additional input and the proposal of structure aware tasks are well motivated adaptations of pre-training to code. \n\n2. The extensive experiments and comparisons supported the claim and additional ablation studies and analysis confirmed the benefits from the additional structure-aware tasks and the data flow representation. \n\n3. The comparison with syntactic structures (AST) is interesting and demonstrated the higher potential of semantic structures like data flow. This could be beneficial to other code related tasks. \n\n4. The paper is well written and easy to follow. \n\nWeakness:\n\n1. Since the proposed approach is adding extra inputs, and given the O(n^2) complexity of Transformer, it would help to add some information regarding the increase in compute cost, for example, additional flops or latency due to additional data flow inputs. \n\nRelated to the point above, although linearizing the data flow into sequence seems the simplest approach, it might be computationally inefficient given that the data flow graph is probably sparse, thus large number of attention position needs to masked. But maybe the data flow input is much smaller than the code input so that it is not adding too much overhead.  \n\nIt would also help to add more statistics about the inputs, for example, the average length of the code, the length of the linearized data flow, the length of the linearized AST, etc. \n\n2. Not so much a weakness, but it would help to show some examples, in appendix if the space is limited, where GraphCodeBERT improves over CodeBERT or other models without semantic structures, to give some more intuition. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper318/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper318/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "authorids": ["~Daya_Guo2", "shuoren@buaa.edu.cn", "lushuai96@pku.edu.cn", "zyfeng@ir.hit.edu.cn", "~Duyu_Tang1", "~Shujie_LIU1", "~Long_Zhou2", "~Nan_Duan1", "~Alexey_Svyatkovskiy1", "shengyfu@microsoft.com", "michele.tufano@microsoft.com", "shao.deng@microsoft.com", "colin.clement@microsoft.com", "dawn.drain@microsoft.com", "neels@microsoft.com", "issjyin@mail.sysu.edu.cn", "djiang@microsoft.com", "~Ming_Zhou1"], "authors": ["Daya Guo", "Shuo Ren", "Shuai Lu", "Zhangyin Feng", "Duyu Tang", "Shujie LIU", "Long Zhou", "Nan Duan", "Alexey Svyatkovskiy", "Shengyu Fu", "Michele Tufano", "Shao Kun Deng", "Colin Clement", "Dawn Drain", "Neel Sundaresan", "Jian Yin", "Daxin Jiang", "Ming Zhou"], "keywords": ["Pre-training", "BERT", "Code Representations", "Code Structure", "Data Flow"], "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.", "pdf": "/pdf/9e81b47417b883d933baaf98c7e08ce4d7b14fa0.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|graphcodebert_pretraining_code_representations_with_data_flow", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021graphcodebert,\ntitle={GraphCode{\\{}BERT{\\}}: Pre-training Code Representations with Data Flow},\nauthor={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jLoC4ez43PZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jLoC4ez43PZ", "replyto": "jLoC4ez43PZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper318/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538145747, "tmdate": 1606915801187, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper318/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper318/-/Official_Review"}}}], "count": 11}