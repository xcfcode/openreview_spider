{"notes": [{"id": "BklpOo09tQ", "original": "BkgDGr59Km", "number": 398, "cdate": 1538087797353, "ddate": null, "tcdate": 1538087797353, "tmdate": 1545355434624, "tddate": null, "forum": "BklpOo09tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxbz6UBxV", "original": null, "number": 1, "cdate": 1545067784751, "ddate": null, "tcdate": 1545067784751, "tmdate": 1545354482134, "tddate": null, "forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper398/Meta_Review", "content": {"metareview": "While the proposed method is novel, the evaluation is not convincing. In particular, the datasets and models used are small. Susceptibility to adversarial examples is tightly related to dimensionality. The study could benefit from more massive datasets (e.g., Imagenet).", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Needs improvement."}, "signatures": ["ICLR.cc/2019/Conference/Paper398/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper398/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper398/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353231523, "tddate": null, "super": null, "final": null, "reply": {"forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper398/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper398/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper398/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353231523}}}, {"id": "ByeVD2kjam", "original": null, "number": 3, "cdate": 1542286428411, "ddate": null, "tcdate": 1542286428411, "tmdate": 1542286428411, "tddate": null, "forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper398/Official_Review", "content": {"title": "Interesting research direction but needs more thorough experiments", "review": "Summary. The authors propose a novel adversarial training method, e2SAD, that relies on a two-step process for generating sets of two training adversarial samples for each clean training sample. The first step is a classical FGSM that yields the first adversarial sample. The second adversarial sample is calculated with a FGSM that is based on the cross-entropy between the probabilities generated by the first adversarial sample and the probabilities generated by the second adversarial sample. The method is computationally efficient (two forward/backward passes per clean sample) w.r.t. powerful iterative attacks such as IFGSM or PGD requiring 40+ steps and the authors claim it gives comparable results to adversarial training with multi-step attacks methods in white and black-box settings.\n\nClarity. Part 1 and 2 of the paper are well written and summarize the existing attacks/defense mechanisms, their pros and cons as well as the contributions clearly. The next sections could be made shorter (see comments below) to match ICLR\u2019s recommended soft limit of 8 pages instead of the 10 pages hard limit. This would also help the reader grasp the key ideas faster and have a standard formatting (no negative spaces for instance).\n\nNovelty. The idea of simulating the effect of iterative attacks using two distinct steps is novel and appealing to me. The first step increases the loss while the second step shifts the probability distributions apart.\n\nPros and cons.\n(+) The paper is clear and easy to follow, although a bit long.\n(+) The idea is interesting and clearly motivated in terms of computational efficiency and in terms of desired properties (Figure 2 illustrates this point well).\n\n(-) Only one aspect of the idea is exploited in the article. It would be interesting to compare this method as an attacker (both in terms of performance and in terms of generated samples, see comment below). Powerful adversarial training should indeed rely on powerful generated adversarial samples.\n(-) The results seem somewhat mitigated in terms of significance and conclusions drawn by the authors. Also, the experimental setup is quite light, notably the used CNN architectures are quite small and other datasets could have been used (also linked to the significance of the results).\n\nComments.\n- Shorter paper. Here are suggested modifications for the paper that could help strengthen the impact of your paper. Section 3.1 could be almost entirely discarded as it brings no new ideas w.r.t sections 1 and 2. Figure 1 summarizes the method well, thus the description in Section 3.2 could be made shorter, especially when displaying Equation (8) right after Figure 1. This would then help reduce the size of Sections 3.2.1 and 3.2.2 (because Equation (8) and Figure 1 would prevent you from repeating claims made earlier in the paper). Algorithm 1 is straightforward and could be placed in Appendix. Conclusion and Result sections could be shortened a little as well (not as much as Section 3 though).\n\n- Significance of the results. The significance of some results is unclear to me. Could the authors provide the standard deviation over 3 or 5 runs? For example, in rows 1, 3, 4, 5, 6 of Table 2, it is not clear it e2SAD performs better than FGSM adversarial training, thus raising the question of the necessity of Step 2 of the attack (which is the core contribution of the paper).\n\n- Experimental setup. The last two rows of Table 1 are encouraging for e2SAD. However, the authors could introduce another dataset, e.g. CIFAR10 or 100 or even ImageNet restricted to 20 or 100 random classes/with fewer samples per class and use deeper modern CNN architectures like ResNets (even a ResNet18). Those models are widely adopted both in the research community and by the industry, thus defense mechanisms that provably work for such models can have a huge impact.\n\n- Defense setup. Is the order of Steps 1 and 2 relevant? What if the authors use only iterations of Step 2?\n\n- Attack setup. Here are a few suggestions for assessing your method in an attack setting: what is the precision of the network, without any defense, given an average dissimilarity L2 budget in the training/test samples, in a white/black box setting? How does it compare to standard techniques (e.g. FGSM, IFGSM, DeepFool, Carlini)? What happens if the authors use their method both for both defense and attack? Could the authors display adversarial samples generated by their method?\n\nConclusion. The idea presented in the paper is interesting, but (1) the experimental results are not entirely satisfactory for the moment and (2) only one aspect of the idea is exploited in the paper, which can be made more interesting and impactful while studying both attack and defense setups. I strongly encourage the authors to continue their research in this area due to the high potential impact and benefits for the whole community.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper398/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper398/Official_Review", "cdate": 1542234470483, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper398/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335715077, "tmdate": 1552335715077, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper398/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1l7bFwG6X", "original": null, "number": 4, "cdate": 1541728506957, "ddate": null, "tcdate": 1541728506957, "tmdate": 1541728506957, "tddate": null, "forum": "BklpOo09tQ", "replyto": "Syey_U8927", "invitation": "ICLR.cc/2019/Conference/-/Paper398/Public_Comment", "content": {"comment": "FGSM and I-FGSM are not strong attacks and evaluating against them does not mean much - a lot of approaches that claim increased robustness simply cause gradient masking by making the optimization landscape more difficult. The proposed defense has all the hallmarks of gradient masking and will likely break when attacked with PGD with several random restarts.", "title": "Weak attacks and gradient masking"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper398/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311849602, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BklpOo09tQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper398/Authors", "ICLR.cc/2019/Conference/Paper398/Reviewers", "ICLR.cc/2019/Conference/Paper398/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper398/Authors", "ICLR.cc/2019/Conference/Paper398/Reviewers", "ICLR.cc/2019/Conference/Paper398/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311849602}}}, {"id": "HkeoqNhshQ", "original": null, "number": 2, "cdate": 1541289107229, "ddate": null, "tcdate": 1541289107229, "tmdate": 1541534029176, "tddate": null, "forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper398/Official_Review", "content": {"title": "Missing any theoretical justification, but encouraging empirical result", "review": "The paper introduces a two-step adversarial defense method, to generate two adversarial examples per clean sample and include them in the actual training loop to achieve robustness. The main claim is that the proposed two-step scheme can outperform more expensive iterative methods such as IFGSM; hence achieving robustness with lower computation. The first example is generated in a standard way (FGSM) method, while the second example is chosen to maximally increase the cross entropy between output distribution of the first and second adversarial example.\n\nThe idea seems simple and practical and the empirical results are encouraging. However, other than experiments, there is no justification why the proposed loss should do better that IFGSM. Ideally, I wanted to see the authors to start from some ideal defense definition (e.,g. Eq 4) and then show that some kind of approximation to that leads to the proposed scheme. In the absence of that, the faith about the proposed method solely must be based on the reported empirical evaluation, which is not ideal due to issues like hyper parameter tuning for each of the methods. I hope at least the authors publish the code so it could tried by others.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper398/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper398/Official_Review", "cdate": 1542234470483, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper398/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335715077, "tmdate": 1552335715077, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper398/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syey_U8927", "original": null, "number": 1, "cdate": 1541199463360, "ddate": null, "tcdate": 1541199463360, "tmdate": 1541534028976, "tddate": null, "forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper398/Official_Review", "content": {"title": "Overall good paper", "review": "Paper summary: The paper presents a 2-step approach to generate strong adversarial examples at a far lesser cost as compared to recent iterative multi-step adversarial attacks. The authors show the improvements of this technique against different attacks and show that the robustness of their 2-step approach is comparable to the iterative multi-step methods. \n\nThe paper presents an interesting technique, is nicely written and easy to read. The fact that their low-cost 2-step method achieves is robust enough to iterative multi-step methods that are expensive is significant.  \n\nPros: \n1) The technique is low-cost as compared to other expensive techniques like PGD and IFGSM \n2) The technique tries to use the categorical distribution of the generated example in the first step to generate an example in the second step, such that the generated image is most different from the first. This is important and different from the most common technique of iteratively maximizing the loss between the generated samples. \n3) The authors show the effetiveness  and improvement of the approach to various attack methods as compared to existing defense techniques\n4) The authors evaluate their technique on MNIST and SVHN datasets\n\n\nCons or shortcomings/things that need more explanation :\n1) It would have been really good to the kind of adversarial examples generated by this technique look like as compared to the examples generated by the other strategies. \n2) In table 2, for the substitute models of FGSM trained on H and S labels (rows 2 and 5), it is unclear why the accuracies are so low when attacked on FGSM (hard) and FGSM(soft) models. \n ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper398/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper398/Official_Review", "cdate": 1542234470483, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper398/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335715077, "tmdate": 1552335715077, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper398/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hygv00zHnm", "original": null, "number": 2, "cdate": 1540857550806, "ddate": null, "tcdate": 1540857550806, "tmdate": 1540857550806, "tddate": null, "forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper398/Public_Comment", "content": {"comment": "Lots of recent defenses have been shown to be causing gradient masking. Given that you are performing adversarial training similar to Tramer et al. (2018), which is not effective in a white-box setting, do you have evidence your defense is not just performing gradient masking? Athalye et al. (2018) suggest a few tests for this, such as trying random noise, or using many iterations of PGD.", "title": "Gradient Masking?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper398/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper398/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311849602, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BklpOo09tQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper398/Authors", "ICLR.cc/2019/Conference/Paper398/Reviewers", "ICLR.cc/2019/Conference/Paper398/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper398/Authors", "ICLR.cc/2019/Conference/Paper398/Reviewers", "ICLR.cc/2019/Conference/Paper398/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311849602}}}, {"id": "H1lXfGukhQ", "original": null, "number": 1, "cdate": 1540485643366, "ddate": null, "tcdate": 1540485643366, "tmdate": 1540485643366, "tddate": null, "forum": "BklpOo09tQ", "replyto": "BklpOo09tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper398/Public_Comment", "content": {"comment": "In Table 3, when attacking e2SAD with IFGSM at 10 iterations you degrade the model to 33% accuracy and at 20 iterations you degrade the model to just 40% accuracy. Is this a statistically significant difference? If it is, this is concerning because more iterations of an attack should produce only stronger results.", "title": "Table 3 Concerns"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper398/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT TWO-STEP ADVERSARIAL DEFENSE FOR DEEP NEURAL NETWORKS", "abstract": "In recent years, deep neural networks have demonstrated outstanding performancein many machine learning tasks. However, researchers have discovered that thesestate-of-the-art models are vulnerable to adversarial examples:  legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples duringthe training process,  is a well known defense to improve the robustness of themodel against adversarial attacks.  However, this robustness is only effective tothe same attack method used for adversarial training.  Madry et al. (2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularlythat projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistanceagainst many other first order attacks.   However,  the computational cost of theadversarial training with PGD and other multi-step adversarial examples is muchhigher than that of the adversarial training with other simpler attack techniques.In this paper, we show how strong adversarial examples can be generated only ata cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to thatof the adversarial training with multi-step adversarial examples.  We empiricallydemonstrate the effectiveness of the proposed two-step defense approach againstdifferent attack methods and its improvements over existing defense strategies.", "keywords": ["Adversarial Examples", "Adversarial Training", "FGSM", "IFGSM", "Robustness"], "authorids": ["tingjui.chang@tamu.edu", "dominiche@tamu.edu", "pli@tamu.edu"], "authors": ["Ting-Jui Chang", "Yukun He", "Peng Li"], "TL;DR": "We proposed a time-efficient defense method against one-step and iterative adversarial attacks.", "pdf": "/pdf/2241b2f1da22d62445f417640b85fbb4117314d2.pdf", "paperhash": "chang|efficient_twostep_adversarial_defense_for_deep_neural_networks", "_bibtex": "@misc{\nchang2019efficient,\ntitle={{EFFICIENT} {TWO}-{STEP} {ADVERSARIAL} {DEFENSE} {FOR} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ting-Jui Chang and Yukun He and Peng Li},\nyear={2019},\nurl={https://openreview.net/forum?id=BklpOo09tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper398/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311849602, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BklpOo09tQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper398/Authors", "ICLR.cc/2019/Conference/Paper398/Reviewers", "ICLR.cc/2019/Conference/Paper398/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper398/Authors", "ICLR.cc/2019/Conference/Paper398/Reviewers", "ICLR.cc/2019/Conference/Paper398/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311849602}}}], "count": 8}