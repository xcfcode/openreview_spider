{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730178614, "tcdate": 1509115226472, "number": 416, "cdate": 1518730178605, "id": "ByzvHagA-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "ByzvHagA-", "original": "BJfvragRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260082614, "tcdate": 1517249988728, "number": 674, "cdate": 1517249988715, "id": "rkpaB1prz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "ByzvHagA-", "replyto": "ByzvHagA-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The novelty of the paper is limited and it lacks on comparisons with relevant baselines, as pointed out by the reviewers. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642446054, "tcdate": 1511776016258, "number": 1, "cdate": 1511776016258, "id": "H1dMyvFgG", "invitation": "ICLR.cc/2018/Conference/-/Paper416/Official_Review", "forum": "ByzvHagA-", "replyto": "ByzvHagA-", "signatures": ["ICLR.cc/2018/Conference/Paper416/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Simple penalty term enforcing decorrelation in the representation. Seems to work, but not fully analyzed -> so-so manuscript", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a penalization term that enforces decorrelation between the dimensions of the representation \nThey show that it can be included as additional term in cost functions to train generic models.\nThe idea is simple and it seems to work for the presented examples.\n\nHowever, they talk about gradient descent using this extra term, but I'd like to see the derivatives of the \nproposed term depending on the parameters of the model (and this depends on the model!). On the other hand, \ngiven the expression of the proposed regulatization,\nit seems to lead to non-convex optimization problems which are hard to solve. Any comment on that?.\n\nMoreover, its results are not quantitatively compared to other Non-Linear generalizations of PCA/ICA designed for similar goals (e.g. those cited in the \"related work\" section or others which have been proved to be consistent non-linear generalizations of PCA such as: Principal Polynomial Analysis, Dimensionality Reduction via Regression that follow the family introduced in the book of Jolliffe, Principal Component Analysis).\n\nMinor points: Fig.1 conveys not that much information.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642445968, "id": "ICLR.cc/2018/Conference/-/Paper416/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper416/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper416/AnonReviewer2", "ICLR.cc/2018/Conference/Paper416/AnonReviewer3", "ICLR.cc/2018/Conference/Paper416/AnonReviewer1"], "reply": {"forum": "ByzvHagA-", "replyto": "ByzvHagA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642445968}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642446019, "tcdate": 1511793896626, "number": 2, "cdate": 1511793896626, "id": "HJbgHsYlM", "invitation": "ICLR.cc/2018/Conference/-/Paper416/Official_Review", "forum": "ByzvHagA-", "replyto": "ByzvHagA-", "signatures": ["ICLR.cc/2018/Conference/Paper416/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Benefits are not clear enough", "rating": "5: Marginally below acceptance threshold", "review": "\nI think the first intuition is interesting. However I think the benefits are not clear enough. Maybe finding better examples where the benefits of the proposed regularization are stressed could help. \n\nThere is a huge amount of literature about ICA, unmixing, PCA, infomax... based on this principle that go beyond of the proposal. I do not see a clear novelty in the proposal.  \n\nFor instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on PCA. As shown in [Szegedy et al 2014, \"Intriguing properties of neural networks\"] adding an extra linear transformation does not change the expressive power of the representation.    \n\n\n- \"Inspired by this, we consider a simpler objective: a representation disentangles the data well when its components do not correlate...\"\n\nThe first paragraph is confusing since jumps from total correlation to correlation without making clear the differences.\nAlthough correlation is a second oder approach to total correlation are not the same. This is extremely important since the whole proposal is based on that.\n\n- Sec 2.1. What prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covariance. I think the definition needs to enforce the out-diagonal terms in C to be small with respect to the terms in the diagonal.   \n\n- All the evaluation measures are based on linear relations, some of them should take into account non-linear relations (i.e. total correlation, mutual information...) in order to show that the method gets something interesting.\n\n- The first experiment (dim red) is not clear to me. The original dimensionality of the data is 4, and only a linear relation is introduced. I do not understand the dimensionality reduction if the dimensionality of the transformed space is 10. Also the data problem is extremely simple, and it is not clear the didactic benefit of using it. I think a much more complicated data would be more interesting. Besides L_1 is not well defined. If it is L_1 norm on the output coefficients the comparison is misleading. \n\n- Sec 3.3. As in general the model needs to be compared with other regularization techniques to stress its benefits.\n\n- Sec 3.4. Here the comparison makes clear that not a real benefit is obtained with the proposal. The idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future samples. However the MSE obtained when not using regularization is the same (or even smaller) than when using it.   \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642445968, "id": "ICLR.cc/2018/Conference/-/Paper416/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper416/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper416/AnonReviewer2", "ICLR.cc/2018/Conference/Paper416/AnonReviewer3", "ICLR.cc/2018/Conference/Paper416/AnonReviewer1"], "reply": {"forum": "ByzvHagA-", "replyto": "ByzvHagA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642445968}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642445983, "tcdate": 1511846596156, "number": 3, "cdate": 1511846596156, "id": "rkhaGO9gG", "invitation": "ICLR.cc/2018/Conference/-/Paper416/Official_Review", "forum": "ByzvHagA-", "replyto": "ByzvHagA-", "signatures": ["ICLR.cc/2018/Conference/Paper416/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting approach but could use more compelling demonstrations", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network. This penalty is meant to disentangle the latent representation by removing shared covariance between each dimension. \n\nWhile the proposed penalty is described as a novel contribution, there are multiple instances of previous work which use the same type of penalty (Cheung et. al. 2014, Cogswell et. al. 2016). Like this work, Cheung et. al. 2014 propose the XCov penalty which penalizes cross-covariance to disentangle subsets of dimensions in the latent representation of autoencoder models. Cogswell et. al. 2016 also proposes a similar penalty (DeCov) to this work for reducing overfitting in supervised learning.\n\nThe novel contribution of the regularizer proposed in this work is that it also penalizes the variance of individual dimensions along with the cross-covariance. Intuitively, this should lead to dimensionality reduction as the model will discard variance in dimensions which are unnecessary for reconstruction. But given the similarity to previous work, the authors need to quantitatively evaluate the value in additionally penalizing variance of each dimension as compared with earlier work. Cogswell et. al. 2016 explicitly remove these terms from their regularizer to prevent the dynamic range of the activations from being unnecessarily rescaled.  It would be helpful to understand how this approach avoids this issues - i.e.,  if you penalize all the variance terms then you could just be arbitrarily rescaling the activities, so what prevents this trivial solution?\n\nThere doesn't appear to be a definition of the L1 penalty this paper compares against and it's unclear why this is a reasonable baseline. The evaluation metrics this work uses (MAPC, CVR, TdV, UD) need to be justified more in the absence of their use in previous work. While they evaluate their method on non-toy dataset such as CIFAR, they do not show what the actual utility of their proposed regularizer serves for such a dataset beyond having no-regularization at all. Again, the utility of the evaluation metrics proposed in this work is unclear.\n\nThe toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets.\n\n> Our method has no penalty on the performance on tasks evaluated in the experiments, while it does disentangle the data\n\nThis needs to be expanded in the results as all the results presented appear to show Mean Squared Error increasing when increasing the weight of the regularization penalty.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642445968, "id": "ICLR.cc/2018/Conference/-/Paper416/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper416/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper416/AnonReviewer2", "ICLR.cc/2018/Conference/Paper416/AnonReviewer3", "ICLR.cc/2018/Conference/Paper416/AnonReviewer1"], "reply": {"forum": "ByzvHagA-", "replyto": "ByzvHagA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642445968}}}, {"tddate": null, "ddate": null, "tmdate": 1515191990498, "tcdate": 1515191990498, "number": 3, "cdate": 1515191990498, "id": "Hy0nCO6XG", "invitation": "ICLR.cc/2018/Conference/-/Paper416/Official_Comment", "forum": "ByzvHagA-", "replyto": "H1dMyvFgG", "signatures": ["ICLR.cc/2018/Conference/Paper416/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper416/Authors"], "content": {"title": "Comparisons and related work", "comment": "Thank you for the comments, we will take them into account. There will be some more evaluations and comparisons added to the final version (see above about Cheung et.al. and Cogswell et.al.)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733999, "id": "ICLR.cc/2018/Conference/-/Paper416/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ByzvHagA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper416/Authors|ICLR.cc/2018/Conference/Paper416/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper416/Authors|ICLR.cc/2018/Conference/Paper416/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper416/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper416/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper416/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper416/Reviewers", "ICLR.cc/2018/Conference/Paper416/Authors", "ICLR.cc/2018/Conference/Paper416/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733999}}}, {"tddate": null, "ddate": null, "tmdate": 1515191817491, "tcdate": 1515191817491, "number": 2, "cdate": 1515191817491, "id": "BkbMROTQG", "invitation": "ICLR.cc/2018/Conference/-/Paper416/Official_Comment", "forum": "ByzvHagA-", "replyto": "HJbgHsYlM", "signatures": ["ICLR.cc/2018/Conference/Paper416/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper416/Authors"], "content": {"title": "Good points!", "comment": "Thank you for the insightful comments, we will take them into account when preparing a final version of our paper!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733999, "id": "ICLR.cc/2018/Conference/-/Paper416/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ByzvHagA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper416/Authors|ICLR.cc/2018/Conference/Paper416/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper416/Authors|ICLR.cc/2018/Conference/Paper416/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper416/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper416/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper416/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper416/Reviewers", "ICLR.cc/2018/Conference/Paper416/Authors", "ICLR.cc/2018/Conference/Paper416/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733999}}}, {"tddate": null, "ddate": null, "tmdate": 1515188498470, "tcdate": 1515188498470, "number": 1, "cdate": 1515188498470, "id": "Sy5z-uTXf", "invitation": "ICLR.cc/2018/Conference/-/Paper416/Official_Comment", "forum": "ByzvHagA-", "replyto": "rkhaGO9gG", "signatures": ["ICLR.cc/2018/Conference/Paper416/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper416/Authors"], "content": {"title": "Clarification on related work", "comment": "Thank you for constructive and well-researched comments. Please consider the following comments on the mentioned related work.\n\nCheung, et.al.  - This paper describes a different type of regularizer that penalize the correlation between hidden units and labels. In contrast we aim to learn a hidden representation that disentangles unknown underlying factors by penalizing correlation between hidden units. Hence, our method use no labels and can go much further in disentangling the signal.\n\nCogswell et.al. - As noted by reviewer one, what sets our work apart from Cogswell et.al., is that we penalize the full covariance matrix while they only penalize the off diagonal elements. The reason that we included the diagonal is that this will lead to a lower variance hypothesis, by removing information not needed to solve the task from the representation, which in turn yields guaranteed lower excess risk (Maurer 2009). Also, we got slightly better empirical results doing this and will include comparisons to Cogswell et.al. in the final version of the paper.\nAnother difference between our model and that of Cogswell et.al. is that we use the L1 matrix norm (in contrast to the frobenius norm) in order to promote a sparse solution. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled activations in deep networks", "abstract": "Deep neural networks have been tremendously successful in a number of tasks.\nOne of the main reasons for this is their capability to automatically\nlearn representations of data in levels of abstraction,\nincreasingly disentangling the data as the internal transformations are applied.\nIn this paper we propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network, something that benefits the disentanglement.\nThis makes the network learn nonlinear representations that are linearly uncorrelated, yet allows the model to obtain good results on a number of tasks, as demonstrated by our experimental evaluation.\nThe proposed technique can be used to find the dimensionality of the underlying data, because it effectively disables dimensions that aren't needed.\nOur approach is simple and computationally cheap, as it can be applied as a regularizer to any gradient-based learning model.", "pdf": "/pdf/1268ebf35db399a5cab1b4f19fefae16d8f1cd06.pdf", "TL;DR": "We propose a novel regularization method that penalize covariance between dimensions of the hidden layers in a network.", "paperhash": "k\u00e5geb\u00e4ck|disentangled_activations_in_deep_networks", "_bibtex": "@misc{\nk\u00e5geb\u00e4ck2018disentangled,\ntitle={Disentangled activations in deep networks},\nauthor={Mikael K\u00e5geb\u00e4ck and Olof Mogren},\nyear={2018},\nurl={https://openreview.net/forum?id=ByzvHagA-},\n}", "keywords": ["representation learning", "disentanglement", "regularization"], "authors": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren"], "authorids": ["kageback@chalmers.se", "olof@mogren.one"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825733999, "id": "ICLR.cc/2018/Conference/-/Paper416/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ByzvHagA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper416/Authors|ICLR.cc/2018/Conference/Paper416/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper416/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper416/Authors|ICLR.cc/2018/Conference/Paper416/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper416/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper416/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper416/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper416/Reviewers", "ICLR.cc/2018/Conference/Paper416/Authors", "ICLR.cc/2018/Conference/Paper416/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825733999}}}], "count": 8}