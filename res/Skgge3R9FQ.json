{"notes": [{"id": "Skgge3R9FQ", "original": "B1eUNq35YX", "number": 1046, "cdate": 1538087912510, "ddate": null, "tcdate": 1538087912510, "tmdate": 1545355435118, "tddate": null, "forum": "Skgge3R9FQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1g_fLwBlE", "original": null, "number": 1, "cdate": 1545070095620, "ddate": null, "tcdate": 1545070095620, "tmdate": 1545354481721, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Meta_Review", "content": {"metareview": "The reviewers agree the paper is not ready for publication at ICLR.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1046/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352985033, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352985033}}}, {"id": "SylJ-xIopQ", "original": null, "number": 8, "cdate": 1542311926640, "ddate": null, "tcdate": 1542311926640, "tmdate": 1542311926640, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "H1gMR8h6h7", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "Asking for more explanations", "comment": "We are thankful for the reviewer 2 to provide us with his/her feedback,\n\nThe reviewer mentioned: \"the interpolation mechanism is also too simple\":\nWe would like to highlight that despite the simplicity of interpolated samples, there has been demonstrated the effectiveness of using such samples on developing more regularized and generalized neural networks (Zhang et al, 2018) as well as on making them more secure (Zhao et. al. 2018).  Thus, we believe that simplicity does not necessarily lead to ineffectiveness. \n\n\nThe reviewer mentioned \u201cmany hidden assumptions on the images source or the base classier\u201d:\nAs this statement is not clear for us, we would appreciate if the reviewer could elaborate more on it. The only assumption we made is on the fact that the out-distribution samples should be statistically and semantically different than the in-distribution samples. Then among such out-distribution sets, we propose a measurement for identifying the most representative one among those available. \n\nThe reviewer stated \"There exists more principled approaches for *selecting* out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors\":\nWhile there are the approaches that aim to detect out-distribution sets, they have not been designed for the selection purposes as we do. By mentioning this statement, if the reviewer means the missing of some principled approaches like ODIN in our comparisons, we would like to inform the inclusion of ODIN results in the revised version of the paper. \n\nReference:\n- Zhao, Jake, and Kyunghyun Cho. \"Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples.\" arXiv preprint arXiv:1802.09502 (2018).\n\n- Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017). mixup: Beyond empirical risk minimization. ICLR 2018 (arXiv preprint arXiv:1710.09412).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "SkevWvSj6m", "original": null, "number": 6, "cdate": 1542309630564, "ddate": null, "tcdate": 1542309630564, "tmdate": 1542310135398, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "On the originality/novelty of the paper", "comment": "The originality of our paper is not on making use of a dustbin class (OOV). As mentioned by the reviewers, this has been proposed not only for NLP and vision tasks but also for adversaries detection (Gross et. al, 2017, Hosseini et. al, 2017) and out-distribution identification (Yu et. al. 2017). \n\nOur main goal is rather to demonstrate the relationship between over-generalization induced by naive CNN and its sensitivity to **both** adversarial examples and out-distribution samples. To this end, we used Augmented CNN (A-CNN) as a tool for controlling over-generalization. Indeed, to our knowledge, we are the first to demonstrate this relationship, i.e. over-generalization and sensitivity to adversaries (as well as out-distribution samples), in an extensive experimental setup. Thus, the originality of our paper does not lie on re-proposing A-CNNs, but we used it as a tool to show that mitigating the effect of over-generalization on the development of more secure neural networks (e.g. CNN) in the presence of adversaries and out-distribution sets. \n\nThe immediate natural key question is how to acquire the training samples for the extra class (from a nearly infinite number of out-distribution samples) in order to cover properly the over-generalized regions induced by a naive CNN. Instead of synthesizing artificial out-distribution samples using a hard-to-train generator (Lee. et al ICLR 2018, Jin et. al NIPS 2017, Yu et. al IJCAI 2017 ), we rather propose the use of a novel measurement for selecting a representative out-distribution dataset among the readily accessible natural ones for training an effective A-CNN. This not only maintains the accuracy on in-distribution samples but also results in identifying both adversaries and unseen out-distribution sets simultaneously.\n\nReference:\n- Grosse, Kathrin, et al. \"On the (statistical) detection of adversarial examples.\" arXiv preprint arXiv:1702.06280 (2017).\n\n-Hosseini, Hossein, et al. \"Blocking transferability of adversarial examples in black-box learning systems.\" arXiv preprint arXiv:1703.04318 (2017).\n\n-Jin, Long, Justin Lazarow, and Zhuowen Tu. \"Introspective classification with convolutional nets.\" Advances in Neural Information Processing Systems. 2017.\n\n-Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detectingout-of-distribution samples. ICLR2018.\n\n- Yu, Yang, et al. \"Open-category classification by adversarial sample generation.\" Proceeding of the 26-th International Joint Conference on Artificial Intelligence (2017).\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "BJeC2_Hspm", "original": null, "number": 7, "cdate": 1542310069817, "ddate": null, "tcdate": 1542310069817, "tmdate": 1542310069817, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "H1eqf5OA3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "More clarification ", "comment": "We appreciate the reviewer 1 for his/her feedback on our paper. \n\nThe reviewer mentioned: \u201cIn general it is very unlikely that you will be able to choose every variation of out-distribution cases\u201d:\nActually, for training A-CNN (Augmented CNN), we did not train it on every variation of out-distribution cases, rather, we recognize a single representative out-distribution set among the available ones according to our measurement. Then using it for training A-CNN with the aim of effectively controlling over-generalization.\n\n\nThe reviewer mentioned: \u201c Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax\u201d:\nWe would be appreciated if the reviewer could provide us with the references that showing using only sigmoid could control such a challenging problem of adversaries. \nPlease note we did not aim to devise a method that is able to reject all adversaries. Rather, we attempted to show that a CNN with less over-generalization is able to reject some of the adversaries while correctly classifies many of the remainder, particularly non-transferable attacks. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "Skl-BtQq6X", "original": null, "number": 3, "cdate": 1542236473278, "ddate": null, "tcdate": 1542236473278, "tmdate": 1542236473278, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Review", "content": {"title": "Interesting research direction but not good enough", "review": "This paper proposed to add an additional label for detecting OOD samples and adversarial examples in CNN models. This research direction seems interesting, however, the idea of using an extra label for OODs is not new and was previously explored in different domains. I would expect the describe how their method is different, and keep the research from that point.\nAdditionally, there are several claims in this paper which I'm not convinced are true, such as the over-generalization of CNNs, the choice of OODs (recent studies have shown NNs are not well calibrated, so using softmax as the confidence might not be the best idea), etc.\nReg. the results, did the authors compare their method to existing adv. example detection methods, such as Ma, Xingjun, et al. ICLR (2018) \"Characterizing adversarial subspaces using local intrinsic dimensionality.\" ? or some other method? \nMoreover, in Table 2. I'm not sure what should I conclude from the \"Naive Model Error\" on OOD samples.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Review", "cdate": 1542234318620, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335860762, "tmdate": 1552335860762, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylwp4Sm67", "original": null, "number": 5, "cdate": 1541784767067, "ddate": null, "tcdate": 1541784767067, "tmdate": 1541784767067, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "rygaTnElhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "Response", "comment": "As you mentioned, we agree that the interpolated samples are not necessarily located on (around) the decision boundaries. There are some reliable approaches for providing an exact solution for this problem, such as DeepFool, that are able to find the samples (adversaries) located around the decision boundaries, but these are computationally expensive. \n\nZhang et. al (2017) also have considered the interpolated samples (in input space) generated with \\alpha=0.5 from the pairs of samples (x_i, x_j) selected from different classes are placed around some virtual decision boundaries as their labels are regarded as the average of their true labels (one-hot vectors i.e. 0.5 y_i + 0.5 y_j ). By regarding such interpolated samples on some virtual decision boundaries, we labeled them as dustbin class. \n\nZhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017). mixup: Beyond empirical risk minimization. ICLR 2018 (arXiv preprint arXiv:1710.09412).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "H1ghn977a7", "original": null, "number": 4, "cdate": 1541778099731, "ddate": null, "tcdate": 1541778099731, "tmdate": 1541778099731, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "rkxFFJ-jsX", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "dustbin regions in white-box adversaries directions", "comment": "Question 1 (fooling regions vs dustbin): From Figure 6, it can be observed that moving in the white-box adversaries directions can end up in the dustbin regions. This can show that some of the fooling regions are assigned to the dustbin label in augmented CNNs.\n\nQuestion 2 (about Sec. 3.2): To generate T-FGS adversaries, we discard the dustbin class as possible fooling target. Adversaries are then generated with a specific epsilon and within a fixed number of iterations (same values as for generating black-box adversaries, see Table 5 of Appendix). To skip over the dustbin regions, the number of iterations for finding adversaries should be increased, which in turn will increase the amount of perturbation.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "SygmR_X7pX", "original": null, "number": 3, "cdate": 1541777611137, "ddate": null, "tcdate": 1541777611137, "tmdate": 1541777611137, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "B1lmogO4j7", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "Computational complexity", "comment": "As mentioned in the paper, the instances are mapped into the feature space generated by the last convolutional layer. We use this space to identify the nearest neighbor using an Euclidean distance, i.e., $min_x\u2019 \\|\\phi(x) - \\phi(x\u2019)\\|_2$, where $\\phi(x)$ is the representation of $x$ in the feature space. As the dimensionality in the feature space is significantly lower than the original pixel space, the computational complexity of finding nearest neighbors in the feature space is lower than that of the pixel space. By using a proper data structure (e.g., k-d tree, which is implemented in scikit-learn), the complexity time of finding the nearest neighbor can be further reduced, compared to that of a naive k-NN implementation. Moreover, using GPUs and parallel computations, it has been shown the computational time for k-NN can be diminished significantly (Johnson et. al., 2017).\n\nReference: Johnson, Jeff, Matthijs Douze, and Herv\u00e9 J\u00e9gou. \"Billion-scale similarity search with GPUs.\" arXiv preprint arXiv:1702.08734 (2017)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "H1eqf5OA3m", "original": null, "number": 2, "cdate": 1541470738281, "ddate": null, "tcdate": 1541470738281, "tmdate": 1541533470219, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Review", "content": {"title": "The work can be improved", "review": "The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored. In fact, in machine learning and NLP there is the OOV class which sometimes people in computer vision also use. Some of the claims in the paper can be further substantiated or explored. For example in abstract there is a simple claim that is presented too strong: We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one. Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax. In general it is very unlikely that you will be able to choose every variation of out-distribution cases. Much easier if you just try to solve the problem using a set of n Sigmoids (n total number of classes) and consider each output a probability distribution. \n\nHowever, the studies in this paper are still valuable and I strongly recommend continuing on the same direction. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Review", "cdate": 1542234318620, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335860762, "tmdate": 1552335860762, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1gMR8h6h7", "original": null, "number": 1, "cdate": 1541420746087, "ddate": null, "tcdate": 1541420746087, "tmdate": 1541533470020, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Review", "content": {"title": "Too many hidden assumptions", "review": "The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs. The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem. The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images. There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.\nIn summary, the quality of the paper is poor and the originality of the work is low. The paper is easily readable.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Review", "cdate": 1542234318620, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335860762, "tmdate": 1552335860762, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygaTnElhQ", "original": null, "number": 6, "cdate": 1540537540917, "ddate": null, "tcdate": 1540537540917, "tmdate": 1540537652953, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "content": {"comment": "The interpolated  samples $ {x}' = \\alpha x_i + (1 \u2212 \\alpha ) x_j $  are not necessarily around the decision boundaries. \n\nBecause the interpolation operation is performed in the input space, but not in the high-level feature space.  Assume that the last convolution layer is f(x), $ f({x}') $ is not equal $ f(\\alpha x_i + (1 \u2212 \\alpha ) x_j) $ in fact.", "title": "Rethink the Interpolated Instances"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311691546, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skgge3R9FQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311691546}}}, {"id": "ByelE-bMjX", "original": null, "number": 3, "cdate": 1539604776010, "ddate": null, "tcdate": 1539604776010, "tmdate": 1540198721591, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "content": {"comment": "You are missing too many baselines. \n\n I could not see why  out-distribution detection methods can be considered as baselines in the paper.\n\nout-distribution detection:\n\n- baseline: [1]  (Hendrycks & Gimpel, 2016)\n- ODIN: [2] (Liang et al., 2017)\n- Bayesian Neural Network: [3] (Gal, 2016)\n- LearningConfidence: [4] (DeVries & Taylor, 2018)\n\nSimilarly,  I could not see why adversarial examples detection methods can be considered as baselines in the paper.\n\n adversarial examples detection:\n\n- KD+PU: [5]\n- LID: [6]\n\n\n[1] A baseline for detecting misclassified and out-of-distribution examples in neural networks.\n[2] Enhancing the reliability of out-of-distribution image detection in neural networks\n[3] Uncertainty in deep learning\n[4] Learning confidence for out-of-distribution detection in neural networks\n[5] Detecting adversarial samples from artifacts.\n[6] Characterizing adversarial subspaces using local intrinsic dimensionality. ", "title": "Missing baselines"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311691546, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skgge3R9FQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311691546}}}, {"id": "rkxFFJ-jsX", "original": null, "number": 5, "cdate": 1540194176791, "ddate": null, "tcdate": 1540194176791, "tmdate": 1540197624388, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "content": {"comment": ">> the fooling classification regions (spanned by the adversary direction and one of its orthogonal random directions) of the naive CNNs are occupied by dustbin regions  (in Section 3.1)\n\nIt doesn't mean that  the fooling classification regions of the Augmented CNNs are occupied by dustbin regions. Could you plot several church-windows where the x-axis of each window is the adversary direction achieved by FGS or DeepFool using  the Augmented CNNs? \n\nFor Section 3.2, have you tried the targeted adversarial attacks to skip over some regions assigned to dustbin class ?", "title": "Some questions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311691546, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skgge3R9FQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311691546}}}, {"id": "B1lmogO4j7", "original": null, "number": 4, "cdate": 1539764379113, "ddate": null, "tcdate": 1539764379113, "tmdate": 1539764843392, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "content": {"comment": "Hi, could you describe the details on how to find the nearest neighbor of x_i in the feature space of a CNN on Section 2.2 ? Is it expensive computationally for large-scale datasets ?\n\nAnd is the last layer referring to the previous layer of the logit layer?", "title": "Details on Section 2.2"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311691546, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skgge3R9FQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311691546}}}, {"id": "Bkeqla1No7", "original": null, "number": 2, "cdate": 1539730673857, "ddate": null, "tcdate": 1539730673857, "tmdate": 1539730722384, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "S1evvljJsQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "Our response ", "comment": "The incapacity of Grosse et al. approach to detect some well-crafted white-box attacks does not warrant that many of the black-box adversaries, which typically includes more perturbations than their white-box counterparts, are not statistically different from the problem distribution (i.e. in-distribution). For example, for creating transferable black-box C&W adversaries, the amount of perturbations required increases (Carlini and Wagner, 2017).  \n\nMoreover, note that we also exhibit in Table 1 that training an augmented CNN on *only I-FGS* (similar to the approach of Grosse et al.) cannot make it robust to other types of (black-box) adversaries like C&W and DeepFool. Plus, this augmented CNN (trained on I-FGS) is also unable to identify out-distribution samples (Table 3 in Appendix). This may be happened as I-FGS only cover partially the over-generalized regions (Fig. 1 (b)). \n\nFinally, we are not claiming that our augmented CNN is an ultimate solution for all possible kinds of adversaries. But, through extensive experiments, we shed some light on how controlling over-generalization effectively can positively influence the development of CNNs that are more robust in the presence of *both* natural out-distribution samples and some strong and highly transferable (black-box) attacks.\n\nReference :\n1) Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. IEEE Symposium on Security and Privacy (SP), 2017.\n2) Grosse, Kathrin, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. \"On the (statistical) detection of adversarial examples.\" arXiv preprint arXiv:1702.06280 (2017)\n\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "S1evvljJsQ", "original": null, "number": 2, "cdate": 1539448927148, "ddate": null, "tcdate": 1539448927148, "tmdate": 1539448927148, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "rkgfQcOksQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "content": {"comment": "It should be noted that the work of Grosse et al. was refuted by https://arxiv.org/abs/1705.07263 which shows that it is only adversarial examples generated with fixed and very simple attack algorithms are statistically detectable as different. However, any recent optimization-based attack (PGD/C&W) will generate adversarial examples that are not statistically different from clean examples as far as the detection Grosse et al. scheme will find.", "title": "Adversarial examples are not drawn from a different distribution"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311691546, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skgge3R9FQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311691546}}}, {"id": "rkgfQcOksQ", "original": null, "number": 1, "cdate": 1539439130123, "ddate": null, "tcdate": 1539439130123, "tmdate": 1539439130123, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "rkelT3a3c7", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "content": {"title": "To respond the anonymous commenter", "comment": "We greatly thank the anonymous commenter, \nThrough statistical testing, Grosse et al. [1] demonstrated that adversarial examples are indeed drawn from a distribution different from the distribution of original (in-distribution) data of a given task. Moreover, the over-generalized (i.e. out-distribution) regions contain the samples statistically/semantically different from in-distribution ones. Thus, adversarial examples can be treated as out-distribution samples. We show that if the over-generalized regions can be effectively reduced, then the risk of being fooled by adversaries can be significantly reduced.\n\nIn other words, by training an augmented CNN with the dustbin class containing only representative natural out-distribution samples, we tend to refine the frontiers (boundaries) of our model regarding out-distribution regions, and as such improving how adversarial samples are processed, by either correctly classifying these samples or classifying them as dustbin (equivalent to rejection).\n\nReference: Grosse, Kathrin, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. \"On the (statistical) detection of adversarial examples.\" arXiv preprint arXiv:1702.06280 (2017)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1046/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619316, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skgge3R9FQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1046/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1046/Authors|ICLR.cc/2019/Conference/Paper1046/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619316}}}, {"id": "rkelT3a3c7", "original": null, "number": 1, "cdate": 1539263671896, "ddate": null, "tcdate": 1539263671896, "tmdate": 1539263845205, "tddate": null, "forum": "Skgge3R9FQ", "replyto": "Skgge3R9FQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "content": {"comment": "Hi, thanks for the nice paper. \n\nThis paper makes a claim:\n>>such that  the samples drawn from many over-generalized regions including a wide-range of out-distribution samples and various types of adversaries are mapped to this \u201cdustbin\u201d sub-manifold.\n\nYour theory seems to assume that adversarial examples exist in the over-generalized regions but not in the in-distribution region, am I right? What is the relationship between the over-generalized regions and the in-distribution regions?", "title": "Confusing the over-generalized regions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1046/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation", "abstract": "Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.", "keywords": ["Convolutional Neural Networks", "Adversarial Instances", "Out-distribution Samples", "Rejection Option", "Over-generalization"], "authorids": ["mahdieh.abbasi.1@ulaval.ca", "rajabia@oregonstate.edu", "azadeh-sadat.mozafari.1@ulaval.ca", "rakesh.bobba@oregonstate.edu", "christian.gagne@gel.ulaval.ca"], "authors": ["Mahdieh Abbasi", "Arezoo Rajabi", "Azadeh Sadat Mozafari", "Rakesh B. Bobba", "Christian Gagn\u00e9"], "TL;DR": "Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.", "pdf": "/pdf/5cfde5d717e354e5e641d4186012a623afdb9480.pdf", "paperhash": "abbasi|controlling_overgeneralization_and_its_effect_on_adversarial_examples_detection_and_generation", "_bibtex": "@misc{\nabbasi2019controlling,\ntitle={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},\nauthor={Mahdieh Abbasi and Arezoo Rajabi and Azadeh Sadat Mozafari and Rakesh B. Bobba and Christian Gagn\u00e9},\nyear={2019},\nurl={https://openreview.net/forum?id=Skgge3R9FQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1046/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311691546, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skgge3R9FQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1046/Authors", "ICLR.cc/2019/Conference/Paper1046/Reviewers", "ICLR.cc/2019/Conference/Paper1046/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311691546}}}], "count": 19}