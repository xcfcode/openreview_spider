{"notes": [{"id": "BkepbpNFwr", "original": "BJednad8wS", "number": 392, "cdate": 1569438980865, "ddate": null, "tcdate": 1569438980865, "tmdate": 1583912036598, "tddate": null, "forum": "BkepbpNFwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Progressive Memory Banks for Incremental Domain Adaptation", "authors": ["Nabiha Asghar", "Lili Mou", "Kira A. Selby", "Kevin D. Pantasdo", "Pascal Poupart", "Xin Jiang"], "authorids": ["nasghar@uwaterloo.ca", "doublepower.mou@gmail.com", "kaselby@uwaterloo.ca", "kevin.pantasdo@uwaterloo.ca", "ppoupart@uwaterloo.ca", "jiang.xin@huawei.com"], "keywords": ["natural language processing", "domain adaptation"], "TL;DR": "We present a neural memory-based architecture for incremental domain adaptation, and provide theoretical and empirical results.", "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is  to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation and progressive neural networks in the experiments.", "pdf": "/pdf/09cab4009ea99ae5ec0339eca94cdcb853cfba52.pdf", "code": "https://github.com/nabihach/IDA", "paperhash": "asghar|progressive_memory_banks_for_incremental_domain_adaptation", "_bibtex": "@inproceedings{\nAsghar2020Progressive,\ntitle={Progressive Memory Banks for Incremental Domain Adaptation},\nauthor={Nabiha Asghar and Lili Mou and Kira A. Selby and Kevin D. Pantasdo and Pascal Poupart and Xin Jiang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkepbpNFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7e3fb050f0162f9dd7abb3a28728cd8b05276c25.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "rZIA_JamId", "original": null, "number": 1, "cdate": 1576798695106, "ddate": null, "tcdate": 1576798695106, "tmdate": 1576800940471, "tddate": null, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "invitation": "ICLR.cc/2020/Conference/Paper392/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper introduces an RNN based approach to incremental domain adaptation in natural language processing, where the RNN is progressively augmented with the parameterized memory bank which is shown to be better than expanding the RNN states.\n\nReviewers and AC acknowledge that this paper is well written with interesting ideas and practical value. Domain adaptation in the incremental setting, where domains come in a streaming way with only the current one accessible, can find some realistic application scenarios. The proposed extensible attention mechanism is solid and works well on several NLP tasks. Several concerns were raised by the reviewers regarding the comparative and ablation studies, which were well resolved in the rebuttal. The authors are encouraged to generalize their approach to other application domains other than NLP to show the generality of their approach.\n\nI recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive Memory Banks for Incremental Domain Adaptation", "authors": ["Nabiha Asghar", "Lili Mou", "Kira A. Selby", "Kevin D. Pantasdo", "Pascal Poupart", "Xin Jiang"], "authorids": ["nasghar@uwaterloo.ca", "doublepower.mou@gmail.com", "kaselby@uwaterloo.ca", "kevin.pantasdo@uwaterloo.ca", "ppoupart@uwaterloo.ca", "jiang.xin@huawei.com"], "keywords": ["natural language processing", "domain adaptation"], "TL;DR": "We present a neural memory-based architecture for incremental domain adaptation, and provide theoretical and empirical results.", "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is  to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation and progressive neural networks in the experiments.", "pdf": "/pdf/09cab4009ea99ae5ec0339eca94cdcb853cfba52.pdf", "code": "https://github.com/nabihach/IDA", "paperhash": "asghar|progressive_memory_banks_for_incremental_domain_adaptation", "_bibtex": "@inproceedings{\nAsghar2020Progressive,\ntitle={Progressive Memory Banks for Incremental Domain Adaptation},\nauthor={Nabiha Asghar and Lili Mou and Kira A. Selby and Kevin D. Pantasdo and Pascal Poupart and Xin Jiang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkepbpNFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7e3fb050f0162f9dd7abb3a28728cd8b05276c25.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729637, "tmdate": 1576800282267, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper392/-/Decision"}}}, {"id": "r1gnymt9cH", "original": null, "number": 3, "cdate": 1572668131838, "ddate": null, "tcdate": 1572668131838, "tmdate": 1576016349556, "tddate": null, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "invitation": "ICLR.cc/2020/Conference/Paper392/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "\n###Summary###\nThis paper introduces incremental domain adaptation for natural language processing, assuming that each domain comes one after another and only the current domain can be accessed in the application scenario.  The basic framework of this paper is based on RNN but augmented with the directly parameterized memory bank. \n\nThe memory bank of this paper is a set of distributed, real-valued vectors capturing domain knowledge. When the model is adapted to the new domain, the model progressively increases the slots in the memory bank. \n\nThe paper evaluates the proposed approach on an NLP classification task, i.e. multi-genre natural language inference (MultiNLI). The dataset used in this paper includes 5 genres: Slate, Fiction, Telephone, Government and Travel. \n\nIn the experiments, the paper performs the dynamics of the progressive memory network for IDA as well as compares the proposed method with variants and previous work in the multi-domain setting. \n\n\n### Novelty ###\n\nThis paper proposes incremental domain adaptation, which is inspired by Li & Hoiem's work. The setting assumes that each domain comes one after another and only one domain can get accessed. This setting is interesting as we will encounter this setting in the real application scenarios, i.e., the domain knowledge in the real domain is unpredictable. Thus, the problem setting provides some novelty. However, I am not sure whether assuming that we can only get access to one domain is reasonable as we can always save the data for the domain we have already seen. \n\nFrom the perspective of the method, this paper incorporates the memory bank to the RNN, which is not new in the machine learning research area, but heuristic enough for the transfer learning community. \n\n\n\n###Clarity###\n\nOverall, the paper is well organized and logically clear. The proposed claims are well supported by the experiments and analysis. The images are well-presented and well-explained by the captions and the text. \n\n\n###Pros###\n\n1) The paper proposes an incremental domain adaptation scenario where one domain appears another and only the data from the current domain can be accessed, which is interesting and heuristic to the domain adaptation research community. \n2) The paper is applicable to many practical scenarios since the data from the real-world application is typically from multiple domains and the data is from one domain at a time. \n3) The paper is overall well-organized and well-written. The claims of the paper are verified by the experimental results.\n\n\n###Cons###\n\n1) The paper has a good motivation for the setting, however, one of the critical drawbacks of this paper is that the papers fail to compare with the state-of-the-art baselines. I understand that this paper has a new setting, but since the authors also compare the proposed method with the \"multi-task\" learning, it will be helpful to compare with state-of-the-art multi-task or multi-source baselines. \n2) The experimental results provided in this paper are weak. In Table 4, we found that sometimes, the IDA method performs worse than the multi-task baselines. \n3) The paper presents no ablation study or analysis of the experimental results. The effectiveness of the memory bank, fine-tuning/freezing learning parameters is unclear. \n\nIt will be also interesting to see how does the proposed method performs on large-scale visual datasets.\n\n\n\nBased on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs. \nI am willing to improve my rating if the authors can address my following concerns. To improve the rating, the author should explain the following questions:\n1). Why assuming that we can only get access to the data from one domain is a reasonable setting, since we can always save the data (or at least the statistics about the data) form the domains we have already observed.\n2). Can the proposed approach generalize to visual domain adaptation, i.e. on the visual task instead of NLP task?\n3). The drawbacks I mentioned in the paper Cons section.\n\n\n\n#################### Updated Review  ###########\nThe authors have addressed most of my concerns I proposed in the initial review, thus I raise my score to 6 weak accept. \n\n1) In the response to the review, the authors claim that from the data privacy's perspective, it's reasonable to assume that we can only get access to one dataset at one time, which makes sense.\n2) I would be really interested in how the similar idea works on vision data, which could be a future work for this paper. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper392/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper392/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive Memory Banks for Incremental Domain Adaptation", "authors": ["Nabiha Asghar", "Lili Mou", "Kira A. Selby", "Kevin D. Pantasdo", "Pascal Poupart", "Xin Jiang"], "authorids": ["nasghar@uwaterloo.ca", "doublepower.mou@gmail.com", "kaselby@uwaterloo.ca", "kevin.pantasdo@uwaterloo.ca", "ppoupart@uwaterloo.ca", "jiang.xin@huawei.com"], "keywords": ["natural language processing", "domain adaptation"], "TL;DR": "We present a neural memory-based architecture for incremental domain adaptation, and provide theoretical and empirical results.", "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is  to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation and progressive neural networks in the experiments.", "pdf": "/pdf/09cab4009ea99ae5ec0339eca94cdcb853cfba52.pdf", "code": "https://github.com/nabihach/IDA", "paperhash": "asghar|progressive_memory_banks_for_incremental_domain_adaptation", "_bibtex": "@inproceedings{\nAsghar2020Progressive,\ntitle={Progressive Memory Banks for Incremental Domain Adaptation},\nauthor={Nabiha Asghar and Lili Mou and Kira A. Selby and Kevin D. Pantasdo and Pascal Poupart and Xin Jiang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkepbpNFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7e3fb050f0162f9dd7abb3a28728cd8b05276c25.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576020407151, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper392/Reviewers"], "noninvitees": [], "tcdate": 1570237752817, "tmdate": 1576020407164, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper392/-/Official_Review"}}}, {"id": "Hklyku7AYB", "original": null, "number": 1, "cdate": 1571858390744, "ddate": null, "tcdate": 1571858390744, "tmdate": 1572972601106, "tddate": null, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "invitation": "ICLR.cc/2020/Conference/Paper392/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an extensible attention mechanism applied on the previous hidden state of an RNN and resulting in supplementary input for the next RNN step. For each added domain, new pairs of attentions key and values can be added to provide more capacity for the model. This method is applied in the context of incremental domain adaptation for NLP without the possibility of storing of old samples (episodic memory).\n\nPros:\n- Extensive ablation study with the different possible combinations of methods\n- Very interesting comparison between expanding the memory (i.e. attention) and expanding the hidden states. Using the attention results in better results for a same number of added parameters and the activations sizes stay the same even when the attention is extended with new pairs.\n- Paper is well written/motivated\n\nWeaknesses:\n- MultiNLI seem to have too much correlation between tasks. It would have been better to be able to observe catastrophic forgetting for the source domain. In the appendix, the metrics have really strong disagreement so it is tough to judge for these two corpuses. \n- When you give the numbers for multi-task learning, you should use your extended memory method to be fair with MT learning. I would just be interested to see it, just as a proper upper bound.\n\nOtherwise, the paper proposes a novel method which works well in practice so I am leaning towards acceptance."}, "signatures": ["ICLR.cc/2020/Conference/Paper392/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper392/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive Memory Banks for Incremental Domain Adaptation", "authors": ["Nabiha Asghar", "Lili Mou", "Kira A. Selby", "Kevin D. Pantasdo", "Pascal Poupart", "Xin Jiang"], "authorids": ["nasghar@uwaterloo.ca", "doublepower.mou@gmail.com", "kaselby@uwaterloo.ca", "kevin.pantasdo@uwaterloo.ca", "ppoupart@uwaterloo.ca", "jiang.xin@huawei.com"], "keywords": ["natural language processing", "domain adaptation"], "TL;DR": "We present a neural memory-based architecture for incremental domain adaptation, and provide theoretical and empirical results.", "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is  to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation and progressive neural networks in the experiments.", "pdf": "/pdf/09cab4009ea99ae5ec0339eca94cdcb853cfba52.pdf", "code": "https://github.com/nabihach/IDA", "paperhash": "asghar|progressive_memory_banks_for_incremental_domain_adaptation", "_bibtex": "@inproceedings{\nAsghar2020Progressive,\ntitle={Progressive Memory Banks for Incremental Domain Adaptation},\nauthor={Nabiha Asghar and Lili Mou and Kira A. Selby and Kevin D. Pantasdo and Pascal Poupart and Xin Jiang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkepbpNFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7e3fb050f0162f9dd7abb3a28728cd8b05276c25.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576020407151, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper392/Reviewers"], "noninvitees": [], "tcdate": 1570237752817, "tmdate": 1576020407164, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper392/-/Official_Review"}}}, {"id": "r1gIw_JlcB", "original": null, "number": 2, "cdate": 1571973214056, "ddate": null, "tcdate": 1571973214056, "tmdate": 1572972601062, "tddate": null, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "invitation": "ICLR.cc/2020/Conference/Paper392/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "*** Summary\n\nThis work proposes to use an augmented RNN model to address the incremental domain adaptation problem. In particular, it designs the progressive memory bank approach which expands the memory capacity by adding parameters every time a new task comes in. The RNN retrieves knowledge from the memory bank via key-value attention. A proof in a highly simplified case is given in addition to empirical results showing that expanding the memory bank is better than expanding the RNN states.\n\n*** Strengths\n\n1. Section 3 is well-written. The methods and motivations are illustrated clearly.\n\n2. Comprehensive experiments are conducted. Supportive results for the arguments presented in Section 3 are therefore demonstrated.\n\n\n*** Weaknesses\n\n1. Regarding Table 2., multiple runs of different sources and targets would be helpful to better understand the effectiveness of the proposed methods in the 2-domain set-up.\n\n2. The choice of key-value memory bank is not intuitive. A comparison between this memory and the traditional attention can help demonstrate the validity of this choice."}, "signatures": ["ICLR.cc/2020/Conference/Paper392/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper392/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Progressive Memory Banks for Incremental Domain Adaptation", "authors": ["Nabiha Asghar", "Lili Mou", "Kira A. Selby", "Kevin D. Pantasdo", "Pascal Poupart", "Xin Jiang"], "authorids": ["nasghar@uwaterloo.ca", "doublepower.mou@gmail.com", "kaselby@uwaterloo.ca", "kevin.pantasdo@uwaterloo.ca", "ppoupart@uwaterloo.ca", "jiang.xin@huawei.com"], "keywords": ["natural language processing", "domain adaptation"], "TL;DR": "We present a neural memory-based architecture for incremental domain adaptation, and provide theoretical and empirical results.", "abstract": "This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is  to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation and progressive neural networks in the experiments.", "pdf": "/pdf/09cab4009ea99ae5ec0339eca94cdcb853cfba52.pdf", "code": "https://github.com/nabihach/IDA", "paperhash": "asghar|progressive_memory_banks_for_incremental_domain_adaptation", "_bibtex": "@inproceedings{\nAsghar2020Progressive,\ntitle={Progressive Memory Banks for Incremental Domain Adaptation},\nauthor={Nabiha Asghar and Lili Mou and Kira A. Selby and Kevin D. Pantasdo and Pascal Poupart and Xin Jiang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BkepbpNFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7e3fb050f0162f9dd7abb3a28728cd8b05276c25.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkepbpNFwr", "replyto": "BkepbpNFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576020407151, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper392/Reviewers"], "noninvitees": [], "tcdate": 1570237752817, "tmdate": 1576020407164, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper392/-/Official_Review"}}}], "count": 5}