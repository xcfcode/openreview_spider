{"notes": [{"tddate": null, "ddate": null, "tmdate": 1519511328392, "tcdate": 1519511328392, "number": 2, "cdate": 1519511328392, "id": "BkdQPwk_G", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Public_Comment", "forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "signatures": ["~Mostapha_Benhenda1"], "readers": ["everyone"], "writers": ["~Mostapha_Benhenda1"], "content": {"title": "Narrow approach (only RL) that ignores previous literature.", "comment": "1. No discussion of previous measures of diversity in the literature, like Guimaraes et al (30 May 2017), or Benhenda (28 August 2017). There are even more diversity metrics in the recent Benhenda et al.  (8 February 2018) here: https://www.authorea.com/users/226673/articles/285209-diversitynet-a-collaborative-benchmark-for-generative-ai-models-in-chemistry\n\nYour definition of diversity is extremely rudimentary, why is it the best one?\n\n2. Your paper proposes 19 new tasks, but completely ignored tasks from the previous literature, like in Olivecrona et al. (25 April 2017), Dieb et al. (20 July 2017), Sanchez-Lengeling et al. (17 August 2017). If you are a staunch advocate of reproductibility, why these omissions?\n\nOn the other hand, after your paper, the literature totally ignored your platform, like Popova et al.  (29 November 2017), Ertl et al. (20 December 2017) and many others. See a list here: https://medium.com/the-ai-lab/diversitynet-a-collaborative-benchmark-for-generative-ai-models-in-chemistry-f1b9cc669cba\n\nAfter 4 months passed, how do you explain the persisting lack of adoption of your platform by the community\u00a0?\nIn my case, I just didn\u2019t hear about it at that time. Lack of marketing. Thanks Nathan Brown for tweeting.\n\n3. However, your approach is too restrictive, you remain within reinforcement learning models. There are important works with autoencoders, like Gomez-Bombarelli et al. (2016), Kadurin et al. (July 2017) and many others, and they deserve to be benchmarked too. Is the OpenAI gym platform restricted to Reinforcement Learning\u00a0?\u00a0\n\nThe ongoing DiversityNet molecule benchmark (here) is using Texygen platform (appeared on the  6th February 2018), which is more inclusive for non-RL algorithms. See: https://github.com/geek-ai/Texygen/issues/5\u00a0\nHowever, Texygen is not generalized yet to the very few non-SMILES generative models in the literature. That\u2019s something to do.\n\n4. Does multi objective optimization always succeed by taking 'any arbitrary balanced weighting' of objectives\u00a0?  Is it always so easy? A general discussion, with different objectives, would have been welcome. Otherwise, the uninformed reader could imagine that it\u2019s a piece of cake to do Multi-objective reinforcement learning.\n\n5. That said, your paper makes an interesting curation of data, models, and tasks. It can be useful for a proper benchmark.\n\nIn conclusion, I don\u2019t think that the ML community should tackle this challenge the way you presented it in this paper 4 months ago."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791682470, "id": "ICLR.cc/2018/Conference/-/Paper646/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkcTe-bR-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper646/Authors", "ICLR.cc/2018/Conference/Paper646/Reviewers", "ICLR.cc/2018/Conference/Paper646/Area_Chair"], "cdate": 1512791682470}}}, {"tddate": null, "ddate": null, "tmdate": 1518730170125, "tcdate": 1509130433985, "number": 646, "cdate": 1518730170113, "id": "HkcTe-bR-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HkcTe-bR-", "original": "SkY6xZWR-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": ["Bk0xiI1Dz"], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260091421, "tcdate": 1517249632050, "number": 373, "cdate": 1517249632036, "id": "B1dDN1pHz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper creates a dataset for exploration of RL for molecular design and I think this makes it a strong contribution to the community at the intersection of the two. For a methods focussed conference such as ICLR however, it may not be the best fit. Hence I would recommend submitting to a workshop track or targeting a more focussed venue such as a bioinformatics conference. ", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516287747762, "tcdate": 1516287747762, "number": 5, "cdate": 1516287747762, "id": "ryhZvNRNM", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "forum": "HkcTe-bR-", "replyto": "HJlpDGpEG", "signatures": ["ICLR.cc/2018/Conference/Paper646/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper646/Authors"], "content": {"title": "\"Relevant\" paper was published one month after ours.", "comment": "The first version our paper was uploaded to openreview on 27th Oct 2017.\n\nThe paper by Popova et al. was put on arXiv on 29th Nov 2017, that is about a month later than our paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730100, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkcTe-bR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper646/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper646/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper646/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers", "ICLR.cc/2018/Conference/Paper646/Authors", "ICLR.cc/2018/Conference/Paper646/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730100}}}, {"tddate": null, "ddate": null, "tmdate": 1516214200175, "tcdate": 1516214200175, "number": 1, "cdate": 1516214200175, "id": "HJlpDGpEG", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Public_Comment", "forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Relevant paper not cited", "comment": "There is a recently published paper on molecular design with deep reinforcement learning which is not addressed in your work:\nPopova, Mariya, Olexandr Isayev, and Alexander Tropsha. \"Deep reinforcement learning for de-novo drug design.\" arXiv preprint arXiv:1711.10907 (2017).\nThis paper discusses alternative to GANs method of novel compounds generation with recurrent neural network and reinforce algorithm, which is relevant to your work. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791682470, "id": "ICLR.cc/2018/Conference/-/Paper646/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HkcTe-bR-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper646/Authors", "ICLR.cc/2018/Conference/Paper646/Reviewers", "ICLR.cc/2018/Conference/Paper646/Area_Chair"], "cdate": 1512791682470}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642485016, "tcdate": 1510228750269, "number": 1, "cdate": 1510228750269, "id": "rkUfmabyM", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Review", "forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "signatures": ["ICLR.cc/2018/Conference/Paper646/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes a set of benchmarks for molecular design, and compares different deep models against them. The main contributions of the paper are 19 molecular design benchmarks (with chembl-23 dataset), including two molecular design evaluation criterias and comparison of some deep models using these benchmarks. The paper does not seem to include any method development.\n\nThe paper suffers from a lack of focus. Several existing models are discussed to some length, while the benchmarks are introduced quite shortly. The dataset is not very clearly defined: it seems that there are 1.2 million training instance, does this apply for all benchmarks? The paper's title also does not seem to fit: this feels like a survey paper, which is not reflected in the title. Biologically lots of important atoms are excluded from the dataset, for instance natrium, calcium and kalium. I don't see any reason to exlude these. What does \"biological activities on 11538 targets\" mean? \n\nThe paper discussed molecular generation and reinforcement learning, but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is used. Are the test molecules somehow generated in a directed or undirected fashion? Shouldn't there also be experiments on comparing ways to generate suitable molecules, and how well they match the proposed criterion? There should be benchmarks for predicting molecular properties (standard regression), and for generating molecules with certain properties. Currently it's unclear which type of problems are solved here.\n\nTable 1 lists 5 models, while fig 3 contains 7, why the discrepancy? In table 1 the plotted runs seem to differ a lot from average results (e.g. -0.43 to 0.15, or 0.32 to 0.83). Variances should be added, and preferably more than 3 initialisations used.\n\nOverall this is an interesting paper, but does not have any methodological contribution, and there is also few insightful results about the compared methods, nor is there meaningful analysis of the problem domain of molecules either.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642484929, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper646/AnonReviewer2", "ICLR.cc/2018/Conference/Paper646/AnonReviewer1", "ICLR.cc/2018/Conference/Paper646/AnonReviewer3"], "reply": {"forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642484929}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642484980, "tcdate": 1511786648334, "number": 2, "cdate": 1511786648334, "id": "rkejdYtxz", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Review", "forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "signatures": ["ICLR.cc/2018/Conference/Paper646/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "This is a solid paper about model evaluation in the chemical domain. ", "rating": "7: Good paper, accept", "review": "Summary:\nThis work is about model evaluation for molecule generation and design. 19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design.\n\non the positive side:\nThe paper is well written, quality and clarity of the work are good. The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation. It is investigated how several RL strategies perform on a large, standardized data set. Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed.  An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym. \n\n\non the negative side:\nThere is no new novel contribution on the methods side.  \n\n\n\nminor comments:\n\nSection 2.1. \nsee Fig.2 \u2014> see Fig.1\npage 4just before equation 8: the the", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642484929, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper646/AnonReviewer2", "ICLR.cc/2018/Conference/Paper646/AnonReviewer1", "ICLR.cc/2018/Conference/Paper646/AnonReviewer3"], "reply": {"forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642484929}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642484944, "tcdate": 1511822057453, "number": 3, "cdate": 1511822057453, "id": "S1ZlQfqeM", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Review", "forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "signatures": ["ICLR.cc/2018/Conference/Paper646/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "empirical evaluation of recurrent models and RL for molecule design", "rating": "6: Marginally above acceptance threshold", "review": "Summary: This paper studies a series of reinforcement learning (RL) techniques in combination with recurrent neural networks (RNNs) to model and synthesise molecules. The experiments seem extensive, using many recently proposed RL methods, and show that most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO is perhaps the only exception.  \n\nOriginality and significance: \n\nThe conclusion from the experiments could be valuable to the broader sequence generation/synthesis field, showing that many current RL techniques can fail dramatically. \n\nThe paper does not provide any theoretical contribution but nevertheless is a good application paper combining and comparing different techniques.\n\nClarity: The paper is generally well-written. However, I'm not an expert in molecule design, so might not have caught any trivial errors in the experimental set-up. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642484929, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper646/AnonReviewer2", "ICLR.cc/2018/Conference/Paper646/AnonReviewer1", "ICLR.cc/2018/Conference/Paper646/AnonReviewer3"], "reply": {"forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642484929}}}, {"tddate": null, "ddate": null, "tmdate": 1515178353596, "tcdate": 1515178353596, "number": 4, "cdate": 1515178353596, "id": "r19_YHamz", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "forum": "HkcTe-bR-", "replyto": "rkUfmabyM", "signatures": ["ICLR.cc/2018/Conference/Paper646/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper646/Authors"], "content": {"title": "We have adapted the manuscript to make the contributions and scope clearer", "comment": "\nWe thank the reviewer for their critical feedback. We have adapted the manuscript to make the contributions and scope clearer.\n\nAlgorithms that allow the generation of small molecules (small graphs) that satisfy given desirable properties are rapidly evolving for medicine, materials and agriculture.  This is currently an area of intense investigation, theoretically as well as practically, as highlighted by several ICLR submissions on molecule/graph generation this year, which all employ different and inconsistent benchmarks and training sets. Currently, it is not possible to compare these results.\n\nWith this paper, we unify the problem space of small molecule generation and thoroughly investigate various approaches (including algorithms which have yet to be explored in this domain, e.g. PPO), and include evaluations of our new benchmarks on pre-existing work.\n\nOur results surpass state-of-the-art results previously reported on a few of the sub-domains, establishing new baselines, and come to the perhaps surprising result that the relatively simple hill-climbing MLE method achieves results on par with the some of the most advanced recently-developed RL algorithms such as PPO.\n\n\nSpecific Comments:\n\n* Regarding lack of focus: we have reorganized the paper to improve the clarity of the work.\n  \n* Regarding purpose: We hope the above responses address your concern of this being a survey work. We believe this work introduces new benchmarks, evaluates pre-existing algorithms, demonstrates novel pairings of algorithm and domain, and establishes a new state-of-the art as well as a few surprising additional insights (hill-climbing MLE supremacy, temperature ineffectiveness).\n\n* Regarding preprocessing steps: the steps taken in this work are standard and in line with the field of computational chemistry [1,2]. This includes the removal of Sodium, Calcium and Potassium, and other counterions.\n\n* Regarding clarity of RL vs. train/test set: for algorithms that rely on pretraining to help navigate the extremely large space of small molecule generation (~10^60), it is important that the algorithms have not been exposed to a correct solution in the training set, hence the train-test split.  As an additional benefit, a train/test split permits the benchmarks to be used with rule-based GOFAI systems, supervised algorithms as well as RL.\n\n* Regarding molecular property prediction: indeed, this is an important sub-field of computational chemistry and is explored under the family of QSAR models [3, 4].  However, that is out-of-scope of this paper, as it attempts to address a different concern.\n\n* Regarding data: the table is a bit overwhelming already, so we chose not to exhaustively show all results for all models and instead focus on representative key models.  Due to time and computational constraints, we had not run more than three initializations, but can do so for the revision.\n\n\nWe hope this addresses your concerns.\n\n[1] Glaab, Enrico. \"Building a virtual ligand screening pipeline using free software: a survey.\" Briefings in bioinformatics 17.2 (2015): 352-366.\n[2] Lionta, Evanthia, et al. \"Structure-based virtual screening for drug discovery: principles, applications and recent advances.\" Current topics in medicinal chemistry 14.16 (2014): 1923-1938.\n[3] Tropsha, Alexander. \"Best practices for QSAR model development, validation, and exploitation.\" Molecular informatics 29.6\u20107 (2010): 476-488.\n[4] Tropsha, Alexander, and Alexander Golbraikh. \"Predictive QSAR modeling workflow, model applicability domains, and virtual screening.\" Current pharmaceutical design 13.34 (2007): 3494-3504.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730100, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkcTe-bR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper646/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper646/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper646/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers", "ICLR.cc/2018/Conference/Paper646/Authors", "ICLR.cc/2018/Conference/Paper646/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730100}}}, {"tddate": null, "ddate": null, "tmdate": 1515177053671, "tcdate": 1515177053671, "number": 3, "cdate": 1515177053671, "id": "HyIP4S6QM", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "forum": "HkcTe-bR-", "replyto": "rkejdYtxz", "signatures": ["ICLR.cc/2018/Conference/Paper646/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper646/Authors"], "content": {"title": "Reply", "comment": "\nWe are grateful for your comments.  We hope your concern about novelty is addressed with our main comment; indeed, the pairing here is in the algorithm to this particular application area.  \nWe further hope that a foundational framework proposed will allow the emergence of future, novel algorithms.  Your minor comments have been addressed in the manuscript.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730100, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkcTe-bR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper646/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper646/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper646/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers", "ICLR.cc/2018/Conference/Paper646/Authors", "ICLR.cc/2018/Conference/Paper646/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730100}}}, {"tddate": null, "ddate": null, "tmdate": 1515176918154, "tcdate": 1515176801677, "number": 1, "cdate": 1515176801677, "id": "Hkqv7raQz", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "forum": "HkcTe-bR-", "replyto": "HkcTe-bR-", "signatures": ["ICLR.cc/2018/Conference/Paper646/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper646/Authors"], "content": {"title": "Global Reply", "comment": "\nWe thank the reviewers for their effort and advice towards improving our submission.  We are pleased that the reviewers have identified our important contributions of dataset curation and preprocessing steps, proposed benchmarks, and baseline results using recently-developed algorithms.  While we introduce no new reinforcement learning algorithms in this work, our primary aim was to substantially lower the barrier towards automated molecular design to allow computer scientists with no prior background in chemistry to develop novel algorithms to improve molecular design.  Indeed, here the novelty lies in the pairing of task and algorithm, and this work is foundational to clearly lay out steps and provide code to apply reinforcement learning algorithms to molecule design.  \n\nFinally, we are able to demonstrate results in this manuscript that establish a new state-of-the-art in single and multi objective physicochemical property optimization and chemical space exploration tasks.  Subsequent work can then build on this set of standardized molecular design benchmarks to introduce new methods. The benchmark framework is general enough to be used with any possible small molecule generation method, whether rule-based or learned, and is not limited to sequence-based generation relying on SMILES. We have therefore amended the manuscript to reflect the importance of our introduced benchmarks.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730100, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkcTe-bR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper646/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper646/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper646/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers", "ICLR.cc/2018/Conference/Paper646/Authors", "ICLR.cc/2018/Conference/Paper646/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730100}}}, {"tddate": null, "ddate": null, "tmdate": 1515176879086, "tcdate": 1515176879086, "number": 2, "cdate": 1515176879086, "id": "H1P3mSpQf", "invitation": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "forum": "HkcTe-bR-", "replyto": "S1ZlQfqeM", "signatures": ["ICLR.cc/2018/Conference/Paper646/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper646/Authors"], "content": {"title": "Reply", "comment": "We thank the referee for their comments and perspective on our work. \n\nWe hope this reviewer\u2019s comments have been addressed in our overall reply and in the responses to the other reviewers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design", "abstract": "The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.", "pdf": "/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf", "TL;DR": "We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.", "paperhash": "neil|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design", "_bibtex": "@misc{\nneil2018exploring,\ntitle={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\nauthor={Daniel Neil and Marwin Segler and Laura Guasch and Mohamed Ahmed and Dean Plumbley and Matthew Sellwood and Nathan Brown},\nyear={2018},\nurl={https://openreview.net/forum?id=HkcTe-bR-},\n}", "keywords": ["reinforcement learning", "molecule design", "de novo design", "ppo", "sample-efficient reinforcement learning"], "authors": ["Daniel Neil", "Marwin Segler", "Laura Guasch", "Mohamed Ahmed", "Dean Plumbley", "Matthew Sellwood", "Nathan Brown"], "authorids": ["daniel.neil@benevolent.ai", "marwin.segler@benevolent.ai", "laura.guasch@benevolent.ai", "mohamed.ahmed@benevolent.ai", "dean.plumbley@benevolent.ai", "matthew.sellwood@benevolent.ai", "nathan.brown@benevolent.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730100, "id": "ICLR.cc/2018/Conference/-/Paper646/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HkcTe-bR-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper646/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper646/Authors|ICLR.cc/2018/Conference/Paper646/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper646/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper646/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper646/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper646/Reviewers", "ICLR.cc/2018/Conference/Paper646/Authors", "ICLR.cc/2018/Conference/Paper646/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730100}}}], "count": 12}