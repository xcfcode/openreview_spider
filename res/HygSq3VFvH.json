{"notes": [{"id": "HygSq3VFvH", "original": "SygUm1bkvr", "number": 115, "cdate": 1569438861433, "ddate": null, "tcdate": 1569438861433, "tmdate": 1577168256292, "tddate": null, "forum": "HygSq3VFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Z2es9fQ2-", "original": null, "number": 1, "cdate": 1576798687818, "ddate": null, "tcdate": 1576798687818, "tmdate": 1576800947274, "tddate": null, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Decision", "content": {"decision": "Reject", "comment": "The paper considers a setting where the state of a (robotics) environment can be divided roughly into \"context states\" (such as variables under the robot's direct control) and \"states of interest\" (such as the state variables of an object to be manipulated), and learn skills by maximizing a lower bound on the mutual information between these two components of the state. Experimental results compare to DDPG/SAC, and show that the learned discriminator is somewhat transferable between environments.\n\nReviewers found the assumptions necessary on the degree of domain knowledge to be quite strong and domain-specific, and that even after revision, the authors were understating the degree to which this was necessary. The paper did improve based on reviewer feedback, and while R3 was more convinced by the follow-up experiments (though remarked that requiring environment variations to obtain new skills was a \"significant step backward from things like [Diversity is All You Need]\"), the other reviewers remained unconvinced regarding domain knowledge and in particular how it interacts with the scalability of the proposed method to complex environments/robots.\n\nGiven the reviewers' concerns regarding applicability and scalability, I recommend rejection in its present form. A future revision may be able to more convincingly demonstrate that limitations based on domain knowledge are less significant than they appear.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717070, "tmdate": 1576800267283, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper115/-/Decision"}}}, {"id": "SkxED7_nKS", "original": null, "number": 1, "cdate": 1571746651727, "ddate": null, "tcdate": 1571746651727, "tmdate": 1574421739241, "tddate": null, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #3", "review": "I take issue with the usage of the phrase \"skill discovery\". In prior work (e.g. VIC, DIAYN), this meant learning a skill-conditional policy. Here, there is only a single (unconditioned) policy, and the different \"skills\" come from modifications of the environment -- the number of skills is tied to the number of environments. This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work. Skill discovery in this context implies being able to have a single agent execute a variety of learned skills, rather than having one agent per environment with each environment designed to elicit a specific skill.\n\nRather than \"skill discovery\", I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1}). Modifying the objective to incorporate domain knowledge (as done in your DIAYN baseline) yields I(a; s_i | s_{t-1}) and is amenable to maximization by either of the lower bounds considered here. Indeed, your DIAYN baseline with skill length set to 1 and the number of skills equal to the number of actions (or same parameterization in the case of continuous actions) should recover this approach. I believe this would be a much more appropriate baseline, and I'd be curious to hear the intuition for why I(s_c ; s_i) should be superior.\n\nApart from this missing baseline, the experimental results seem convincing. However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction). Indeed, an appendix would be greatly appreciated, as many experimental details were omitted. Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn't done as incorporating domain knowledge is the main contribution of the paper. That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned).\n\nRebuttal EDIT:\n\nThe language around skills and the extent of prior knowledge still downplays things a bit too much for my liking. Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non-trivial to specify or learn for harder problems (e.g. pixel observations).\n\nThat said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work. The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would've also been nice, the experimental results pass the bar for acceptance in my view.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper115/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper115/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575853057695, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper115/Reviewers"], "noninvitees": [], "tcdate": 1570237756853, "tmdate": 1575853057708, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper115/-/Official_Review"}}}, {"id": "Hyg3B8z5oB", "original": null, "number": 5, "cdate": 1573688899942, "ddate": null, "tcdate": 1573688899942, "tmdate": 1573688915538, "tddate": null, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment", "content": {"title": "Revision", "comment": "\n- We did new experiments in a navigation task, see Figure 1, with three different settings including a single object, no object, and multiple objects.\n- A new video is uploaded to show the new experimental results at\u00a0https://youtu.be/l5KaYJWWu70?t=104\n- We also added a theoretical proof of the connection between our method MISC and the empowerment method in the Appendix.\n- We added new experiments in the navigation task to compare MISC, empowerment and ICM methods, see Figure 4 in the paper.\n- Experimental details are now added in the Appendix.\n- We updated the paper about the new experiments mentioned above.\n- We updated the paper according to review comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygSq3VFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper115/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper115/Authors|ICLR.cc/2020/Conference/Paper115/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176185, "tmdate": 1576860544830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment"}}}, {"id": "rkgrTHMcjB", "original": null, "number": 4, "cdate": 1573688764977, "ddate": null, "tcdate": 1573688764977, "tmdate": 1573688764977, "tddate": null, "forum": "HygSq3VFvH", "replyto": "SkxED7_nKS", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment", "content": {"title": "To reviewer\u2019s comments", "comment": "Thank you for the comments!\n\nTo review\u2019s feedback:\n\n- We pay attention to the term \u201cskill discovery\u201d and made it more clear about the connection between prior works and the current work in the revised version. Our method can also be combined with DIAYN to learn the skill-conditioned policy as mentioned in the paper.\n\n- We added both a theoretical connection and new experimental results to compare MISC and the empowerment method in the revised version. In the navigation tasks, we show that our method outperforms the empowerment method.\n\n- An intuition for why I(s_c, s_i) could be superior to I(a, s_i) is that in robotic tasks, the mutual information between the robotic sates, s_c, and the object states, s_i, could be easier to be estimated than the mutual information between the action, a, and the object states, s_i, as shown in Figure 4 in the paper. Therefore, the agent receives a higher MI reward more easily and learns to control s_i more efficiently. \nThe context states can be seen as the summary information of the agent\u2019s action and the transition model of the environment, which could be more relevant in terms of estimating the object states in comparison to the agent\u2019s actions. \n\n- VIME and PER are used as described in their original papers.\n\n- We have added an appendix to provide more information about experiment details.\n\n- We also newly evaluated our method on gazebo-based robotic simulations, including the cases when there is no object, a single object of interest, and multiple objects of interests. \nA video showing new experimental results is available at https://youtu.be/l5KaYJWWu70?t=104\nIn this experiment, we also compare MISC with two additional baselines, including ICM and empowerment (with state of interest), see Figure 4 in the paper.\n\n- We now mention that the states of interests vs context are given in the revised paper. However, when they are not given. They can also be automatically learned/selected by iterating over all possible combinations.  Afterwards, an optimal combination can be chosen by the user via testing in the task at hand."}, "signatures": ["ICLR.cc/2020/Conference/Paper115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygSq3VFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper115/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper115/Authors|ICLR.cc/2020/Conference/Paper115/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176185, "tmdate": 1576860544830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment"}}}, {"id": "SJgUsHzqsr", "original": null, "number": 3, "cdate": 1573688734054, "ddate": null, "tcdate": 1573688734054, "tmdate": 1573688734054, "tddate": null, "forum": "HygSq3VFvH", "replyto": "SJxy9Ndy5r", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment", "content": {"title": "To reviewer\u2019s comments", "comment": "Thank you for the comments!\n\nTo review\u2019s questions:\n\n- As the experimental results shows, with position information alone, the agent is able to learn to push or pick up the object, therefore we consider position information alone (without velocity information) is sufficient in our case.\n\n- For MISC, the method needs to know what are the states of interests and what are the context state. While, the states of interest can be any states that users are interested in, such as a part of the robot states or the object states. The context states are some other states, which are different from the states of interest. In robotic tasks, the states of the robot and the object states are normally available [Andrychowicz et al 2018, Plappert et al 2018]. \n\nTo automatically detect the state of interests and the context states, we can train the agent with random state splits and then chose the combination, which is suitable for the tasks at hand.\n\nReferences:\n[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.\n[2] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow- ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce- ment learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.\n\n- Yes, MISC can deal with the case, when there are multiple objects of interest. We added new experiments showing the agent can learn to manipulate two balls. We define the mutual information intrinsic reward as I(S^i_{1}, S^c)+I(S^i_{2}, S^c). The experimental results are shown in the new video at https://youtu.be/l5KaYJWWu70?t=148, where we show that a robot car can learn to manipulate two balls in the same episode.\n\n- Equation (4) is not the mutual information between two trajectories of states. It is an estimation of mutual information between two sets of states. And the states are sampled from the same trajectory. Therefore, we do not need to decompose Equation (4) to evaluate Equation (3).\n\n- We add the experimental details in the Appendix.\n\n- The discriminator is trained along with the policy. For example, in the case that we update the agent 200 times in each epoch, then we also update the MISC 200 times per epoch. For more detailed information, please refer to our code at https://github.com/misc-project/misc\n\n- Compared to the dense reward, with the negative L2 distance between the robot and the object, the robot can only learn to reach the object but will not learn to push or pick up the object because when the robot reaches the object, the negative L2 distance is already zero. However, MISC has the advantage that it not only enables the agent to learn to reach but also learn to push and pick & place.\n\n- If we train the MISC and DIAYN at the same time, the DIAYN reward might be dominant. Subsequently, The agent might not learn to control the states of interests with MISC.\n\n- MISC-p works similarly to PER. The main difference is that MISC-p uses the estimated mutual information quantity as a priority, while PER uses the TD-error as a priority for replay. For more detail on PER, please refer to the original PER paper [Schaul et al 2016].\n\nReference:\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, 2016.\n\n- We first scale the intrinsic and the extrinsic reward between 0 and 1 and then use equal weights for these two rewards.\n\n- For the opposite situation, we can use negative mutual information rewards to encourage the agent to learn to \u201cavoid\u201d some objects.\n\n- The discriminator uses the same amount of (s,a,s') experience as VIME consumes because the discriminator is fixed after pre-training. VIME can only be trained along with the policy. VIME cannot be pre-trained, otherwise, it won\u2019t detect novel states.\n\n- Transfer the learned discriminator from Push to the Pick&Place should still help the agent to learn the pick & place task because the transferred discriminator will help the agent to learn to reach the object at least. As long as the state inputs for the discriminator are the same, then MI discriminator can be transferred among different tasks."}, "signatures": ["ICLR.cc/2020/Conference/Paper115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygSq3VFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper115/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper115/Authors|ICLR.cc/2020/Conference/Paper115/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176185, "tmdate": 1576860544830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment"}}}, {"id": "HJeVtBG5sB", "original": null, "number": 2, "cdate": 1573688699723, "ddate": null, "tcdate": 1573688699723, "tmdate": 1573688699723, "tddate": null, "forum": "HygSq3VFvH", "replyto": "Hkgcm01WqB", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment", "content": {"title": "To reviewer\u2019s comments", "comment": "Thank you for the comments!\n \nTo reviewer\u2019s concerns:\n \n- First of all, the state of interest does not have to be the object state. It can be the state of the robot, for example, the state of actuators. Maximizing the mutual information between two sets of actuator states can help the agent to learn to control itself. We did a new experiment in navigation environments, where train the agent to maximize the mutual information between its left wheel states and its right wheel states. The agent learns to run in a straight line instead of in random directions. The video showing experiment results is available at https://youtu.be/l5KaYJWWu70?t=134\n \n- Although we evaluated our method in robotic manipulation tasks, it does not mean it won\u2019t work for other tasks. We added additional experiments in a new navigation task, see the video at https://youtu.be/l5KaYJWWu70?t=104 \nWe consider our algorithm as a general-purpose skill learning algorithm in the sense that it guides the agent to learn any skills to control the states of interests. The states of interest could be any states, such as the robot states, the object states, or the states of the environment.\n \n- The state of interest is specified by the user with little domain knowledge. However, when there is no clear divide from the user, the agent can learn from different combinations of the states of interest and the context states. In the end, the user can choose skills from the learned skill sets that are useful for the task at hand.\n \n- The combination of our method and DIAYN enables DIAYN to learn manipulation skills efficiently, while DIAYN alone did not learn. Furthermore, compared to MISC, the combined method enjoys the benefits brought by DIAYN, such as learning combinable motion primitive with skill-conditioned policy for hierarchical reinforcement learning [1].\n \nReference:\n[1] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm."}, "signatures": ["ICLR.cc/2020/Conference/Paper115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygSq3VFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper115/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper115/Authors|ICLR.cc/2020/Conference/Paper115/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176185, "tmdate": 1576860544830, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper115/Authors", "ICLR.cc/2020/Conference/Paper115/Reviewers", "ICLR.cc/2020/Conference/Paper115/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper115/-/Official_Comment"}}}, {"id": "SJxy9Ndy5r", "original": null, "number": 2, "cdate": 1571943559017, "ddate": null, "tcdate": 1571943559017, "tmdate": 1572972636859, "tddate": null, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a self-supervised reinforcement learning approach, Mutual Information-based State-Control (MISC), which maximizes the mutual information between the context states (i.e. robot states) and the states of interest (i.e. states of an object to manipulate). For this, they first split the entire state into two mutually exclusive sets of the context states and the states of interest. Then, the neural discriminator is trained to estimate the (lower-bound of) mutual information between the two states. The (mutual-information) intrinsic reward is computed by the trained neural discriminator, which is used for policy pre-training. Experimental results show that MISC helps to improve the performance of DDPG/SAC and the learned discriminator can be transferred to different environments.\n\n\nDetailed comments and questions:\n\n- In the paper, the states are represented by only object positions (x, y, z). Is this sufficient? (e.g. velocity is unnecessary?)\n\n- For MISC, the additional assumption is required: the agent should know that which parts of the states are its own controllable state and object's state respectively. Is this additional assumption realistic enough and has it been adopted in other previous works? Is there any way to discriminate robot states and object states automatically?\n\n- Can MISC deal with the problems where the number of objects of interest is more than two? In this case, how can we define mutual information?\n\n- In Eq. (4), T(x_1:N, y_1:N) is assumed to be decomposable into the sum of T(x_t, y_t) / N. Can this make the lower bound (Eq. (3)) arbitrarily loose since the class of functions becomes very limited?\n\n- Detailed experimental setups are missing. e.g. network architecture, hyper-parameters (e.g. I_tran^max), and how they were searched.\n\n- Similarly to the problem of sparse reward, if the robot and the object are far apart and it is difficult to reach the object with random exploration, it would also be difficult to train the mutual information discriminator. How was the discriminator trained? How many time steps were used to train MI discriminator?\n\n- It seems that the MI discriminator learns to estimate the 'proximity' between the robot and the object. Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.\n\n- For the MISC+DIAYN, what if we train the agent using MISC and DIAYN at the same time, instead of pre-training MISC first and fine-tuning DIAYN later?\n\n- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.\n\n- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.\n\n- It seems that MISC is beneficial when the robot should get closer to the object for the success of the task. Then, how about the opposite situation? What if the task requires that the robot should 'avoid' the object of interest? Does MISC still work? Is it helpful for the improvement of sample efficiency?\n\n- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.\n\n- In section 4.3, what happens if we transfer the learned discriminator to Pick&Place from Push that has a gripper fixed to be closed, rather than the opposite direction (i.e. from Pick&Place to Push)? Does the MISC-t still well work? Can the learned MI discriminator be transferred to different tasks even when the state space is different?\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper115/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper115/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575853057695, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper115/Reviewers"], "noninvitees": [], "tcdate": 1570237756853, "tmdate": 1575853057708, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper115/-/Official_Review"}}}, {"id": "Hkgcm01WqB", "original": null, "number": 3, "cdate": 1572040226189, "ddate": null, "tcdate": 1572040226189, "tmdate": 1572972636816, "tddate": null, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "invitation": "ICLR.cc/2020/Conference/Paper115/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper paper proposes a mutual information maximization objective for discovering unsupervised robotic manipulation skills. The paper assumes that the state space can be divided into two parts - the state of the robot (\u201ccontext states\u201d) which is controllable via actions and the state of an object (\u201cstates of interest\u201d) which must be manipulated by the robot. Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully.\n\nI vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an \u201cobject\u201d or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.\n\nMy main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others. The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption. It doesn\u2019t seem like a surprising discovery that maximizing the mutual information between the robot state and object state will lead to skills that actually make the robot move the object.\n\nGiven that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper. Can the authors elaborate on why this choice should intuitively be better than the proposed method alone?\n\nThe paper does not talk about how these skills can be used as primitive actions by a higher level controller (in a hierarchical RL setup), which would help in demonstrating the usefulness of these skills - e.g.: are these skills sequentially composable such that they can solve a complex task?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper115/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper115/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Supervised State-Control through Intrinsic Mutual Information Rewards", "authors": ["Rui Zhao", "Volker Tresp", "Wei Xu"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com", "wei.xu@horizon.ai"], "keywords": ["Intrinsic Reward", "Deep Reinforcement Learning", "Skill Discovery", "Mutual Information", "Self-Supervised Learning", "Unsupervised Learning"], "TL;DR": "This paper introduces Mutual Information-based State-Control, a self-supervised reinforcement learning framework for discovering robotic manipulation skills.", "abstract": "Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, we propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. We formulate the intrinsic objective as rewarding the skills that maximize the mutual information between the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. We evaluate our approach for different simulated robotic manipulation tasks from OpenAI Gym. We show that our method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. Our results show that the mutual information between the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU", "pdf": "/pdf/d07eb5f3d4fdcd06d6c363f14683e085a9c6af80.pdf", "code": "https://github.com/misc-project/misc", "paperhash": "zhao|selfsupervised_statecontrol_through_intrinsic_mutual_information_rewards", "original_pdf": "/attachment/caf6769ad1575657d79a342a06bb9ec7a02038d7.pdf", "_bibtex": "@misc{\nzhao2020selfsupervised,\ntitle={Self-Supervised State-Control through Intrinsic Mutual Information Rewards},\nauthor={Rui Zhao and Volker Tresp and Wei Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HygSq3VFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygSq3VFvH", "replyto": "HygSq3VFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575853057695, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper115/Reviewers"], "noninvitees": [], "tcdate": 1570237756853, "tmdate": 1575853057708, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper115/-/Official_Review"}}}], "count": 9}