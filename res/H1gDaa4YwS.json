{"notes": [{"id": "H1gDaa4YwS", "original": "BkgXb0euPH", "number": 822, "cdate": 1569439167192, "ddate": null, "tcdate": 1569439167192, "tmdate": 1577168257028, "tddate": null, "forum": "H1gDaa4YwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["you.xie@tum.de", "nils.thuerey@tum.de"], "title": "Learning General and Reusable Features via Racecar-Training", "authors": ["You Xie", "Nils Thuerey"], "pdf": "/pdf/8243209efd8018b95a872576089bb6f721fc10d9.pdf", "TL;DR": "We propose a novel bi-directional training approach for learning of general features.", "abstract": "We propose a novel training approach for improving the learning of generalizing features in neural networks. We augment the network with a reverse pass which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, i.e. racecar training, leads to significantly more general features to be extracted from a given data set. We demonstrate in our paper that a network obtained in this way is continually trained for the original task, it outperforms baseline models trained in a regular fashion. This improved performance is visible for a wide range of learning tasks from classification, to regression and stylization. In addition, networks trained with our approach exhibit improved performance for task transfers. We additionally analyze the mutual information of our networks to explain the improved generalizing capabilities.", "keywords": ["transfer learning", "neural networks", "generalization", "reusable features"], "paperhash": "xie|learning_general_and_reusable_features_via_racecartraining", "original_pdf": "/attachment/329f8eabd07b1451d2f6a6e19dc5600838af21fb.pdf", "_bibtex": "@misc{\nxie2020learning,\ntitle={Learning General and Reusable Features via Racecar-Training},\nauthor={You Xie and Nils Thuerey},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gDaa4YwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rDiH2krpwA", "original": null, "number": 1, "cdate": 1576798707120, "ddate": null, "tcdate": 1576798707120, "tmdate": 1576800929192, "tddate": null, "forum": "H1gDaa4YwS", "replyto": "H1gDaa4YwS", "invitation": "ICLR.cc/2020/Conference/Paper822/-/Decision", "content": {"decision": "Reject", "comment": "Both reviewers (we apologize for the lack of a 3rd review) did not feel the paper should be accepted. The rebuttal offered did not change the reviewer scores. So the paper cannot be accepted unfortunately. But the authors should use the feedback to improve their paper and resubmit.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["you.xie@tum.de", "nils.thuerey@tum.de"], "title": "Learning General and Reusable Features via Racecar-Training", "authors": ["You Xie", "Nils Thuerey"], "pdf": "/pdf/8243209efd8018b95a872576089bb6f721fc10d9.pdf", "TL;DR": "We propose a novel bi-directional training approach for learning of general features.", "abstract": "We propose a novel training approach for improving the learning of generalizing features in neural networks. We augment the network with a reverse pass which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, i.e. racecar training, leads to significantly more general features to be extracted from a given data set. We demonstrate in our paper that a network obtained in this way is continually trained for the original task, it outperforms baseline models trained in a regular fashion. This improved performance is visible for a wide range of learning tasks from classification, to regression and stylization. In addition, networks trained with our approach exhibit improved performance for task transfers. We additionally analyze the mutual information of our networks to explain the improved generalizing capabilities.", "keywords": ["transfer learning", "neural networks", "generalization", "reusable features"], "paperhash": "xie|learning_general_and_reusable_features_via_racecartraining", "original_pdf": "/attachment/329f8eabd07b1451d2f6a6e19dc5600838af21fb.pdf", "_bibtex": "@misc{\nxie2020learning,\ntitle={Learning General and Reusable Features via Racecar-Training},\nauthor={You Xie and Nils Thuerey},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gDaa4YwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gDaa4YwS", "replyto": "H1gDaa4YwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709924, "tmdate": 1576800258800, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper822/-/Decision"}}}, {"id": "SylPFQeojB", "original": null, "number": 3, "cdate": 1573745535309, "ddate": null, "tcdate": 1573745535309, "tmdate": 1573812086155, "tddate": null, "forum": "H1gDaa4YwS", "replyto": "B1lfbmsKtH", "invitation": "ICLR.cc/2020/Conference/Paper822/-/Official_Comment", "content": {"title": "Reply to reviewer #2", "comment": "Dear reviewer, thank you for your review. We are glad you agree that our proposed method can extract general features from the data set. Below we will answer your comments one by one.\n\n--- Regarding presentation: Yes, we agree that what the neural networks learn and what we reused for transfer learning effectively are the parameters and weights of the network. However, the parameters typically represent features in the form of convolutional kernels. For example, when we reuse parameters from a CNN, and this network can extract information from natural image content, such as eyes, then it is common practice to refer to such a feature as an \u201ceye feature\u201d, see, e.g. [1-4].\n\n--- Mutual information (MI) plane visualization: This is a powerful tool for neural networks analysis but admittedly not very intuitive, hence we include more explanations in the corresponding part of the appendix. \nWe use the MI plane of Std_{A} in figure 2 as an example to illustrate MI plane in details. The neural network in this numerical task has 5 middle layers ($L_{1\\sim5}$) and one output layer ($L_{6}$). The $X$ axis of the MI plane represents the quantity $I(X;L)$, i..e the mutual information between input variable $X$ and output of each layer $L$. The $Y$ axis of the MI plane represents $I(L;Y)$, the mutual information between output of each layer $L$ and output variable $Y$. Besides, every point in the graphs is colored w.r.t. the training epochs, i.e., initially black, and yellow once the training is finished. Hence, we can see six lines changing from black to yellow in figure 14 of the updated version. According to the information bottleneck principle [5], the outputs of the early layers contain more information from the input, which means a high value for  $I(X;L)$ and $I(L;Y)$. \n\nIn figure 14 we can see that early layers $L_{1\\sim3}$ are located in the top right part of the graph. For later layers, such as $L_{6}$, parameters of $L_{6}$ are randomly initialized before training, so there are almost no direct relationships between the output of $L_{6}$ and $X$ or $Y$, which means low values of $I(X;L)$ and $I(L;Y)$. Once the model is well trained, $L_{6}$ of $Std_A$ is able to generate data which has the same distribution with $Y$, so $I(L;Y)$ has increased and moved to the top left corner of the graph. \n\nIn figure 14, we can see that the line of $L_{6}$ starts from the bottom left part of the graph, and moves to the top left part during training. For $RR_{A}^{6}$ in figure 2, the L2 constraint is explicitly applied to decrease difference between $L$ and $L^{'}$, and the output is pushed to recover lost information of the input, so $I(X;L)$ of later layers is  increased, and $I(X;L)$ of early layers is decreased to make the task easier.\n\n--- Typos and symbols, thank you for pointing this out. We corrected the typos and improved the unclear parts.\n\n--- Performance: We repeat tests in figure 3,4,6,7 five times and tests in figure 9 three times. Details about the results are shown in the supplementary materials. The trends and derived conclusions are still consistent with our original submission. We can see that models trained with the racecar loss perform significantly better than standard models.\n\n[1] Heming Liang and Qi Li. Hyperspectral imagery classification using sparse representations of convolutional neural network features. Remote Sensing, 8(2):99, 2016.\n[2] Zetao Chen, Obadiah Lam, Adam Jacobson, and Michael Milford. Convolutional neural network-based place recognition. arXiv preprint arXiv: 1411.1509, 2014.\n[3] Wenyuan Dai, Yuqiang Chen, Gui-Rong Xue, Qiang Yang, and Yong Yu. Translated learning: Transfer learning across different feature spaces. In Advances in neural information processing systems, pp. 353-360, 2019.\n[4] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural Information processing systems, pp. 3320-3328, 2014.\n[5] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2015.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["you.xie@tum.de", "nils.thuerey@tum.de"], "title": "Learning General and Reusable Features via Racecar-Training", "authors": ["You Xie", "Nils Thuerey"], "pdf": "/pdf/8243209efd8018b95a872576089bb6f721fc10d9.pdf", "TL;DR": "We propose a novel bi-directional training approach for learning of general features.", "abstract": "We propose a novel training approach for improving the learning of generalizing features in neural networks. We augment the network with a reverse pass which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, i.e. racecar training, leads to significantly more general features to be extracted from a given data set. We demonstrate in our paper that a network obtained in this way is continually trained for the original task, it outperforms baseline models trained in a regular fashion. This improved performance is visible for a wide range of learning tasks from classification, to regression and stylization. In addition, networks trained with our approach exhibit improved performance for task transfers. We additionally analyze the mutual information of our networks to explain the improved generalizing capabilities.", "keywords": ["transfer learning", "neural networks", "generalization", "reusable features"], "paperhash": "xie|learning_general_and_reusable_features_via_racecartraining", "original_pdf": "/attachment/329f8eabd07b1451d2f6a6e19dc5600838af21fb.pdf", "_bibtex": "@misc{\nxie2020learning,\ntitle={Learning General and Reusable Features via Racecar-Training},\nauthor={You Xie and Nils Thuerey},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gDaa4YwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gDaa4YwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference/Paper822/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper822/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper822/Reviewers", "ICLR.cc/2020/Conference/Paper822/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper822/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper822/Authors|ICLR.cc/2020/Conference/Paper822/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165682, "tmdate": 1576860544523, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference/Paper822/Reviewers", "ICLR.cc/2020/Conference/Paper822/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper822/-/Official_Comment"}}}, {"id": "HJxBHXlisH", "original": null, "number": 2, "cdate": 1573745469052, "ddate": null, "tcdate": 1573745469052, "tmdate": 1573745581075, "tddate": null, "forum": "H1gDaa4YwS", "replyto": "rkxHedg05S", "invitation": "ICLR.cc/2020/Conference/Paper822/-/Official_Comment", "content": {"title": "Reply to reviewer #1", "comment": "Dear reviewer, we would like to thank you for your comments. We are glad you like our stylization and super resolution results. Below we will answer your comments one by one.\n\n---- Regarding the repeatability: We repeat tests in figure 3,4,6,7 five times and tests in figure 9 three times. Details about the results are shown in the supplementary materials. The trends and derived conclusions are still consistent with our original submission. We can see that models trained with the racecar loss perform significantly better than standard models.\n\n---- Regarding the illustrative examples: Since the  mutual information (MI) tests used the MI plane to highlight the properties of our method, we used more illustrative examples in later sections to visually show the impact of our method.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["you.xie@tum.de", "nils.thuerey@tum.de"], "title": "Learning General and Reusable Features via Racecar-Training", "authors": ["You Xie", "Nils Thuerey"], "pdf": "/pdf/8243209efd8018b95a872576089bb6f721fc10d9.pdf", "TL;DR": "We propose a novel bi-directional training approach for learning of general features.", "abstract": "We propose a novel training approach for improving the learning of generalizing features in neural networks. We augment the network with a reverse pass which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, i.e. racecar training, leads to significantly more general features to be extracted from a given data set. We demonstrate in our paper that a network obtained in this way is continually trained for the original task, it outperforms baseline models trained in a regular fashion. This improved performance is visible for a wide range of learning tasks from classification, to regression and stylization. In addition, networks trained with our approach exhibit improved performance for task transfers. We additionally analyze the mutual information of our networks to explain the improved generalizing capabilities.", "keywords": ["transfer learning", "neural networks", "generalization", "reusable features"], "paperhash": "xie|learning_general_and_reusable_features_via_racecartraining", "original_pdf": "/attachment/329f8eabd07b1451d2f6a6e19dc5600838af21fb.pdf", "_bibtex": "@misc{\nxie2020learning,\ntitle={Learning General and Reusable Features via Racecar-Training},\nauthor={You Xie and Nils Thuerey},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gDaa4YwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gDaa4YwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference/Paper822/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper822/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper822/Reviewers", "ICLR.cc/2020/Conference/Paper822/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper822/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper822/Authors|ICLR.cc/2020/Conference/Paper822/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165682, "tmdate": 1576860544523, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference/Paper822/Reviewers", "ICLR.cc/2020/Conference/Paper822/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper822/-/Official_Comment"}}}, {"id": "BkgPyzljir", "original": null, "number": 1, "cdate": 1573745118537, "ddate": null, "tcdate": 1573745118537, "tmdate": 1573745405956, "tddate": null, "forum": "H1gDaa4YwS", "replyto": "H1gDaa4YwS", "invitation": "ICLR.cc/2020/Conference/Paper822/-/Official_Comment", "content": {"title": "General response to Reviewers", "comment": "We would like to thank all reviewers for their comments - they are very important for us in order to improve our paper. We are glad you agree that parts of our results are promising and our proposed method can extract more general features from the data set. To support our conclusion, we have repeated all tests of our original paper, and the summarized performance results will be included in the supplementary materials. Below, we list some of the most important numbers.\n\nTo summarize, the key observation is that our approach robustly, over many repeated runs, yields improvements in terms of performance for the original task, and especially for task transfers.\nMutual Information (MI) tests accuracy, 5 runs:\n+--------------------Std_{AA}-----RR_{AA}-----Std_{AB}-----RR_{AB}-----+\n|       Mean      |      0.973    |      0.994      |    0.136     |       0.997     |\n| ___Std. Dev._|____0.013__|____0.004____|__    0.0_  __|__0.003_____|\nMNIST tests accuracy , 5 runs:\n+--------------------Std_{AA}-----RR_{AA}-----Std_{AB}-----RR_{AB}-----+\n|        Mean   |      0.984    |      0.986     |    0.952      |       0.961       |\n| __Std. Dev._|___0.0009  |____0.0006__|__0.0051___|____0.0044____|\nCifar tests accuracy, 5 runs:\n+--------------------Std_{AA}-----RR_{AA}-----Std_{AB}-----RR_{AB}-----+\n|        Mean     |      0.805    |      0.856     |    0.183      |       0.276      |\n| __Std. Dev.__|___0.0340__|___0.0130___|__0.0141___|____0.0138__|\nSmoke tests L2 loss, 3 runs:\n+------------------Std_{AB1}----RR_{AB1}----Std_{AB2}---RR_{AB2}----+\n|        Mean     |      8401.4  |      187.0     |    7.56e8      |       2.36e7  |\n| __Std. Dev.__|_12559.15__|___41.61 ___|__ 1.19e9     |____5.78e6 _|\n\nWe can see that racecar trained models performance significantly better (higher average performance and reduced standard deviation) than models trained with standard procedures. For instance, in MI tests, the accuracy of RR_{AB} is 6.3 times higher than Std_{AB}; for the Cifar tests, the accuracy of RR_{AB} is 50.82% higher than Std_{AB}; for the smoke AutoEncoder tests, the L2 loss of RR_{AB1}  is 300.83 times lower than Std_{AB1}, and the L2 loss of RR_{AB2}  is 31.03 times lower than Std_{AB2}.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper822/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["you.xie@tum.de", "nils.thuerey@tum.de"], "title": "Learning General and Reusable Features via Racecar-Training", "authors": ["You Xie", "Nils Thuerey"], "pdf": "/pdf/8243209efd8018b95a872576089bb6f721fc10d9.pdf", "TL;DR": "We propose a novel bi-directional training approach for learning of general features.", "abstract": "We propose a novel training approach for improving the learning of generalizing features in neural networks. We augment the network with a reverse pass which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, i.e. racecar training, leads to significantly more general features to be extracted from a given data set. We demonstrate in our paper that a network obtained in this way is continually trained for the original task, it outperforms baseline models trained in a regular fashion. This improved performance is visible for a wide range of learning tasks from classification, to regression and stylization. In addition, networks trained with our approach exhibit improved performance for task transfers. We additionally analyze the mutual information of our networks to explain the improved generalizing capabilities.", "keywords": ["transfer learning", "neural networks", "generalization", "reusable features"], "paperhash": "xie|learning_general_and_reusable_features_via_racecartraining", "original_pdf": "/attachment/329f8eabd07b1451d2f6a6e19dc5600838af21fb.pdf", "_bibtex": "@misc{\nxie2020learning,\ntitle={Learning General and Reusable Features via Racecar-Training},\nauthor={You Xie and Nils Thuerey},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gDaa4YwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gDaa4YwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference/Paper822/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper822/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper822/Reviewers", "ICLR.cc/2020/Conference/Paper822/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper822/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper822/Authors|ICLR.cc/2020/Conference/Paper822/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165682, "tmdate": 1576860544523, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper822/Authors", "ICLR.cc/2020/Conference/Paper822/Reviewers", "ICLR.cc/2020/Conference/Paper822/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper822/-/Official_Comment"}}}, {"id": "B1lfbmsKtH", "original": null, "number": 1, "cdate": 1571562234484, "ddate": null, "tcdate": 1571562234484, "tmdate": 1572972548044, "tddate": null, "forum": "H1gDaa4YwS", "replyto": "H1gDaa4YwS", "invitation": "ICLR.cc/2020/Conference/Paper822/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper attempts to learn general and reusable features for transfer learning tasks. The authors propose a training paradigm called Racecar Training. The core idea of it is to operate a reverse pass for the network. The authors use mutual information to analyze the network for its improved generalizing capabilities. They also conduct experiments on classification, regression and stylization to validate their method\u2019s effectiveness.\n\nPros:\nThe reverse pass idea is similar to auto-encoder paradigm that discards redundant information and only save the essential low dimensional one by comparing the original data and the decoding data. The general feature the authors mentioned is like the essential low dimensional information, which is reasonable.\n\nCons:\n1.\tI think the main drawbacks of this paper is that the authors make a poor presentation. The authors talk about learning general features with which the model can use on new task in the title, introduction and even the whole paper. However, there are no explicit general features learned during the learning procedure. They only perform the reverse pass when learning in the original task. Even the structure of networks does not change at all. The general and reusable feature is only an explanation of the improved performance. I think the authors should change this explanation to a more convincing one. For example, the network may learn general and reusable weights or parameters since it is the model learned that will be applied to new tasks instead of the features. \n2.\tThe analysis by mutual information makes the paper hard to follow. The figures such as figure 2 are so confusing. There are so many points and lines in each picture. What do they mean? What are the x-axis and y-axis? The authors also do not explain what the meaning of different mutual information are. \n3.\tThe symbols are chaotic. The authors explain \u201cRR^3\u201d means n=1 in equation 1. Then what does n equal to in RR^1? The authors explain \u201cAB\u201d means the model was trained for task A during phase I, and is then trained for task B as transfer in phase II. Then what does AA/AB in \u201cStd_{AA/AB}\u201d mean? \n4.\tIn the last paragraph of page 3, the authors mention orange color. However, there are only green, blue and yellow color in Figure 1.\n5.\tIn experiments, I do not see obvious advantage of the proposed method. For example, in Figure 6, the test accuracy of Std_{AA} is 0.9842 while that of RR^3_{AA} is only 0.9859. In Figure 7, the test accuracy of Std_{AA} is 0.8377 while that of RR^13_{AA} is 0.8711. The transfer tasks (referred as AB) also show minor advantages. With minor advantages but increased requirements for memory and additional computations (e.g., 61.13% slower per epoch for the MNIST tests mentioned by the authors), the proposed method shows very limited values.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper822/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper822/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["you.xie@tum.de", "nils.thuerey@tum.de"], "title": "Learning General and Reusable Features via Racecar-Training", "authors": ["You Xie", "Nils Thuerey"], "pdf": "/pdf/8243209efd8018b95a872576089bb6f721fc10d9.pdf", "TL;DR": "We propose a novel bi-directional training approach for learning of general features.", "abstract": "We propose a novel training approach for improving the learning of generalizing features in neural networks. We augment the network with a reverse pass which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, i.e. racecar training, leads to significantly more general features to be extracted from a given data set. We demonstrate in our paper that a network obtained in this way is continually trained for the original task, it outperforms baseline models trained in a regular fashion. This improved performance is visible for a wide range of learning tasks from classification, to regression and stylization. In addition, networks trained with our approach exhibit improved performance for task transfers. We additionally analyze the mutual information of our networks to explain the improved generalizing capabilities.", "keywords": ["transfer learning", "neural networks", "generalization", "reusable features"], "paperhash": "xie|learning_general_and_reusable_features_via_racecartraining", "original_pdf": "/attachment/329f8eabd07b1451d2f6a6e19dc5600838af21fb.pdf", "_bibtex": "@misc{\nxie2020learning,\ntitle={Learning General and Reusable Features via Racecar-Training},\nauthor={You Xie and Nils Thuerey},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gDaa4YwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gDaa4YwS", "replyto": "H1gDaa4YwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575398741829, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper822/Reviewers"], "noninvitees": [], "tcdate": 1570237746497, "tmdate": 1575398741841, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper822/-/Official_Review"}}}, {"id": "rkxHedg05S", "original": null, "number": 2, "cdate": 1572894700837, "ddate": null, "tcdate": 1572894700837, "tmdate": 1572972548010, "tddate": null, "forum": "H1gDaa4YwS", "replyto": "H1gDaa4YwS", "invitation": "ICLR.cc/2020/Conference/Paper822/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a scheme for training layered feedforward neural networks with backwards accumulation of gradients. In the proposed scheme, the intermediate activations during a forward pass are constrained, using an L2 norm penalty, to be close to the activations of a model that inverts the operations of each layer and transforms the data in reverese order (from output to input). The second, inverse, network shares all parameters with the feedforward network.\n\nThe paper hypothesizes that such training, which they call racecar training, results in features that are transferable between tasks; i.e. using racecar training to learn the features, then applying regular training on a novel task. To support this hypothesis, the paper provides an empirical analysis of the mutual information between inputs and intermediate features, and between intermediate features and outputs (as proposed in the information bottleneck literature). Under this analysis, the paper shows that using racecar training the intermediate layers contain less information about the outputs than standard feedforward training. The paper states that this makes the features learned by racecar training better for transfer: features that enable the network to achieve high predictive accuracy on a particular tasks but that carry little information about the output distribution, thus less specialized. The paper continues this analysis to the transfer setting, first to the same initial task, and then the a new task. In both cases one of the variants of the proposed method appears to produce higher accuracy than standard pre-training.\n\nI'm inclining to reject this paper given that the results on the main hypothesis (i.e. transferability of features) seem to provide only marginal improvement, and we have no idea about the repeatability of the results ( how many times did the authors run the experiments for figures 3,4,6,7,9? What's the spread of the results? Are these results significant?). The results on style transfer and super resolution are promising, but these are only illustrative examples: these results do not provide insights on how well the method works in general, or when does it fail.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper822/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper822/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["you.xie@tum.de", "nils.thuerey@tum.de"], "title": "Learning General and Reusable Features via Racecar-Training", "authors": ["You Xie", "Nils Thuerey"], "pdf": "/pdf/8243209efd8018b95a872576089bb6f721fc10d9.pdf", "TL;DR": "We propose a novel bi-directional training approach for learning of general features.", "abstract": "We propose a novel training approach for improving the learning of generalizing features in neural networks. We augment the network with a reverse pass which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, i.e. racecar training, leads to significantly more general features to be extracted from a given data set. We demonstrate in our paper that a network obtained in this way is continually trained for the original task, it outperforms baseline models trained in a regular fashion. This improved performance is visible for a wide range of learning tasks from classification, to regression and stylization. In addition, networks trained with our approach exhibit improved performance for task transfers. We additionally analyze the mutual information of our networks to explain the improved generalizing capabilities.", "keywords": ["transfer learning", "neural networks", "generalization", "reusable features"], "paperhash": "xie|learning_general_and_reusable_features_via_racecartraining", "original_pdf": "/attachment/329f8eabd07b1451d2f6a6e19dc5600838af21fb.pdf", "_bibtex": "@misc{\nxie2020learning,\ntitle={Learning General and Reusable Features via Racecar-Training},\nauthor={You Xie and Nils Thuerey},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gDaa4YwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gDaa4YwS", "replyto": "H1gDaa4YwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper822/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575398741829, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper822/Reviewers"], "noninvitees": [], "tcdate": 1570237746497, "tmdate": 1575398741841, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper822/-/Official_Review"}}}], "count": 7}