{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458061671344, "tcdate": 1458061671344, "id": "3Qx7joOmLCp7y9wltPvx", "invitation": "ICLR.cc/2016/workshop/-/paper/116/review/11", "forum": "L7VOOy8B6hRNGwArs4Bn", "replyto": "L7VOOy8B6hRNGwArs4Bn", "signatures": ["ICLR.cc/2016/workshop/paper/116/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/116/reviewer/11"], "content": {"title": "This paper studies the effect of additive noise combined with adversarial noise", "rating": "3: Clear rejection", "review": "This short paper studies how to improve the stability of CNN architectures to adversarial noise. For that purpose, it proposes a Gaussian additive noise model applied to each layer, and the result is combined with adversarial training on large scale classification.\n\nPros:\n- The model is simple\n\nCons:\n- Unfortunately, this paper suffers from lack of quality, clarity, originality and significance.\n- The paper does not clearly motivate what is stated in the title. My understanding after reading the paper several times is that the authors train using a noise model that is not the adversarial one, but a gaussian additive noise injected at each layer, but then evaluate the performance at test time using the adversarial noise. What is the motivation to use Gaussian noise then? Is there any principled reason to consider this stochastic model in order to approximate the adversarial noise? \n- Section 2 presents a series of qualitative approximations to justify why one can use Gaussian noise after rectifications and poolings. The resulting noise model thus seems to be the superposition of gaussian noise injected at each layer. The statements involving the Central Limit theorem are too imprecise -- if one considers for example recent CNN architectures with 3x3 spatial kernels, it is unclear to me why the output of the ReLu is well approximated with a Gaussian, since what matters is not how many terms you average, but how many independent (or uncorrelated) terms you average.\n- Section 3 presents the numerical experiments, but we see no error bars in the results. How statistically significant are these numbers? \n- Also, I do not really understand the Imagenet results. How come the accuracy using purely Gaussian noise is significantly better than the error using Gaussian noise + adversarial noise, given that the validation set is corrupted with adversarial noise? This behavior is strange, and does not appear to happen in the cifar experiments. \n- How do you interpret that the gaussian regularization indeed slightly better (if we believe the differences are statistically significant) when test examples are corrupted by adversarial noise, but slightly worse when no noise is added? \n\nOverall, my impression is that this paper does not give enough rigorous insights into the problem they are addressing, neither theoretical nor experimental. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Robust Convolutional Neural Networks under Adversarial Noise", "abstract": "Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called \"adversarial examples\". In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise.", "pdf": "/pdf/L7VOOy8B6hRNGwArs4Bn.pdf", "paperhash": "jin|robust_convolutional_neural_networks_under_adversarial_noise", "conflicts": ["purdue.edu"], "authors": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "authorids": ["jhjin@purdue.edu", "adundar@purdue.edu", "euge@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580016052, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580016052, "id": "ICLR.cc/2016/workshop/-/paper/116/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "L7VOOy8B6hRNGwArs4Bn", "replyto": "L7VOOy8B6hRNGwArs4Bn", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/116/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457630307056, "tcdate": 1457630307056, "id": "E8VYgYX7JH31v0m2iDPz", "invitation": "ICLR.cc/2016/workshop/-/paper/116/review/10", "forum": "L7VOOy8B6hRNGwArs4Bn", "replyto": "L7VOOy8B6hRNGwArs4Bn", "signatures": ["ICLR.cc/2016/workshop/paper/116/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/116/reviewer/10"], "content": {"title": "This paper proposes a method to makes CNNs more robust to noisy inputs. ", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a new method to make CNNs more robust to adversarial noise by adding Gaussian noise to the input images and CNN layers. The paper is unclear in many parts  making it difficult to determine the exact computation being proposed. The best best of my understanding, the authors are proposing adding Gaussian noise to the input images and then at each subsequent layer sampling from a Gaussian with the mean and variance computed from the mean and variance of that layers input. This is not clearly explained and it is also not clear how to train this model. \n\nPros:\n- The stochasticity results in a slight improvement of the same deterministic network.\n\nCons:\n- The title and introduction suggest the authors are proposing a method to make CNNs robust to adversarial noise. However, adversarial examples are never used in the method and it seems to me that the method being proposed is perhaps just a general regularizing technique not specifically related to adversarial examples. This would be a fine contribution, but the way the method is motivated is very misleading.\n- The proposed method performs significantly worse than Goodfellow et al.'s (2014) method of training with adversarial examples (when evaluated on adversarial examples, which is the setting in which the authors are claiming to address). Combing the proposed method with Goodfellow et al.'s (2014) method provides a very slight improvement however this could simply be due to the regularizing effects of adding stochasticity.\n- Comparisons with other methods of introducing stochasticity to a network are missing. Some questions remain such as: what happens for noise distributions other than Gaussian? How does this compare to the simpler method of just adding noise to the input (and/or hidden units) and forward propagating as normal during training?\n\nSome additional minor comments:\n- typo: page one \"...during training but not has been applied...\"\n- page 2 you say X in R^3, when I think you mean X in R. If R^3 was in fact meant then the channel index should be dropped I think?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Robust Convolutional Neural Networks under Adversarial Noise", "abstract": "Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called \"adversarial examples\". In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise.", "pdf": "/pdf/L7VOOy8B6hRNGwArs4Bn.pdf", "paperhash": "jin|robust_convolutional_neural_networks_under_adversarial_noise", "conflicts": ["purdue.edu"], "authors": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "authorids": ["jhjin@purdue.edu", "adundar@purdue.edu", "euge@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580016323, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580016323, "id": "ICLR.cc/2016/workshop/-/paper/116/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "L7VOOy8B6hRNGwArs4Bn", "replyto": "L7VOOy8B6hRNGwArs4Bn", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/116/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457581442231, "tcdate": 1457581442231, "id": "K1VgzwpE4S28XMlNCVA7", "invitation": "ICLR.cc/2016/workshop/-/paper/116/review/12", "forum": "L7VOOy8B6hRNGwArs4Bn", "replyto": "L7VOOy8B6hRNGwArs4Bn", "signatures": ["~Jiashi_Feng1"], "readers": ["everyone"], "writers": ["~Jiashi_Feng1"], "content": {"title": "This paper propose a systematic method, randomize the inputs by adding Gaussian noise,  to robustify feedforward CNN to adversarial noise. ", "rating": "6: Marginally above acceptance threshold", "review": "This paper is investigating an interesting problem for deep learning, i.e. how to robustify a deep neural network. The paper is well written and easy to follow. The method proposed in this work is novel to me. \n\nPros\n* An interesting problem\n* A feasible and efficient solution\n\nCons\n* There is no justification why adding Gaussian noise into the input is a better choice, compared with other random distribution. I understand Gaussian distribution is easy for computation. But it is not so straightforward why adding Gaussian randomness could improve the robustness of the method. \n* The experiments do not show benefits of introducing randomness into inputs, for standard training/test setting. Compared with baseline and standard training + stochastic FF, the performance actually drops by more than 1% using the stochastic FF on CIFAR 10. Although certain robustness to adversarial noise is demonstrated, I am more glad to see the proposed method can improve the performance of standard setting and realistic problem.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Robust Convolutional Neural Networks under Adversarial Noise", "abstract": "Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called \"adversarial examples\". In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise.", "pdf": "/pdf/L7VOOy8B6hRNGwArs4Bn.pdf", "paperhash": "jin|robust_convolutional_neural_networks_under_adversarial_noise", "conflicts": ["purdue.edu"], "authors": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "authorids": ["jhjin@purdue.edu", "adundar@purdue.edu", "euge@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580015640, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580015640, "id": "ICLR.cc/2016/workshop/-/paper/116/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "L7VOOy8B6hRNGwArs4Bn", "replyto": "L7VOOy8B6hRNGwArs4Bn", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/116/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455819184655, "tcdate": 1455819184655, "id": "L7VOOy8B6hRNGwArs4Bn", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "L7VOOy8B6hRNGwArs4Bn", "signatures": ["~Jonghoon_Jin1"], "readers": ["everyone"], "writers": ["~Jonghoon_Jin1"], "content": {"CMT_id": "", "title": "Robust Convolutional Neural Networks under Adversarial Noise", "abstract": "Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to a small perturbation of input called \"adversarial examples\". In this work, we propose a new feedforward CNN that improves robustness in the presence of adversarial noise. Our model uses stochastic additive noise added to the input image and to the CNN models. The proposed model operates in conjunction with a CNN trained with either standard or adversarial objective function. In particular, convolution, max-pooling, and ReLU layers are modified to benefit from the noise model. Our feedforward model is parameterized by only a mean and variance per pixel which simplifies computations and makes our method scalable to a deep architecture. From CIFAR-10 and ImageNet test, the proposed model outperforms other methods and the improvement is more evident for difficult classification tasks or stronger adversarial noise.", "pdf": "/pdf/L7VOOy8B6hRNGwArs4Bn.pdf", "paperhash": "jin|robust_convolutional_neural_networks_under_adversarial_noise", "conflicts": ["purdue.edu"], "authors": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "authorids": ["jhjin@purdue.edu", "adundar@purdue.edu", "euge@purdue.edu"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}