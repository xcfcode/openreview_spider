{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363840020000, "tcdate": 1363840020000, "number": 6, "id": "UNlcNgK7BCN9v", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yGgjGkkbeFSbt", "replyto": "yGgjGkkbeFSbt", "signatures": ["Ross Goroshin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revised paper is now available on arXiv."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363749480000, "tcdate": 1363749480000, "number": 1, "id": "__krPw9SreVyO", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yGgjGkkbeFSbt", "replyto": "yGgjGkkbeFSbt", "signatures": ["Ross Goroshin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their constructive comments.\r\n\r\nA revised version of the paper has been submitted to arXiv and should be available shortly. \r\n\r\nIn addition to minor corrections and additions throughout the paper, we have added three new subsections:\r\n\r\n(1) a potential extension of the SATAE framework to include differentiable\r\nfunctions without a zero-gradient region\r\n\r\n(2) experiments on the CIFAR-10 dataset\r\n\r\n(3) future work.\r\n\r\nWe have also expanded the introduction to better motivate our approach."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363043520000, "tcdate": 1363043520000, "number": 1, "id": "MAHULigTUZMSF", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "yGgjGkkbeFSbt", "replyto": "pn6HDOWYfCDYA", "signatures": ["Sixin Zhang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'complementary nonlinerity' is very interesting, it makes me think of wavelet, transforming autoencoder. one question i was asking is how to make use of the information that's 'thrown' away (say after applying the nonlinearity, or the low path filter), or maybe those information are just noise? in saturating AE, the complementary nonlinerity is the residue of the projection (formula 1). What's that projective space? why the projection is defined elementwise (cf. softmax -> simplex)? how general can the non-linearity be extended for general signal representation (say Scattering Convolution Networks) , and classfication. I am just curious ~"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362779100000, "tcdate": 1362779100000, "number": 2, "id": "pn6HDOWYfCDYA", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yGgjGkkbeFSbt", "replyto": "yGgjGkkbeFSbt", "signatures": ["Rostislav Goroshin, Yann LeCun"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In response to 5bc2: the principle behind SATAE is a unification of the principles behind sparse autoencoders (and sparse coding in general) and contracting autoencoders. \r\n\r\nBasically, the main question with unsupervised learning is how to learn a contrast function (energy function in the energy-based framework, negative log likelihood in the probabilistic framework) that takes low values on the data manifold (or near it) and higher values everywhere else. \r\n\r\nIt's easy to make the energy low near data points. The hard part is making it higher everywhere else. There are basically 5 major classes of methods to do so: \r\n1. bound the volume of stuff that can have low energy (e.g. normalized probabilistic models, K-means, PCA); \r\n2 use a regularizer so that the volume of stuff that has low energy is as small as possible (sparse coding, contracting AE, saturating AE); \r\n3. explicitly push up on the energy of selected points, preferably outside the data manifold, often nearby (MC and MCMC methods, contrastive divergence); \r\n4. build local minima of the energy around data points by making the gradient small and the hessian large (score matching); \r\n5. learn the vector field of gradient of the energy (instead of the energy itself) so that it points away from the data manifold (denoising autoencoder). \r\n\r\nSATAE, just like contracting AE and sparse modeling falls in category 2.\r\n\r\nBasically, if you auto-encoding function is G(X,W), X being the input, and W the trainable parameters, and if your unregularized energy function is E(X,W) = ||X - G(X,W)||^2, if G is constant when X varies along a particular direction, then the energy will grow quadratically along that direction (technically, G doesn't need to be constant, but merely to have a gradient smaller than one). The more directions G(X,W) has low gradient, the lower the volume of stuff with low energy.\r\n\r\nOne advantage of SATAE is its extreme simplicity. You could see it as a version of Contracting AE cut down to its bare bones.\r\n\r\nWe can always obfuscate this simple principle with complicated math, but how would that help? At some point it will become necessary to make more precise theoretical statements, but for now we are merely searching for basic principles."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362593760000, "tcdate": 1362593760000, "number": 5, "id": "zOUdY11jd_zJr", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yGgjGkkbeFSbt", "replyto": "yGgjGkkbeFSbt", "signatures": ["anonymous reviewer 5bc2"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Saturating Auto-Encoder", "review": "Although this paper proposes an original (yet trivial) approach to regularize auto-encoders, it does not bring sufficient insights as to why saturating the hidden units should yield a better representation. The authors do not elaborate on whether the SATAE is a more general principle than previously proposed regularized auto-encoders(implying saturation as a collateral effect) or just another auto-encoder in an already well crowded space of models (ie:Auto-encoders and their variants). In the last years, many different types of auto-encoders have been proposed and most of them had no or little theory to justify the need for their existence, and despite all the efforts engaged by some to create a viable theoretical framework (geometric or probabilistic) it seems that the effectiveness of auto-encoders in building representations has more to do with a lucky parametrisation or yet another regularization trick.\r\n\r\nI feel the authors should motivate their approach with some intuitions about why should I saturate my auto-encoders, when I can denoise my input, sparsify my latent variables or do space contraction? It's worrisome that most of the research done for auto-encoders has mostly focused in coming up with the right regularization/parametrisation that would yield the best 'filters'. Following this path will ultimately make the majority of people reluctant to use auto-encoders because of their wide variety and little knowledge about when to use what. The auto-encoder community should backtrack and clear the intuitive/theoretical noise left behind, rather than racing for the next new model."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362361200000, "tcdate": 1362361200000, "number": 4, "id": "NNd3mgfs39NaH", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yGgjGkkbeFSbt", "replyto": "yGgjGkkbeFSbt", "signatures": ["anonymous reviewer 5955"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Saturating Auto-Encoder", "review": "This paper proposes a novel kind of penalty for regularizing autoencoder training, that encourages activations to move towards flat (saturated) regions of the unit's activation function. It is related to sparse autoencoders and contractive autoencoders that also happen to encourage saturation. But the proposed approach does so more directly and explicitly, through a 'complementary nonlinerity' that depends on the specific activation function chosen. \r\n\r\nPros:\r\n+ a novel and original regularization principle for autoencoders that relates to earlier approaches, but is, from a certain perspective, more general (at least for a specific subclass of activation functions). \r\n+ paper yields significant insight into the mechanism at work in such regularized autoencoders also clearly relating it to sparsity and contractive penalties. \r\n+ provides a credible path of explanation for the dramatic effect that the choice of different saturating activation functions has on the learned filters, and qualitatively shows it.\r\n\r\nCons:\r\n- Proposed regularization principle, as currently defined, only seems to make sense for activation functions that are piecewise linear and have some perfectly flat regions (e.g. a sigmoid activation would yield no penalty!) This should be discussed.\r\n- There is no quantitative measure of the usefulness of the representation learned with this principle. The usual comparison of classification or denoising performance based on the learned features, with those obtained with other autoencoder regularization principles would be a most welcome addition."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361946900000, "tcdate": 1361946900000, "number": 3, "id": "BSYbBsx9_5Suw", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yGgjGkkbeFSbt", "replyto": "yGgjGkkbeFSbt", "signatures": ["anonymous reviewer 3942"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Saturating Auto-Encoder", "review": "This paper proposes a regularizer for auto-encoders with\r\nnonlinearities that have a zegion with zero-gradient. The paper\r\nmentions three nonlinearities that fit into that category: shrinkage,\r\nsaturated linear, rectified linear.\r\n\r\nThe regularizer basically penalizes how much the activation deviates\r\nfrom saturation. The insight is that at saturation, the unit conveys\r\nless information compared to when it is in a non-saturated region.\r\n\r\nWhile I generally like the paper, I think it could be made a lot\r\nstronger by having more experimental results showing the practical\r\nbenefits of the nonlinearities and their associated regularizers.\r\n\r\nI am particularly interested in the case of saturated linear\r\nfunction. It will be interesting to compare the results of the\r\nproposed regularizer and the sparsity penalty. More concretely, f(x) =\r\n1 would incur some loss under the conventional sparsity; whereas, the\r\nnew regularizer does not. From the energy conservation point of view,\r\nit is not appealing to maintain the neuron at high activation, and the\r\nnew regularizer does not capture that. But it may be the case that,\r\nfor a network to generalize, we need to only restrict the neurons to\r\nbe in the saturation regions. Any numerical comparisons on some\r\nclassification benchmarks would be helpful.\r\n\r\nIt would also be interesting that the method is tested on a\r\nclassification dataset to see if it makes a different to use the new\r\nregularizers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361902200000, "tcdate": 1361902200000, "number": 7, "id": "x9pbTj7Nbg9Qs", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "yGgjGkkbeFSbt", "replyto": "yGgjGkkbeFSbt", "signatures": ["Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This is a cool investigation in a direction that I find fascinating, and I only have two remarks about minor points made in the paper.\r\n\r\n* Regarding the energy-based interpretation (that reconstruction error can be thought of as an energy function associated with an estimated probability function), there was a recent result which surprised me and challenges that view. In http://arxiv.org/abs/1211.4246 (What Regularized Auto-Encoders Learn from the Data Generating Distribution), Guillaume Alain and I found that denoising and contractive auto-encoders (where we penalize the Jacobian of the encoder-decoder function r(x)=decode(encode(x))) estimate the *score* of the data generating function in the vector r(x)-x (I should also mention Vincent 2011 Neural Comp. with a similar earlier result for a particular form of denoising auto-encoder where there is a well-defined energy function).  So according to these results,  the reconstruction error ||r(x)-x||^2 would be the magnitude of the score (derivative of energy wrt input). This is quite different from the energy itself, and it would suggest that the reconstruction error would be near zero both at a *minimum* of the energy (near training examples) AND at a *maximum* of the energy (e.g. near peaks that separate valleys of the energy). We have actually observed that empirically in toy problems where one can visualize the score in 2D.\r\n\r\n* Regarding the comparison in section 5.1 with the contractive auto-encoder, I believe that there is a correct but somewhat misleading statement. It says that the contractive penalty costs O(d * d_h) to compute whereas the saturating penalty only costs O(d_h) to compute. This is true, but since computing h in the first place also costs O(d * d_h) the overhead of the contractive penalty is small (it basically doubles the computational cost, which is much less problematic than multiplying it by d as the remark could lead a naive reader to believe)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358468100000, "tcdate": 1358468100000, "number": 39, "id": "yGgjGkkbeFSbt", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "yGgjGkkbeFSbt", "signatures": ["rgoroshin@gmail.com"], "readers": ["everyone"], "content": {"title": "Saturating Auto-Encoder", "decision": "conferencePoster-iclr2013-conference", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "pdf": "https://arxiv.org/abs/1301.3577", "paperhash": "goroshin|saturating_autoencoder", "keywords": [], "conflicts": [], "authors": ["Ross Goroshin", "Yann LeCun"], "authorids": ["rgoroshin@gmail.com", "ylecun@gmail.com"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 9}