{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392734280000, "tcdate": 1392734280000, "number": 5, "id": "OV0_ZIkXpHOXu", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "ssDPnHvkedao6", "replyto": "ssDPnHvkedao6", "signatures": ["Ashutosh Modi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have submitted a new version of the paper. We made the changes we promised above."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392069840000, "tcdate": 1392069840000, "number": 4, "id": "27T0Aaudf37di", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "ssDPnHvkedao6", "replyto": "ssDPnHvkedao6", "signatures": ["Ashutosh Modi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank both reviewers for the comments. See our feedback above. We will upload the revised version by the end of the week."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392063900000, "tcdate": 1392063900000, "number": 1, "id": "UiBqvVVsPEv5t", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "ssDPnHvkedao6", "replyto": "IhlvUSBbaVUbQ", "signatures": ["Ashutosh Modi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "-- Keeping embeddings fixed to the ones produced by SENNA: We have just tried doing this, and obtained about the same results (slightly better: 84.3 average F1 vs. 84.1 F1 in the paper). In theory, keeping them fixed may not be a good idea, as learning in the (mostly) language modeling context (as in SENNA) tends to assign similar representations to antonyms/opposites (e.g., open and close). And the opposites tend to appear at different positions in event sequences. However, the fact the results are similar may suggest that our dataset is not large enough to learn meaningful refinements. Perhaps, using SENNA embeddings to define an informative prior on the representation would be a better idea but we will leave this for future work. We will add a footnote mentioning the above experiment. \r\n\r\n-- F1 vs. accuracy: The binary classification problem is fairly balanced, so we would not expect much of a difference between accuracy (which we essentially optimize) and F1;   we chose to use the same metric in evaluation as considered in the previous work. \r\n\r\n-- We are not familiar with the reordering metric used in Birch et al. (2009), thanks for the pointer."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392063900000, "tcdate": 1392063900000, "number": 1, "id": "CdfIWqqXIkdT-", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "ssDPnHvkedao6", "replyto": "vvxMv-uZQWDNr", "signatures": ["Ashutosh Modi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "-- Why call it 'unlabeled'?: We used unlabeled in the sense that we used texts without any kind of semantic annotation on top of them. But we agree that this is confusing, especially given that the texts were written by Amazon turkers specifically for this task. We will edit the paper accordingly. \r\n-- Representing more complex sentences?: We will clarify this point in the paper. The example 'fill water in coffee maker' contains 2 phrases as arguments ( 'water' and 'in coffee maker'), we use their 'lexical' heads (i.e. 'water' and 'maker') only. The embeddings of these two words and  of the predicate ('fill') are then used as the input to the hidden layer (see Figure 2). The same procedure is used for predicates with more than 2 arguments (just more arguments are used as inputs to the hidden layer). In other words, we use a bag-of-arguments model. \r\n-- Too much related work:  Given that there have been much work on this or related task in NLP, we believe that we should explain how our approach (and the general representation learning framework) is different. We would prefer not to shorten this section."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391787960000, "tcdate": 1391787960000, "number": 3, "id": "IhlvUSBbaVUbQ", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "ssDPnHvkedao6", "replyto": "ssDPnHvkedao6", "signatures": ["anonymous reviewer 60ec"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Semantic Script Knowledge with Event Embeddings", "review": "This paper investigates a model which aims at predicting the order of\r\nevents; each event is an english sentence. While previous methods\r\nrelied on a graph representation to infer the right order, the\r\nproposed model is made of two stages. The first stage use a continuous\r\nrepresentation of a verb frame, where the predicate and its arguments\r\nare represented by their word embeddings. A neural network is used to\r\nderive this continuous representation in order to capture the\r\ncompositionality within the verb frame. The second stage uses a large\r\nmargin extension of PRank. The learning scheme is very interesting:\r\nthe error made by the ranker is used to update the ranker parameters,\r\nbut is also back-propagated to update the NN parameters. \r\n\r\nThis paper is well written and describes a nice idea to solve a\r\ndifficult problem. The experimental setup is convincing (including the\r\ndescription of the task and how the learning resources were built).  I\r\nonly have a few suggestions/questions.\r\n\r\nFor a conference that is focused on representation learning, it could\r\nbe interesting to discuss whether the word embeddings provided by\r\nSENNA need to be updated. For instance, the authors could compare\r\ntheir performances to a system where the initial word embeddings are\r\nfixed. Moreover, the evaluation metric is F1, but how the objective\r\nfunction is related to this metric. Maybe a footnote could say a few\r\nwords about that and I'm curious to see how the objective function\r\nevolves during training. The ranking error function is quite similar\r\nto metrics used in MT for reordering evaluation (see for instance the\r\nwork of Alexandra Birch in 2009)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391516760000, "tcdate": 1391516760000, "number": 2, "id": "vvxMv-uZQWDNr", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "ssDPnHvkedao6", "replyto": "ssDPnHvkedao6", "signatures": ["anonymous reviewer b099"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Semantic Script Knowledge with Event Embeddings", "review": "The authors propose a model that takes a set of events (written as English text) as input, and outputs the temporal ordering of those events. As opposed to a previous DAG based method (also used as a baseline here), in this work words are represented as vectors (initialized with Collobert's SENNA embeddings) and are input into a two layer neural net whose output is also a vector embedding.  The output is then taken as input to an online ranking model (PRank) and the whole thing (including the word vectors) are trained using backprop.  A dataset containing short sequences of events (e.g. the process of making coffee) gathered for previous work using MTurk is used for train and test.  The proposed embedding method shows a substantial improvement over the DAG baseline.\r\n\r\nThis is interesting work, I thought the execution was good, and the results are impressive. I only have a few suggestions/questions: first, why, in the abstract and elsewhere, do you claim to be using unlabeled data?  The data is labeled by order of events (by MTurkers), is it not?  I suspect that you mean that no further labeling was done, but this is confusing.  Second, your model (Fig. 1) shows one predicate (i.e. verb) and two arguments (i.e. nouns), but some of the examples from the ESD data are more complex (e.g. 'fill water in coffee maker').  How are these more complex phrases mapped to your model? Finally, you use a lot of space on previous work; I think that the paper would be improved by adding more details on your method, and shortening the previous work sections (1 and 2.1) by better focusing them.\r\n\r\nA minor issue: at the end of Section 1, some unnecessary extra space has been inserted."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390323900000, "tcdate": 1390323900000, "number": 1, "id": "_9Q-_PbJ6d95N", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "ssDPnHvkedao6", "replyto": "ssDPnHvkedao6", "signatures": ["Ashutosh Modi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Corrected a typo in the paper and updated version is available at : http://arxiv.org/abs/1312.5198v2"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387467900000, "tcdate": 1387467900000, "number": 3, "id": "ssDPnHvkedao6", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "ssDPnHvkedao6", "signatures": ["modiashutosh@gmail.com"], "readers": ["everyone"], "content": {"title": "Learning Semantic Script Knowledge with Event Embeddings", "decision": "submitted, no decision", "abstract": "Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from unlabeled texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.", "pdf": "https://arxiv.org/abs/1312.5198", "paperhash": "modi|learning_semantic_script_knowledge_with_event_embeddings", "keywords": [], "conflicts": [], "authors": ["Ashutosh Modi", "ivan titov"], "authorids": ["modiashutosh@gmail.com", "titov@mmci.uni-saarland.de"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 8}