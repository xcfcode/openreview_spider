{"notes": [{"id": "rJxMM2C5K7", "original": "r1guQco9Fm", "number": 1245, "cdate": 1538087946152, "ddate": null, "tcdate": 1538087946152, "tmdate": 1545355409977, "tddate": null, "forum": "rJxMM2C5K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HygN3VQJx4", "original": null, "number": 1, "cdate": 1544660140087, "ddate": null, "tcdate": 1544660140087, "tmdate": 1545354504146, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Meta_Review", "content": {"metareview": "The reviewers found that the paper needs more compelling empirical study.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1245/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352910718, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1245/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1245/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352910718}}}, {"id": "B1gs7jQ0AQ", "original": null, "number": 6, "cdate": 1543547683204, "ddate": null, "tcdate": 1543547683204, "tmdate": 1543547683204, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "rJeqaFp30m", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "content": {"title": "on the complexity of the algorithm", "comment": "Thanks for the feedback.\n\nI would like to mention that the complexity of the dithered quantization at the workers is similar to the other stochastic quantization methods such as TernGrad and QSGD. Hence, at the worker side, the complexity of the algorithm would be the same.\nHowever, for dequantization, our method requires the server (or aggregation node) to regenerate the random numbers and then taking their average, which is not of much computational complexity. This can be done while the workers are computing their SGs and the server is waiting to receive their data. Hence, we believe that at each iteration of distributed training, using dithered quantization would not increase the complexity or the required time for computing the average of received SGs from workers."}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615144, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxMM2C5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1245/Authors|ICLR.cc/2019/Conference/Paper1245/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615144}}}, {"id": "rJeqaFp30m", "original": null, "number": 5, "cdate": 1543457217828, "ddate": null, "tcdate": 1543457217828, "tmdate": 1543457217828, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "ryxWehzSC7", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "content": {"title": "Not enough for me to change my review. ", "comment": "Thanks for the rebuttal. I still find that the practical impact of this method is not clear.\n\nFor one thing, this method needs low level change in the training framework. And if there is no clear quality or performance gain, it becomes hard to justify the extra complexity."}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1245/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615144, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxMM2C5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1245/Authors|ICLR.cc/2019/Conference/Paper1245/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615144}}}, {"id": "SygVWg7BCX", "original": null, "number": 4, "cdate": 1542955004075, "ddate": null, "tcdate": 1542955004075, "tmdate": 1542955004075, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "BylmK1YB3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "content": {"title": "added simulations", "comment": "Thanks for the comments and suggestions to improve the paper.\n\nWe have added two more figures in the Experiments section to highlight how the dithered quantization scheme may improve the convergence rate of the distributed training. We would like to point out that for the presented simulation results, the convergence speed of our proposed method is better than the existing methods and including the baseline (communication without quantization). We argue that this is mainly due to the fact that our method basically adds a controlled independent noise to the stochastic gradients which may help with the convergence, consistent with the findings in [Neelakantan et al. 2015] and [Noh et al. 2017]."}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615144, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxMM2C5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1245/Authors|ICLR.cc/2019/Conference/Paper1245/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615144}}}, {"id": "BJgVE2fr0X", "original": null, "number": 3, "cdate": 1542954028360, "ddate": null, "tcdate": 1542954028360, "tmdate": 1542954028360, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "r1gMEiLihm", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "content": {"title": "clarifications on the contribution of the work", "comment": "We appreciate the reviewer's comment. However, as there is no direct comments or questions on the paper by the reviewer to justify his/her ratings, we just briefly state some of our contributions in this paper:\n\n1- we have considered using dithered quantization for the communication of SG (or in general, parameters' updates) in a distributed training setting. The advantage of using our proposed qunatization scheme is that unlike all other existing methods where the added noise due to the quantization depends on the values of SG, here the quantization noise is inpendent of the SG values. This ensures that almost all existing convergence results on training with SG or its variants readily applicable to the quantized distributed training algorithm, without much modification. (see e.g., Thm. 4)\n\n2- We have analyzed how the number of workers and quantization precision (or equivalently number of bits) affect the training times (see. Thm 5 and equation 5)\n\n3- We provided a nested scheme to further reduce communication without sacrificing the precision of quantization. For example, theoretically we could achieve the accuracy of two bits quantization with only 1 bit in a distributed setting. (see Thm. 6 and the discussion after)\n\n4- Finally, we provided some simulation results to experimentally verify the algorithm. \n\n\n\nNote that the proofs of all the claims and theoretical results are provided in the appendix."}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615144, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxMM2C5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1245/Authors|ICLR.cc/2019/Conference/Paper1245/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615144}}}, {"id": "ryxWehzSC7", "original": null, "number": 2, "cdate": 1542953961400, "ddate": null, "tcdate": 1542953961400, "tmdate": 1542953961400, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "rJlDh7B0h7", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "content": {"title": "clarification on the simulations and comparisons w.r.t. other methods", "comment": "We appreciate the reviewer's comments and suggestions.\n\n- Regarding the comparison of dithered quantization with One-bit, TernGrad and QSGD:\n\nWe would like to point out that, due to indecency of noise from SGs in our scheme, the proposed distributed training algorithm is expected to behave consistently well irrespective of the database or the neural network.\n\nThe results shown in the paper only reflects the final accuracy of the model after enough iterations of the training algorithm that the models have almost converged. Hence, they merely show the effect of the distributed training on the final accuracy, not how fast the models converge. To address this issue, we have added two figures in the paper showing the accuracy vs iteration for CIFAR10 with 4 and 8 workers, for the first 500 iterations of training. We would like to point out that (a) the convergence speed of our proposed method is better than the existing methods and including the baseline method (communication without quantization). We argue that this is mainly due to the fact that our method basically adds a controlled independent noise to the stochastic gradients which may help with the convergence, consistent with the findings in [Neelakantan et al. 2015] and [Noh et al. 2017]. (b) As the number of workers increases, since the average of received quantized SG is computed and used for training, the quantization noise would be decreased proportionately. Hence, the performance gap between almost all of the quantization methods for the distributed training will vanish eventually.\n\n\n\n- Regarding the comment on the NDQSG:\n\nNote that the main contribution of our work is the dithered quantization and its theoretical analysis in distributed training. However, we mentioned NDQSG to further reduce the communication by exploiting the correlations among SGs computed by the workers. The performance of NDQSG depends on the amount of correlation among SGs computed by the workers. The probability of error in distributed communication using NDQSG is bounded by Thm. 6 and equation 8. We advise on using NDQSG whenever the correlation is significant. As shown in Figues 6(a) and 6(b), when the correlations among the SGs computed by the workers are high, using NDQSG can reduce the communication cost. However, in Fig. 6(c), since the noise in SG computation is high, using NDQSG failes to estimate the true SG sometimes, adding some error into the estimation. This can slow down the convergence speed of the distributed training algorithm in some situations."}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615144, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxMM2C5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1245/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1245/Authors|ICLR.cc/2019/Conference/Paper1245/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers", "ICLR.cc/2019/Conference/Paper1245/Authors", "ICLR.cc/2019/Conference/Paper1245/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615144}}}, {"id": "rJlDh7B0h7", "original": null, "number": 3, "cdate": 1541456815160, "ddate": null, "tcdate": 1541456815160, "tmdate": 1541533299455, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Review", "content": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "review": "In this paper, the authors propose to apply dithered quantization (DQ) to the stochastic gradients computed through the training process. Though an extra noise is added to the gradient, it improves the quantization error. Hence after the noise is removed at the update server, it achieves superior results when compared against unquantized baseline.\n\nThe authors also propose a nested scheme to further reduce communication cost.\n\nThis method strictly improves over previous approaches such as QSGD and TernGrad in terms of quantization error. However, the improved quantization performance does not show up in the experiments. In Table 3, it is clear that DQSG does not significantly improve over QSG and TernGrad once there are 8 workers. And they all use the same amount of bits in communication.\n\nThe proposed NDQSG though capable of reducing the communication cost by 30%, its accuracy on CIFAR-10 shows noticeable drop.\n\nOverall, I think this method is promising, but further tuning is required to make it practical.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Review", "cdate": 1542234272172, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335904834, "tmdate": 1552335904834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gMEiLihm", "original": null, "number": 2, "cdate": 1541266218421, "ddate": null, "tcdate": 1541266218421, "tmdate": 1541533299248, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Review", "content": {"title": "Interesting paper but the contribution is not good enough", "review": "Overall, this paper is well written and clearly present their contribution.\nAlthough the idea seems to be interesting and novel, but not enough evidence to prove the efficiency, from both theoretical and numerical perspective, even though many numerical experiments are proposed.\nIn general, this paper is high level in the articles assigned to me.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Review", "cdate": 1542234272172, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335904834, "tmdate": 1552335904834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BylmK1YB3m", "original": null, "number": 1, "cdate": 1540882299294, "ddate": null, "tcdate": 1540882299294, "tmdate": 1541533299043, "tddate": null, "forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1245/Official_Review", "content": {"title": "Establishes a useful connection between distributed optimization and dithered quantization", "review": "Authors establish a connection between communication reduction in distributed optimization and dithered quantization. This allows us to understand prior approaches in a new perspective, and also motivates authors to develop two new distributed training algorithms which communication overhead is significantly reduced. The first algorithm, DQSG, uses dithered quantization to reduce the communication bits. The second algorithm, NDQSG, uses nested dithered quantization to further reduce the amount of needed communication. The usefulness of these algorithms are empirically validated by computing the raw communication bits and average entropy of them. Therefore, dithered communication seems to provide both theory and algorithm which are useful.\n\nThe paper is clearly written. It provides a succinct review of dithered quantization and previous works, and figures provide a good insight into why the algorithm works, especially Figure 3.\n\nTheorems in this paper are mostly about plugging in properties of dithered quantization into standard results in stochastic optimization, but they are still useful. The analysis of NDQSG does not seem to be as complete as that of DQSG, however. With NQSG, now workers are divided into two groups, and there would be an interesting tradeoff between assignments to these two: how should we balance two groups? This might be tricky to analyze, but it is still useful to clarify limitations and provide conjectures. At least, this could be analyzed empirically.\n\npros:\n* establishing a connection to other topic of research often facilitates productive collaboration between two fields\n* provides a new perspective to understand prior work\n* provides new useful algorithms\n\ncons:\n* experiments were conducted on small models and small datasets\n* unclear models are large enough to demonstrate the need for communication reduction; in other words, it is unclear wall-time would actually be reduced with these algorithms.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1245/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Nested Dithered Quantization for Communication Reduction in Distributed Training", "abstract": "In distributed training, the communication cost due to the transmission of gradients\nor the parameters of the deep model is a major bottleneck in scaling up the number\nof processing nodes. To address this issue, we propose dithered quantization for\nthe transmission of the stochastic gradients and show that training with Dithered\nQuantized Stochastic Gradients (DQSG) is similar to the training with unquantized\nSGs perturbed by an independent bounded uniform noise, in contrast to the other\nquantization methods where the perturbation depends on the gradients and hence,\ncomplicating the convergence analysis. We study the convergence of training\nalgorithms using DQSG and the trade off between the number of quantization\nlevels and the training time. Next, we observe that there is a correlation among the\nSGs computed by workers that can be utilized to further reduce the communication\noverhead without any performance loss. Hence, we develop a simple yet effective\nquantization scheme, nested dithered quantized SG (NDQSG), that can reduce the\ncommunication significantly without requiring the workers communicating extra\ninformation to each other. We prove that although NDQSG requires significantly\nless bits, it can achieve the same quantization variance bound as DQSG. Our\nsimulation results confirm the effectiveness of training using DQSG and NDQSG\nin reducing the communication bits or the convergence time compared to the\nexisting methods without sacrificing the accuracy of the trained model.", "keywords": ["machine learning", "distributed training", "dithered quantization", "nested quantization", "distributed compression"], "authorids": ["abdi@ece.gatech.edu", "fekri@ece.gatech.edu"], "authors": ["Afshin Abdi", "Faramarz Fekri"], "TL;DR": "The paper proposes and analyzes two quantization schemes for communicating Stochastic Gradients in distributed learning which would reduce communication costs compare to the state of the art while maintaining the same accuracy.  ", "pdf": "/pdf/95f2aebc33c07de020be728a725c30834620ef7f.pdf", "paperhash": "abdi|nested_dithered_quantization_for_communication_reduction_in_distributed_training", "_bibtex": "@misc{\nabdi2019nested,\ntitle={Nested Dithered Quantization for Communication Reduction in Distributed Training},\nauthor={Afshin Abdi and Faramarz Fekri},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxMM2C5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1245/Official_Review", "cdate": 1542234272172, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxMM2C5K7", "replyto": "rJxMM2C5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1245/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335904834, "tmdate": 1552335904834, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1245/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}