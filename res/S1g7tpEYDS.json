{"notes": [{"id": "S1g7tpEYDS", "original": "HJg5_hnvwS", "number": 663, "cdate": 1569439098891, "ddate": null, "tcdate": 1569439098891, "tmdate": 1583912043187, "tddate": null, "forum": "S1g7tpEYDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "stdCLb7Z5", "original": null, "number": 1, "cdate": 1576798702680, "ddate": null, "tcdate": 1576798702680, "tmdate": 1576800933319, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of VAEs to use deterministic autoencoders with an explicit regularization term on the latent representations. While the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. The authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in ICLR-2020. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795703943, "tmdate": 1576800251424, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper663/-/Decision"}}}, {"id": "rkxPP-2itH", "original": null, "number": 1, "cdate": 1571696991015, "ddate": null, "tcdate": 1571696991015, "tmdate": 1574268743207, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper propose an extension to deterministic autoencoders. Motivated from VAEs, the authors propose RAEs, which replace the noise injection in the encoders of VAEs with an explicit regularization term on the latent representations. As a result, the model becomes a deterministic autoencoder with a L_2 regularization on the latent representation z. To make the model generalize well, the authors also add a decoder regularization term L_REG. In addition, due to the encoder in RAE is deterministic, the authors propose several ex-post density estimation techniques for generating samples.\n\nThe idea of transferring the variational to deterministic autoencoders is interesting. Also, this paper is well-written and easy to understand. However, in my opinion, this paper needs to consider more cases for autoencoders and needs more rigorous empirical and theoretical study before it can be accepted. Details are as follow:\n\n1. The RAEs are motivated from VAEs, or actually CV-VAEs as in this paper. More precisely, the authors focus on VAEs with a constant covariance Gaussian distribution as the variational distribution and a Gaussian distribution with the identity matrix as the covariance matrix as the model likelihood. However, there might be many other settings for VAEs. For example, the model likelihood can be a Gaussian distribution with non-constant covariance, or even some other distributions (e.g. Multinomial, Bernoulli, etc). Similarly, the variational distribution can be a Gaussian distribution with non-constant covariance, or even some more complicated distributions that do not follow the mean-field assumption. Any of these more complex models may not be easily transferred to the RAE models that are mentioned in this paper. Perhaps it is better if the authors can consider RAEs for some more general VAE settings.\n\n2. Perhaps the authors needs more empirical study, especially on the gain of RAE over CV-VAE and AE. \na) As the motivated model (CV-VAE) and the most related model in the objective (AE), they are not appearing in the structured input experiment (Section 6.2). It will be great if they can be compared with in this experiment.\nb) The authors did not show us clearly whether the performance gain of RAE over VAE, AE and CV-VAE is due to the regularization on z (the term L_z^RAE) or the decoder regularization (the term L_REG) in the experiments. In table 1, the authors only compare the standard RAE with RAE without decoder regularization, but did not compare with RAE without the regularization on z (i.e. equivalent to AE + decoder regularization) and CV-VAE + decoder regularization. The authors would like to show that the explicit regularization on z is better than injecting the noise, hence the decoder regularization term should appear also in the baseline methods. It is totally possible that perhaps AE + decoder regularization or CV-VAE + decoder regularization perform better than RAE. \nc) The authors did not show how they tune the parameter \\sigma for CV-VAE. Since the parameter \\beta in the objective of RAE is tunable, for fair comparison, the authors needs to find the best \\sigma for CV-VAE in order to get the conclusion that explicit regularization is better than CV-VAE.\nd) Although the authors mention that the 3 regularization techniques perform similarly, from Table 1, it is still hard to decide which one should we use in practice in order to get a performance at least not too much worse compared to the baseline methods. RAE-GP and RAE-L2 perform not well on CelebA while RAE-SN perform not well on MNIST, compared to the baseline methods. We know that the best performance over the 3 methods is always comparable to or better than the baselines, but not none of the single methods do. It is better if the authors can provide more suggestions on the choice for decoder regularization for different datasets.\n\n3. The authors provided a theoretical derivation for the objective L_RAE (Equation 11), but this is only for the L_GP regularization. Besides, this derivation (in Appendix B) has multiple technique issues. For example, in the constraints in Equation 12, the authors wrote ||D_\\theta(z1) - D_\\theta(z2)|| < epsilon for all z1, z2 ~ q_\\phi(z | x), this is impossible for CV-VAE since this constraint requires D_theta() to be bounded while q_\\phi(z | x) in CV-VAE has an unbounded domain. Moreover, in the part  (||D_\\theta(z1) - D_\\theta(z2)||_p=\\nabla D_\\theta(\\tilde z)\\cdot ||z_1-z_2||_p) of Equation 13, \\nabla D_\\theta(\\tilde z) is a vector well the other two terms are scalars, which does not make sense. There are many other issues as well. Please go through the proof again and solve these issues.\n\n\nQuestions and additional feedback:\n\n1. Can the authors provide more intuitions why do you think the explicit regularization works better compared to the noise injection? Can you provide a theoretical analysis on that?\n\n2. Can the authors provide some additional experiments as mentioned above? Also, can the authors provide more details about how do they tune the parameters \\beta and \\lambda?\n\n========================================================================================================\n\nAfter the rebuttal:\n\nThanks the authors for the detailed response and the additional experiments. I agree that the additional experiment results help to support the claims from the authors, especially for the CV-VAE for the structured data experiments and the AE + L2 experiment. So I think now the authors have more facts to support that RAE is performing better compared to the baseline methods. \n\nTherefore, I agree that after the revision, the proposed method RAE is supported better empirically. So I am changing my score from \"weak reject\" to \"weak accept\". But I still think the baseline CV-VAE + regularization is important for Table 1 and the technical issues in the theoretical analysis needs to be solved. Hope the authors can edit them in the later version.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper663/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859637571, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper663/Reviewers"], "noninvitees": [], "tcdate": 1570237748885, "tmdate": 1575859637585, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Review"}}}, {"id": "BkgpIOihYS", "original": null, "number": 2, "cdate": 1571760212922, "ddate": null, "tcdate": 1571760212922, "tmdate": 1574248449156, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "The paper analyzes Variational Autoencoders and formulates an alternative perspective on many common problems in the VAE framework like a mismatch between the aggregated posterior and the marginal distribution over latent variables. The authors perform an autopsy of the VAE objective and show how it could be formulated as a constrained optimization problem. Interestingly, the paper proposes to remove all stochasticity from an encoder and a decoder and use different regularizers instead. In order to be able to sample from the model, the marginal distribution over latents is trained after the encoder and decoder are learned.  In general, I find the paper very interesting and important for the VAE community.\n\n- (An open question) The marginal distribution p(z) allows to turn the AE into a generative model. However, it also serves as a code model in a compression framework (i.e., it is used in the adaptive entropy coding). It would be interesting to see how the proposed approach would compare to other compression methods.\n\n- It would be interesting to see how the post-training of p(z) performs. Maybe it would be a good idea to run some toyish problem (e.g., dim(z)=2 on MNIST) and see how the GMM fits samples from the aggregated posterior.\n\n- Did the authors try a mix of different regularizers they proposed to use? For instance, L2 + SN?\n \n- Could the authors comment on the choice of hyperparameters (weights in the objective)?\n\n======== AFTER REBUTTAL ========\nI would like to thank the authors for answering my questions. I really appreciate all effort to provide more empirical evidence. I still believe that this paper is extremely important for the AE/VAE community and it sheds some new light on representation learning. In my opinion the paper should be accepted. Therefore, I decided to rise my score from \"Weak Accept\" to \"Accept\".", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper663/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859637571, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper663/Reviewers"], "noninvitees": [], "tcdate": 1570237748885, "tmdate": 1575859637585, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Review"}}}, {"id": "rJgNoi5nsS", "original": null, "number": 12, "cdate": 1573854107700, "ddate": null, "tcdate": 1573854107700, "tmdate": 1573854107700, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "New revision: more experiments", "comment": "We have uploaded a new revision of the paper including the following experiments and visualizations:\n\n+ all PRD curves for all models on all image datasets, showing the positive impact of performing XPDE (Appendix E)\n\n+ all possible combinations of the regularizers applied to RAEs (Appendix I)"}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "r1egWs9hjB", "original": null, "number": 11, "cdate": 1573853943710, "ddate": null, "tcdate": 1573853943710, "tmdate": 1573853943710, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "BkgpIOihYS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "All combinations of regularization schems ", "comment": "We added results for all possible combinations of the regularization schemes proposed on our RAEs in Appendix I.\n\nThe   improvement in reconstruction, random sample quality and   interpolated test samples is generally comparable, but hardly much better.  This can be explained with the fact that the additional regularization losses make tuning their hyperparameters more difficult, in practice.\n\nAs stated in the paper, we would recommend practitioners to use RAE+L2 for simplicity."}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "S1gILqc3oB", "original": null, "number": 10, "cdate": 1573853774068, "ddate": null, "tcdate": 1573853774068, "tmdate": 1573853774068, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "ByxI2wx9or", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "More PRD curves", "comment": "We uploaded all the PRD curves for all the models on all the image data experiments.\nThe positive effect of performing XPDE is clearly visible from these plots."}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "BkgjBne5sr", "original": null, "number": 9, "cdate": 1573682243278, "ddate": null, "tcdate": 1573682243278, "tmdate": 1573825113205, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "New revision uploaded: more experiments and plots", "comment": "We thank again all the reviewers for the valuable feedback.\n\nWe have uploaded a new revision of the paper including the following changes:\n\n+ we investigated the PRD values for the WAE on CelebA and provided a better insight of the phenomenon by visualizing the whole PRD curves (see Appendix E)\n\n+ we built more intuition on XPDE and the aggregated posterior mismatch problem by visualizing the learned latent space and the fit by different estimators on 2D MNIST and the models employed in the experiments via t-sne projections (see Appendix H)\n\n+ we have run the AE+L2 model for all image datasets and XPDE variants, reporting FIDs for reconstructions, random samples and interpolation (see Table 1)\n\n+ we have extended the structured object experiments to the CVVAE, generating and scoring equations and molecules in Figure 2  \n\nPlease refer to our individual comments for more details and do not hesitate to ask for clarifications and more experiments!"}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "SJltgjgcsS", "original": null, "number": 8, "cdate": 1573681904690, "ddate": null, "tcdate": 1573681904690, "tmdate": 1573683563189, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "rkxPP-2itH", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "Additional experiments and evidence to support our claims", "comment": "Q2a) As requested, we run the CV-VAE variants for the structured data experiments, which we named GCVVAE. Please see the new revision we uploaded, we reported results in Figure 2 in our experimental section. As one can see from there,  making the variance constant as is the case with CVVAEs, provides a boost in the expression and molecule datasets when compared to the GVAE. While no significant difference is perceivable w.r.t. a GRAE for expressions, on the more challenging molecules datasets, RAE still deliver better average scores and more significant molecules (while the number of valid molecules is slightly better for the CVVAE, even though not significantly, given the std).\n\nQ2b) Furthermore, we run as requested the AE model plus a regularization term, on the image data. We employed the L_L2 regularization for all datasets and with all the different XPDE estimates. We reported results in Table 1 in our original experimental setting. It is visible from there that by not constraining the Zs during optimization results in a latent space that is highly non-isotropic-Gaussian. Fortunately, XPDE with GMMs helps deliver better samples by fixing this mismatch. Interpolations are only very slightly worse, but still comparable with other models. As expected, this is due that the latent codes to be allowed to \"strech\" as much as they want. As in our claims, regularization on the decoder helps the interpolations to still deliver very reasonable samples.\n\nWe will report additional combinations of AE+REG in a new revision of the paper.\n\nWe welcome any additional suggestion from the reviewer to strengthen our claims empirically.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "ByxI2wx9or", "original": null, "number": 6, "cdate": 1573681069947, "ddate": null, "tcdate": 1573681069947, "tmdate": 1573683441556, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "r1lYVM27cr", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "Update on PRD scores and more experiments", "comment": "Q1+Q3) We unraveled the WAE's FID scores \"mystery\". We reported values in Table 2 in Appendix E that are all subject to the same beta value for computing PRD [cf. Sajjadi M. et al. 2018]. These values flatten the understanding of the trade-off between precision and recall, which is otherwise perceivable by looking at their curves. \nTherefore, we have augmented Appendix E to now include the PRD curves for these models and added a discussion in Figure 4. \nIndeed, from Figure 4 it is visible how WAE+GMM achieves better recall, but much less precision overall than a RAE_SN+GMM. This trade-off is also not visible by the single FID score, where the WAE+GMM achieves something slightly less than RAEs+GMMs, and is also distorted while fixing a single beta, as we report in the Table.\n\nWe will report all PRD curves for all models and datasets in the final revision of the paper, as well as extensively commenting on this caveat of the evaluation. We thank the reviewer for pointing this out.\n\nFurthermore, we have expanded our experimental setting as requested by other reviewers, please see our general comment on the updated revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "HJxWtux5oS", "original": null, "number": 7, "cdate": 1573681273056, "ddate": null, "tcdate": 1573681273056, "tmdate": 1573681273056, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "BkgpIOihYS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "Additional experiments and visualizations to provide insight", "comment": "Q2) We provided the suggested visualization in Appendix H. There, we visualize how XPDE affects the estimation of the latent space on MNIST when employing either 2 dimensions or 16 as in the paper (for which we employ t-sne to project to a 2D plane). \n\nIn both cases we plot the original test set samples against the random samples generated by the model when equipped with an isotropic Gaussian, or a Gaussian whose mean and covariance are estimated from data, or by employing a 10-component GMM.\nThe positive effect of XPDE is clearly visualized: a better estimator over the latent space delivers samples that better match the original density. This is ultimately translated into better samples.\n\nQ3) We are still in the process of running additional experiments for combinations of regularizers. We will report them in the next revision of the paper.\n\nPlease see our general comments on the updated revision. We welcome further suggestions to improve presentation and experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "HygPryzMjB", "original": null, "number": 5, "cdate": 1573162815463, "ddate": null, "tcdate": 1573162815463, "tmdate": 1573162815463, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "rkxPP-2itH", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "A valuable stepping stone towards alternative generative modeling", "comment": "Q1) <RAE compared against all VAE settings> We agree that having a deterministic framework that is able to extend and perform better than *any possible VAE variant* would be highly relevant and convenient. However, we believe that even if our paper does not achieve this ambitious goal, it has great value. Indeed, we introduce RAEs for a subset of VAEs parametrizations, we demonstrate their practical effectiveness as replacements for generative modeling and pave the way to future interesting extensions.\n\nWe start our analogy on the Gaussian VAE with isotropic prior and constant variance. Therefore, the natural competitors of our RAE framework are the Gaussian VAEs. As argued in Section 2, this is arguably still the most commonly employed VAE for generative modeling (partly because more complex variants are provably tougher to train, cf. our discussion in Section 2).\nFurthermore, please note that i) we extend our empirical comparison also to WAEs, 2-stage VAEs and GVAEs and ii) some trivial change in the distributional assumptions over the VAE decoder can be easily ported in the RAE framework, e.g., a Bernoulli distribution would require to change the MSE to a CE loss.\n\n\nQ2) <More experiments> We agree that more experiments can be helpful in strengthening the outcomes of our \"autopsy\". Unfortunately, the possible space of all experimental setting combinations is exponential. We believe that our experimental design is adequately devised to highlight the effectiveness of our framework, and most importantly in a rigorous way. We will explain how and why in the following points. Nevertheless, we remark that we are open to run additional experiments during the rebuttal.\n\na) <CV-VAE for SOP> Please note that the structure output prediction (SOP) experiments are circa one order of magnitude longer to run, given the repetitions of the costly BO. This is why we selected a single RAE scheme, and we selected the fastest one with L_L2 reg. We are in the process of running one additional case involving the CV-VAE as requested.\n\nb) <Effect of L_Z> In our experimental setting one can quantify the impact of introducing the L_Z term by looking at experiments involving AEs (not adding L_Z in their objective) and vanilla RAEs (including only L_Z). AEs achieve comparable, or only slightly less, random sample FIDs. Time permitting, we will run an additional experiment with only AE + explicit regularization.  We remark that the outcome of these additional experiments are not invalidating our claim: implicit or explicit decoder regularization on a deterministic autoencoder deliver a generative model competitive with VAEs and variants.\n\nc) <tuning hyperparameter>  We perform the same grid search by binary search over some reasonable interval for all hyperparameters involved and spent a reasonably equal amount of time for all models involved in our comparison. See also our answer to Reviewer#1.\n\nd) <which RAE?> Indeed, there is no clear winner, overall. As stated in the paper, we recommend RAE-L2 for its simplicity. We adopted it for the more involved SOP experiments, delivering far better results than GVAEs. Furthermore, we would like to highlight that use of different types of regularization emphasizes the point that as predicted by the theory a regularization is necessary and any implicit or explicit one works well in practice.\n\nQ3) <Derivation only for L_GP> We agree with the reviewer that providing a general theoretical derivation for every possible regularization schemes would be very sensible. As it is highly non-trivial and impactful, it could potentially be worth a best paper award. However, we believe this perspective should not diminish the value of our contribution. We point out that, while L_GP and L_SN are introduced more as heuristics from the GAN literature, here we provide a sound theoretical derivation for L_GP in the autoencoder framework. Furthermore, the L_L2 regularization is theoretically backed by the Tikhonov regularization framework for general function approximators.\n\nQ4) <unbounded domain> We appreciate and thank that the reviewer for providing such a thorough check of our proof. As we mention after eq.13, we are interested in q_{\\phi}(z | x) with bounded support, e.g., a uniform distribution over a hypersphere in z. We will specify this before eq. 13, to make the assumption more clear as suggested.\n\n<dimension mismatch> We thank the reviewer for having spotted this typo! Indeed, we are missing the p-norm over the (vector) gradient, which appears on the right side of same equation. This does not hinder in any way our proof (the right side of the equation is correct as the remaining part). As suggested, we will thoroughly proofread the derivation and update the manuscript accordingly.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "B1xzMA-fir", "original": null, "number": 4, "cdate": 1573162506002, "ddate": null, "tcdate": 1573162506002, "tmdate": 1573162506002, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "BkgpIOihYS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "Interesting experimental suggestions", "comment": "Q1) <relation to compression> We thank the reviewer for this very interesting perspective. We are not experts in adaptive entropy coding, but we will try to assess the meaningfulness of the learned codes by some down-stream task metric. For instance, one could rank the Z codes learned without supervision by RAEs and VAEs on MNIST by the accuracy scored by a logistic regressor.\n    We welcome further suggestions and pointers to other similar settings to augment our experimental design.\n\nQ2) <visual intuition on XPDE> This is a valuable suggestion and we are in the process of running the suggested 2D visual comparison. We will update the paper accordingly. Moreover, we can extend this visual inspection to the higher dimensional cases by showing a 2D-projection, e.g., via t-sne, provided they are meaningful.\n\nQ3) <mixing regularizers>  As recent works in the GAN literature have shown, combining different regularizers might lead to better results (e.g., cf. [1], [2]).  We therefore agree that this is a valuable direction. In our experimental design we strove for rigor and, most importantly, for simplicity: our aim is to prove that even a single explicit regularization scheme (or no explicit like for AE) can deliver a smooth latent space and high quality samples. Furthermore, combining more penalties would make training a RAE less practical: more hyperparameters to be tuned and longer training time.\n    Nevertheless, we will add the suggested experiments on some regularizer combinations to the manuscript as soon as we have the results.\n\nQ4) <choice of hyperparameters> We optimized all hyperparameters for each model separately on each dataset, by optimizing the random sample FID on a held-out set. To this aim, we executed a grid search by performing a binary search starting from some common intervals. We will provide all details to reproduce this search in the Appendix. Note that we reserved the same effort to tune the hyperparameters of all models, achieving better FIDs for our competitors w.r.t. other papers (e.g. our VAEs generate better samples than reported in the original WAE paper).\n\n[1] - Lucic et al, \"Are GANs Created Equal? A Large-Scale Study\", NeurIPS 2018\n[2] - Kurach et al. \"A Large-Scale Study on Regularization and Normalization in GANs\", ICML 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "r1g1rpZziB", "original": null, "number": 3, "cdate": 1573162295182, "ddate": null, "tcdate": 1573162295182, "tmdate": 1573162295182, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "r1lYVM27cr", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "Remarks on our empirical evaluation", "comment": "Q1) <meaningfulness of interpolation> We agree with the reviewer that interpolation experiments serve the purpose to prove that the learned latent space is smooth and contains no \"holes\". Note that RAEs do not define an explicit probability distribution p(X), therefore assessing the distance between the true data distribution can only be done through statistical tests on samples (e.g. FID, PRD).\n\nQ2) <FID golden standard> We agree with the reviewer that the evaluation of generative models is a nontrivial problem. Among many recent alternatives, FID is still the most popular criterion adopted to assess the distance between two distribution via their samples. We have reported PRD in the appendix but we are open to run any other metric the reviewer suggests.\n    <XPDE boosts FID> We agree that eX-Post Density Estimation (XPDE) boosts FID scores. This is done by design, since XPDE is introduced to fix the posterior mismatch. Note that we also apply XPDE to all VAE variants (see Table 1 under the GMM column) to demonstrate that it effectively reduces the gap between the prior and the aggregate posterior. This per se might be a highly relevant contribution for the generative modeling community and its simplicity might have let it pass unnoticed so far.\n\nQ3> <better PRD for WAEs> We are in the process or recomputing the PRD scores on WAE to assess if it is a typo or it depends on the sensitivity of PRD w.r.t. the clustering we adopted (in which case we will discuss this sensitivity issue in the paper).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "rJxoPh-fsS", "original": null, "number": 2, "cdate": 1573162082798, "ddate": null, "tcdate": 1573162082798, "tmdate": 1573162082798, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment", "content": {"title": "General comments to all reviews", "comment": "We thank all the reviewers for appreciating the direction of our work and its relevance for the generative modeling community. \nWe agree that the evaluation of generative models is a nontrivial problem and a very active field.\nWe appreciate the constructive criticisms and would gladly accommodate additional experiments that would improve the paper's quality. To this end we would like to engage with every question individually and distill the core addendum to the paper.\nHowever before that, we would like to emphasize that the scope of the paper is not to introduce a model that always supersedes all possible VAE variants and extensions. Instead, we aim to provide a simpler alternative to the most popular variants, that yields a as-good-as or even better generative mechanism. Most importantly, by doing so we intend to shed light over some common unquestioned assumptions and open a novel perspective in the current research landscape of generative modeling."}, "signatures": ["ICLR.cc/2020/Conference/Paper663/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1g7tpEYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper663/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper663/Authors|ICLR.cc/2020/Conference/Paper663/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168118, "tmdate": 1576860551326, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper663/Authors", "ICLR.cc/2020/Conference/Paper663/Reviewers", "ICLR.cc/2020/Conference/Paper663/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Comment"}}}, {"id": "r1lYVM27cr", "original": null, "number": 3, "cdate": 1572221488581, "ddate": null, "tcdate": 1572221488581, "tmdate": 1572972567481, "tddate": null, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "invitation": "ICLR.cc/2020/Conference/Paper663/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies (the more conventional) deterministic auto-encoders, as they are easier to train than VAE. To then try to maintain the model's capability of approximating the data distribution and to draw/synthesize new unseen samples, the paper both looks at imposing additional regularization terms towards a smooth decoder and proposes to sample from a latent distribution that's induced from empirical embeddings (similar to an aggregate posterior in VAE). Experiments are mostly around contrasting VAEs with the proposed RAEs in terms of comparing the quality of the generated samples.\n\nThe paper is trying to answer an important and meaningful question, i.e. can a deterministic auto-encoder learn a meaningful latent space and approximates the data distribution just as well as the much more complicated VAEs. And they've made some interesting and reasonable trials. The methods developed in the paper are mostly easy to understand and also well motivated in general.\nHowever my main concern is on the empirical studies, as they don't seem particularly convincing to justify the claimed success of the RAE over VAE. That said, the evaluation of generative models is by itself still an active research topic in development, which certainly adds to the difficulty of coming up with a sensible and rigorous comparative study in this regard.\n\nDetailed comments and questions.\n1. On \"interpolation\": it might provide some insights on how well the decoder can \"generalize\" to unseen inputs (which is still important), but otherwise seems to provide very little insight on how well the actual \"distribution\" is being approximated, as it's much more about \"coverage\" of the support of the distribution than the actual probabilities themselves?\n2. I'm not so sure FID can be regarded as the golden standard when it comes to comparing data generation qualities, and especially for the given setting, as it might give all those methods employing the so-called \"ex-post density estimation\" an edge over VAEs due to the scraping of the gap between the aggregate posterior and the prior (by design).\n3. From Table 2, according to the \"Precision/Recall\" criteria, WAE clearly out-performs RAE on the CelebA dataset, contradicting the result with FID in Table 1. I think this might need a closer look (as to why this happened) and certainly should be worth some discussions in the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper663/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper663/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["partha.ghosh@tuebingen.mpg.de", "msajjadi@tue.mpg.de", "antonio.vergari@tuebingen.mpg.de", "black@tue.mpg.de", "bs@tue.mpg.de"], "title": "From Variational to Deterministic Autoencoders", "authors": ["Partha Ghosh", "Mehdi S. M. Sajjadi", "Antonio Vergari", "Michael Black", "Bernhard Scholkopf"], "pdf": "/pdf/4855d3d40c16853fcf3844059592f0c5783a60fe.pdf", "TL;DR": "Deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior (i.e., Gaussian).", "abstract": " Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. ", "keywords": ["Unsupervised learning", "Generative Models", "Variational Autoencoders", "Regularization"], "paperhash": "ghosh|from_variational_to_deterministic_autoencoders", "code": "https://github.com/ParthaEth/Regularized_autoencoders-RAE-", "_bibtex": "@inproceedings{\nGhosh2020From,\ntitle={From Variational to Deterministic Autoencoders},\nauthor={Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael Black and Bernhard Scholkopf},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1g7tpEYDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e347fb88b9b7a25ba052d329ecd0971365223b05.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1g7tpEYDS", "replyto": "S1g7tpEYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper663/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859637571, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper663/Reviewers"], "noninvitees": [], "tcdate": 1570237748885, "tmdate": 1575859637585, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper663/-/Official_Review"}}}], "count": 16}