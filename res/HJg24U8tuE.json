{"notes": [{"id": "HJg24U8tuE", "original": "Hyggix1wuV", "number": 34, "cdate": 1553716787551, "ddate": null, "tcdate": 1553716787551, "tmdate": 1562083041353, "tddate": null, "forum": "HJg24U8tuE", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Smoothing Nonlinear Variational Objectives with Sequential Monte Carlo", "authors": ["Antonio Moretti", "Zizhao Wang", "Luhuan Wu", "Itsik Pe'er"], "authorids": ["amoretti@cs.columbia.edu", "zizhao.wang@columbia.edu", "lw2827@columbia.edu", "itsik@cs.columbia.edu"], "keywords": ["sequential monte carlo", "variational inference", "time series"], "abstract": "The task of recovering nonlinear dynamics and latent structure from a population recording is a challenging problem in statistical neuroscience motivating the development of novel techniques in time series analysis. Recent work has focused on connections between Variational Inference and Sequential Monte Carlo for performing inference and parameter estimation on sequential data. Inspired by this work, we present a framework to develop Smoothed Variational Objectives (SVOs) that condition proposal distributions on the full time-ordered sequence of observations. SVO maintains both expressiveness and tractability by sharing parameters of the transition function between the proposal and target. We apply the method to several dimensionality reduction/expansion tasks and examine the dynamics learned with a quantitative metric. SVO performs favorably against the state of the art.", "pdf": "/pdf/399951e6fb39c0bada616f02c21091abbbe91738.pdf", "paperhash": "moretti|smoothing_nonlinear_variational_objectives_with_sequential_monte_carlo"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "rylXFG749N", "original": null, "number": 2, "cdate": 1555473019337, "ddate": null, "tcdate": 1555473019337, "tmdate": 1556906115969, "tddate": null, "forum": "HJg24U8tuE", "replyto": "HJg24U8tuE", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper34/Official_Review", "content": {"title": "Review", "review": "This paper describes a framework called Smoothed Variational Objectives (SVOs) for performing inference and parameter estimation in nonlinear dynamical systems. The proposed method is evaluated on three benchmarks (Fitzhugh-Nagumo, Lorenz Attractor, and electrophysiology data) in terms of R^2_k, and shows favorable results compared to previous algorithms. \n\nOverall, I think this paper is well-written and well-structured. It provides enough background on variational inference and Sequential Monte Carlo methods and is more or less self-contained. Unfortunately, I am not an expert on this topic and won\u2019t be able to provide more insightful opinions. \n\nOther questions/comments:\nIs the \\delta term in Eq defined anywhere?\n", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing Nonlinear Variational Objectives with Sequential Monte Carlo", "authors": ["Antonio Moretti", "Zizhao Wang", "Luhuan Wu", "Itsik Pe'er"], "authorids": ["amoretti@cs.columbia.edu", "zizhao.wang@columbia.edu", "lw2827@columbia.edu", "itsik@cs.columbia.edu"], "keywords": ["sequential monte carlo", "variational inference", "time series"], "abstract": "The task of recovering nonlinear dynamics and latent structure from a population recording is a challenging problem in statistical neuroscience motivating the development of novel techniques in time series analysis. Recent work has focused on connections between Variational Inference and Sequential Monte Carlo for performing inference and parameter estimation on sequential data. Inspired by this work, we present a framework to develop Smoothed Variational Objectives (SVOs) that condition proposal distributions on the full time-ordered sequence of observations. SVO maintains both expressiveness and tractability by sharing parameters of the transition function between the proposal and target. We apply the method to several dimensionality reduction/expansion tasks and examine the dynamics learned with a quantitative metric. SVO performs favorably against the state of the art.", "pdf": "/pdf/399951e6fb39c0bada616f02c21091abbbe91738.pdf", "paperhash": "moretti|smoothing_nonlinear_variational_objectives_with_sequential_monte_carlo"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper34/Official_Review", "cdate": 1554234173757, "reply": {"forum": "HJg24U8tuE", "replyto": "HJg24U8tuE", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234173757, "tmdate": 1556906090609, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "S1eEnWqJqV", "original": null, "number": 1, "cdate": 1555173803797, "ddate": null, "tcdate": 1555173803797, "tmdate": 1556906115756, "tddate": null, "forum": "HJg24U8tuE", "replyto": "HJg24U8tuE", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper34/Official_Review", "content": {"title": "Not new, not smoothing, still an interesting question", "review": "This paper proposes two methods to extend recent work in filtering SMC-based variational objectives to the smoothing case. The first approach is a Monte Carlo objective (MCO) based on Forward Filtering Backward Smoothing (FFBS), and the second technique gives the SMC proposal distribution access to all observations. The authors evaluate both techniques experimentally and find that the FFBS-based technique does not perform well, while giving the proposal access to all observations improves performance on several tasks. \n\nWhile the paper is generally written well and easy to follow, the main technique (giving the proposal access to all observations) has been presented previously in the literature and is misrepresented as a smoothing algorithm when it is not.\n\nTo expand on these points:\n\n1. Filtering Variational Objectives (Maddison et al. 2017) section 6.4 presents experiments that run FIVO with a proposal that conditions on the state of a bidirectional RNN run over the observations. They find that it does not reliably help on their tasks.\n2. Changing the information that the proposal distribution has access to does not change SMC\u2019s sequence of target distributions. If only the proposal is changed and the form of the weights is not changed, then the algorithm is still based on filtering SMC and is not smoothing. Unfortunately, it is not entirely clear what SMC scheme the authors use with the new proposal. It seems that they use the future-conditioned proposal with the weights defined in equation (12), but if this is not the case it should be clarified. \n\nFurther feedback:\n\n1. As discussed in Maddison et al. 2017, an MCO based on filtering SMC cannot become tight, even when q is set to the true smoothing distribution. Because of this, it is useful to compare the performance of the proposed algorithm to the IWAE bound (with the same proposal) which can become tight and allow the proposal to make full use of the information available to it. The authors should consider incorporating this comparison in their experiments.\n2. On page 4 the authors state \u201cAs with the IWAE, increasing K yields a tighter bound L_SMC defined below\u201d. I am not aware of a proof that L_SMC is provably tighter as K increases. The authors should provide a proof or citation or remove the statement.\n3. There is prior work on developing variational objectives based on smoothing SMC, including\u2028\u2028\n\nGraphical model inference: Sequential Monte Carlo meets deterministic approximations, Lindsten et al 2018\u2028\nTwisted Variational Sequential Monte Carlo, Lawson et al. 2018\u2028\u2028\n\nThe authors should consider incorporating this in their related works.\n\nOverall, it is still interesting to consider why an MCO based on filtering SMC would perform better when the proposal is given access to all observations. If the authors change their paper to address the points above, I will consider changing my score.\n", "rating": "2: Marginally below acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing Nonlinear Variational Objectives with Sequential Monte Carlo", "authors": ["Antonio Moretti", "Zizhao Wang", "Luhuan Wu", "Itsik Pe'er"], "authorids": ["amoretti@cs.columbia.edu", "zizhao.wang@columbia.edu", "lw2827@columbia.edu", "itsik@cs.columbia.edu"], "keywords": ["sequential monte carlo", "variational inference", "time series"], "abstract": "The task of recovering nonlinear dynamics and latent structure from a population recording is a challenging problem in statistical neuroscience motivating the development of novel techniques in time series analysis. Recent work has focused on connections between Variational Inference and Sequential Monte Carlo for performing inference and parameter estimation on sequential data. Inspired by this work, we present a framework to develop Smoothed Variational Objectives (SVOs) that condition proposal distributions on the full time-ordered sequence of observations. SVO maintains both expressiveness and tractability by sharing parameters of the transition function between the proposal and target. We apply the method to several dimensionality reduction/expansion tasks and examine the dynamics learned with a quantitative metric. SVO performs favorably against the state of the art.", "pdf": "/pdf/399951e6fb39c0bada616f02c21091abbbe91738.pdf", "paperhash": "moretti|smoothing_nonlinear_variational_objectives_with_sequential_monte_carlo"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper34/Official_Review", "cdate": 1554234173757, "reply": {"forum": "HJg24U8tuE", "replyto": "HJg24U8tuE", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234173757, "tmdate": 1556906090609, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper34/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "rkxsbN_wqN", "original": null, "number": 1, "cdate": 1555690499459, "ddate": null, "tcdate": 1555690499459, "tmdate": 1556906115498, "tddate": null, "forum": "HJg24U8tuE", "replyto": "HJg24U8tuE", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper34/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept", "comment": "This paper  studies sequential MC for training deep generative models. The paper is well-written but the authors should connect the work to existing works as mentioned by reviewer 1"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Smoothing Nonlinear Variational Objectives with Sequential Monte Carlo", "authors": ["Antonio Moretti", "Zizhao Wang", "Luhuan Wu", "Itsik Pe'er"], "authorids": ["amoretti@cs.columbia.edu", "zizhao.wang@columbia.edu", "lw2827@columbia.edu", "itsik@cs.columbia.edu"], "keywords": ["sequential monte carlo", "variational inference", "time series"], "abstract": "The task of recovering nonlinear dynamics and latent structure from a population recording is a challenging problem in statistical neuroscience motivating the development of novel techniques in time series analysis. Recent work has focused on connections between Variational Inference and Sequential Monte Carlo for performing inference and parameter estimation on sequential data. Inspired by this work, we present a framework to develop Smoothed Variational Objectives (SVOs) that condition proposal distributions on the full time-ordered sequence of observations. SVO maintains both expressiveness and tractability by sharing parameters of the transition function between the proposal and target. We apply the method to several dimensionality reduction/expansion tasks and examine the dynamics learned with a quantitative metric. SVO performs favorably against the state of the art.", "pdf": "/pdf/399951e6fb39c0bada616f02c21091abbbe91738.pdf", "paperhash": "moretti|smoothing_nonlinear_variational_objectives_with_sequential_monte_carlo"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper34/Decision", "cdate": 1554814604723, "reply": {"forum": "HJg24U8tuE", "replyto": "HJg24U8tuE", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814604723, "tmdate": 1556906100529, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}