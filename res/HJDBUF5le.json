{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488368036665, "tcdate": 1478297152035, "number": 495, "id": "HJDBUF5le", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJDBUF5le", "signatures": ["~Harrison_Edwards1"], "readers": ["everyone"], "content": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396618256, "tcdate": 1486396618256, "number": 1, "id": "rJGC3MIOx", "invitation": "ICLR.cc/2017/conference/-/paper495/acceptance", "forum": "HJDBUF5le", "replyto": "HJDBUF5le", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This is an interesting paper that adds nicely to the literature on VAEs and one-shot generalisation. This will be of interest to the community and will contribute positively to the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396618780, "id": "ICLR.cc/2017/conference/-/paper495/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJDBUF5le", "replyto": "HJDBUF5le", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396618780}}}, {"tddate": null, "tmdate": 1484702416502, "tcdate": 1484702416502, "number": 5, "id": "rydAGB3Il", "invitation": "ICLR.cc/2017/conference/-/paper495/public/comment", "forum": "HJDBUF5le", "replyto": "Sy8k9FbNg", "signatures": ["~Harrison_Edwards1"], "readers": ["everyone"], "writers": ["~Harrison_Edwards1"], "content": {"title": "Reply to AnonReviewer3", "comment": "Thanks for your review and useful comments.\n\n\"- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior.\"\n\nPlease see our reply to AnonReviewer1. We think the point about statistician is fair, but that statistic network is reasonable. The statistic network is an inference network, the output of which is a parameterization of an approximate posterior over the context latent variable. The use of the word statistic is to remind the reader that the input to this kind of inference network is a dataset and that it's output is invariant to the order of its inputs.\n\n\"(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior?\" \nWhen generating samples we first take the dataset, pass it through the statistic network to get an approximate posterior over c. We then set c to be the mean of this posterior, and sample from p(x | c). This means that the datapoint encoders q(z | x,c) are not used (which is I think what you are asking). This process is detailed in pseudo-code in the appendix (algorithm 2) and referred to at the beginning of section 5 (Experimental results). Based on your feedback we have made it clearer in the first paragraph of section 5 that this is how we obtain samples in the experiments.\n\n\"(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities.\"\n\nOur quantitative evidence for our approach consists in our few-shot classification results, complemented by qualitative investigations of the representations learned. Because of some of the choices we have made with regard to data augmentation, it is difficult to make fair comparison in terms of likelihoods. We take your point that it would have been better to have this comparison, and will make this a priority in future investigations of this area."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287553296, "id": "ICLR.cc/2017/conference/-/paper495/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJDBUF5le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper495/reviewers", "ICLR.cc/2017/conference/paper495/areachairs"], "cdate": 1485287553296}}}, {"tddate": null, "tmdate": 1484635384004, "tcdate": 1484635384004, "number": 1, "id": "Hyxba4o8g", "invitation": "ICLR.cc/2017/conference/-/paper495/official/comment", "forum": "HJDBUF5le", "replyto": "HJLoyZdIe", "signatures": ["ICLR.cc/2017/conference/paper495/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper495/AnonReviewer4"], "content": {"title": "response", "comment": "\"As I understand your next question, you are asking about whether this would pose a problem if your datasets are characterized by rare examples in the datasets. I think in this kind of case the statistic network will struggle to learn useful representations if only presented with small numbers of examples. The problem is that it will become computationally infeasible to pass large datasets through the network for every parameter update. One direction towards addressing these kinds of problems is to use small minibatches but to have some persistent state between them. For instance the statistic network could use a running mean that is updated everytime we pass a minibatch from the same dataset. This is similar to truncated backprogagation through time that is used to handle long sequences in recurrent networks.\"\n\nI don't see how having persistent states addresses this issue.  As I see it, the reason why your estimator is biased is because you're optimizing a term including a log of a sum of non-negative values (over a function of examples in the dataset, in the loss L_D in equations 9 and 10).  \n\nIf your q(c|D) needs to be determined by a single example in the dataset, then I think you'll have an issue where minibatches where it hasn't happened to appear yet will have incorrect gradients.  This will be the case even if you have a persistent state between minibatches.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287553152, "id": "ICLR.cc/2017/conference/-/paper495/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJDBUF5le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper495/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper495/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper495/reviewers", "ICLR.cc/2017/conference/paper495/areachairs"], "cdate": 1485287553152}}}, {"tddate": null, "tmdate": 1484436778778, "tcdate": 1484436778778, "number": 4, "id": "S1QVSVuUx", "invitation": "ICLR.cc/2017/conference/-/paper495/public/comment", "forum": "HJDBUF5le", "replyto": "rkfZJCVVl", "signatures": ["~Harrison_Edwards1"], "readers": ["everyone"], "writers": ["~Harrison_Edwards1"], "content": {"title": "Reply to AnonReviewer1", "comment": "Thank you for your review! \n\nFirstly regarding your comments on 'neural statistician'. We accept your criticism (shared by AnonReviewer3) that 'statistician' is overbroad. We do defend the use of the term statistic network, as this appropriately captures the fact that the inferential network is exchangeable across the elements in the set - i.e. it does not depend on the order that elements are presented. However extending that to the title \"towards a neural statistician was perhaps overenthusiastic, though done with no desire to mislead or slight real statisticians! On the other hand since the paper has been on arxiv for some time now, and is referred to by name by several other submissions to ICLR, we feel it would be a bit awkward at this point to change the title. I have added a line to the abstract that I hope clarifies the scope of what we mean by neural statistician.\n\nRegarding spatial mnist, we agree that it would make a fun benchmark for future work in this area. Regarding the number of samples used in the experiment. It is true that the model can learn useful representations with around 25 samples per digit, I choose the number of samples such that I could nearly always recognize a digit from its samples. If we do use the data as a benchmark in future work I will explore the sample size more carefully. As to whether the model selects meaningful points: that may have been confirmation bias on my part. I have weakened the language to just say that 'sensible subsets' are often selected.\n\nFinally regarding the code we plan to release it upon acceptance.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287553296, "id": "ICLR.cc/2017/conference/-/paper495/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJDBUF5le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper495/reviewers", "ICLR.cc/2017/conference/paper495/areachairs"], "cdate": 1485287553296}}}, {"tddate": null, "tmdate": 1484423070339, "tcdate": 1484423070339, "number": 3, "id": "HJLoyZdIe", "invitation": "ICLR.cc/2017/conference/-/paper495/public/comment", "forum": "HJDBUF5le", "replyto": "HyWm1orEx", "signatures": ["~Harrison_Edwards1"], "readers": ["everyone"], "writers": ["~Harrison_Edwards1"], "content": {"title": "Reply to AnonReviewer4", "comment": "Firstly thank you for taking the time to read our paper and write your review.\n\n\"\n-When training the statistic network, are minibatches (i.e. subsets of the examples) used? \n\"\nIn our experiments we typically consider the datasets to be small and use the entire dataset, but if you had large datasets then something like  minibatches would have to be used.\n\n\"\ndoes using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?\n\"\n\nYou raise a good point, because of the nonlinearity in the statistic network, there can be no guarantee of an unbiased gradient.\n\nAs I understand your next question, you are asking about whether this would pose a problem if your datasets are characterized by rare examples in the datasets. I think in this kind of case the statistic network will struggle to learn useful representations if only presented with small numbers of examples. The problem is that it will become computationally infeasible to pass large datasets through the network for every parameter update. One direction towards addressing these kinds of problems is to use small minibatches but to have some persistent state between them. For instance the statistic network could use a running mean that is updated everytime we pass a minibatch from the same dataset. This is similar to truncated backprogagation through time that is used to handle long sequences in recurrent networks.\n\nThank you for your suggestion regarding hierarchical forecasting we will look into whether our approach could give some advantage in this area.\n\n\n \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287553296, "id": "ICLR.cc/2017/conference/-/paper495/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJDBUF5le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper495/reviewers", "ICLR.cc/2017/conference/paper495/areachairs"], "cdate": 1485287553296}}}, {"tddate": null, "tmdate": 1482170137348, "tcdate": 1482170137348, "number": 3, "id": "HyWm1orEx", "invitation": "ICLR.cc/2017/conference/-/paper495/official/review", "forum": "HJDBUF5le", "replyto": "HJDBUF5le", "signatures": ["ICLR.cc/2017/conference/paper495/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper495/AnonReviewer4"], "content": {"title": "Solid Contribution", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets.  The basic idea is to use a \"double\" variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples.  \n\nHierarchical modeling is an important and high impact problem, and I think that it's under-explored in the Deep Learning literature.  \n\nPros:\n  -The few-shot learning results look good, but I'm not an expert in this area.  \n  -The idea of using a \"double\" variational bound in a hierarchical generative model is well presented and seems widely applicable.  \n\nQuestions: \n  -When training the statistic network, are minibatches (i.e. subsets of the examples) used?  \n  -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)?  For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization.  This seems to fit the graphical model on the right side of figure 1.  If your statistic network is trained on minibatches, it won't be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset.  Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model.  \n\nSuggestions: \n  -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  ", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512565828, "id": "ICLR.cc/2017/conference/-/paper495/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper495/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper495/AnonReviewer3", "ICLR.cc/2017/conference/paper495/AnonReviewer1", "ICLR.cc/2017/conference/paper495/AnonReviewer4"], "reply": {"forum": "HJDBUF5le", "replyto": "HJDBUF5le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512565828}}}, {"tddate": null, "tmdate": 1482116857725, "tcdate": 1482116857725, "number": 2, "id": "rkfZJCVVl", "invitation": "ICLR.cc/2017/conference/-/paper495/official/review", "forum": "HJDBUF5le", "replyto": "HJDBUF5le", "signatures": ["ICLR.cc/2017/conference/paper495/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper495/AnonReviewer1"], "content": {"title": "a nice addition to the one-/few-shot learning literature", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Sorry for the late review -- I've been having technical problems with OpenReview which prevented me from posting.\n\nThis paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to \"learn to learn\" by acquiring the ability to learn distributions from small numbers of examples.\n\nOverall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read.\n\nThe name of the paper is overly grandiose relative to what was done; the proposed method doesn\u2019t seem to have much in common with a statistician, unless one means by that \"someone who thinks up statistics\". \n\nThe experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method.\n\nThe spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn\u2019t the method be able to recognize the distribution with fewer samples?  (Nitpick: the red points in Figure 4 don\u2019t seem to correspond to meaningful points as was claimed in the text.) \n\nWill the authors release the code?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512565828, "id": "ICLR.cc/2017/conference/-/paper495/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper495/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper495/AnonReviewer3", "ICLR.cc/2017/conference/paper495/AnonReviewer1", "ICLR.cc/2017/conference/paper495/AnonReviewer4"], "reply": {"forum": "HJDBUF5le", "replyto": "HJDBUF5le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512565828}}}, {"tddate": null, "tmdate": 1481902558037, "tcdate": 1481902558037, "number": 1, "id": "Sy8k9FbNg", "invitation": "ICLR.cc/2017/conference/-/paper495/official/review", "forum": "HJDBUF5le", "replyto": "HJDBUF5le", "signatures": ["ICLR.cc/2017/conference/paper495/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper495/AnonReviewer3"], "content": {"title": "Interesting paper that starts to expand the repertoire of variational autoencoders", "rating": "6: Marginally above acceptance threshold", "review": "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512565828, "id": "ICLR.cc/2017/conference/-/paper495/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper495/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper495/AnonReviewer3", "ICLR.cc/2017/conference/paper495/AnonReviewer1", "ICLR.cc/2017/conference/paper495/AnonReviewer4"], "reply": {"forum": "HJDBUF5le", "replyto": "HJDBUF5le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512565828}}}, {"tddate": null, "tmdate": 1481235173286, "tcdate": 1481235173280, "number": 2, "id": "BkaJoUvXg", "invitation": "ICLR.cc/2017/conference/-/paper495/public/comment", "forum": "HJDBUF5le", "replyto": "BJCRwsfXx", "signatures": ["~Harrison_Edwards1"], "readers": ["everyone"], "writers": ["~Harrison_Edwards1"], "content": {"title": "one-shot task", "comment": "Thank you for reading the paper and your question.\n\nThe task is really the same for all experiments: produce a summary of each dataset as an approximate posterior over the context variables. We probe these summaries by looking at synthesized samples, and using the representations to do one-shot classification. The log-probability of a new dataset under the model would be hard to interpret given the extensive data augmentation we have used.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287553296, "id": "ICLR.cc/2017/conference/-/paper495/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJDBUF5le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper495/reviewers", "ICLR.cc/2017/conference/paper495/areachairs"], "cdate": 1485287553296}}}, {"tddate": null, "tmdate": 1480927189828, "tcdate": 1480927189820, "number": 2, "id": "BJCRwsfXx", "invitation": "ICLR.cc/2017/conference/-/paper495/pre-review/question", "forum": "HJDBUF5le", "replyto": "HJDBUF5le", "signatures": ["ICLR.cc/2017/conference/paper495/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper495/AnonReviewer3"], "content": {"title": "one-shot task for generative models", "question": "Thanks for the interesting paper, can you clarify more precisely what the \"one-shot\" task for generative models is? If it is essentially a transfer type task (assign high probability to unseen but valid regions of the space), then why not evaluate the log-probability of these inputs?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959249696, "id": "ICLR.cc/2017/conference/-/paper495/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper495/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper495/AnonReviewer1", "ICLR.cc/2017/conference/paper495/AnonReviewer3"], "reply": {"forum": "HJDBUF5le", "replyto": "HJDBUF5le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959249696}}}, {"tddate": null, "tmdate": 1480702386017, "tcdate": 1480702386010, "number": 1, "id": "B1chFN17x", "invitation": "ICLR.cc/2017/conference/-/paper495/public/comment", "forum": "HJDBUF5le", "replyto": "B1b6XIoGl", "signatures": ["~Harrison_Edwards1"], "readers": ["everyone"], "writers": ["~Harrison_Edwards1"], "content": {"title": "Reply to pre-review questions", "comment": "Thanks for your questions!\n\nClassifying an MNIST digit from a sample of its coordinates is considerably more difficult than from an image because it is ambiguous: you don't know if there are no black pixels in a given region or whether they are simply missing from your sample by chance. Using a nearest neighbor classifier on the context variables gives a classification accuracy of around 85%. This is much better than nearest neighbors on raw images, but a representation from a VAE trained on MNIST images, for example, would likely fare better. We could likely do well if we used some labels for the digits to inform our representations, that is modeling the joint distribution over datasets and dataset labels p(D, y). It may be interesting to investigate such a supervised or semi-supervised approach for 3d shapes given as point clouds in the future. There is some related work on classifying datasets that we mention in the paper, such as the support measure machine, which it would be interesting to compare against in future work.\n\nRegarding your second question, in this context by 'statistician' we mean someone who chooses and computes statistics of datasets. We agree that this covers only a small part of what human statisticians do!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287553296, "id": "ICLR.cc/2017/conference/-/paper495/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJDBUF5le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper495/reviewers", "ICLR.cc/2017/conference/paper495/areachairs"], "cdate": 1485287553296}}}, {"tddate": null, "tmdate": 1480446904865, "tcdate": 1480446904860, "number": 1, "id": "B1b6XIoGl", "invitation": "ICLR.cc/2017/conference/-/paper495/pre-review/question", "forum": "HJDBUF5le", "replyto": "HJDBUF5le", "signatures": ["ICLR.cc/2017/conference/paper495/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper495/AnonReviewer1"], "content": {"title": "MNIST classification performance?", "question": "I don't have any major clarification questions -- the exposition seems clear and well motivated. But I am curious how well you can classify spatial MNIST digits from the summary statistics. Is it competitive with algorithms that are given the original image?\n\nPedantic question: in what sense can this network be called a \"statistician\"?  The tasks under consideration don't seem very representative of what statisticians do.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards a Neural Statistician", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "pdf": "/pdf/ab975c81ca2637f798440bd40c1d09f3410c8a16.pdf", "TL;DR": "Learning representations of datasets with an extension of VAEs.", "paperhash": "edwards|towards_a_neural_statistician", "keywords": [], "conflicts": ["ed.ac.uk", "openai.com"], "authors": ["Harrison Edwards", "Amos Storkey"], "authorids": ["h.l.edwards@sms.ed.ac.uk", "amos.storkey@ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959249696, "id": "ICLR.cc/2017/conference/-/paper495/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper495/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper495/AnonReviewer1", "ICLR.cc/2017/conference/paper495/AnonReviewer3"], "reply": {"forum": "HJDBUF5le", "replyto": "HJDBUF5le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper495/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959249696}}}], "count": 13}