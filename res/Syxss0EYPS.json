{"notes": [{"id": "Syxss0EYPS", "original": "SJgQQ7tuPS", "number": 1331, "cdate": 1569439394680, "ddate": null, "tcdate": 1569439394680, "tmdate": 1577168261190, "tddate": null, "forum": "Syxss0EYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "RQIVVZPpOZ", "original": null, "number": 1, "cdate": 1576798720756, "ddate": null, "tcdate": 1576798720756, "tmdate": 1576800915843, "tddate": null, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose an agent that can act in an RL environment to verify hypotheses about it, using hypotheses formulated as triplets of pre-condition, action sequence, and post-condition variables. Training then proceeds in multiple stages, including a pretraining phase using a reward function that encourages the agent to learn the hypothesis triplets. \n\nStrengths: Reviewers generally agreed it\u2019s an important problem and interesting approach\n\nWeaknesses: There were some points of convergence among reviewer comments: lack of connection to existing literature (ie to causal reasoning and POMDPs), and concerns about the robustness of the results (which were only reporting the max seeds).  Two reviewers also found the use of natural language to unnecessarily complicate their setup. Overall, clarity seemed to be an issue. Other comments concerned lack of comparisons, analyses, and suggestions for alternate methods of rewarding the agent (to improve understandability).  \n\nThe authors deserve credit for their responsiveness to reviewer comments and for the considerable amount of additional work done in the rebuttal period. However, these efforts ultimately didn\u2019t satisfy the reviewers enough to change their scores. Although I find that the additional experiments and revisions have significantly strengthened the paper, I don't believe it's currently ready for publication at ICLR. I urge the authors to focus on clearly presenting and integrating these new results in a future submission, which I look forward to.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711672, "tmdate": 1576800260918, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Decision"}}}, {"id": "BkgYVAox5B", "original": null, "number": 3, "cdate": 1572023856986, "ddate": null, "tcdate": 1572023856986, "tmdate": 1574513497601, "tddate": null, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper looks into the problem of training agents that can interact with their environments to verify hypotheses about it. It first formulates the problem as a MDP, where the agent takes actions to explore the environment and has two special actions (Answer_True, and Answer_False) to indicate that the agent has made a prediction about the validity of the hypothesis. The reward depends on how correct the agent's prediction is. A second formulation uses MDP to explore the states and has a special action (Answer), which predicts the validity of the hypothesis based on the last sequence of N states visited. This is one side of the problem. The authors carry out such experiments and conclude that this doesn't work. \n\nThen, the authors exploit the structure of some hypotheses (such as triplet hypotheses of the form pre_condition, action_sequence, post_condition), which are easier to test. They conclude that taking this structure into account helps. \n\nOverall, the paper is well-written and the literature review section is quite excellent. However, I have reservations against the formulations that the authors used. I would appreciate it if the authors present their argument in the rebuttal. \n\nFirst, in the plain formulation of MDP, a policy produces an action according to the current state only. The authors add (Answer_True, and Answer_False) to the list of actions in MDP. So, if the agent is trained on some hypotheses, the agent will essentially learn to identify for each h which state s that can be used to to verify h (either prove or disprove it). To me, this is essentially memorization, and the agent cannot learn to predict the validity of new hypotheses. So, it seems that formulating the problem using MDP is not reasonable to begin with. \n\nSecond, when the agent exploits the structure of the hypotheses, the problem becomes nearly trivial. It would have been interesting if, somehow, the agent learned the strategy of trying to alter the preconditions or postconditions on its own, but this is not the case in the paper. The formulation essentially tells the agent that it should alter the preconditions and postconditions so that we have enough information about the validity of h that can be fed into a prediction network. I think that the fact this works is not that interesting. \n\nSome minor comments:\n- I suggest that all acronyms be defined in the paper before they are used. \n- In the reward functions, why did the authors use C instead of just using 1. \n- In Page 4, \"The agent is is spawned\" has a typo. \n- In Page 5, \"so we can in principal only\" has a typo. \n- In Page 7, \"as it paves the towards\" has a typo. \n\n================== \n#Post Rebuttal Remark\n\nI have gone through the authors' response and I thank them for it, particularly for making some of the suggested enhancements. However, my score remains unchanged. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575422448215, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Reviewers"], "noninvitees": [], "tcdate": 1570237738928, "tmdate": 1575422448227, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Review"}}}, {"id": "r1xcgUU0YH", "original": null, "number": 2, "cdate": 1571870193909, "ddate": null, "tcdate": 1571870193909, "tmdate": 1574445372779, "tddate": null, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper trains agents which are able to verify hypotheses, such as \u201cthe blue switch causes the door to open\u201d. It does this by first pretraining the agent to perform interventions in the environment which change the states of the objects of interest, and then finetuning the agent to actually make a decision about whether the given hypothesis is correct. The paper shows that agents trained using this procedure are able to not only verify the types of hypotheses seen during pre-training, but also learn to verify more complex hypotheses. In contrast, an agent which is trained directly on the hypothesis verification task is unable to learn to do it. \n\nOverall, I enjoyed reading this paper and thought that it provided an interesting take on the question of how to train agents that can appropriately gather information about their environments. However, (1) the paper lacks any discussion of related work in terms of causal reasoning and partial observability, and (2) the experiments and analysis seem weak. I thus am giving a score of \u201cweak reject\u201d, though it is possible I could increase my score is some of my concerns can be addressed.\n\nFirst, I was very surprised to see that the paper included no discussion at all about either causal reasoning or partial observability. The whole notion of verifying hypotheses\u2014particularly those in the triplet form as presented in the paper\u2014is equivalent to the idea of performing inference about the structure of a causal graph with three variables. The choice of which interventions to perform in order to make these inferences is a well-studied problem [1] and has been recently explored in the context of RL as well [2]. The novelty here seems to be in embedding the problem of causal reasoning in harder credit assignment problems (i.e. longer time horizon), though see [3]. Similarly, the setup of the MDP in the paper is actually a POMDP, where the state includes the truth value of the hypothesis but where observations do not include this information. Yet, there is no mention of POMDPs or discussion of the literature on partial observability in the paper.\n \nSecond, I felt that the setup was overly complex in places making it difficult to draw conclusions, that there were a lack of comparisons, and that the analysis was not as in depth as it could have been. For example, why is it necessary to represent the hypothesis with natural language? Why not use a symbolic representation? It seems like including the pseudo-natural language adds unnecessary complexity and makes it difficult to distentangle what about the problem is hard (Understanding the hypothesis? Choosing the right interventions? Parsing the observations correctly?). The utility of having it be closer to language is that you might see generalization between related hypotheses, but this isn\u2019t really something that is actually tested for since all hypotheses are trained on either during pretraining or finetuning.\n \nI also feel like the choice of pretraining reward feels somewhat arbitrary, and it would have been nice to see comparisons to other alternatives (and even better, to other forms of intrinsic motivation). For example, here are a few alternate ways of rewarding the agent that seem intuitively like they could also work:\nReward the agent for changing the state of any of the objects in the environment\nReward the agent for changing the state of any object referenced in the hypothesis\nReward the agent for observing a state of the world it has not seen before (i.e. count-based exploration)\nIn other words, how important is the fact that the reward is given based on the pre and postconditions?\n\nI thought the paper would benefit from more detailed analyses to tease apart the behavior of the agent. For example, I am curious how many errors are a result of errors in the predictor versus poor exploration behavior by the policy. Could you report (1) how frequently the policy\u2019s behavior results in the right observations necessary to make a decision, and (2) results with a policy which uses an oracle predictor (i.e. which will always report the correct answer, if there was enough data in the last N frames to detect that answer)?\n\nOn the more practical side, I also thought the quality of the evaluations was not very thorough. For instance, it looks like the pretraining proceeds for 1e8 steps and finetuning for 5e7 steps, based on the plots (these values should be stated more explicitly in the paper). However, this is a bit of an unfair comparison for the \u201cRL Baseline\u201d, as it only is trained for 5e7 steps while the other agents are trained for 1.5e8 steps. I would like to see a comparison where the RL Baseline agent is trained for 1.5e8 steps as well. Similarly, on the bottom of page 6 the paper says \u201cwe show the max out of five for each of the methods shown\u201d. However, only reporting the max value is considered bad practice and can result in misleading comparisons (see Joelle Pineau\u2019s talk on \u201cReproducible, Reusable, and Robust Reinforcement Learning\u201d at NeurIPS 2018). I\u2019d like to see the data in all figures and tables reported with means or medians across seeds, rather than best seeds.\n\nA few minor comments:\n\n- Please state in the main text which RL algorithm you use.\n- Can you clarify whether Figure 2 show the proxy rewards or the true rewards?\n- For R_pre and R_ppost, what values do you use for C and N?\n\n[1] Pearl, J. (2000). Causality: models, reasoning and inference (Vol. 29). Cambridge: MIT press.\n[2] Dasgupta, I., Wang, J., Chiappa, S., Mitrovic, J., Ortega, P., Raposo, D., ... & Kurth-Nelson, Z. (2019). Causal reasoning from meta-reinforcement learning. arXiv preprint arXiv:1901.08162.\n[3] Denil, M., Agrawal, P., Kulkarni, T. D., Erez, T., Battaglia, P., & de Freitas, N. (2016). Learning to perform physics experiments via deep reinforcement learning. arXiv preprint arXiv:1611.01843.\n\n--\nUpdate after rebuttal:\n\nThank you very much for your response. However, I do not feel that all of my concerns have been addressed and thus will keep my score as it is. In particular, I still feel the paper lacks sufficient discussion of the literature on causal reasoning. I also do not think it is sufficient to add an appendix with the results across multiple seeds: these results should be in the main paper. I'm not sure I follow the justification that max seeds make sense because \"the reward distribution is quite binary in nature\"---the plots shown in Figure 3 and 4, for example, span a range of values from 0 to 1. I find the plots that have both variance and max seed very hard to interpret---in some cases the mean is so much lower than the max seed that the variance region doesn't overlap at all. More broadly, it might be easier to compare using bar plots showing final performance, rather than training curves\n\nI appreciate the additional results, especially with different pretraining schemes---thanks for adding these! I have a bit of hard time interpreting the results though since there are no direct comparisons with the triplet pretraining scheme; it would be helpful if these results could be included in these figures too.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575422448215, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Reviewers"], "noninvitees": [], "tcdate": 1570237738928, "tmdate": 1575422448227, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Review"}}}, {"id": "B1l3DYTjsr", "original": null, "number": 5, "cdate": 1573800292148, "ddate": null, "tcdate": 1573800292148, "tmdate": 1573801853759, "tddate": null, "forum": "Syxss0EYPS", "replyto": "r1xcgUU0YH", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your helpful comments and suggestions. We have updated the paper and made some general comments above. We will now answer your specific concerns and suggestions.\n\n\u201cthe paper included no discussion at all about either causal reasoning or partial observability\u201d: \nYes, you are correct; the setting is POMDP, not an MDP. We have fixed this in the text. All models (including all baselines) were provided with a history of observations. Also, not contextualizing our work with the papers you suggest was an omission on our part that has been remedied in the updated version. Note that https://arxiv.org/pdf/1611.01843.pdf also takes the strategy of using standard RL with models that use a history of observations (e.g. RNN). \n\n\u201cwhy is it necessary to represent the hypothesis with natural language? Why not use a symbolic representation?\u201d \nAs remarked to R2: we agree that for the experiments reported, we could have used simpler symbolic representations. It is not clear that these would actually be more convenient, and they would be less general. More importantly, we choose templated language because we believe it is scalable in the sense that it allows extension to more complicated and richer hypotheses.  In our view the combinatorial nature of the hypotheses is an important feature of our framing.  \n\n\u201conly reporting the max value is considered bad practice ...I\u2019d like to see the data in all figures and tables reported with means or medians across seeds, rather than best seeds\u201d: \nSee Appendix J for the main result (with more random seeds) with the mean and variance.\n\nHowever, the reward distribution is quite binary in nature (either it works, or fails completely) thus neither mean, median or max measures provide an adequate characterization. However, we are able to side-step this issue by using our pre-training mechanism to select good seeds: in a new experiment.\n\nTo address this concern, we performed the methodology in Appendix E which we hope you will take a look at. Here we show that the selection of the good seeds can be done automatically using our pre-training templates to select good seeds: in a new experiment we show that the performance on the pre-training templates can be used to reliably pick seeds/models that will result in high performance after fine-tuning.\n\n\u201cCould you report (1) how frequently the policy\u2019s behavior results in the right observations necessary to make a decision, and (2) results with a policy which uses an oracle predictor\u201d:\nThis was a very good suggestion. We did this analysis and experiments in Appendix G for the crafting environment (in the interest of time, we did not have time to do more). We will provide this for all environments on revision.\n\n\u201cI would like to see a comparison where the RL Baseline agent is trained for 1.5e8 steps as well.\u201d\nAnother good point and suggestion. We provide this in appendix H. \n\n\u201calternate ways of rewarding the agent\u201d\nThis was another good suggestion, and we really appreciate this suggestion.\nWe followed two of these suggestions: \n1. Reward the agent for changing the state of any of the objects in the environment\n2. Reward the agent for changing the state of any object referenced in the hypothesis\n\nWe didn\u2019t do the count-based exploration for a couple reasons: (time and because even in 5x5 gridworlds, there are an exponential number of states and we don\u2019t believe that this would have done very well).\n\nThis experiment is in Appendix F. We provide details and analysis there.\n\nTo answer your question though: \u201chow important is the fact that the reward is given based on the pre and postconditions?\u201d\nIt\u2019s pretty important. \nSome versions of intrinsic motivation you suggest do pretty well, especially the crafting one because it turns out to be a pretty good proxy for how to verify the hypothesis, but much less well on color switch and pushblock. See the paper for more analysis here.\n\nMinor points:\n\u201cwhich RL algorithm you use\u201d: \nPPO\n\u201cwhether Figure 2 show the proxy rewards or the true rewards?\u201d\nProxy rewards.\n\u201cFor R_pre and R_ppost, what values do you use for C and N?\u201d\nC=10, N=5"}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syxss0EYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1331/Authors|ICLR.cc/2020/Conference/Paper1331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157633, "tmdate": 1576860542810, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment"}}}, {"id": "HkgIb_6ssS", "original": null, "number": 4, "cdate": 1573799933535, "ddate": null, "tcdate": 1573799933535, "tmdate": 1573801592028, "tddate": null, "forum": "Syxss0EYPS", "replyto": "HkgdawpsiB", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (cont)", "comment": "\u201caccuracy of all the experiments are shown using the max of top-5, however appendix D shows quite a significant variance for the methods\u201d: \nYes, this is correct.\nTo address this concern, we performed the experiments described in Appendix E which we hope you will take a look at.  There we show that the selection of the good seeds can be done automatically using our pre-training templates to select good seeds:  that is, the performance on the pre-training templates can be used to reliably pick seeds/models that will result in high performance after fine-tuning. Thus even though the variance is high, bad runs can be weeded out in the first phase with the pre-training templates.\n\n\u201cwhat happens if the methods are trained on more seeds?\u201d: \nIn Appendix J we rerun the final accuracy experiments using more random seeds. We also are sure in the other experiments (including our method of random seed selection in Appendix E) to run with more random seeds.\n\nThis does not appear to substantially change the results, at least for this experiment. However, in Appendix H, when we run more random seeds AND train the baselines for longer, the pushblock RL baseline gets one good random seed, although it still performs worse than our methods (~75% versus 85%). No amount of additional training or random seeds gives good results for baselines on the other environments.\n\n[Would like an experiment realted to] the difference in performance on each environment with different pre-training reward function (only one in show in the paper right now):\nAs we said above, as requested by R3, we have added new experiments to the paper that explores different ways of doing pre-training. See Appendix F for figures and details.\n\n\u201cthe title is somewhat misleading\u2026\u201d: \nWe would like to retain \u201cScientist\u201d in the title, since it conveys the ultimate goal of this line of work. That said, we understand your concern. We therefore intend to change the title to: \u201cToward a Scientist Agent: Learning to verify hypotheses\u201d"}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syxss0EYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1331/Authors|ICLR.cc/2020/Conference/Paper1331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157633, "tmdate": 1576860542810, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment"}}}, {"id": "HkgdawpsiB", "original": null, "number": 3, "cdate": 1573799871648, "ddate": null, "tcdate": 1573799871648, "tmdate": 1573799871648, "tddate": null, "forum": "Syxss0EYPS", "replyto": "B1xsnehaKH", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your helpful comments and suggestions. We have updated the paper and made some general comments above. We will now answer your specific concerns and suggestions.\n\n\u201cThe environments seem to be all fairly similar\u2026would have been better to also present problems with fairly different settings (e.g. much different - sparser and/or denser - types of reward function)\u201d: \nWe agree and have now included new experiments that explore different forms of pre-training (something that was possible to do in the rebuttal period), which adds to the diversity of experiments presented. More generally, we are working to demonstrate our approach on a broader set of environments, which we hope to include in a future update of the paper. \n\n\u201ca \"RL Baseline\" is mentioned in principle, however (1) it is unclear whether it was pre-trained similarly to the proposed methods\u201d: \nPre-training was not used for the RL baseline, but the deep network structure and fine-tuning procedures were the same as our approach. \n\n\u2026 \u201cI would expect the same policy to be able to learn the same function given enough memory and training steps.\u201d: \n\u201cA baseline employing some form of memory (such as heavy usage of frame stacking or recurrency), to attempt at figuring out whether it's really not reasonable to learn the whole problem simply using RL\u201d\nOur revised paper includes new experiments that ran the RL baseline for longer (Appendix H), and another experiment that gave the RL baseline more \u201cmemory\u201d (Appendix I). These show that the extra training did not help, except for the pushblock task where some limited performance was achieved, although still not as much as our methods. See those appendices for figures and more analysis. \n\nFor clarity, all methods already keep a state memory. In all original experiments, the policy and hypothesis predictor get the last N=5 states. In Appendix I we show that increasing N furtehr does not improve performance.\n\nAlso, as mentioned to R1, and as we show in Appendix G: when we have an oracle hypothesis verification predictor, the RL baseline with its stacked observations can pretty easily solve the problem. So we don\u2019t believe that the use of memory stacking to sidestep the partial observability is the main difficulty of this problem.\n\n\u201cI'm confused by the way the authors decomposed the action space for the policy and the predictor in section 3.1. Does the policy use ans_T and ans_F at any point during training? Does the actor effectively decide (i.e. by choosing \"ans\") when to query the prediction network?\u201d\nWe apologize for the confusion. The policy does not use ans_T and ans_F, it has a single ans action which then lets the actor query the prediction network. We will make this more clear on revision.\n\n\u201cBetter clarity on how the hypothesis setup stands to previous literature\u201d... \n\u201cIdeally, I would like to see some comparisons between this type of hypothesis and other decompositions used in previous literature, since it seems like the method exploits this particular structure quite heavily\u201d: \nAs far as we are aware, ours is the first attempt to decompose a hypothesis in this fashion for the purposes of facilitating ML based approach, thus there is no previous literature with which a comparison can be made. However, if R2 knows of any relevant works we would be most interested. \n\nAs requested by R3, we have added new experiments to the paper that explores different forms of decomposition, by adjusting the type of intrinsic motivation used. These experiments that the original form of factorization chosen is more effective than other types. See Appendix F for figures and details.\n\n\u201cthe usage of natural language overkill and not necessarily well justified\u201d: \nWe agree that for the experiments reported, we could have used simpler symbolic representations.  It is not clear that these would actually be more convenient, and they would be less general.  More importantly, we choose templated language because we believe it is scalable in the sense that it allows extension to more complicated and richer hypotheses.  In our view the combinatorial nature of the hypotheses is an important feature of our framing.  \n\n\u201cconfused by how the pre-training is done\u201d: \nApologies for the confusion. We will make this clearer in the next revision. When we use both pre and prepost reward functions (which we do on pushblock and crafting) they are added. Your interpretation here was indeed correct."}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syxss0EYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1331/Authors|ICLR.cc/2020/Conference/Paper1331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157633, "tmdate": 1576860542810, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment"}}}, {"id": "BkgE98TiiH", "original": null, "number": 2, "cdate": 1573799564233, "ddate": null, "tcdate": 1573799564233, "tmdate": 1573799564233, "tddate": null, "forum": "Syxss0EYPS", "replyto": "BkgYVAox5B", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your helpful comments and suggestions. We have updated the paper and made some general comments above. We will now answer your specific concerns and suggestions.\n\n\u201cwhen the agent exploits the structure of the hypotheses, the problem becomes nearly trivial...\u201d: \nWe agree that the structure imposed during pre-training makes the problem far simpler than the original one (but not trivial -- our RL approach still only succeeds intermittently). But rather than a weakness, it should be viewed as a strength of the way we have chosen to frame the problem. As noted, the original problem of hypothesis verification is just too hard by itself (RL baseline makes little progress at all). Our idea of decomposing the hypothesis into (pre,post,reward) is an important contribution since it makes the problem amenable to ML approaches. Importantly, once pre-trained using this assistance, the model can be fine-tuned on the original task, i.e. without the need to use the imposed structure. Hence the use of our imposed structure as a pre-training step makes a hitherto inaccessible problem tractable. \n\nR3 requested an exploration of other forms of structure, imposed via different intrinsic forms of motivation to the one in the paper. We have followed these suggestions, revising the paper to include these new experiments (see Appendix F). These show that alternate structures (which provide weaker assistance), are in general not sufficient to make the problem tractable, lending support to our original formulation.\n\n\u201cessentially memorization...cannot learn to predict the validity of new hypotheses\u201d: \nOur approach is clearly not memorizing: (i) the environment is randomized (locations of agents, objects, hypotheses and ground truth) each episode giving rise to an exponential number of states and (ii) in Table 1, we demonstrate the learning is able to generalize to new templates. \n\n\u201cFirst, in the plain formulation of MDP, a policy produces an action according to the current state only.\u201d \u2026. \u201cat formulating the problem using MDP is not reasonable to begin with\u201d:\n\nWe are sorry for the confusion- we incorrectly wrote that the problem is an MDP, we should have written POMDP; see also our response to reviewer 3.  \n\nThere are quite a few works which do frame stacking, or use a recurrent network structure as a way of using RL on problems which may require more than the current observation frame. Frame stacking is very commonly used in Atari for instance (Mnih, Volodymyr, et al. \"Playing Atari with Deep Reinforcement Learning.\" arXiv preprint arXiv:1312.5602 (2013).) Lots of works, such as the previously cited (Das et al., 2018) keep this history implicitly using RNNs.\n\nAs we show in Appendix G, when we have an oracle hypothesis verification predictor, the RL baseline with its stacked observations can pretty easily solve the problem. So we don\u2019t believe that the use of memory stacking to sidestep the partial observability is the main difficulty of this problem.\n\n\u201cTypos, undefined acronyms\u201d\nThank you. Those typos have been fixed. We will be sure to read carefully before the next revision to find more typos and to define all our acronyms.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syxss0EYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1331/Authors|ICLR.cc/2020/Conference/Paper1331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157633, "tmdate": 1576860542810, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment"}}}, {"id": "HkeUjrpojr", "original": null, "number": 1, "cdate": 1573799325997, "ddate": null, "tcdate": 1573799325997, "tmdate": 1573799325997, "tddate": null, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment", "content": {"title": "Response to all reviewers", "comment": "We would like to express our sincere thanks to reviewers, whose feedback is among the most constructive, useful and detailed that we\u2019ve ever received at ICLR and similar venues. Your collective input has caused us to make many changes to the paper, in terms of writing as well as new experiments that result in what we believe to be greatly strengthened work.   We are pleased to report that we have done *ALL* the experiments that you asked for (except running on completely new environment which would require substantial more time and experiments). These have all been added to the paper as appendices, but can be moved to the main body of the paper later (subject to space constraints). \n\nThe experiments include:\nA different way of pruning and analyzing the variance of our methods by using the triplet hypotheses as a sort of validation set to choose random seeds to address concerns about variance and displaying max results. (Appendix E).\nExperiments using different pre-training reward functions suggested by one of the reviewers. (Appendix F)\nAn experiment training an RL agent using an oracle hypothesis predictor, and an evaluation of our methods using this oracle to see where our methods and baselines do well or badly. (Appendix G)\nExperiments where we train our baseline methods for longer to account for the baselines not receiving the pre-training time (Appendix H). \nAn experiment seeing whether giving the baseline a longer state memory improves performance (Appendix I)\nRerunning our results on more random seeds to see if there is a substantial change in the results (Appendix J)\n\nThe goal of this work is to train agents that can learn to verify different kinds of hypotheses. In our view, the key contribution of this work is showing that the structure of the hypothesis verification problem gives an opening for solving it, and that the problem is difficult without using the particular structure. The reviewers\u2019 comments and the results of their suggested experiments strengthen this conclusion.\n\nGiven the detailed nature of the comments, we will respond separately to each reviewer separately on their specific concerns. We encourage all reviewers to look at the new experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syxss0EYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1331/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1331/Authors|ICLR.cc/2020/Conference/Paper1331/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157633, "tmdate": 1576860542810, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Authors", "ICLR.cc/2020/Conference/Paper1331/Reviewers", "ICLR.cc/2020/Conference/Paper1331/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Comment"}}}, {"id": "B1xsnehaKH", "original": null, "number": 1, "cdate": 1571827890693, "ddate": null, "tcdate": 1571827890693, "tmdate": 1572972482758, "tddate": null, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "invitation": "ICLR.cc/2020/Conference/Paper1331/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors present a framework for testing a set of structured hypotheses about environment dynamics by learning an exploratory policy using Reinforcement Learning and a evaluator through supervised learning. They propose a formulation that decomposes environment hypotheses into sets of pre-conditions, required actions, and post-conditions. They then exploit this decomposition to (a) decouple the problem into both RL and supervised learning, and (b) provide localised pre-training to make the problem more tractable.\n\nOverall, I really wanted to like this paper. The problem is interesting, and it certainly provides a great venue for interesting and impactful research in RL, language-conditioned decision making, structured / symbolic learning, and so on. However, I've found it relatively difficult to understand good parts of the methods and part of the experimental section, due to missing or misleading details.\n\nIn particular:\n\n1. the justification for splitting the problem in a _exploratory_ / verification policy and a predictor is sound in principle, however it's unclear to me whether the problem is after all that intractable. In the experiment section a \"RL Baseline\" is mentioned in principle, however (1) it is unclear whether it was pre-trained similarly to the proposed methods, and (2) if the policy has learnt enough about the problems that its poking methodology provides enough signal to the predictor, I would expect the same policy to be able to learn the same function given enough memory and training steps.\n\n2. I'm confused by the way the authors decomposed the action space for the policy and the predictor in section 3.1. Does the policy use ans_T and ans_F at any point during training? Does the actor effectively decide (i.e. by choosing \"ans\") when to query the prediction network? \n\n3. The way the authors split the templates is confusing to me. Up to section 3.3.1 (and - really - until I read the appendix...), the writing sort of led me to assume that (1) the \"(pre-condition, action sequence) -> post-condition\" split was a fairly standard manner of compose a hypothesis, and that (2) the templates were mostly symbolic. However after reading the appendix, I found the imposed structure to be fairly arbitrary, and the usage of natural language overkill and not necessarily well justified. Ideally, I would like to see some comparisons between this type of hypothesis and other decompositions used in previous literature, since it seems like the method exploits this particular structure quite heavily and I don't quite understand how it generalises to other tasks.\n\n4. The environments seem to be all fairly similar, both in terms of overall complexity, size, and features. It would have been better to also present problems with fairly different settings (e.g. much different - sparser and/or denser - types of reward function), rather than evaluating multiple times on effectively the same grid-world. I was though encouraged to see that one of the environment seemed to require slightly different setting in the pre-training reward setup, however the authors didn't follow up with some analysis on why there was such a difference.\n\n5. I'm confused by how the pre-training is done. I understand that R_{pre} is used by itself in one environment, but I couldn't figure out whether it's both reward functions at the same time that are used in the rest of them, or just R_{ppost}. Looking at the scale of the (average?) reward, the former seems to be the case, but it would be good to be certain about such things.\n\n6. The final accuracy of all the experiments are shown using the max of top-5, however appendix D shows quite a significant variance for the methods. Thus I'm not sure the analysis and final considerations are reasonable. What happens if the methods are trained on more seeds?\n\n6. [nit] the title is somewhat misleading: in the introduction, a scientist is defined as being both a proposer and a verifier of hypotheses, which is a reasonable, however the authors fundamentally propose to solve only arguably the more straightforward of the two problems. A less _flashy_ title would go a long way towards providing reasonable expectations for the reader.\n\n\nTo improve this paper, I would like to see:\n\n- Better clarity on how the hypothesis setup stands to previous literature.\n- The difference in performance on each environment with different pre-training reward function (only one in show in the paper right now)\n- At least one more environments with significantly different dynamics, or an explanation of how the existing settings differ in qualitative terms.\n- A baseline employing some form of memory (such as heavy usage of frame stacking or recurrency), to attempt at figuring out whether it's really not reasonable to learn the whole problem simply using RL, with ablation of pre-training (which I suspect might make a significant difference).\n\nAt this point, I cannot recommend the article for acceptance, but I'd be willing to change my rating if the authors were to address some of the above points. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1331/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1331/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Agent as Scientist: Learning to Verify Hypotheses", "authors": ["Kenneth Marino", "Rob Fergus", "Arthur Szlam", "Abhinav Gupta"], "authorids": ["kdmarino@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com", "abhinavg@cs.cmu.edu"], "keywords": [], "abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.", "pdf": "/pdf/9048a84e46ecb65831e1e06a8d0988018d13e87a.pdf", "paperhash": "marino|agent_as_scientist_learning_to_verify_hypotheses", "original_pdf": "/attachment/39d733e4800ef577cac31cd3abdeb5f895fbc02c.pdf", "_bibtex": "@misc{\nmarino2020agent,\ntitle={Agent as Scientist: Learning to Verify Hypotheses},\nauthor={Kenneth Marino and Rob Fergus and Arthur Szlam and Abhinav Gupta},\nyear={2020},\nurl={https://openreview.net/forum?id=Syxss0EYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syxss0EYPS", "replyto": "Syxss0EYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575422448215, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1331/Reviewers"], "noninvitees": [], "tcdate": 1570237738928, "tmdate": 1575422448227, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1331/-/Official_Review"}}}], "count": 10}