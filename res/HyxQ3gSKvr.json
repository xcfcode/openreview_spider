{"notes": [{"id": "HyxQ3gSKvr", "original": "SJlJSg-twr", "number": 2531, "cdate": 1569439915059, "ddate": null, "tcdate": 1569439915059, "tmdate": 1577168289663, "tddate": null, "forum": "HyxQ3gSKvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ygtugur@gmail.com", "george.arvanitakis@huawei.com", "abdellatif.zaidi@u-pem.fr"], "title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "authors": ["Yigit Ugur", "George Arvanitakis", "Abdellatif Zaidi"], "pdf": "/pdf/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "abstract": "In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders\u2019 mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.", "keywords": ["clustering", "Variational Information Bottleneck", "Gaussian Mixture Model"], "paperhash": "ugur|variational_information_bottleneck_for_unsupervised_clustering_deep_gaussian_mixture_embedding", "original_pdf": "/attachment/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "_bibtex": "@misc{\nugur2020variational,\ntitle={Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding},\nauthor={Yigit Ugur and George Arvanitakis and Abdellatif Zaidi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxQ3gSKvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DCJH7cc14u", "original": null, "number": 1, "cdate": 1576798751390, "ddate": null, "tcdate": 1576798751390, "tmdate": 1576800884299, "tddate": null, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "invitation": "ICLR.cc/2020/Conference/Paper2531/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to use a mixture of Gaussians to variationally encode high-dimensional data through a latent space. The latent codes are constrained using the variational information bottleneck machinery. \n\nWhile the paper is well-motivated and relatively well-written, it contains minimal novel ideas. The consensus in reviews and lack of rebuttal make it clear that this paper should be significantly augmented with novel material before being published to ICLR.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ygtugur@gmail.com", "george.arvanitakis@huawei.com", "abdellatif.zaidi@u-pem.fr"], "title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "authors": ["Yigit Ugur", "George Arvanitakis", "Abdellatif Zaidi"], "pdf": "/pdf/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "abstract": "In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders\u2019 mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.", "keywords": ["clustering", "Variational Information Bottleneck", "Gaussian Mixture Model"], "paperhash": "ugur|variational_information_bottleneck_for_unsupervised_clustering_deep_gaussian_mixture_embedding", "original_pdf": "/attachment/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "_bibtex": "@misc{\nugur2020variational,\ntitle={Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding},\nauthor={Yigit Ugur and George Arvanitakis and Abdellatif Zaidi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxQ3gSKvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724654, "tmdate": 1576800276333, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2531/-/Decision"}}}, {"id": "HJl3w-spFH", "original": null, "number": 1, "cdate": 1571823972290, "ddate": null, "tcdate": 1571823972290, "tmdate": 1572972326415, "tddate": null, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "invitation": "ICLR.cc/2020/Conference/Paper2531/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the autoencoder model combining the usual information bottleneck and the Gaussian mixture model (GMM). Using an approximation to deal with GMMs, the authors derive a bound on the cost function generalizing the ELBO. The performance of the proposed method is tested on three benchmark datasets and compared with existing methods combining VAE with GMM.\n\nWhile the framework and the performance of the proposed method are interesting and promising, some of its main parts are unclearly explained.\n\n- Although Remark 1 well explains the difference between VaDE and the proposed objective functions, little is discussed how this difference affects the learnt model.\n- I wonder why Q_\\phi(x|u) in Eq. (11) doesn\u2019t have to be a distribution. What does the expression [\\hat{x}] mean?\n- Eq. (17) is not explained clearly. Since the equality symbol is used, it is unclear where is approximation. Although this approximation is one of the main parts of the proposed method, little is discussed on the influence of this approximation.\n- Are the information plane and latent representations in Figs 5 and 6 also available for DEC and VaDE and not limited to the proposed method?\n\nMinor comments:\np.5, the math expression between Eqs. (16) and (17): The distribution q(x_i|u_i,m) is undefined.\np.6, l.4 from the bottom: ACC is defined later.\np.7, l.14: Should J be n_u?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2531/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2531/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ygtugur@gmail.com", "george.arvanitakis@huawei.com", "abdellatif.zaidi@u-pem.fr"], "title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "authors": ["Yigit Ugur", "George Arvanitakis", "Abdellatif Zaidi"], "pdf": "/pdf/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "abstract": "In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders\u2019 mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.", "keywords": ["clustering", "Variational Information Bottleneck", "Gaussian Mixture Model"], "paperhash": "ugur|variational_information_bottleneck_for_unsupervised_clustering_deep_gaussian_mixture_embedding", "original_pdf": "/attachment/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "_bibtex": "@misc{\nugur2020variational,\ntitle={Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding},\nauthor={Yigit Ugur and George Arvanitakis and Abdellatif Zaidi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxQ3gSKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2531/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2531/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575744807822, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2531/Reviewers"], "noninvitees": [], "tcdate": 1570237721535, "tmdate": 1575744807833, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2531/-/Official_Review"}}}, {"id": "Byx_WBDRtr", "original": null, "number": 2, "cdate": 1571874048187, "ddate": null, "tcdate": 1571874048187, "tmdate": 1572972326367, "tddate": null, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "invitation": "ICLR.cc/2020/Conference/Paper2531/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The author(s) posit a Mixture of Gaussian's prior for a compressed latent space representation of high-dimensional data (e.g. images and documents). They propose fitting this model using the Variational Information Bottleneck paradigm and explicate its derivation and tie it to the variational objective used by similar models. They empirically showcase their model and optimization methodology on the MNIST, STL-10, and Reuters10k benchmarks.\n\nThe idea of using a latent mixture of Gaussian's to variationally encode high-dimensional data is not new. The author(s) appropriately cite VaDE (Jiang, 2017) and DEC (Xie, 2016), \"Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders\" (Dilokthanakul, 2017). However, the author(s) unfortunately did not mention \"Approximate Inference for Deep Latent Gaussian Mixtures\" (Nalisnick, 2016). Nalisnick et al. consider the exact generative process as the proposed by the author(s), but with the addition of a Dirichlet prior on the Categorical distribution. Nalisnick et al. fit their model with a VAE and circumvent Dirichlet intractability by positing a Kumarswamy stick-breaking variational posterior (the resulting distribution is on the simplex). They achieve 91.58% accuracy, but do so after fitting a KNN to the latent space. New techniques such as \"Implicit Reparameterization Gradients\" (Figurnov, 2018) and \"Pathwise Derivatives Beyond the Reparameterization Trick\" (Jankowiak, 2018) allow direct use of a Dirichlet posterior in VAEs. These methods are respectively implemented in the TensorFlow and PyTorch APIs. My point here is that there are many ways to fit this pre-existing model. From my review of these other works we have, for \"best run\" on MNIST, that author(s) > VaDE > Nalisnick > Dilokthanakul > DEC. Thus, the author(s) are SoTA for this particular generative process for their best run. It would be nice to see their standard deviation to gauge how statistically significant their results are. However, I am dubious of the SoTA claim as I detail later.\n\nThe author(s) derivation was sound, but a bit confusing for me. In particular, I found keeping track of P's and Q's very burdensome after reading sections 2.1 and 2.2. In my experience, Q is typically reserved for a variational distribution that approximates some intractable P distribution, while P is used to describe the generative process (i.e. likelihood) and other exact distributions. Furthermore, one typically introduces the generative process first using P distributions. Once, I got past this confusion everything else made sense. I might suggest introducing the generative process first and with P distributions instead of Q's. The variational model can follow with the Q distributions as the author(s) have it. Equations 4, 5, and 6 all exactly match the unsupervised information bottleneck objective (Alemi, 2017)--see appendix B. I am therefore confident in those equations. I carefully checked their derivation of the VADe comparison (equation 10 and appendix B). Their derivation is straightforward. The principle trick is using the MC assumption C->X->U for P distributions to claim p(u|x,c) = p(u|x). Equations 12-16 all follow naturally from equation 11. If equation 17 is indeed an approximation or bound approximation, I would suggest not using the equals sign. Instead, consider another appropriate operator or rename Dkl to indicate it is the approximated version (just as in equation 24).\n\nIf the author(s) like my suggestion regarding generative vs variational nomenclature, I would also change the second sentence of page 6 to something like \"We use our variational Qc|u distribution to compute assignments.\" Thereafter, I would drop the star indicator for optimal parameters. These parameters are not necessarily optimal given the non-convexity of the DNN. Replace with, \"after convergence, we ...\" Similarly, drop optimal from line 2 of algorithm 1.\n\nCircling back to the experiments, the author(s) use reported values from DEC and VaDE. Those works compute cluster assignments using a KNN classifier on the latent space. This paper however uses the arg max of equation 19. I much prefer this article's method, but for comparison purposes, the author(s) should similarly use a KNN classifier on their latent space to compute accuracy in the same manner. The use of KNN in these other works allows them to consider a number of latent clusters larger than the number of true classes (e.g. 20 clusters for MNIST). I like that the author(s) stick to 10 clusters for MNIST, but for comparison I would have liked to see a KNN generated accuracy alongside their equation 19 based accuracy. It looks like the author(s) implemented VADe for STL-10 based on table 1. If so, it seems they could easily implement equation 19 for their STL-10 value for VADe. If not, please correct this. Not to belabor further, but I would really like to see a table 2 that reports both equation 19 and KNN accuracies when available. Namely, the author(s) should report both values for their model and can leave equation 19 accuracies blank for reported values.\n\nLastly, I always raise an eyebrow when I see tSNE latent space representations. STL-10 was used to generate figure 6, where one needs dim(u) >> 2. However, MNIST can be well reconstructed using just 2 latent dimensions. In this case, tSNE is unnecessary. The author(s) state \"Colors are used to distinguish between clusters.\" This statement is unclear as to whether the author(s) are using the class label or learned latent cluster. If it is the former, figure 6 makes sense in that it shows misclassifications (i.e. red dots in the green cluster). However, if it is the latter then I am concerned tSNE is doing something weird.\n\nTo summarize, I enjoyed the paper. My only concern is novelty. Being the first to pair an existing model with an existing method, in my eyes, does not necessarily meet the ICLR bar. The author(s) seemingly achieve SoTA, but without KNN-based accuracies for their model it is hard to compare to cited works. Having these KNN results and error bars would strengthen their case substantially to: achieving SoTA for an existing model by being the first to pair it with an existing method.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2531/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2531/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ygtugur@gmail.com", "george.arvanitakis@huawei.com", "abdellatif.zaidi@u-pem.fr"], "title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "authors": ["Yigit Ugur", "George Arvanitakis", "Abdellatif Zaidi"], "pdf": "/pdf/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "abstract": "In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders\u2019 mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.", "keywords": ["clustering", "Variational Information Bottleneck", "Gaussian Mixture Model"], "paperhash": "ugur|variational_information_bottleneck_for_unsupervised_clustering_deep_gaussian_mixture_embedding", "original_pdf": "/attachment/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "_bibtex": "@misc{\nugur2020variational,\ntitle={Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding},\nauthor={Yigit Ugur and George Arvanitakis and Abdellatif Zaidi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxQ3gSKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2531/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2531/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575744807822, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2531/Reviewers"], "noninvitees": [], "tcdate": 1570237721535, "tmdate": 1575744807833, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2531/-/Official_Review"}}}, {"id": "rkgw2B4Jqr", "original": null, "number": 3, "cdate": 1571927471262, "ddate": null, "tcdate": 1571927471262, "tmdate": 1572972326321, "tddate": null, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "invitation": "ICLR.cc/2020/Conference/Paper2531/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper purposes to cluster data in an unsupervised manner that estimates the distribution with GMM in latent space instead of original data space. Also, to better describe the distribution of latent code, the authors involve VIB to constraint the latent code.\n\nThe whole pipeline is clear and makes sense. And the experiment proves it's effective. \n\nHowever, in my eyes, it's almost like an extension of VaDE which combines VIB and GMM too. The main difference is the authors use some hyperparameters to control the optimization of the whole model and make some more proper hypothesis. All of those make sense but may not contribute much to the research field."}, "signatures": ["ICLR.cc/2020/Conference/Paper2531/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2531/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ygtugur@gmail.com", "george.arvanitakis@huawei.com", "abdellatif.zaidi@u-pem.fr"], "title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "authors": ["Yigit Ugur", "George Arvanitakis", "Abdellatif Zaidi"], "pdf": "/pdf/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "abstract": "In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders\u2019 mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.", "keywords": ["clustering", "Variational Information Bottleneck", "Gaussian Mixture Model"], "paperhash": "ugur|variational_information_bottleneck_for_unsupervised_clustering_deep_gaussian_mixture_embedding", "original_pdf": "/attachment/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "_bibtex": "@misc{\nugur2020variational,\ntitle={Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding},\nauthor={Yigit Ugur and George Arvanitakis and Abdellatif Zaidi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxQ3gSKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2531/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2531/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575744807822, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2531/Reviewers"], "noninvitees": [], "tcdate": 1570237721535, "tmdate": 1575744807833, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2531/-/Official_Review"}}}, {"id": "BJx9ZbkrdS", "original": null, "number": 1, "cdate": 1570201858485, "ddate": null, "tcdate": 1570201858485, "tmdate": 1570201858485, "tddate": null, "forum": "HyxQ3gSKvr", "replyto": "Byl8Wur9Dr", "invitation": "ICLR.cc/2020/Conference/Paper2531/-/Official_Comment", "content": {"comment": "Since there is no exact calculation of the KL divergence between two Gaussian mixtures (in our case between a single component multivariate Gaussian and a Gaussian Mixture Model) we have to use an approximation. Our requirements for this approximation are 1) to be closed-form (and differentiable), in order to be able to back-propagate on the cost function by using stochastic gradient descent and 2) the approximation should not depend on the assignment probability $P_{C|X}$ because this probability is not known and as we mentioned in the paper the assumption $P_{C|X} = Q_{C|U}$, affects negatively the training process. The lower bound approximation of Hershey & Olsen (2007) full fills our requirements and works well enough in our experiments.\n\nIf there is another approximation of KL divergence between Gaussian mixtures, that satisfies the aforementioned requirements is definitely worth to give it a try to see how the performance of our proposed algorithm changes. \n\nFor sure the framework could be extended to other mixtures of distributions such as Bernoulli mixtures. The main modification will be the calculation of the new KL divergence term.\n\nI hope that we cover your questions.\n", "title": "Reply regarding KL approximation and  extendibility of the model"}, "signatures": ["ICLR.cc/2020/Conference/Paper2531/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2531/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ygtugur@gmail.com", "george.arvanitakis@huawei.com", "abdellatif.zaidi@u-pem.fr"], "title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "authors": ["Yigit Ugur", "George Arvanitakis", "Abdellatif Zaidi"], "pdf": "/pdf/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "abstract": "In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders\u2019 mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.", "keywords": ["clustering", "Variational Information Bottleneck", "Gaussian Mixture Model"], "paperhash": "ugur|variational_information_bottleneck_for_unsupervised_clustering_deep_gaussian_mixture_embedding", "original_pdf": "/attachment/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "_bibtex": "@misc{\nugur2020variational,\ntitle={Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding},\nauthor={Yigit Ugur and George Arvanitakis and Abdellatif Zaidi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxQ3gSKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxQ3gSKvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2531/Authors", "ICLR.cc/2020/Conference/Paper2531/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2531/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2531/Reviewers", "ICLR.cc/2020/Conference/Paper2531/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2531/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2531/Authors|ICLR.cc/2020/Conference/Paper2531/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139978, "tmdate": 1576860531418, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2531/Authors", "ICLR.cc/2020/Conference/Paper2531/Reviewers", "ICLR.cc/2020/Conference/Paper2531/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2531/-/Official_Comment"}}}, {"id": "Byl8Wur9Dr", "original": null, "number": 1, "cdate": 1569507326434, "ddate": null, "tcdate": 1569507326434, "tmdate": 1569518920436, "tddate": null, "forum": "HyxQ3gSKvr", "replyto": "HyxQ3gSKvr", "invitation": "ICLR.cc/2020/Conference/Paper2531/-/Public_Comment", "content": {"comment": "There must be other methods to approximate KL divergence b/w mixture distributions, is Variational the best? The Variational Approximation to KL divergence proof seemed to be true for general distributions, so can ur work be extended to a mixture distributions other than Gaussians, like Mixture of Bernoulli's (just an example)\n\n\n", "title": "Great Work, just a simple doubt"}, "signatures": ["~Pranav_Poduval1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Pranav_Poduval1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ygtugur@gmail.com", "george.arvanitakis@huawei.com", "abdellatif.zaidi@u-pem.fr"], "title": "Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding", "authors": ["Yigit Ugur", "George Arvanitakis", "Abdellatif Zaidi"], "pdf": "/pdf/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "abstract": "In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders\u2019 mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.", "keywords": ["clustering", "Variational Information Bottleneck", "Gaussian Mixture Model"], "paperhash": "ugur|variational_information_bottleneck_for_unsupervised_clustering_deep_gaussian_mixture_embedding", "original_pdf": "/attachment/9119343fd57e297acd3a2f2eb992c64a12353527.pdf", "_bibtex": "@misc{\nugur2020variational,\ntitle={Variational Information Bottleneck for Unsupervised Clustering: Deep Gaussian Mixture Embedding},\nauthor={Yigit Ugur and George Arvanitakis and Abdellatif Zaidi},\nyear={2020},\nurl={https://openreview.net/forum?id=HyxQ3gSKvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyxQ3gSKvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504178958, "tmdate": 1576860565066, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2531/Authors", "ICLR.cc/2020/Conference/Paper2531/Reviewers", "ICLR.cc/2020/Conference/Paper2531/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2531/-/Public_Comment"}}}], "count": 7}