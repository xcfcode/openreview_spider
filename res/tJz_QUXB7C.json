{"notes": [{"id": "tJz_QUXB7C", "original": "_7CQ6RNao6", "number": 2662, "cdate": 1601308294896, "ddate": null, "tcdate": 1601308294896, "tmdate": 1614985721580, "tddate": null, "forum": "tJz_QUXB7C", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generating Plannable Lifted Action Models for Visually Generated Logical Predicates", "authorids": ["~Masataro_Asai1"], "authors": ["Masataro Asai"], "keywords": ["Object-Centric Representation", "Planning", "Discrete VAE"], "abstract": "We propose FOSAE++, an unsupervised end-to-end neural system that generates a compact discrete state transition model (dynamics / action model) from raw visual observations. Our representation can be exported to Planning Domain Description Language (PDDL), allowing symbolic state-of-the-art classical planners to perform high-level task planning on raw observations. FOSAE++ expresses states and actions in First Order Logic (FOL), a superset of so-called object-centric representation. It is the first unsupervised neural system that fully supports FOL in PDDL action modeling, while existing systems are limited to continuous, propositional, or property-based representations, and/or require manually labeled input for actions/predicates/propositions.", "one-sentence_summary": "From pixel data of objects, FOSAE++ generates a symbolic planning domain description generalized over objects. The system is fully neural and requires no manual tagging.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "asai|generating_plannable_lifted_action_models_for_visually_generated_logical_predicates", "pdf": "/pdf/8ae9f19fc92a7c11679d7d30c28d9415d3551a05.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=d1A6zmIss", "_bibtex": "@misc{\nasai2021generating,\ntitle={Generating Plannable Lifted Action Models for Visually Generated Logical Predicates},\nauthor={Masataro Asai},\nyear={2021},\nurl={https://openreview.net/forum?id=tJz_QUXB7C}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zryV4TEr162", "original": null, "number": 1, "cdate": 1610040419882, "ddate": null, "tcdate": 1610040419882, "tmdate": 1610474018530, "tddate": null, "forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "invitation": "ICLR.cc/2021/Conference/Paper2662/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper described a system for deriving PDDL (Planning Domain Description Language) operator descriptions from unlabeled visual image pairs.  The goal is to construct STRIPS-like descriptions with preconditions, add lists and delete lists for operators that can explain the state transitions seem in the image pairs.  This is combined with a neural form of inductive logic programming (ILP) which historically performs a similar task from logical descriptions rather than images.\n\nWhile this topic is appropriate for ICLR, the work is fairly incremental and the experiments are limited to the 8-puzzle which, according to a reviewer, is the easiest of the tasks.  In spite of boarder line scores, no rebuttal was provided.  So my recommendation is to not accept the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Plannable Lifted Action Models for Visually Generated Logical Predicates", "authorids": ["~Masataro_Asai1"], "authors": ["Masataro Asai"], "keywords": ["Object-Centric Representation", "Planning", "Discrete VAE"], "abstract": "We propose FOSAE++, an unsupervised end-to-end neural system that generates a compact discrete state transition model (dynamics / action model) from raw visual observations. Our representation can be exported to Planning Domain Description Language (PDDL), allowing symbolic state-of-the-art classical planners to perform high-level task planning on raw observations. FOSAE++ expresses states and actions in First Order Logic (FOL), a superset of so-called object-centric representation. It is the first unsupervised neural system that fully supports FOL in PDDL action modeling, while existing systems are limited to continuous, propositional, or property-based representations, and/or require manually labeled input for actions/predicates/propositions.", "one-sentence_summary": "From pixel data of objects, FOSAE++ generates a symbolic planning domain description generalized over objects. The system is fully neural and requires no manual tagging.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "asai|generating_plannable_lifted_action_models_for_visually_generated_logical_predicates", "pdf": "/pdf/8ae9f19fc92a7c11679d7d30c28d9415d3551a05.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=d1A6zmIss", "_bibtex": "@misc{\nasai2021generating,\ntitle={Generating Plannable Lifted Action Models for Visually Generated Logical Predicates},\nauthor={Masataro Asai},\nyear={2021},\nurl={https://openreview.net/forum?id=tJz_QUXB7C}\n}"}, "tags": [], "invitation": {"reply": {"forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040419869, "tmdate": 1610474018510, "id": "ICLR.cc/2021/Conference/Paper2662/-/Decision"}}}, {"id": "IPIoGgCo6Fv", "original": null, "number": 1, "cdate": 1603752290839, "ddate": null, "tcdate": 1603752290839, "tmdate": 1605024158729, "tddate": null, "forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "invitation": "ICLR.cc/2021/Conference/Paper2662/-/Official_Review", "content": {"title": "A solid contribution, but with sparse results", "review": "This work presents FOSAE++, an end-to-end system capable of producing \"lifted\" action models provided only bounding box annotations of image pairs before and after an unknown action is executed. Building on recent work in the space, the primary contribution of this work is to generate PDDL action rules. To accomplish this, the authors introduce novel 'params' function that use the Gumbel-Softmax function to implement a differentiable mechanism for selecting which entities are relevant to the current action and feeds them into the new 'bind' and 'unbind' functions that select those elements in the tensor predicting their relevance. Overall, this work is a meaningful contribution in the direction of generated lifted action models without direct labeled data.\n\nThe biggest flaw in the paper is the rather sparse results section. Additionally, I do somewhat take issue with the statement in 4.2 that \"due to the time constraint\" planning experiments beyond the 8-Puzzle domain. Though I appreciate the author's candor in this regard, at the moment the paper is rather weakened by the lack of inclusion of additional planning experiments, particularly since the 8-Puzzle domain is arguably the easiest domain from which one might generated a lifted action representation, due to the black background surrounding the digits. Seeing planning experiments in the Sokoban domain would greatly strengthen the paper. The reconstructed Sokoban environments in Figure 4 look rather good, though the slightly shifted tiles in the reconstructed scene results in black lines/gaps between the cells, raising questions about the ability of the system to perform planning in these domains.\n\nRelatedly, though I appreciate that the authors are space-constrained at the moment, the results section (and in particular the planning section) is quite short and lacking in detail. More detail, including discussion of the limitations of this approach, would strengthen the paper.\n\nThe Appendix is incredibly thorough and a welcome addition to the paper. It provides helpful additional content that, while not necessary for understanding the paper, aid in understanding and implementation.\n\nSmaller comments:\n- The caption for Fig. 2 should be extended or more annotations should be added to the figure itself. Right now, it is only clear from the body text how these components are used or where these models were introduced in other papers. This change would help clarity.\n- A passage addressing the limitations of the proposed system would be a welcome addition as well. In particular, there seems to be a general assumption that only the regions within the provided bounding boxes will change after an action is executed, something this is not generally true (nor is true in the Photo-realistic Blocksworld domain, in which shadows can change outside the bounding boxes).\n- The caption in Fig. 3 is (I think) incorrect: the final note should read that `move` was simplified to `?from, ?to`.\n- Fig. 12 in the appendix is a duplicate of another figure earlier in the paper. It should be replaced with the _actual_ figure before publication.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2662/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2662/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Plannable Lifted Action Models for Visually Generated Logical Predicates", "authorids": ["~Masataro_Asai1"], "authors": ["Masataro Asai"], "keywords": ["Object-Centric Representation", "Planning", "Discrete VAE"], "abstract": "We propose FOSAE++, an unsupervised end-to-end neural system that generates a compact discrete state transition model (dynamics / action model) from raw visual observations. Our representation can be exported to Planning Domain Description Language (PDDL), allowing symbolic state-of-the-art classical planners to perform high-level task planning on raw observations. FOSAE++ expresses states and actions in First Order Logic (FOL), a superset of so-called object-centric representation. It is the first unsupervised neural system that fully supports FOL in PDDL action modeling, while existing systems are limited to continuous, propositional, or property-based representations, and/or require manually labeled input for actions/predicates/propositions.", "one-sentence_summary": "From pixel data of objects, FOSAE++ generates a symbolic planning domain description generalized over objects. The system is fully neural and requires no manual tagging.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "asai|generating_plannable_lifted_action_models_for_visually_generated_logical_predicates", "pdf": "/pdf/8ae9f19fc92a7c11679d7d30c28d9415d3551a05.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=d1A6zmIss", "_bibtex": "@misc{\nasai2021generating,\ntitle={Generating Plannable Lifted Action Models for Visually Generated Logical Predicates},\nauthor={Masataro Asai},\nyear={2021},\nurl={https://openreview.net/forum?id=tJz_QUXB7C}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2662/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091262, "tmdate": 1606915778157, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2662/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2662/-/Official_Review"}}}, {"id": "46YDsi-KYm", "original": null, "number": 2, "cdate": 1603767458845, "ddate": null, "tcdate": 1603767458845, "tmdate": 1605024158665, "tddate": null, "forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "invitation": "ICLR.cc/2021/Conference/Paper2662/-/Official_Review", "content": {"title": "Review of Generating Plannable Lifted Action Models for Visually Generated Logical Predicates", "review": "## Summary\n \nThis paper builds on Latplan (Asai & Fukunaga 2018) and proposes FOSAE++, a lifted action model expressed in First-Order Logic (FOL), which differs from object-centric representation by being generalized over objects and environments. According to the authors, FOSAE++ is the first system that : \n* uses a white-box action model which is trivially convertible to STRIPS formalism, \n* is invariant to the number/order of objects, \n* uses unsupervised generation of multi-arity predicate symbols.\n\nFOSAE++ is demonstrated on three environments: \n* Photo-realistic Blocksworld\n* MNIST 8-puzzle\n* Sokoban\nIn each environment the input is the set objects obtained using a domain-specific segmentation of the image.\n\n## Reason for score\n\nI'm sorry but by expertise in the domain of this paper is way too limited to be able to do a fair assessment. From the introduction I was not able to clearly understand the contributions of the paper. This is not necessarily due to the introduction, but rather to my lack of expertise in the domain. Many of the concepts are alien to me (PDDL, classical planning, lifted action models, object-centric representations, STRIPS). \n\nThat being said, I did find the introduction particularly opaque and it was difficult for me to clearly identify the contributions of the paper. I was not able to make sense of Table 1 and believe the authors could have done a better job of introducing the different aspects that distinguish their methods from previous work.\n\nI did give a good read to the rest of the paper but, again, am unable to comment deeply on the content. I did find the figures particularly dense and difficult to parse. I was also surprised that more than 2 pages were dedicated to background and previous work. Despite that long exposition, I would have to read a lot of the referenced work in order to understand these preliminaries. Again, this may be due to my lack of expertise however I can't help but think that the authors could have done a better job at bringing me up to speed.\n\nAlso, I'm a bit surprised that the authors claim to have achieved \"unsupervised end-to-end neural system that generates a\ncompact discrete state transition model (dynamics / action model) from raw visual observations.\" given that they rely on domain-specific segmentation code to extract objects, and that their technique only operate on these objects. They claim that \"in principle [the domain specific code] can be replaced by the output of object-detectors such as YOLO\" but I find this unconvincing.\n\n## Conclusion\n\nGiven my very limited understanding, I would only consider my rating as an educated guess based on a small number of details I was able to evaluate.", "rating": "5: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper2662/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2662/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Plannable Lifted Action Models for Visually Generated Logical Predicates", "authorids": ["~Masataro_Asai1"], "authors": ["Masataro Asai"], "keywords": ["Object-Centric Representation", "Planning", "Discrete VAE"], "abstract": "We propose FOSAE++, an unsupervised end-to-end neural system that generates a compact discrete state transition model (dynamics / action model) from raw visual observations. Our representation can be exported to Planning Domain Description Language (PDDL), allowing symbolic state-of-the-art classical planners to perform high-level task planning on raw observations. FOSAE++ expresses states and actions in First Order Logic (FOL), a superset of so-called object-centric representation. It is the first unsupervised neural system that fully supports FOL in PDDL action modeling, while existing systems are limited to continuous, propositional, or property-based representations, and/or require manually labeled input for actions/predicates/propositions.", "one-sentence_summary": "From pixel data of objects, FOSAE++ generates a symbolic planning domain description generalized over objects. The system is fully neural and requires no manual tagging.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "asai|generating_plannable_lifted_action_models_for_visually_generated_logical_predicates", "pdf": "/pdf/8ae9f19fc92a7c11679d7d30c28d9415d3551a05.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=d1A6zmIss", "_bibtex": "@misc{\nasai2021generating,\ntitle={Generating Plannable Lifted Action Models for Visually Generated Logical Predicates},\nauthor={Masataro Asai},\nyear={2021},\nurl={https://openreview.net/forum?id=tJz_QUXB7C}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2662/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091262, "tmdate": 1606915778157, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2662/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2662/-/Official_Review"}}}, {"id": "ZXpN5AGj_bq", "original": null, "number": 3, "cdate": 1603885280880, "ddate": null, "tcdate": 1603885280880, "tmdate": 1605024158598, "tddate": null, "forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "invitation": "ICLR.cc/2021/Conference/Paper2662/-/Official_Review", "content": {"title": "Inspiring paper on a less studied problem", "review": "This paper addresses the problem of learning dynamics model directly from raw sensory inputs. The authors propose an unsupervised end-to-end model that can perform high-level tasks planning on raw observations. This work extends Asai et al. 2020, 2019 etc, and with improved symbol generation and lifted PDDL. The authors follow the experimental setup as seen in prior work, where three artificial environments (blocksworld, MNIST 8-puzzle, and sokoban) are used for planning. \n\nPros.\n+ interesting and important problem of high-level tasks planning with potentially practical usage\n+ detailed analysis of experiments in the supplementary\n+ the model seems to be working really well without much supervision\n+ training details are provided for re-implementation\n\nCons. \n- The notations used throughout the paper can be a little bit confusing. Readers who have not read supplementary and prior works may find it difficult to understand. \n- Experiments are limited to relatively simple environments. It would be interesting to see how the proposed model can be applied to the more realistic scenarios as seen in Konidaris et al. \n\nOverall the reviewer thinks this paper addresses an important yet less studied topic, and will likely inspire follow-up works towards this direction. While the paper has some problems in terms of presentation, the reviewer recommends its acceptance. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2662/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2662/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generating Plannable Lifted Action Models for Visually Generated Logical Predicates", "authorids": ["~Masataro_Asai1"], "authors": ["Masataro Asai"], "keywords": ["Object-Centric Representation", "Planning", "Discrete VAE"], "abstract": "We propose FOSAE++, an unsupervised end-to-end neural system that generates a compact discrete state transition model (dynamics / action model) from raw visual observations. Our representation can be exported to Planning Domain Description Language (PDDL), allowing symbolic state-of-the-art classical planners to perform high-level task planning on raw observations. FOSAE++ expresses states and actions in First Order Logic (FOL), a superset of so-called object-centric representation. It is the first unsupervised neural system that fully supports FOL in PDDL action modeling, while existing systems are limited to continuous, propositional, or property-based representations, and/or require manually labeled input for actions/predicates/propositions.", "one-sentence_summary": "From pixel data of objects, FOSAE++ generates a symbolic planning domain description generalized over objects. The system is fully neural and requires no manual tagging.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "asai|generating_plannable_lifted_action_models_for_visually_generated_logical_predicates", "pdf": "/pdf/8ae9f19fc92a7c11679d7d30c28d9415d3551a05.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=d1A6zmIss", "_bibtex": "@misc{\nasai2021generating,\ntitle={Generating Plannable Lifted Action Models for Visually Generated Logical Predicates},\nauthor={Masataro Asai},\nyear={2021},\nurl={https://openreview.net/forum?id=tJz_QUXB7C}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tJz_QUXB7C", "replyto": "tJz_QUXB7C", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2662/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538091262, "tmdate": 1606915778157, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2662/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2662/-/Official_Review"}}}], "count": 5}