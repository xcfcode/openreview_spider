{"notes": [{"id": "ryG8UsR5t7", "original": "HylownL_Ym", "number": 178, "cdate": 1538087758133, "ddate": null, "tcdate": 1538087758133, "tmdate": 1545355378943, "tddate": null, "forum": "ryG8UsR5t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1l89Hh-eN", "original": null, "number": 1, "cdate": 1544828302256, "ddate": null, "tcdate": 1544828302256, "tmdate": 1545354530233, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Meta_Review", "content": {"metareview": "Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper178/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353309881, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper178/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper178/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper178/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353309881}}}, {"id": "Byejusor0Q", "original": null, "number": 5, "cdate": 1542990706864, "ddate": null, "tcdate": 1542990706864, "tmdate": 1542990706864, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "SkgNvoLlCm", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "content": {"title": "More experiments ", "comment": "Thanks for your reply. Without knowing the performance on the new experiments my opinion of the paper is however unchanged, so I will maintain the score as is."}, "signatures": ["ICLR.cc/2019/Conference/Paper178/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper178/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605010, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryG8UsR5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper178/Authors|ICLR.cc/2019/Conference/Paper178/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605010}}}, {"id": "rJenao8lC7", "original": null, "number": 4, "cdate": 1542642628115, "ddate": null, "tcdate": 1542642628115, "tmdate": 1542642628115, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "ryeyZSlc37", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "content": {"title": "More analysis and experiments", "comment": "Thank you for your time and critical feedback.\nThe score you propose is indeed interesting and we would need to look further into it. However, it seems that its main disadvantage compared to our proposed metric is the lack of robustness to outliers. Indeed, there is no direct way to handle a known amount of outliers and the correlation itself is error-prone and will be sensitive to erroneous data. \n\nWe believe that robustness to outliers is of major importance, especially for problems where the learning is based on potentially erroneous ground-truth. Depth estimation is one such typical case since it involves sensor measurement uncertainty. To highlight this issue and also to address your concern about our qualitative evaluation made on a single image, we plan to add several images in the appendix with and without ground truth problems.\n\nWe finally would like to respectfully disagree concerning the novelty of the work since there is no reliable metric used in the deep learning community to assess the quality of a predictive uncertainty with respect to the true error, while the need for it is growing.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605010, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryG8UsR5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper178/Authors|ICLR.cc/2019/Conference/Paper178/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605010}}}, {"id": "SkgNvoLlCm", "original": null, "number": 3, "cdate": 1542642523768, "ddate": null, "tcdate": 1542642523768, "tmdate": 1542642523768, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "SygUVxWc3X", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "content": {"title": "More experiments", "comment": "Thank you for your considerate review and insightful comments. \nAbout the concepts of calibration and sharpness, we will follow your advice and introduce them earlier in the paper to be more easily understandable for a broader audience.\n\nWe also understand your concerns that the experiments do not support strongly enough our theoretical claims. As proposed by the reviewer 1, we plan to conduct similar experiments on several other datasets from the UCI benchmark. In this way, we should be able to demonstrate that our findings generalize well to other real-life setups.\n\nFinally, your last point is also very relevant. We indeed disregarded the means of the predictive uncertainty methods in our tests for comparison purposes. Instead, we use a unique prediction for all methods, represented in Figure 4. and obtained with the standard network described in section 7.1. However, we also computed all the results using both the mean and variance for each method to unbias the comparisons. Because the results were not mixed up, we originally decided to omit this part for simplicity and clarity. We realize that we should have left it and we will reintroduce the corresponding section in our next update.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605010, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryG8UsR5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper178/Authors|ICLR.cc/2019/Conference/Paper178/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605010}}}, {"id": "SyxT09LeAX", "original": null, "number": 2, "cdate": 1542642389456, "ddate": null, "tcdate": 1542642389456, "tmdate": 1542642488406, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "BkgP0S052m", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "content": {"title": "More references and experiments", "comment": "Thank you for your helpful review and constructive comments.\nBecause we proposed a new evaluation metric, we were concerned by the question of how to compare very different predictive uncertainty. For example, the use of Bayesian deep learning techniques leads to compute variations that don\u2019t have meaning in terms of absolute error. That is the reason why we took sides for the insensitivity to scale.\n\nThank you also for pointing out this very relevant ICML paper that we were not aware of. We still need to study it further but it seems that they take into account calibration and sharpness separately, while we were driven by the will of assessing both these properties jointly. For example, the sharpness score they propose in section 3.5 seem very close to MeRCI as it corresponds to the mean variance of the evaluated method. However this only assesses the sharpness of the predictive uncertainty and do not allow to compare techniques with different ranges of uncertainties.\n\nWe indeed made incorrect claims about some prior work and apologize for this mistake. We rather wanted to point out that systematic assessment of the uncertainty estimate itself was largely underestimated in the deep learning community. We will rephrase this properly in the next update.\n\nFinally we plan to use several other datasets from the UCI regression benchmark, as you advised. We believe that proving the generalization of the metrics behavior will support our claims more strongly. We will also add several other qualitative analysis in the appendix for a fairer comparison. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605010, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryG8UsR5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper178/Authors|ICLR.cc/2019/Conference/Paper178/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605010}}}, {"id": "BJeLx9UeRX", "original": null, "number": 1, "cdate": 1542642157944, "ddate": null, "tcdate": 1542642157944, "tmdate": 1542642157944, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "content": {"title": "Preface", "comment": "Foremost, we would like to thank the reviewers for their time and thorough reviews. Globally, all reviewers unveiled similar issues to the submitted work. We will address each specific point in detailed responses below. Just as a recall and to introduce our discussion, we would like to insist on the properties we wanted our metric to have:\n\n       \u2022\tInsensitivity to scale, so that we can compare predictive uncertainty techniques with various ranges that are often not in relation with the absolute error;\n       \u2022\tRobustness to outliers, so that we can fairly evaluate tasks for which data are known to be noisy;\n       \u2022\tEasy applicability to deep learning, where we usually get two scalars to define the couple prediction/uncertainty instead of a whole distribution;\n       \u2022\tRepresentativity of the true error, in the sense that we want it to evaluate jointly calibration and sharpness, ensuring that the estimated uncertainty varies with the true error.\n\nWe will also notify each submission updates we propose below.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605010, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryG8UsR5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper178/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper178/Authors|ICLR.cc/2019/Conference/Paper178/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers", "ICLR.cc/2019/Conference/Paper178/Authors", "ICLR.cc/2019/Conference/Paper178/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605010}}}, {"id": "BkgP0S052m", "original": null, "number": 3, "cdate": 1541232079117, "ddate": null, "tcdate": 1541232079117, "tmdate": 1541534218781, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Review", "content": {"title": "Ok, but not good enough", "review": "The authors propose mean rescaled confidence interval (MERCI) as a way to measure the quality of predictive uncertainty for regression problems. The main idea is to rescale confidence intervals, and use average width of the confidence intervals as a measure for calibration. Due to the rescaling, the MERCI score is insensitive to the absolute scale; while this could be a feature in some cases, it can also be problematic in applications where the absolute scale of uncertainty matters. \n\nOverall, the current draft feels a bit preliminary. The current draft misses discussion of other relevant papers, makes some incorrect claims, and the experiments are a bit limited. I encourage the authors to revise and submit to a different venue. \n\t\nThere\u2019s a very relevant ICML 2018 paper on calibrating regression using similar idea: \nAccurate Uncertainties for Deep Learning Using Calibrated Regression\nhttps://arxiv.org/pdf/1807.00263.pdf\nCan you clarify if/how the proposed work differs from this? I\u2019d also like to see a discussion of calibration post-processing methods such as Platt scaling and isotonic regression.\n\nThe paper unfairly dismisses prior work by making factually incorrect claims, e.g. Section 2 claims\n\u201cIndeed, papers like (Hernandez-Lobato & Adams, 2015; Gal & Ghahramani, 2016; Lakshminarayanan et al., 2017; Kendall & Gal, 2017) propose quantitative evaluations on several datasets, those classically used for evaluating the task, but only compare their average test performances in terms of RMSE. It is the quality of the prediction which is measured, and not the quality of the estimated uncertainty. They also show some qualitative results, where maps of the estimated uncertainty are displayed as images and visually evaluated. Yet, to the best of our knowledge, the literature on deep neural networks does not propose any method for the quantitative evaluation of the uncertainty estimates.\u201d\nThis is incorrect.  To just name a few examples of prior work quantitatively evaluating the quality of uncertainty: (Hernandez-Lobato & Adams, 2015) and (Gal & Ghahramani, 2016) report log-likelihoods on regression tasks, (Lakshminarayanan et al. 2017) report log-likelihoods and Brier score on classification and regression tasks. There are many more examples. \n\nThe experiments are a bit limited. Figure 1 is a toy dataset and Table 2 / Figure 4 focus on a single test case which does not seem like a fair comparison of the different methods. The authors should at least compare their method to other work on the UCI regression benchmarks used by (Hernandez-Lobato & Adams, 2015).", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Review", "cdate": 1542234521380, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335665671, "tmdate": 1552335665671, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygUVxWc3X", "original": null, "number": 2, "cdate": 1541177389675, "ddate": null, "tcdate": 1541177389675, "tmdate": 1541534218579, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Review", "content": {"title": "Interesting paper for the deep learning community, but the experimental section is not convincing enough", "review": "This works presents an overview of different techniques to obtain uncertainty estimates for regression algorithms, as well as metrics to assess the quality of these uncertainty estimates.\nIt then introduces MeRCI, a novel metric that is more suitable for deep learning applications.\n\nBeing able to build algorithms that are good not only at making predictions, but also at reliably assessing the confidence of these predictions is fundamental in any application. While this is often a focus in many communities, in the deep learning community however this is not the case, so I really like that the authors of this paper want to raise awareness on these techniques. The paper is well written and I enjoyed reading it. \nI feel that to be more readable for a broader audience it would be relevant to introduce more in depth key concepts such as sharpness and calibration, an not just in a few lines as done in the end of page 2. \n\nWhile I found the theoretical explanation interesting, I feel that the experimental part does not support strongly enough the claims made in the paper. First of all, for this type of paper I would have expected more real-life experiments, and not just the monocular depth estimation one. This is in fact the only way to assess if the findings of the paper generalize.\nThen, I am not convinced that keeping the networks predictions fixed in all experiments is correct. The different predictive uncertainty methods return both a mean and a variance of the prediction, but it seems that you disregard the information on the mean in you tests. If I understood correctly, I would expect the absolute errors to change for each of the methods, so the comparisons in Figure 4 can be very misleading. \nWith which method did you obtain the predictions in Figure 4.c? \n\nTypos:\n- \"implies\" -> \"imply\" in first line of page 3\n- \"0. 2\" -> \"0.2\" in pag 6, also you should clarify if 0.2 refers to the fraction of units that are dropped or that are kept\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Review", "cdate": 1542234521380, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335665671, "tmdate": 1552335665671, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeyZSlc37", "original": null, "number": 1, "cdate": 1541174518626, "ddate": null, "tcdate": 1541174518626, "tmdate": 1541534218374, "tddate": null, "forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper178/Official_Review", "content": {"title": "Lacking novelty and rigor.", "review": "The main contribution of this paper is a new proposed score to evaluate models that yield uncertainty values for regression.\n\nAs constituted, the paper can not be published into one of the better ML conferences. The novelty here is very limited. Furthermore there are other weaknesses to the study.\n\nFirst, the stated goal of the \"metric\" is that \"reflects the correlation between the true error and the estimated uncertainty ... (and is) scale independent and robust to outliers.\" Given this goal (and the name of the paper) it is perplexing why the correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) of true error and predicted error (\\sigma_i) was not tried as baseline score. The correlation would have some \"scale independence\" and I'm sure that there are robust estimates (a simple thing would be to consider the median instead of mean, but there are probably more sophisticated approaches). This just feels like an obvious omission. If one wants to mix both predictive quality and correctness of uncertainty assessments then one could just scale the mean absolute error by the correlation: MAE/Corr, which would lead to a direct comparison to the  proposed MeRCI.\n\nSecond, the paper does a poor job of justifying MeRCI. On toy data MeRCI is justified by a confluence with existing scores. Then on the depth prediction task, where there are discrepancies among the scores, MeRCI is largely justified qualitatively  on a single image (Figure 4). A qualitative argument on a single instance in a single task can not cut it. The paper must put forth some systematic and more comprehensive comparison of scores.\n\nEven with the above issues resolved, the paper would have to do more for publication. I would want to see either some proof of a property of the proposed score(s), or to use the proposed score to inform training, etc.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper178/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR", "abstract": "As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. ", "keywords": ["evaluation metric", "predictive uncertainty", "deep learning"], "authorids": ["michel.moukari@unicaen.fr", "loic.simon@ensicaen.fr", "sylvaine.picard@safrangroup.com", "frederic.jurie@unicaen.fr"], "authors": ["michel moukari", "lo\u00efc simon", "sylvaine picard", "fr\u00e9d\u00e9ric jurie"], "TL;DR": "We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning", "pdf": "/pdf/7941286d780c0fd6c55ee421c9619038519cb8a9.pdf", "paperhash": "moukari|merci_a_new_metric_to_evaluate_the_correlation_between_predictive_uncertainty_and_true_error", "_bibtex": "@misc{\nmoukari2019merci,\ntitle={{MERCI}: A {NEW} {METRIC} {TO} {EVALUATE} {THE} {CORRELATION} {BETWEEN} {PREDICTIVE} {UNCERTAINTY} {AND} {TRUE} {ERROR}},\nauthor={michel moukari and lo\u00efc simon and sylvaine picard and fr\u00e9d\u00e9ric jurie},\nyear={2019},\nurl={https://openreview.net/forum?id=ryG8UsR5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper178/Official_Review", "cdate": 1542234521380, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryG8UsR5t7", "replyto": "ryG8UsR5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper178/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335665671, "tmdate": 1552335665671, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper178/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}