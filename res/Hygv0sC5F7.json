{"notes": [{"id": "Hygv0sC5F7", "original": "S1xAXZq5YX", "number": 903, "cdate": 1538087887027, "ddate": null, "tcdate": 1538087887027, "tmdate": 1545355418752, "tddate": null, "forum": "Hygv0sC5F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?", "abstract": "We study the implicit bias of gradient descent methods in solving a binary classification problem over a linearly separable dataset. The classifier is described by a nonlinear ReLU model and the objective function adopts the exponential loss function. We first characterize the landscape of the loss function and show that there can exist spurious asymptotic local minima besides asymptotic global minima. We then show that gradient descent (GD) can converge to either a global or a local max-margin direction, or may diverge from the desired max-margin direction in a general context. For stochastic gradient descent (SGD), we show that it converges in expectation to either the global or the local max-margin direction if SGD converges. We further explore the implicit bias of these algorithms in learning a multi-neuron network under certain stationary conditions, and show that the learned classifier maximizes the margins of each sample pattern partition under the ReLU activation.", "keywords": ["gradient method", "max-margin", "ReLU model"], "authorids": ["xu.3260@osu.edu", "zhou.1172@osu.edu", "ji.367@osu.edu", "liang.889@osu.edu"], "authors": ["Tengyu Xu", "Yi Zhou", "Kaiyi Ji", "Yingbin Liang"], "TL;DR": "We study the implicit bias of gradient methods in solving a binary classification problem with nonlinear ReLU models.", "pdf": "/pdf/f4b7688b3c9bea0e4a93a377dad1dccc965aab65.pdf", "paperhash": "xu|when_will_gradient_methods_converge_to_maxmargin_classifier_under_relu_models", "_bibtex": "@misc{\nxu2019when,\ntitle={When Will Gradient Methods Converge to Max-margin Classifier under Re{LU} Models?},\nauthor={Tengyu Xu and Yi Zhou and Kaiyi Ji and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygv0sC5F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1eotByYy4", "original": null, "number": 1, "cdate": 1544250755457, "ddate": null, "tcdate": 1544250755457, "tmdate": 1545354496365, "tddate": null, "forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper903/Meta_Review", "content": {"metareview": "The reviewers and AC note the following potential weaknesses: 1) the proof techniques largley follow from previous work on linear models 2) it\u2019s not clear how signficant it is to analyze a one-neuron ReLU model for linearly separable data. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper903/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper903/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?", "abstract": "We study the implicit bias of gradient descent methods in solving a binary classification problem over a linearly separable dataset. The classifier is described by a nonlinear ReLU model and the objective function adopts the exponential loss function. We first characterize the landscape of the loss function and show that there can exist spurious asymptotic local minima besides asymptotic global minima. We then show that gradient descent (GD) can converge to either a global or a local max-margin direction, or may diverge from the desired max-margin direction in a general context. For stochastic gradient descent (SGD), we show that it converges in expectation to either the global or the local max-margin direction if SGD converges. We further explore the implicit bias of these algorithms in learning a multi-neuron network under certain stationary conditions, and show that the learned classifier maximizes the margins of each sample pattern partition under the ReLU activation.", "keywords": ["gradient method", "max-margin", "ReLU model"], "authorids": ["xu.3260@osu.edu", "zhou.1172@osu.edu", "ji.367@osu.edu", "liang.889@osu.edu"], "authors": ["Tengyu Xu", "Yi Zhou", "Kaiyi Ji", "Yingbin Liang"], "TL;DR": "We study the implicit bias of gradient methods in solving a binary classification problem with nonlinear ReLU models.", "pdf": "/pdf/f4b7688b3c9bea0e4a93a377dad1dccc965aab65.pdf", "paperhash": "xu|when_will_gradient_methods_converge_to_maxmargin_classifier_under_relu_models", "_bibtex": "@misc{\nxu2019when,\ntitle={When Will Gradient Methods Converge to Max-margin Classifier under Re{LU} Models?},\nauthor={Tengyu Xu and Yi Zhou and Kaiyi Ji and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygv0sC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper903/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353042515, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper903/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper903/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper903/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353042515}}}, {"id": "SJg80NOj2Q", "original": null, "number": 3, "cdate": 1541272781898, "ddate": null, "tcdate": 1541272781898, "tmdate": 1541533592782, "tddate": null, "forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper903/Official_Review", "content": {"title": "A theoretical paper with very stringent assumptions.", "review": "This paper considers the binary classification problem with exponential loss and ReLu activation function (single neuron). The authors characterize the asymptotic loss landscape by three different types of critical points. They prove that gradient descent (GD) will result in four different regions and provide convergence rates for GD to converge to an asymptotic global minimum, asymptotic local minimum and local minimum under certain assumptions. The authors also provide convergence results for stochastic gradient descent (SGD) and provide extensions to leaky ReLu activation and multi-neuron networks. The paper is well written and the results are mostly clearly presented. This paper mostly follows the line of research by Soudry et al. (2017, 2018), while it has its own merit due to the ReLu activation function considered. However, there are many strong assumptions that are not carefully verified and I really have concerns about the contribution of this paper since they simplify their analysis and results merely by imposing stringent conditions. In particular, I have the following major comments about the paper:\n\n1.\tIn the definition of max-margin direction, why you use \\argmin_{w} max_{i} (w^{\\top}x_i)? It seems to me that the definition should be \\argmax_{w} min_{i} (w^{\\top}x_i). This definition keeps appearing in multiple places in the main paper. \n2.\tIn the proof of Theorem 3.2, I am confused by the argument of the case that \\hat w^{+} is not in the linearly separable region. More clarification is needed to make the proof rigorous.\n3.\tIn the analysis of Theorem 3.3 and 3.4, the authors make a very stringent assumption that the iterate w_t staying in linear separable region for all t>\\mathcal{T}. This assumption seems too strong, which should be verified rather than imposed in analysis of SGD. Note that even the example shown in Proposition 2 is still very restrictive (you require all the positive examples or negative examples are very close to one another).\n4.\tFurthermore, in the analysis of SGD, the authors did not specify the assumption that \\hat w^{+} lies in the linear separable region, which is also required in this theorem and also very strong. Given such strong assumptions, the analytic results seem to be trivial and it is hard to evaluate the authors\u2019 contribution.  \n5.\tFor the convergence results of SGD, the current rate is derived on the distance between \\|E[w_t] - \\hat{w}\\|^2. Can you provide similar results for mean square error (E\\| w_t - \\hat{w} \\|^2)? \n6.\tIn multi-neuron case, the authors again make very strong assumptions that all the neurons have unchanging activation status. This is not easily achievable without careful characterization or other rigorous assumptions. Under such strong assumptions, the extension to multi-neuron again seems not very meaningful.\n\nOther minor comments:\n1.\tThe references are not correctly cited. For instance, please correct the use of parenthesis in \u201c\u2026 which is different from that in (Soudry et al., 2017, Corollary 8)\u201d and \u201c\u2026 hold for various other types of gradient-based algorithms Gunasekar et al. (2018)\u201d.\n2.\tThe sentence \u201c\u2026, which the nature of convergence is different from \u2026\u201d does not read well. Should it be \u201cwhere\u201d or \u201cof which\u201d?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper903/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?", "abstract": "We study the implicit bias of gradient descent methods in solving a binary classification problem over a linearly separable dataset. The classifier is described by a nonlinear ReLU model and the objective function adopts the exponential loss function. We first characterize the landscape of the loss function and show that there can exist spurious asymptotic local minima besides asymptotic global minima. We then show that gradient descent (GD) can converge to either a global or a local max-margin direction, or may diverge from the desired max-margin direction in a general context. For stochastic gradient descent (SGD), we show that it converges in expectation to either the global or the local max-margin direction if SGD converges. We further explore the implicit bias of these algorithms in learning a multi-neuron network under certain stationary conditions, and show that the learned classifier maximizes the margins of each sample pattern partition under the ReLU activation.", "keywords": ["gradient method", "max-margin", "ReLU model"], "authorids": ["xu.3260@osu.edu", "zhou.1172@osu.edu", "ji.367@osu.edu", "liang.889@osu.edu"], "authors": ["Tengyu Xu", "Yi Zhou", "Kaiyi Ji", "Yingbin Liang"], "TL;DR": "We study the implicit bias of gradient methods in solving a binary classification problem with nonlinear ReLU models.", "pdf": "/pdf/f4b7688b3c9bea0e4a93a377dad1dccc965aab65.pdf", "paperhash": "xu|when_will_gradient_methods_converge_to_maxmargin_classifier_under_relu_models", "_bibtex": "@misc{\nxu2019when,\ntitle={When Will Gradient Methods Converge to Max-margin Classifier under Re{LU} Models?},\nauthor={Tengyu Xu and Yi Zhou and Kaiyi Ji and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygv0sC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper903/Official_Review", "cdate": 1542234350481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper903/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335829380, "tmdate": 1552335829380, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper903/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgRSG1t3X", "original": null, "number": 2, "cdate": 1541104198152, "ddate": null, "tcdate": 1541104198152, "tmdate": 1541533592543, "tddate": null, "forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper903/Official_Review", "content": {"title": "Importance of ReLU networks and max-margin used in this paper are unclear.", "review": "Recently, the implicit bias where gradient descent converges the max-margin classifier was shown for linear models without an explicit regularization.\nThis paper tries to extend this result to ReLU network, which is more challenging because of the non-convexity.\nMoreover, a similar property of stochastic gradient descent is also discussed.\n\nThe implicit bias is a key property to ensure the superior performance of over-parameterized models, hence this line of research is also important.\nHowever, I think there are several concerns as summarized below.\n\n1. I'm not sure about the significance of the ReLU model (P) considered in the paper.\nIndeed, the problem (P) is challenging, but an obtained model is linear defined by $w$.\nTherefore, an advantage of this model over linear models is unclear.\n\nMoreover, since the max-margin in this paper is defined by using part of dataset and it is different from the conventional max-margin, the generalization guarantees are not ensured by the margin theory.\nTherefore, I cannot figure out the importance of an implicit bias in this setting (, which ensures the convergence to this modified max-margin solution).\nIn addition, the definition of the max-margin seems to be incorrect: argmin max -> argmax min.\n\n2. Proposition 1 (variance bound) gives a bound on the sum of norms of stochastic gradients.\nHowever, I think this bound is obvious because stochastic gradients of the ReLU model (P) are uniformly bounded by the ReLU activation.\nCombining this boundedness and decreasing learning rates, the bound in Proposition 1 can be obtained immediately.\nMoreover, the validity of an assumption on $w_t$ made in the proposition should be discussed.\n\n3. Lemma F.2 is key to show the main theorem, but I wonder whether this lemma is correct.\nI think the third equation in the proof seems to be incorrect.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper903/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?", "abstract": "We study the implicit bias of gradient descent methods in solving a binary classification problem over a linearly separable dataset. The classifier is described by a nonlinear ReLU model and the objective function adopts the exponential loss function. We first characterize the landscape of the loss function and show that there can exist spurious asymptotic local minima besides asymptotic global minima. We then show that gradient descent (GD) can converge to either a global or a local max-margin direction, or may diverge from the desired max-margin direction in a general context. For stochastic gradient descent (SGD), we show that it converges in expectation to either the global or the local max-margin direction if SGD converges. We further explore the implicit bias of these algorithms in learning a multi-neuron network under certain stationary conditions, and show that the learned classifier maximizes the margins of each sample pattern partition under the ReLU activation.", "keywords": ["gradient method", "max-margin", "ReLU model"], "authorids": ["xu.3260@osu.edu", "zhou.1172@osu.edu", "ji.367@osu.edu", "liang.889@osu.edu"], "authors": ["Tengyu Xu", "Yi Zhou", "Kaiyi Ji", "Yingbin Liang"], "TL;DR": "We study the implicit bias of gradient methods in solving a binary classification problem with nonlinear ReLU models.", "pdf": "/pdf/f4b7688b3c9bea0e4a93a377dad1dccc965aab65.pdf", "paperhash": "xu|when_will_gradient_methods_converge_to_maxmargin_classifier_under_relu_models", "_bibtex": "@misc{\nxu2019when,\ntitle={When Will Gradient Methods Converge to Max-margin Classifier under Re{LU} Models?},\nauthor={Tengyu Xu and Yi Zhou and Kaiyi Ji and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygv0sC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper903/Official_Review", "cdate": 1542234350481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper903/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335829380, "tmdate": 1552335829380, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper903/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkx8w0vw3X", "original": null, "number": 1, "cdate": 1541008989990, "ddate": null, "tcdate": 1541008989990, "tmdate": 1541533592345, "tddate": null, "forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper903/Official_Review", "content": {"title": "Review", "review": "This paper studies ReLU model, or equivalently, one-layer-one-neuron model, for the classification problem. This paper shows if the data is linearly separable, gradient descent may converge to either a global minimum or a sub-optimal local minimum, or diverges. This paper further studies the implicit bias induced by GD and SGD and shows if they converge, they can have a maximum margin solution. \n\nComments:\n1. Using ReLU model for linearly separable data doesn't make sense to me. When ReLU is used, I expect some more complicated separable condition. \n2. This paper only studies one-layer-one-neuron model, which is a very restricted setting. It's hard to see how this result can be generalized to the multiple-neuron case.\n3. The analysis follows closely with previous work in studying the implicit bias for linear models.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper903/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?", "abstract": "We study the implicit bias of gradient descent methods in solving a binary classification problem over a linearly separable dataset. The classifier is described by a nonlinear ReLU model and the objective function adopts the exponential loss function. We first characterize the landscape of the loss function and show that there can exist spurious asymptotic local minima besides asymptotic global minima. We then show that gradient descent (GD) can converge to either a global or a local max-margin direction, or may diverge from the desired max-margin direction in a general context. For stochastic gradient descent (SGD), we show that it converges in expectation to either the global or the local max-margin direction if SGD converges. We further explore the implicit bias of these algorithms in learning a multi-neuron network under certain stationary conditions, and show that the learned classifier maximizes the margins of each sample pattern partition under the ReLU activation.", "keywords": ["gradient method", "max-margin", "ReLU model"], "authorids": ["xu.3260@osu.edu", "zhou.1172@osu.edu", "ji.367@osu.edu", "liang.889@osu.edu"], "authors": ["Tengyu Xu", "Yi Zhou", "Kaiyi Ji", "Yingbin Liang"], "TL;DR": "We study the implicit bias of gradient methods in solving a binary classification problem with nonlinear ReLU models.", "pdf": "/pdf/f4b7688b3c9bea0e4a93a377dad1dccc965aab65.pdf", "paperhash": "xu|when_will_gradient_methods_converge_to_maxmargin_classifier_under_relu_models", "_bibtex": "@misc{\nxu2019when,\ntitle={When Will Gradient Methods Converge to Max-margin Classifier under Re{LU} Models?},\nauthor={Tengyu Xu and Yi Zhou and Kaiyi Ji and Yingbin Liang},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygv0sC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper903/Official_Review", "cdate": 1542234350481, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygv0sC5F7", "replyto": "Hygv0sC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper903/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335829380, "tmdate": 1552335829380, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper903/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}