{"notes": [{"id": "Hye5TaVtDH", "original": "rJelUWWdDB", "number": 828, "cdate": 1569439169762, "ddate": null, "tcdate": 1569439169762, "tmdate": 1577168256801, "tddate": null, "forum": "Hye5TaVtDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "scY8IKeJyh", "original": null, "number": 1, "cdate": 1576798707260, "ddate": null, "tcdate": 1576798707260, "tmdate": 1576800929077, "tddate": null, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "invitation": "ICLR.cc/2020/Conference/Paper828/-/Decision", "content": {"decision": "Reject", "comment": "This paper introduces a novel architecture and loss for estimating PSD matrices using neural networks.  There is some theoretical justification for the architecture, and a small-scale but encouraging experiment.\n\nOverall, I think there is a sensible contribution here, but there are so many architectural and computational choices presented together at once that it's hard to tell what the important parts are.\n\nThe main problems with this paper are:\n1) The scalability of the approach O(N^3)\n2) The derivation of the architecture and gradient computations wasn't clear about what choices were available and why.  Several alternative choices were mentioned but not evaluated.  I think the authors also need to improve their understanding of automatic differentiation.  Backprop through eigendecomposition is already available in most autodiff packages.  It was claimed that a certain kind of matrix derivative provided better generalization, which seems like a strong claim to make in general.\n3) The experimental setup seemed contrived, except for the heteroskedastic regression experiments, which lacked competitive baselines.  Why were the GP and MLPs homoskedastic?\n\nAs a matter of personal preference, I found that having 4 different \"H\"s differing only in font and capitalization for the network architecture was hard to keep track of.\n\nI agree that R1 had some unjustified comments and R2's review was contentless.  I apologize for these inadequate reviews. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728410, "tmdate": 1576800280821, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper828/-/Decision"}}}, {"id": "Hygwg9QjoB", "original": null, "number": 3, "cdate": 1573759470941, "ddate": null, "tcdate": 1573759470941, "tmdate": 1573759470941, "tddate": null, "forum": "Hye5TaVtDH", "replyto": "ByxIxKl6YS", "invitation": "ICLR.cc/2020/Conference/Paper828/-/Official_Comment", "content": {"title": "Thank you for reviewing our paper. ", "comment": "Thank you for reviewing our paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper828/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye5TaVtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference/Paper828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper828/Reviewers", "ICLR.cc/2020/Conference/Paper828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper828/Authors|ICLR.cc/2020/Conference/Paper828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165592, "tmdate": 1576860544611, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference/Paper828/Reviewers", "ICLR.cc/2020/Conference/Paper828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper828/-/Official_Comment"}}}, {"id": "rklnOFmijS", "original": null, "number": 2, "cdate": 1573759348300, "ddate": null, "tcdate": 1573759348300, "tmdate": 1573759348300, "tddate": null, "forum": "Hye5TaVtDH", "replyto": "BJg1Bzy0Yr", "invitation": "ICLR.cc/2020/Conference/Paper828/-/Official_Comment", "content": {"title": "Thank you for reviewing our paper. We have addressed your comments in below. We hope our explanation helps clarifying the unclear points.  ", "comment": ">> First, in the synthetic data the mMLP with the l_QRE loss outperforms the l_quad loss with regards to both E_QRE and E_quad. Why does mMLP/l_QRE outperform on E_quad? As l_quad is actually designed to minimize this error, this is surprising and needs additional explanation.  \n\nNote that the evaluation is on the test set and not on the train set. When we use l_quad in mMLP, the generalization is poor on the test set. So it is very much expected from mMLP/l_QRE to outperform on E_QRE, E_quad and in fact any other relevant metric. This highlights that the mMLP/l_QRE has a good generalization on the test set which is really what matters. \n\n>> Additionally, there are two major changes between mMLP and the MLP.  One is the network structure, and the second is the trace being normalized; which change really induces the improvements in the performance? If the l_QRE loss was used in the MLP, would you get similar improvements?\n\nWe used l_QRE in MLP in Section 5.1.3. Table 2 shows the result of the evaluation. The result showed that MLP benefits from l_QRE but still under-performs in comparison to the mMLP/l_QRE. That suggests both the loss and the architecture are important.\n\n>> The network structure as a whole needs greater validation. A major difference in (3) is that the weight matrices W_l are applied as a both a left and right multiplication.  Given that the \\mathcal{H} operation symmetrizes and normalizes the matrix, a symmetric operation isn't strictly necessary here.  Using both multiplications leads to quadratic properties, which in my experience are less stable in the optimization.  Can the authors validate this structure versus the simpler structure of simply using left multiplications?  Or, in other words, is this weight multiplication structure helpful or do the benefits really come from the \\mathcal{H} operation.\n\nWhat you are suggesting is similar to the experiment that we carried out in Section 5.1.3. There, we compared the architecture of (3) with (18) which is a simpler architecture with only left multiplication. Table 2 summarizes the result of the experiment. \n\n>> I think that the heteroscedastic regression experiments don't evaluate on one of the key issues, which is uncertainty estimation. The trace-1 normalization is highly restrictive, so I...\n\nThere is a misunderstanding here. Please take a second look at equation (16) and (17). As you can see the information about the trace is all captured by the scale parameter \\eta. That means for the Gaussian case in (16), the Var[y|X] is \\eta\\Omega where \\omega is a trace-1 matrix and \\eta includes the trace term. This is stated in the line below equation (16) and below equation (17). So we are indeed learning the full covariance including the trace term. We are basically learning the full covariance as two individual terms: trace term \\eta and trace-1 matrix \\Omega. The model should capture uncertainties just as expected and trace-1 normalization imposes no restriction since, in addition to the trace-one matrix \\omega, we are also learning its normalization \\eta.\n\n >> Also, heteroscedastic regression has a long history in neural networks, dating back to at least Nix and Wiegand in 1994.  The manuscript needs to be updated to...\n\nThank you for the comment, we will update the related work. \n\n>> Please check Table 5(a), which states that you are only using a small number of training samples.  Also, given the relatively small sample size of these datasets, please comment on the uncertainty of the results.  How confident are you that the methods actually improve the prediction?  How were the competing models tuned and optimized?\n\nIndeed one of the central advantages of the mMLP is that it works well even for the cases with small data size. This was stated in the abstract and empirically validated throughout the paper. For the experiment in Table 5.a, we ran experiments for 10 times and performed a t-test as shown in Fig 5.b. In cases where they are statistical significance (p-value 0.05) we have indicated the results with bold-faced numbers. \n\n>> How were the competing models tuned and optimized?\n\nTable 3 shows the model specifications of the various methods used in the analysis. For the MLP and mMLP methods, we used early stopping on the validation set. For both methods, we used Adam as the optimizer, and considered scheduling of the learning rate. We experimented with various batch sizes, and found that in both cases smaller batch-sizes are preferred though the results are not sensitive to the tuning of this hyperparameter. For the NBCR, the key tuning parameter with the most effect was the truncation. We tried different values ranging from 5 to 20 (step size of 5). In general, due to the need for sampling, the tuning of the NBCR is quite time-consuming. For some cases, one might find better hyperparameter settings for the truncation. However, that would only marginally influence the results at least in our experience. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper828/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye5TaVtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference/Paper828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper828/Reviewers", "ICLR.cc/2020/Conference/Paper828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper828/Authors|ICLR.cc/2020/Conference/Paper828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165592, "tmdate": 1576860544611, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference/Paper828/Reviewers", "ICLR.cc/2020/Conference/Paper828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper828/-/Official_Comment"}}}, {"id": "Bke_a_misS", "original": null, "number": 1, "cdate": 1573759168447, "ddate": null, "tcdate": 1573759168447, "tmdate": 1573759168447, "tddate": null, "forum": "Hye5TaVtDH", "replyto": "BkgR_zORKS", "invitation": "ICLR.cc/2020/Conference/Paper828/-/Official_Comment", "content": {"title": "Thank you for reviewing the paper. We hope our explanation helps clarifying the unclear points. ", "comment": "\n1- With respect to your comment on the prior papers, we would like to ask you if in all fairness the suggested papers are comparable out of the box or even if they are at all comparable? The fact that our paper shares few keywords including 2nd-order neural network does not mean that they are designed for the same purpose and can be compared against each other. We could of course cite these papers as example of the papers that have used 2nd-order neural network but we do not see how these papers can be compared within the scope of our paper and the problem we are solving. \n\n2- It is an interesting paper for the sake of comparison, however, the implementation of this work is not available. That being said, the referred paper uses the Cholesky decomposition for preserving the SPD constraint. As discussed in our paper, the Cholesky-based solutions are in general heuristic as they do not consider the non-Euclidean geometry of the SPD matrices. This was evaluated in Section 5.1.2 of our paper.\n\n3- Please have a look at Figure 1 and Table 1. You can see that the gain in some cases is almost more than 100 times over the quadratic loss. In page 3 and page 4, we have derived a deterministic solution to the derivative of the von-Neumann divergence. We have not explicitly tried the suggested Jensen-Bregman log-det divergence. But we have tried Stein divergence, in our arxiv version of this paper, which is closely related to the log-det divergence and that has resulted in considerably weaker performance in comparison to the von-Neumann divergence. With respect to the computational complexity, in this work we focus on problems with modest dimensionalities. The other point is that there are indeed quite a few other Bregmann divergences. \n\n4- We think the experiments are compelling enough given that the paper\u2019s main contribution is theoretical. Table 5 lists experiments on 6 real datasets, with various size and dimensionality, and shows comparison with three other methods: multilayer perceptron, Gaussian process, and NBCR. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper828/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hye5TaVtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference/Paper828/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper828/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper828/Reviewers", "ICLR.cc/2020/Conference/Paper828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper828/Authors|ICLR.cc/2020/Conference/Paper828/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504165592, "tmdate": 1576860544611, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper828/Authors", "ICLR.cc/2020/Conference/Paper828/Reviewers", "ICLR.cc/2020/Conference/Paper828/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper828/-/Official_Comment"}}}, {"id": "ByxIxKl6YS", "original": null, "number": 1, "cdate": 1571780846502, "ddate": null, "tcdate": 1571780846502, "tmdate": 1572972547289, "tddate": null, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "invitation": "ICLR.cc/2020/Conference/Paper828/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper generalized neural networks into case where a semidefinite positive matrix is learned at the output. The paper presents theoretical derivations that look sound, and validating experiments on synthetic and real data. I must say my expertise does not really correspond to what is done in this paper, but I do not see any obvious flaws and the results look solid. I appreciated the discussion of limitations in section 6. I vote for acceptance with the weakest possible confidence level since it is likely I missed many important points. "}, "signatures": ["ICLR.cc/2020/Conference/Paper828/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper828/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575784871930, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper828/Reviewers"], "noninvitees": [], "tcdate": 1570237746409, "tmdate": 1575784871947, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper828/-/Official_Review"}}}, {"id": "BJg1Bzy0Yr", "original": null, "number": 2, "cdate": 1571840567510, "ddate": null, "tcdate": 1571840567510, "tmdate": 1572972547245, "tddate": null, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "invitation": "ICLR.cc/2020/Conference/Paper828/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This manuscript proposes a novel formulation of the MLP to address predicting a symmetric positive definite (SPD) matrix from an input vector or matrix.  While the field has had methods for years to estimate SPD matrices (such as the covariance matrix estimate in the reparameterization trick), this manuscript proposes a markedly different approach based on a different layer structure and repeated normalization steps based on Mercer Kernels.  Additionally, the loss used can be modified to use more PSD-specific losses, such as the symmetrized von Neumann divergence, rather than the traditional quadratic loss.  This loss appears to give significantly better solutions on synthetic data.\n\nWhile there is some interesting and potentially useful novelty in the approach, I have some concerns about the empirical evidence and modeling to truly determine the mMLP's utility.\n\nFirst, in the synthetic data the mMLP with the l_QRE loss outperforms the l_quad loss with regards to both E_QRE and E_quad.  Why does mMLP/l_QRE outperform on E_quad? As l_quad is actually designed to minimize this error, this is surprising and needs additional explanation.  Additionally, there are two major changes between mMLP and the MLP.  One is the network structure, and the second is the trace being normalized; which change really induces the improvements in the performance? If the l_QRE loss was used in the MLP, would you get similar improvements?\n\nThe network structure as a whole needs greater validation.  A major difference in (3) is that the weight matrices W_l are applied as a both a left and right multiplication.  Given that the \\mathcal{H} operation symmetrizes and normalizes the matrix, a symmetric operation isn't strictly necessary here.  Using both multiplications leads to quadratic properties, which in my experience are less stable in the optimization.  Can the authors validate this structure versus the simpler structure of simply using left multiplications?  Or, in other words, is this weight multiplication structure helpful or do the benefits really come from the \\mathcal{H} operation.\n\nI think that the heteroscedastic regression experiments don't evaluate on one of the key issues, which is uncertainty estimation.  The trace-1 normalization is highly restrictive, so I imagine that that this method is getting the uncertainty incorrect.  Also, heteroscedastic regression has a long history in neural networks, dating back to at least Nix and Wiegand in 1994.  The manuscript needs to be updated to reflect the historical work and current literature on the topic.\n\nPlease check Table 5(a), which states that you are only using a small number of training samples.  Also, given the relatively small sample size of these datasets, please comment on the uncertainty of the results.  How confident are you that the methods actually improve the prediction?  How were the competing models tuned and optimized?"}, "signatures": ["ICLR.cc/2020/Conference/Paper828/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper828/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575784871930, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper828/Reviewers"], "noninvitees": [], "tcdate": 1570237746409, "tmdate": 1575784871947, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper828/-/Official_Review"}}}, {"id": "BkgR_zORKS", "original": null, "number": 3, "cdate": 1571877493839, "ddate": null, "tcdate": 1571877493839, "tmdate": 1572972547186, "tddate": null, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "invitation": "ICLR.cc/2020/Conference/Paper828/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper explores the problem of deep heteroskedastic multivariate regression where the goal is to regress over symmetric positive definite matrices; that is, the deep learning model should take as input data points, and produce a conditional covariance matrix as the output. The key challenge in this setting is how to ensure the predicted matrix is positive definite (and thus follows the non-linear geometry of these matrices), how the neural network can be trained for this task, and what loss function can be used for effective training. The paper proposes a neural network with bilinear layers in this regard, and uses the von Neumann divergence as the loss function to regress the predicted covariance against a ground truth SPD matrix. The gradients of the von Neumann divergence are provided for learning via backpropagation. Experiments on several synthetic datasets and small scale datasets are provided, showcasing some benefits. \n\nPros: \n1. The use of von Neumann divergence as a loss for this task is perhaps novel.\n2. The use of \\alpha-derivatives, while computationally demanding, is perhaps novel in this context as well. \n\nCons:\n1. I do not think the problem setting or the proposed framework is entirely new or is the best choice of its ingredients. Specifically, the idea of using second-order neural networks have been attempted in several prior papers, including the ones the paper cite (such as Ionescu et al. ,2015). Several other papers in this regard are listed below. \n[a] Second-order Temporal pooling for action recognition, Cherian and Gould, IJCV, 2018\n[b] Deep manifold-to-manifold transforming network, Zhang et al, ICIP, 2018\n[c] Statistically motivated second-order pooling, Yu and Salzmann, ECCV, 2018\n\nIn comparison to these methods, it is not clear how the proposed setup is novel, or in what way is method better. There are no comparisons to these methods, and thus it is difficult to judge the benefits even empirically. \n\n2. There are also models that predict the mean and covariance matrices directly from the model, such as the works below. The paper should also include and perhaps compare to their datasets.\n[d] Deep Inference for Covariance Estimation: Learning Gaussian Noise Models for State Estimation, Liu et al, ICRA, 2018\n\n3. I do not think the use of von Neumann divergence as a loss is the best choice one could have, esp. for a deep neural network learning setting. This divergence includes the matrix logarithm, which is perhaps computationally expensive. This is a problem when using other popular loss/similarity functions on SPD matrices (such as the log-Euclidean metric). Perhaps a better option is to use the Jensen-Bregman log-det divergence, as suggested in [a] above; this divergence is symmetric and also has computationally efficient gradients. It is unclear why the paper decided to use von Neumann. \n\n4. The experiments are not compelling, there are no comparisons to alternative models and the datasets used are small scale. Thus, it is unclear if the design choices in the paper have any strong bearing in the empirical performances.\n\nOverall, the paper makes an attempt at designing neural networks for learning SPD matrices. While, there are some components in the model that are perhaps new, the paper lacks any justifications for their choices, and as such these choices seem inferior to alternatives that have been proposed earlier. Also, the experimental results are not convincing against prior works. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper828/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper828/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Matrix Multilayer Perceptron", "authors": ["Jalil Taghia", "Maria B\u00e5nkestad", "Fredrik Lindsten", "Thomas Sch\u00f6n"], "authorids": ["jalil.taghia@ericsson.com", "maria.bankestad@ri.se", "fredrik.lindsten@liu.se", "thomas.schon@it.uu.se"], "keywords": ["Multilayer Perceptron", "symmetric positive definite", "heteroscedastic regression", "covariance estimation"], "abstract": "Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets. ", "pdf": "/pdf/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "paperhash": "taghia|matrix_multilayer_perceptron", "original_pdf": "/attachment/864f74e8327942d5d6053038c6087f80f91fb210.pdf", "_bibtex": "@misc{\ntaghia2020matrix,\ntitle={Matrix Multilayer Perceptron},\nauthor={Jalil Taghia and Maria B{\\r{a}}nkestad and Fredrik Lindsten and Thomas Sch{\\\"o}n},\nyear={2020},\nurl={https://openreview.net/forum?id=Hye5TaVtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hye5TaVtDH", "replyto": "Hye5TaVtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper828/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575784871930, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper828/Reviewers"], "noninvitees": [], "tcdate": 1570237746409, "tmdate": 1575784871947, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper828/-/Official_Review"}}}], "count": 8}