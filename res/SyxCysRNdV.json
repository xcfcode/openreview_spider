{"notes": [{"id": "SyxCysRNdV", "original": "Skg1KoeEdV", "number": 44, "cdate": 1553423077677, "ddate": null, "tcdate": 1553423077677, "tmdate": 1562082115944, "tddate": null, "forum": "SyxCysRNdV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Data Augmentation for Rumor Detection Using Context-Sensitive Neural Language Model With Large-Scale Credibility Corpus", "authors": ["Sooji Han", "Jie Gao", "Fabio Ciravegna"], "authorids": ["sooji.han@sheffield.ac.uk", "j.gao@sheffield.ac.uk", "f.ciravegna@sheffield.ac.uk"], "keywords": ["Rumor Detection", "Data Augmentation", "Social Media", "Neural Language Models", "Weak Supervision"], "TL;DR": "We propose a methodology of augmenting publicly available data for rumor studies based on samantic relatedness between limited labeled and unlabeled data.", "abstract": "In this paper, we address the challenge of limited labeled data and class imbalance problem for machine learning-based rumor detection on social media. We present an offline data augmentation method based on semantic relatedness for rumor detection. To this end, unlabeled social media data is exploited to augment limited labeled data. A context-aware neural language model and a large credibility-focused Twitter corpus are employed to learn effective representations of rumor tweets for semantic relatedness measurement. A language model fine-tuned with the a large domain-specific corpus shows a dramatic improvement on training data augmentation for rumor detection over pretrained language models. We conduct experiments on six different real-world events based on five publicly available data sets and one augmented data set. Our experiments show that the proposed method allows us to generate a larger training data with reasonable quality via weak supervision. We present preliminary results achieved using a state-of-the-art neural network model with augmented data for rumor detection.", "pdf": "/pdf/5d8a74c4a7952e9cad7d8bb94bb71798fa76d6ff.pdf", "paperhash": "han|data_augmentation_for_rumor_detection_using_contextsensitive_neural_language_model_with_largescale_credibility_corpus"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "BJgTv4QbtV", "original": null, "number": 1, "cdate": 1554228325040, "ddate": null, "tcdate": 1554228325040, "tmdate": 1555512028318, "tddate": null, "forum": "SyxCysRNdV", "replyto": "SyxCysRNdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper44/Official_Review", "content": {"title": "A useful augmentation method, persuasively tested.", "review": "This paper addresses the problem of limited data in rumor detection. They augment  data by identifying unlabeled data as paraphrases of  labeled rumors based on semantic similarity. They build a rumor detection model by fine-tuning a pretrained language model.\n\nI recognize space is limited, but a brief explanation of the rumor detection task and  specifics about  the class imbalance would help.\n\nPreprocessing as described removes critical meta information about whether the tweet is citing a particular source (url/rt). I'm skeptical that removing this information is necessary to build a model.\n\nI would like to see some exploration of whether sentence cosine similarity is actually a good metric for semantic similarity. What properties are captured by cosine similarity?\n\nDoes this manner of augmenting data create bias towards detecting the same sort of rumors as are in the corpus? That is, will topic be relied on more than other markers of credibility? Perhaps holding out specific events from augmented training data would be a good way to test.\n\nThe models that serve as a test bed show a sound methodology, and this paper strikes me as a solid work in progress.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Augmentation for Rumor Detection Using Context-Sensitive Neural Language Model With Large-Scale Credibility Corpus", "authors": ["Sooji Han", "Jie Gao", "Fabio Ciravegna"], "authorids": ["sooji.han@sheffield.ac.uk", "j.gao@sheffield.ac.uk", "f.ciravegna@sheffield.ac.uk"], "keywords": ["Rumor Detection", "Data Augmentation", "Social Media", "Neural Language Models", "Weak Supervision"], "TL;DR": "We propose a methodology of augmenting publicly available data for rumor studies based on samantic relatedness between limited labeled and unlabeled data.", "abstract": "In this paper, we address the challenge of limited labeled data and class imbalance problem for machine learning-based rumor detection on social media. We present an offline data augmentation method based on semantic relatedness for rumor detection. To this end, unlabeled social media data is exploited to augment limited labeled data. A context-aware neural language model and a large credibility-focused Twitter corpus are employed to learn effective representations of rumor tweets for semantic relatedness measurement. A language model fine-tuned with the a large domain-specific corpus shows a dramatic improvement on training data augmentation for rumor detection over pretrained language models. We conduct experiments on six different real-world events based on five publicly available data sets and one augmented data set. Our experiments show that the proposed method allows us to generate a larger training data with reasonable quality via weak supervision. We present preliminary results achieved using a state-of-the-art neural network model with augmented data for rumor detection.", "pdf": "/pdf/5d8a74c4a7952e9cad7d8bb94bb71798fa76d6ff.pdf", "paperhash": "han|data_augmentation_for_rumor_detection_using_contextsensitive_neural_language_model_with_largescale_credibility_corpus"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper44/Official_Review", "cdate": 1553713414463, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "SyxCysRNdV", "replyto": "SyxCysRNdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713414463, "tmdate": 1555511824625, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper44/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "r1l0VF88F4", "original": null, "number": 2, "cdate": 1554569526478, "ddate": null, "tcdate": 1554569526478, "tmdate": 1555512023293, "tddate": null, "forum": "SyxCysRNdV", "replyto": "SyxCysRNdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper44/Official_Review", "content": {"title": "A straight-forward data augmentation method for rumor detection employing ELMo", "review": "The authors present a data augmentation technique for rumor detection using recently introduced contextualized word representations, like ELMo. Last, they fine-tune them with diverse datasets (tweets at their majority) in order to build rumor-specific embeddings.\n\nThe paper is very clear and easy to comprehend. The authors present a very analytical data augmentation technique for the task of rumor detection by employing semantic relatedness fine-tuning on a large Twitter corpus that they collected. This way the effectively address the labeled data scarcity and class imbalance problems.\n\nPros:\n- using state-of-the-art neural language models\n- semantic relatedness fine-tuning\n\nCons: \n- not compared with other data augmentation techniques (other than using Kochkina's method)\n- considerable time for collecting the data, fine-tuning, and kxn pair comparison\n\nWhat was the time (as well as resources) requested to fine-tune on the CREDBANK corpus, as well as the time required for the whole process? Will the data and methods be available to the public? Could other methods be used than semantic relatedness? What about involving transfer learning for similar tasks?", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Augmentation for Rumor Detection Using Context-Sensitive Neural Language Model With Large-Scale Credibility Corpus", "authors": ["Sooji Han", "Jie Gao", "Fabio Ciravegna"], "authorids": ["sooji.han@sheffield.ac.uk", "j.gao@sheffield.ac.uk", "f.ciravegna@sheffield.ac.uk"], "keywords": ["Rumor Detection", "Data Augmentation", "Social Media", "Neural Language Models", "Weak Supervision"], "TL;DR": "We propose a methodology of augmenting publicly available data for rumor studies based on samantic relatedness between limited labeled and unlabeled data.", "abstract": "In this paper, we address the challenge of limited labeled data and class imbalance problem for machine learning-based rumor detection on social media. We present an offline data augmentation method based on semantic relatedness for rumor detection. To this end, unlabeled social media data is exploited to augment limited labeled data. A context-aware neural language model and a large credibility-focused Twitter corpus are employed to learn effective representations of rumor tweets for semantic relatedness measurement. A language model fine-tuned with the a large domain-specific corpus shows a dramatic improvement on training data augmentation for rumor detection over pretrained language models. We conduct experiments on six different real-world events based on five publicly available data sets and one augmented data set. Our experiments show that the proposed method allows us to generate a larger training data with reasonable quality via weak supervision. We present preliminary results achieved using a state-of-the-art neural network model with augmented data for rumor detection.", "pdf": "/pdf/5d8a74c4a7952e9cad7d8bb94bb71798fa76d6ff.pdf", "paperhash": "han|data_augmentation_for_rumor_detection_using_contextsensitive_neural_language_model_with_largescale_credibility_corpus"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper44/Official_Review", "cdate": 1553713414463, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "SyxCysRNdV", "replyto": "SyxCysRNdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper44/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713414463, "tmdate": 1555511824625, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper44/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "HygV3LkFtE", "original": null, "number": 1, "cdate": 1554736811789, "ddate": null, "tcdate": 1554736811789, "tmdate": 1555510987164, "tddate": null, "forum": "SyxCysRNdV", "replyto": "SyxCysRNdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper44/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Augmentation for Rumor Detection Using Context-Sensitive Neural Language Model With Large-Scale Credibility Corpus", "authors": ["Sooji Han", "Jie Gao", "Fabio Ciravegna"], "authorids": ["sooji.han@sheffield.ac.uk", "j.gao@sheffield.ac.uk", "f.ciravegna@sheffield.ac.uk"], "keywords": ["Rumor Detection", "Data Augmentation", "Social Media", "Neural Language Models", "Weak Supervision"], "TL;DR": "We propose a methodology of augmenting publicly available data for rumor studies based on samantic relatedness between limited labeled and unlabeled data.", "abstract": "In this paper, we address the challenge of limited labeled data and class imbalance problem for machine learning-based rumor detection on social media. We present an offline data augmentation method based on semantic relatedness for rumor detection. To this end, unlabeled social media data is exploited to augment limited labeled data. A context-aware neural language model and a large credibility-focused Twitter corpus are employed to learn effective representations of rumor tweets for semantic relatedness measurement. A language model fine-tuned with the a large domain-specific corpus shows a dramatic improvement on training data augmentation for rumor detection over pretrained language models. We conduct experiments on six different real-world events based on five publicly available data sets and one augmented data set. Our experiments show that the proposed method allows us to generate a larger training data with reasonable quality via weak supervision. We present preliminary results achieved using a state-of-the-art neural network model with augmented data for rumor detection.", "pdf": "/pdf/5d8a74c4a7952e9cad7d8bb94bb71798fa76d6ff.pdf", "paperhash": "han|data_augmentation_for_rumor_detection_using_contextsensitive_neural_language_model_with_largescale_credibility_corpus"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper44/Decision", "cdate": 1554736075915, "reply": {"forum": "SyxCysRNdV", "replyto": "SyxCysRNdV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736075915, "tmdate": 1555510962932, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}