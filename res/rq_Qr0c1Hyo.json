{"notes": [{"id": "rq_Qr0c1Hyo", "original": "pZK1mwfnpSL", "number": 3269, "cdate": 1601308363095, "ddate": null, "tcdate": 1601308363095, "tmdate": 1612185763700, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1DmMKraiP5", "original": null, "number": 1, "cdate": 1610040403573, "ddate": null, "tcdate": 1610040403573, "tmdate": 1610473999815, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Dear authors,\n\nall reviewers found many interesting contributions in your paper and also pointed out some minor/major issues. In your rebuttal discussions, you addressed most of them to their satisfaction and I hope you will incorporate them carefully also in your final submission.\n\nI hence recommend accepting this paper\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040403559, "tmdate": 1610473999798, "id": "ICLR.cc/2021/Conference/Paper3269/-/Decision"}}}, {"id": "7E7DuxOQcp", "original": null, "number": 15, "cdate": 1606230581503, "ddate": null, "tcdate": 1606230581503, "tmdate": 1606230581503, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "We have updated the manuscript", "comment": "We thank the reviewers again for their insightful comments. We have now uploaded an updated version of the paper to openreview which incorporates most of the comments raised. We believe that this has significantly improved the paper. To summarize, the main changes in the new version are:\n\n1. We have added a new subsection in section 2.2 (titled \u201cRemarks on the Analysis\u201d) which discusses the assumptions made in our analysis in more depth. Specifically, we clarify the following:\n - We make clear that we study the mean evolution of the iterates, not the variance of individual training runs. We discuss why we make this choice by comparing the roles of bias and variance in the random sampling strategy and the random shuffling strategy.\n - We clarify that the \u201creverse-epoch strategy\u201d can enable us to suppress the variance of individual training runs at $O(\\epsilon^2)$.\n - We provide a discussion of our assumption that terms of $O(m^3 \\epsilon^3)$ are small and its implications.\n\n\n2. We have added a set of new experimental results on a 3-layer MLP classifying Fashion-MNIST in appendix section D. We chose to use Fashion-MNIST as the implicit regularization effect of large learning rates/small batch sizes for MNIST is relatively small. Note that these results are provisional as we currently perform a single training run for each set of hyper-parameters. We will update the figures to provide the average performance over multiple runs in time for the camera ready.\n\n\n3. We have made several other minor changes throughout the text. To list a few:\n - We clarify that backward error analysis assumes that the original flow is analytic in the vicinity of the parameters.\n - We have softened the language describing our empirical results in section 4.\n - We clarify the precise meaning of the phrase \u201csmall but finite learning rates\u201d.\n - We cleaned up the references to refer to published versions of papers.\n\nWe will continue working on the text to incorporate the remaining suggestions of the reviewers in time for the camera ready.\n\nBest wishes,\nThe Authors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "f2ookzGQv_r", "original": null, "number": 2, "cdate": 1603941508396, "ddate": null, "tcdate": 1603941508396, "tmdate": 1606093281938, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review", "content": {"title": "Borderline Accept Paper", "review": "Summary: To analyze why the generalization error of SGD with larger learning rates achieves better test error, this paper analyzes the implicit regularization of SGD (with a finite step size) via a first order backward error analysis. Under this analysis the paper shows that the mean position of SGD with $m$ minibatches effectively follows the flow according to Eq (20) for a small but finite step size, while GD effectively follows the last inline equation in section 2.1. The paper shows empirically on an image classification task that by explicitly including the (implicit SGD) regularizer, SGD on the modified loss behaves similarly to using a larger learning rate when evaluating on the test set. The paper then extends this results to consider varying the batch size in section 3, showing that for small batchsizes the implicit regularization scales with the ratio of learning rate and batchsize $\\epsilon/B$. Finally in section 4, the paper analyzes SGD when for each sampled minibatch in an epoch, we apply $n$ gradient steps with a stepsize $\\epsilon/n$ and show that performance degrades as $n$ increases, suggesting that the benefit of SGD with larger learning rates is due to the implicit regularizer and not the temperature of an associated SDE. \n\nThis paper is clearly written and well edited. I find the main result and the analysis technique interesting and novel. Although the experiments are well explained and help support the theory developed, there is only one experiment setting making it difficult to believe strong general claims such as those in section 4. I do have concerns about equating the \"mean\" behavior of SGD with the actual behavior of SGD and. \n \nRecommendation:\nI recommend accepting this paper. As it currently stands, this paper is borderlin on the acceptance threshold for me. I like the novel use of the backward error analysis to gain insight into the behavior of SGD and I believe it would be of interest to ICLR readers. My main concerns are the papers' narrow focus on the mean behavior of SGD and the single experiment setting used to validate results. I would much more strongly support this paper if the theoretical analysis was stronger (e.g. analyzing the variance of individual SGD flows/regularizers to the mean SGD flow/regularizer) or if more experiments (in different settings) supported the results.  \n\nQuestions: \nIf we don't take the expectation over $\\xi(m)$ in Section 2.2, the theory suggests that there exist a (random) modified flow for each (random) ordering of minibatches $\\hat{C}_0, \\ldots, \\hat{C}_m$ by equating equations (14) and (19). The main result Eq (20) would correspond to the expected value over the (random) modified flow. I believe this paper would be much stronger if there was some discussion of how the variance / deviations of these random flows from the mean flow (i.e the variance of $\\xi(m)$) affects the implicit regularization and how this scales with batch size and properties of the loss. Would the implicit regularization break down for some experiments? \n\nIs the assumption that $m \\epsilon$ is small reasonable (so that we can ignore the higher order $O(m^3 \\epsilon^3)$ terms in the analysis)? Isn't $m = N/B$ the number of updates per epochs very large in practice since $N >> B$?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078876, "tmdate": 1606915773596, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3269/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review"}}}, {"id": "aPBSDYIl1xA", "original": null, "number": 14, "cdate": 1605895231598, "ddate": null, "tcdate": 1605895231598, "tmdate": 1605895231598, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "EGOVOnlHkRP", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you! We are really grateful for the effort you have put into reviewing our work. \n\nWe are working to make the changes in the text as promised."}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "TB_kAY949zf", "original": null, "number": 13, "cdate": 1605894756167, "ddate": null, "tcdate": 1605894756167, "tmdate": 1605894756167, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "wjNPs4eR-Ij", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Additional clarifications", "comment": "Thank you for your reply!\n\nWe respond to your questions below. Please let us know if anything is unclear or if there are other points you would like to discuss.\n\n**Is the approximation $m \\epsilon$ is small reasonable for large datasets:**\n\n- Yes, our analysis neglects terms at $O(m^3\\epsilon^3)$. However in our main analysis in section 2.2, we assume the dataset size $N$ and the batch size $B$ are fixed, and consequently $m =N/B$ is a constant. In this context we can equivalently say that we neglect terms at $O(\\epsilon^3)$. \n\n- If the dataset size is not fixed, the accuracy of the approximation that $m \\epsilon = N \\epsilon/B$ is small may degrade as $N$ increases, on the basis of the reasonable assumption that the ratio $\\epsilon/B$ which achieves the optimal test accuracy is constant as $N$ rises. \n\n- Interestingly, we note that prior work [1] found evidence to suggest that for SGD without learning rate decay, if we want to maximize the test accuracy, then we should keep $N\\epsilon/B$ constant as N increases (i.e., when the dataset size increase the optimal value of the ratio $\\epsilon / B$ falls). If this were the case, then the accuracy of our approximation would not fall as $N$ increases, but we note that [1] only shows results for a simple MLP on MNIST and we are not aware of any follow up work which verifies this claim on larger models.\n\n- To summarize, we agree that it is plausible that the accuracy of our approximations may degrade as the size of the dataset grows, and we will clarify this in the text. However there is also some empirical evidence to suggest this may not occur in practice. \n\n**When do we expect our analysis to break down:**\n\nThere are three main situations in which we might expect our analysis to break down:\n\n1) The variance in the SGD iterates at $O(m^2 \\epsilon^2)$ may be large. In this case, while the mean SGD iterate over one epoch will coincide with our analysis, it may not be a good estimate of the iterates of any specific SGD run. (Note however that we can resolve this failure mode using the reverse epoch strategy).\n\n2) If $m\\epsilon$ is too large, then higher order terms will also contribute to the dynamics.\n\n3) There may be situations in which these higher order terms, while small over a single epoch, accumulate over many epochs and cause the mean SGD iterate to slowly diverge from the path of the modified loss on long timescales (even though the SGD iterates in any single epoch stay close to the path of the modified loss, conditioned on the initial parameters of that epoch).\n\nWe anticipate that situation 3 will be quite common in practice; i.e., the dynamics of SGD at practical learning rates will not exactly coincide with the modified loss over an entire training run, however the modified loss will provide a good approximation to the path of SGD locally over a few epochs, and consequently our analysis will still provide useful insights into the implicit regularization phenomenon, as shown by our experiments.\n\n**Reverse epoch strategy:**\n\nOur apologies. Yes, we meant to say that the reverse epoch strategy coincides exactly with gradient flow on the modified loss for terms below $O(m^3 \\epsilon^3)$, because it eliminates the variance at $O(m^2 \\epsilon^2)$. However it does not exactly match the modified flow for higher order terms, and therefore this equivalence only holds if $m \\epsilon$ is sufficiently small.\n\nBest wishes,\nThe Authors\n\n[1] A Bayesian Perspective on Generalization and SGD, Smith and Le\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "wjNPs4eR-Ij", "original": null, "number": 12, "cdate": 1605821358256, "ddate": null, "tcdate": 1605821358256, "tmdate": 1605821358256, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "hFLNxg-g87F", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Questions to the response.", "comment": "For the approximation $m\\epsilon = N\\epsilon/B$, I agree that $\\epsilon/B$ is typically kept constant (e.g. large batch sizes $B$ allow for larger step sizes $\\epsilon$), but wouldn't $m$ still increase as $N$ increases (while $B$ and $\\epsilon$ are held fixed)? Doesn't your analysis neglects terms at $O(m^3 \\epsilon^3)$ instead of $O(\\epsilon^3)$? The first 2 reasons in your response are perhaps motivate why the results are useful (regardless), but I don't understand why I shouldn't expect things to breakdown for large $m$.\n\nDid you have any thoughts of what conditions we might expect the implicit regularization / low order flow correction to break down? I agree that the implicit regularization appears to explain a lot of the benefit in your experiment settings, but I am hoping to understand / let other readers know what conditions, where SGD + GD with your regularization may differ (e.g. when the variance is not controlled or when $m$ is large).  It may be the case that SGD does not perform as well as GD + regularization when the approximation breaks down, which would further support this implicit regularization as being helpful.  \n\nFinally, the \"reverse epoch\" strategy paragraph in the response does not allow the flow to \"coincide exactly\" with discrete SGD because it's neglecting higher order terms $O(m^3 \\epsilon^3)$, right? (Or only coincide up to $O(m^3 \\epsilon^3)$ terms). The actual trace of SGD would have the $\\omega_k$ varying at each step. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "uJhk1VSHz-P", "original": null, "number": 1, "cdate": 1603809941643, "ddate": null, "tcdate": 1603809941643, "tmdate": 1605811332548, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review", "content": {"title": "Missing a discussion of the scope of the results and the assumptions that go into it", "review": "## Summary\n\nUsing backward error analysis, the paper argues that SGD with small but finite step sizes stays on the path of a gradient flow ODE of a modified loss, which penalizes the squared norms of the mini-batch gradients. This offers a possible explanation of the empirically observed positive effect of (relatively) large step sizes on generalization performance. The paper further contests previous findings based on a vanishing step size assumption.\n\n\n## Rating\n\nSimilar to several recent works, this paper tries to explain certain aspects of stochastic gradient descent using a continuous time approximation. In contrast to existing works, it explicitly accounts for the effect of finite step sizes, which I think is a very interesting direction and surfaces several interesting aspects. I also welcome and endorse the critical discussion of prior work based on infinitesimal step size assumptions. Overall, the paper was interesting and pleasant to read. To the very best of my knowledge, all mathematical derivations are technically correct.\n\nHowever\u2014as the authors themselves note in their critique of SDE approximations to SGD\u2014the devil is in the details with continuous time approximations. In my opinion, that makes is absolutely crucial to discuss the scope of the results carefully and transparently, including a critical discussion on assumptions made and simplifications that go into the continuous-time model. In my opinion, this paper fails to deliver that, which is why I recommend rejection. Below, I am asking for clarification on various points and would encourage the authors to respond to the major points in the rebuttal phase.\n\n\n## Major Comments\n\n\n1) The main result says that the *expected* SGD iterate after a *single* epoch lands close to the path of a gradient flow ODE on a modified loss. Unless I am missing something, this fundamentally fails to capture the behavior over multiple epochs. The analysis only guarantees that, from any given starting point $\\omega_0$, the expected iterate after one epoch of SGD ends up close to the ODE path starting from $\\omega_0$. Unless I am missing something, this does *not* imply that two epochs of SGD starting from $\\omega_0$ end up on that path. We can not simply chain two epochs together: The first epoch only stays on the path in expectation, but any realization of that random variable will deviate from the path, which affects the initial condition of the next epoch. Intuitively, one needs to get a handle on the variance of the iterate as well in order to give guarantees for multiple epochs. Is this understanding correct? If so, to what extent can insights about a single epoch of SGD be transferred to practical settings?\n\n2) Comment (1) hints at a larger (but vague) point that the paper is trying to characterize a *stochastic* optimization procedure with a solution of a *deterministic* gradient flow ODE. It does so by focusing on the *expectation* of the iterate, which might be an approach to highlight certain aspects, but it will never give a full picture. Why wouldn\u2019t we also be interested in the covariance of the iterates? The limitations of this characterization should be discussed thoroughly in the paper.\n\n3) In Section 2, the composition of the minibatches is assumed to be fixed and the randomness only comes from their ordering. The paper says: \"It is standard practice to shuffle the dataset once per epoch, but this step does not affect our analysis and we omit it for brevity.\u201c I don\u2019t think that statement is justified with respect to the result in Eq. (1), given that the modified loss depends on the minibatch composition. Therefore, would we reshuffle the dataset after each epoch, the modified loss would change from one epoch to the next. Later, in Section 3, the expectation is additionally taken over the composition of the batches. Why is the result presented in these two distinct steps? None of the key findings of the paper seems to rely on the intermediate fixed-composition result. It also doesn\u2019t reflect the common practice of reshuffling the entire dataset and then traversing it, which simultaneously randomizes the composition and ordering of batches. So why not give the result of Eq. (22) directly? It is also the more intuitive result, invoking the trace of the gradient covariance matrix, which also appears in prior work on continuous time approximations of SGD.\n\n4) While the analysis tries to account for finite step sizes, it still seems to assume step sizes that are orders of magnitude smaller than those used in practice. In particular, when going from Eq. (12) to Eq. (13), each minibatch cost function is equated with its second-order Taylor approximation around the starting point $\\omega_0$. This is a *drastic* approximation and I don\u2019t see any justification for why this should be anywhere near accurate for practical settings. For large datasets and moderate batch sizes, the number of updates in one epoch will be in the thousands. For realistic step size choices, a second-order Taylor expansion around the starting point will probably be rather poor after a handful of SGD updates, no?\n\n5) The paper strongly emphasizes the assumption of sampling data points without replacement. While sampling without replacement is indeed the usual setting in practice, most of the stochastic optimisation literature builds on the assumption of sampling with replacement. And to my knowledge, no major differences (in terms of generalization performance) have been reported in the literature between the two approaches.\n    a) Can the analysis presented in the paper be extended to setting of sampling with replacement? It seems to me that this should be straight-forward. Equations (12) and (13) should hold also when each minibatch is obtained from sampling with replacement. In that case, the expectation of the second-order correction term should directly give a result akin to Eq. (22). If that is in fact possible, it should definitely be added to the paper.\n    b) If that is not possible, what prevents the application and is this a technicality or would you actually expect substantially different behavior in terms of generalization?\n    c) It would also have been nice to see the experiments repeated with sampling with replacement to check empirically whether the findings hold in that case?\n\n6) Something that bugs me from an optimization perspective is that the smoothness properties of the problem do not enter this analysis at all. For example, you write (near the bottom of page 4) that \u201cour analysis assumes $m\\epsilon = N\\epsilon / B$ is small.\u201d However, any given loss function $C(w)$ can be rescaled by a constant $M\\gg 1$ while scaling the step size with $1/M$. This leaves the behavior of SGD unaffected while making the step size arbitrarily small. Why does that not enter into the analysis? It probably relates to my comment (4), seeing that the step sizes are assumed to be so small that they are not restricted by the smoothness of the function.\n\n\n## Minor Comments\n\n7) The paper derives the implicit regularizer and provides empirical evidence that it can partially explain the benefits of large step sizes for generalization. However, very little attention is given to the regularization term itself and to the question *why* this regularizer might be beneficial. The only comment speaking to that is that the regularizer penalizes \u201csharp\u201d regions. I would like to see this discussion expanded and connected to the recent literature.\n\n8) At the end of page 6, you write about the large batch size regime and say that the \u201cwe expect the optimal learning rate to be independent of the batch size in this limit.\u201d It would have been great to substantiate that conjecture with an experiment and/or to refer to specific experiments done in prior work.\n\n9) You repeatedly use the phrase \u201csmall but finite learning rates\u201d. If my understanding is correct, that has phrase has a very precise meaning in the context of this work, namely that terms of order $O(\\epsilon^3)$ are vanishingly small while terms that a quadratic or linear in $\\epsilon$ can not be ignored. (This is in contrast to prior work that also ignores quadratic terms.) Maybe this could be stated clearly the first time you use this phrase.\n\n\n## Typos / Style\n\n- I think you should capitalize references to sections, equations, figures, et cetera.\n- The bib file could really need some love. You are citing the arXiv versions for several papers that have been published in peer-reviewed venues. Capitalization in paper titles is messed up (e.g., \u201csgd\u201d).\n\n## Edit after Rebuttal\n\nI thank the authors for their engagement with my review. Many of my comments and questions have been resolved and, consequently, I have increase my score and **recommend accepting this paper.**", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078876, "tmdate": 1606915773596, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3269/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review"}}}, {"id": "EGOVOnlHkRP", "original": null, "number": 11, "cdate": 1605811164102, "ddate": null, "tcdate": 1605811164102, "tmdate": 1605811164102, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "TS7u9RooUX-", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Reviewer response", "comment": "Thanks for the in-depth response. This clarifies almost all of the open questions I had. I still can't fully wrap my head around the discrepancy between random sampling and shuffle-and-traverse. But irrespective of that, I've become convinced that this analysis provides many interesting insights and I will raise my score and **recommend accepting the paper.** Please implement the changes/clarifications as indicated in your responses!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "TS7u9RooUX-", "original": null, "number": 10, "cdate": 1605701121723, "ddate": null, "tcdate": 1605701121723, "tmdate": 1605701166384, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "9fej7JeQG2", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Reply part 2", "comment": "6. Note that the $n$-th order terms in the Taylor expansion of the modified flow have the form $\\epsilon^n f_{n}(C(\\omega))$, where $f_{n}(C(\\omega))$ is a function of the loss $C(\\omega)$. When scaling $C$ by $M$, one would obtain $f_{n}(MC(\\omega)) = M^{n+1} f_{n}(C(\\omega))$. As an example of this, if you inspect equation 9 in the text, we find $f_1(MC(\\omega)) = (1/2) \\nabla \\nabla MC(\\omega) \\nabla MC(\\omega) = M^2 f_1(C(\\omega))$. \n\n Consequently if we scale the loss by a factor $M$ and divide the learning rate by the same factor, then the $n$-th order correction term $(\\epsilon/M)^n f_n(MC(\\omega)) = M \\epsilon^n f_n(C(\\omega)$ also increases by a factor of $M$. Therefore if we scale the loss by $M$ and the learning rate by $1/M$ then the modified flow will also scale by a factor of $M$, and therefore the dynamics of the parameters are unaffected as required.\n\n The functions $f_{n}(C(\\omega))$ are composed of derivatives of the loss $C(\\omega)$, therefore these derivatives must also be bounded, and this is implicit in our analysis. A standard result in backward error analysis [3] shows that the smoother the original vector field (i.e., the larger the ball about the current parameters for which the gradient of the loss is analytic), the larger the learning rate for which backward error analysis will be accurate. We will clarify this point in the updated text.\n\n7. We call the first order correction a regularizer since it appears as an additive term in the modified loss with a tunable coefficient (the learning rate), and because tuning the scale of this term enhances the test accuracy (at least for some tasks/datasets). We note that it is common in deep learning to use the word regularizer in a fairly loose sense. For instance, early stopping and batch normalization are commonly referred to as sources of regularization.\n\n While the norm of the full batch gradient vanishes at local minima it does not vanish in the vicinity of local minima, and consequently it may introduce a dynamical penalty favouring flatter minima. However, we agree that this intuition, while interesting, is not compelling.\n\n[1] Stochastic modified equations and adaptive stochastic gradient algorithms, Li et al.\n\n[2] Random Shuffling Beats SGD after Finite Epochs, Haochen and Sra.\n\n[3] Geometric numerical integration: structure-preserving algorithms for ordinary differential equations, Hairer et al.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "9fej7JeQG2", "original": null, "number": 9, "cdate": 1605700816956, "ddate": null, "tcdate": 1605700816956, "tmdate": 1605700816956, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "wjN1kaoc2A-", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Reply part 1", "comment": "Thank you for your rapid reply! We really appreciate the level of detail in your review.\n\nWe clarify our responses below. Please feel free to message us again if anything remains unclear. We are currently working on an updated version of the paper incorporating your and the other reviewers\u2019 comments, which we will upload before the end of the rebuttal period.\n\n1. We will make clear in the updated version of the paper that we analyze the mean behaviour of the iterates, and discuss this in more depth. \n\n We apologize for not directly addressing the question of whether we can chain together multiple epochs directly in our first response. As you suggest in your reply, the answer to this question is a little subtle (and interacts with our response to point 4):\n\n - We can repeat our analysis over any finite number of epochs $n$ by taking a Taylor expansion in terms of $O(nm\\epsilon)$ rather than $O(m\\epsilon)$. For any number of epochs, the lowest order correction to the mean SGD iterate will lie on the modified loss (equation 1), where the implicit regularization term is an average over all minibatches sampled during the $n$ epochs.\n\n - However, the Taylor expansion above requires $\\epsilon < \\epsilon_{max}$ where $\\epsilon_{max} \\propto 1/n$, and therefore for large $n$ the analysis above would not hold for practical learning rates.\n\n To summarize, our analysis does identify the bias in the lowest order correction to the dynamics over multiple epochs, however the more epochs we consider the more likely higher order terms are to contribute to the dynamics, and consequently our analysis is most accurate in practice locally over a small number of epochs. We will clarify this point in the text.\n\n Our experimental results suggest that studying the mean SGD iterate over one epoch up to $O(\\epsilon^2)$ can explain most of the generalization benefit of finite learning rate SGD over multiple epochs, at least for Wide-ResNets/CIFAR-10. However, we agree that higher order terms and the variance in the updates may play a role in other practical problems. \n\n4. Yes, we agree. We intend to extend the discussion of the assumption that $m\\epsilon$ is small when we update the text to incorporate these points.\n\n5. This is a very subtle point, which we hope we can clarify below. (We assume you mean random sampling with replacement, instead of without, in your most recent comment.)\n\n - To see why random sampling changes our analysis, look at equations 15 and 16. If the minibatches are randomly sampled with replacement, then in equation 15 there would be an expectation on the RHS over the composition of the minibatches. Since the minibatches $j$ and $k$ are now random and independent, the RHS of equation 16 simplifies to $(m^2 - m)/2 \\nabla \\nabla C \\nabla C$. Crucially, the term in $\\nabla \\nabla \\hat{C_j} \\nabla \\hat{C}_j$ vanishes. In addition, random shuffling will of course also introduce a source of variance at $O(\\epsilon)$.\n\n - Consequently, for random sampling the dynamics up to $O(\\epsilon^2)$ are described by the modified stochastic differential equation of equation 7 in [1]. \n\n - However, we are not claiming that there is a qualitative difference in terms of generalization between the random sampling and the random shuffling cases. We believe that in practice this SDE may often have a very similar bias to the ODE identified in our work. Our intuition is that the noise source in the SDE, although it appears as a variance term at $O(\\epsilon)$, could implicitly introduce a bias at $O(\\epsilon^2)$ when integrated over a finite timescale. \n\n - The main point we were trying to make in our original response is that, although random sampling and random shuffling may behave similarly in practice, it is much easier to interpret the dynamics for random shuffling, because the mean evolution is described by an ODE, whereas for random sampling we have to study the properties of an SDE, which is more challenging. \n\n - In addition, random shuffling is more commonly used in practice, and it has better convergence properties since it suppresses the variance at $O(\\epsilon)$ [2]. In practice, we found on our Wide-ResNet/CIFAR-10 setup that random shuffling achieves slightly higher test accuracies than random sampling."}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "wjN1kaoc2A-", "original": null, "number": 8, "cdate": 1605605138167, "ddate": null, "tcdate": 1605605138167, "tmdate": 1605605293348, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "jC4tuadGAdb", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Thanks for the reply", "comment": "Dear authors,\n\nthanks for the detailed reply. While you have answered some of my questions, several points remain unclear to me. So, I'd like to ask another round of clarifying questions. I'm maintaining the same numbering of the issues and you can consider the points I'm not replying to as resolved.\n\n1. After reading your response, I agree that analyzing the mean behavior is a solid contribution and a good first step, but I think this restriction should be made a bit clearer to the reader. However, you didn't address my main question here; to quote myself: *The analysis only guarantees that, from any given starting point $\\omega_0$ , the expected iterate after one epoch of SGD ends up close to the ODE path starting from $\\omega_0$ . Unless I am missing something, this does not imply that two epochs of SGD starting from $\\omega_0$ end up on that path. We can not simply chain two epochs together: The first epoch only stays on the path in expectation, but any realization of that random variable will deviate from the path, which affects the initial condition of the next epoch. Intuitively, one needs to get a handle on the variance of the iterate as well in order to give guarantees for multiple epochs. Is this understanding correct? If so, to what extent can insights about a single epoch of SGD be transferred to practical settings?* I presume the answer is that this type of analysis only describes the local behavior anyway due to the restrictions discussed in point 4, is that fair? In any case, the (non-)transfer to multiple epochs should be mentioned in the paper!\n\n2. -\n\n3. Personally, I would give the result in Eq. (22) directly, but I understand your arguments for doing it in two stages.\n\n4. You give good justifications for the approximation here. These should be in the paper! In particular something along the lines of *\"We derive the lowest order correction to gradient flow for SGD with finite learning rate.\"*\n\n5. This is somewhat surprising to me. I'm not aware of any prior work that suggests a qualitative difference **in terms of generalization** between random sampling without replacement and the shuffle-and-traverse strategy. Are you? Or is this just a technical limitation of this type of analysis? This might be a somewhat too technical discussion for this forum, but I don't see where things go wrong when we would go through Equations (12)--(17) and replace the fixed-composition batches with randomly-sampled ones? Shouldn't we arrive at a result similar to Eq. (22)??\n\n6. I have probably phrased my question ambiguously. My main point was that *\"$m\\epsilon$ is small\"* can not be the full story. The scale of $m\\epsilon$ should somehow be related to some regularity of the objective in order to guarantee that the approximation is good, no? To quote myself again: *\"Any given loss function can be rescaled by a constant $M\\gg 1$ while scaling the step size with $1/M$. This leaves the behavior of SGD unaffected while making the step size arbitrarily small. Why does that not enter into the analysis?\"* I might be missing something obvious here, but this seems like a crucial point to me. Can you clarify this?\n\n7. Agreed that understanding these terms can be deferred to future work, but I would maybe be a bit more careful with calling it a \"regularizer\" then. Just a quick comment: The intuition you've outlined doesn't seem compelling to me. The batch gradient norm is unrelated to \"sharpness\"; any local minimum of the batch loss has zero batch gradient.\n\nEdit: Fixing the enumeration."}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "jC4tuadGAdb", "original": null, "number": 7, "cdate": 1605542199394, "ddate": null, "tcdate": 1605542199394, "tmdate": 1605542270327, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "X8HWIi9WMJ-", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Response part 2", "comment": "5) One can apply backwards analysis to consider SGD under random sampling (with replacement), and indeed this analysis was already presented in [1]. We will clarify this point in the text. However in this case, SGD is described by a (modified) stochastic differential equation which exhibits variance at $O(\\epsilon)$. This stochastic differential equation is significantly more challenging to analyse than the ODE presented in our work, since the variance in the iterates at $O(\\epsilon)$ may also give rise to a bias in the iterates at $O(\\epsilon^2)$. This bias is difficult to identify in closed form, and therefore the implicit regularization effect of SGD is harder to uncover for this case. By instead requiring that each minibatch is sampled once per epoch, we eliminate the variance at $O(\\epsilon)$, and consequently the bias in the iterates at $O(\\epsilon^2)$ becomes explicit.\n\n We will repeat our Wide-ResNet experiments sampling with replacement, and add these to the appendices (unfortunately these experiments may not be complete before the end of the rebuttal period).\n\n6) The curvature of the loss does play a role in the analysis, and we will update the text to clarify this point. Note that the implicit regularization term arises from a Hessian-vector product (see equation 9), and therefore the implicit regularization itself arises from the curvature of the loss. Furthermore, our analysis assumes that $m\\epsilon = N\\epsilon/B$ is small, which allows us to neglect terms at $O(\\epsilon^3)$ and above. However the neglected terms at $O(\\epsilon^3)$ are composed of first, second and third derivatives of the minibatch losses, and the accuracy of this approximation will depend on the scales of these derivatives.\n\n7) Our main goal in this work is simply to derive the lowest order corrections to gradient flow, which arise from finite learning rates. Remarkably, these corrections have a natural interpretation as an implicit regularizer, but we are not aware of any principled reason why this implicit regularizer is beneficial in practice.\n\n Intuitively, the implicit regularizer is composed of two components, one of which penalizes the euclidean norm of the full batch gradient, and which might therefore be interpreted as penalizing \"sharp minima\". The second component, which dominates in the small batch limit, penalizes the trace of the covariance matrix of the per example gradients, and it might therefore be interpreted as favouring consistent solutions which perform well on all training examples. We will extend our discussion of this point to refer to related works.\n\n8) We refer to figure 1c in [2], and figure 5 in [3], for examples where the optimal learning rate is independent of batch size in the large batch limit. See also figure 8 in [4].\n\n9) Yes, this is correct. We use a small but finite learning rate to mean that terms at $O(\\epsilon^2)$ are non-negligible while terms at $O(\\epsilon^3)$ remain small, and we will clarify this in the text as suggested.\n\nMinor comments: We will resolve the issues raised with the references and formatting.\n\nPlease let us know if you have any further comments or questions.\n\nBest wishes,\nThe Authors\n\n[1] Stochastic modified equations and adaptive stochastic gradient algorithms, Li et al.\n\n[2] On the Generalization Benefit of Noise in Stochastic Gradient Descent, Smith et al.\n\n[3] An Empirical Model of Large-Batch Training, McClandish et al.\n\n[4] Measuring the Effects of Data Parallelism on Neural Network Training, Shallue et al."}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "X8HWIi9WMJ-", "original": null, "number": 6, "cdate": 1605542147865, "ddate": null, "tcdate": 1605542147865, "tmdate": 1605542147865, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "uJhk1VSHz-P", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Response part 1", "comment": "We would like to thank you for your review, and for your constructive feedback which we believe will help us to significantly improve the paper. We address all of your comments below, and we are currently working to incorporate the changes you suggest in the text.\n\nFor readability, the numbering on our responses below match the numbering of the comments raised in your review:\n\n1) Our analysis considers the expected value of the SGD iterates, but neglects the variance of individual training runs. We agree that this point should have been clearer in the original submission, and we will update the text to clarify.\n\n We note that most prior work studying SGD in the limit of small learning rates has focused on the variance of the iterates, rather than the bias. This is because, under a random sampling strategy, the variance arises at $O(\\epsilon)$, while the bias arises at $O(\\epsilon^2)$. However under the random shuffling strategy we consider, both the variance and the bias arise only at $O(\\epsilon^2)$. We therefore anticipate that the variance will play a less important role than is commonly supposed, which is why we choose to focus on the bias of the expected iterate in this work. Our experiments suggest that the bias in the iterates up to $O(\\epsilon^2)$ can explain most of the generalization benefit of finite learning rate SGD, at least for Wide-ResNets/CIFAR-10. However we do agree that studying the variance of the iterates is an interesting avenue for future work, and we will update the text to clarify this point.\n\n Intriguingly, there is a specific sequence of minibatches which leaves the bias at $O(\\epsilon^2)$ unchanged, but for which the SGD iterates only exhibit variance at $O(\\epsilon^3)$ and above. To achieve this we shuffle the dataset, perform a single epoch, then reverse the dataset and perform a second epoch, iterating through the same minibatches but in the opposite order. We then shuffle again and repeat. If you inspect equations 13-15 in our analysis, you will see that reversing the sequence of the minibatches every second epoch has the same effect as taking the expectation over all possible sequences (it replaces the sum over $k < j$ by a sum over $k \\neq j$). Under this \"reverse epoch\" strategy the path taken by the modified flow will coincide exactly with the discrete iterates of SGD (for a specific training run, at the end of every second epoch).\n\n2) See point 1.\n\n3) We agree that the line \"It is standard practice to shuffle the dataset once per epoch, but this step does not affect our analysis and we omit it for brevity\u201c was misleading and we will remove it from the updated text. \n\n We also agree that equation 22 is more intuitive than equation 1, since it does not refer to specific minibatches and clarifies the role of batch size. We debated before submission whether to derive equation 22 directly or to provide both steps as in the manuscript, however we chose eventually to provide both steps. This is because equation 1 can be expressed directly as a sum over minibatch losses, and it is therefore clearer how to implement the modified loss from equation 1 in our experiments. We also believe that the derivation is easier to follow in two stages. We will add a note to clarify this in the paper.\n\n4) Our analysis assumes that $m\\epsilon = N\\epsilon/B$ is small, in order to neglect terms at $O(m^3\\epsilon^3)$. We agree that this is at first sight an extreme approximation and that, as we already note in the text, higher order terms are also likely to play a role in practice. We anticipate that these terms could be an interesting avenue for future work, and we will make this point clearer in the updated text. However, we believe the approximation is reasonable for the following reasons:\n\n  - Our work identifies the lowest order correction to gradient flow for SGD with finite learning rates. We believe that this is in itself a significant theoretical contribution, especially since this lowest order correction has a natural interpretation as an implicit regularizer.\n\n  - Our experiments suggest that, at least for Wide-ResNets/CIFAR-10, our analysis is sufficient to describe most of the generalization benefit of large learning rates.\n\n  - Furthermore, as we show in section 3, the optimal learning rate is usually proportional to the batch size for small batch sizes, and therefore N\\epsilon/B does not grow as the batch size falls. Consequently, our approximation may be surprisingly accurate in practice, even when the number of updates in an epoch is large.\n\n  - Many previous well-cited works in this area make even more extreme approximations, yet these works have still yielded useful insights. For instance, the SDE analogy neglects all terms of $O(\\epsilon^2)$ and above, while our analysis only neglects terms at $O(\\epsilon^3)$.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "hFLNxg-g87F", "original": null, "number": 5, "cdate": 1605539864195, "ddate": null, "tcdate": 1605539864195, "tmdate": 1605539864195, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "f2ookzGQv_r", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your helpful feedback, and your recommendation to accept our work. \n\nYour review raises three important points, which we discuss below. We are currently working on an updated manuscript to incorporate your comments, and we believe that this will significantly improve the paper.\n\n**Additional experiments:**\nWe apologize that the original submission appears to make overly strong empirical claims in section 4. This was not our intention. Our goal was simply to provide intriguing evidence that the SDE analogy does not capture the generalization benefit of SGD in at least one case (Wide-ResNets/CIFAR-10). We will update the language in this section to resolve this issue.\n\nWe are also currently preparing additional experiments on MNIST using a fully connected network. We hope to add these to the appendices before the end of the rebuttal period. If they are not complete in time we will add them to the final version.\n\n**Bias and Variance:**\n\nOur analysis focuses on the mean evolution of the SGD iterates. The reviewer is correct to comment that we do not study the variance of the iterates, and this is an important point. We will update the text to make sure that this is explained clearly.\n\nWe note that, under the random sampling strategy (common in previous theoretical analyses but rarely used in practice), finite learning rates introduce variance in the evolution at $O(\\epsilon)$, while the bias arises at $O(\\epsilon^2)$, so it is natural to study the variance in this setting. However under the random shuffling strategy we consider, both the bias and the variance arise at $O(\\epsilon^2)$. We therefore anticipate that the bias will play a more important role for the random shuffling strategy than is commonly supposed, which was one of the key inspirations for our analysis. Our experiments suggest that the bias in the iterates up to $O(\\epsilon^2)$ can explain most of the generalization benefit of finite learning rates, at least for Wide-ResNets/CIFAR-10.\n\nIntriguingly, there is a simple strategy for sampling minibatches which entirely suppresses the variance in the SGD iterates at $O(\\epsilon^2)$, while leaving the bias at $O(\\epsilon^2)$ unchanged. To achieve this, we shuffle the dataset, perform one epoch of updates, then reverse the dataset and perform a second epoch, iterating through the same minibatches but in the opposite order. We then shuffle again and repeat. If you inspect equations 13-15 in our analysis, you will see that reversing the sequence of the minibatches every second epoch has the same effect as taking the expectation over all possible sequences (it replaces the sum over $k < j$ by a sum over $k \\neq j$). Under this \"reverse epoch\" strategy the path taken by the modified flow will coincide exactly with the discrete iterates of SGD (for a specific training run, at the end of every second epoch).\n\n**Is the approximation reasonable:**\nOur analysis assumes that $m\\epsilon = N\\epsilon/B$ is small, in order to neglect terms at $O(m^3\\epsilon^3)$. We agree that this is at first sight an extreme approximation and that, as we already note in the text, higher order terms are also likely to play a role in practice. We anticipate that these terms could be an interesting avenue for future work, and we will make this point clearer in the updated text. However, we believe the approximation is reasonable for the following reasons:\n\n1) Our work identifies the lowest order correction to gradient flow for SGD with finite learning rates. We believe that this is in itself a significant theoretical contribution, especially since this lowest order correction has a natural interpretation as an implicit regularizer.\n\n2) Our experiments suggest that, at least for Wide-ResNets/CIFAR-10, our analysis is sufficient to describe most of the generalization benefit of large learning rates.\n\n3) Furthermore, as we show in section 3, the optimal learning rate is usually proportional to the batch size for small batch sizes, and therefore N\\epsilon/B does not grow as the batch size falls. Consequently, our approximation may be surprisingly accurate in practice, even when the number of updates in an epoch is large.\n\n4) Many previous well-cited works in this area make even more extreme approximations, yet these works have still yielded useful insights. For instance, the SDE analogy neglects all terms of $O(\\epsilon^2)$ and above, while our analysis only neglects terms at $O(\\epsilon^3)$.\n\nWe will update the text to clarify all of the points above. Please let us know if there are any other issues you would like us to address.\n\nBest wishes,\nThe Authors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "cvs2qe68wEC", "original": null, "number": 4, "cdate": 1605538840039, "ddate": null, "tcdate": 1605538840039, "tmdate": 1605538840039, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "N0BP1f9P5ZW", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your positive feedback and useful comments! \n\n**Bias and Variance:**\nYou raise an important point. Our analysis in section 2 focuses on the mean evolution of SGD, averaged over all possible sequences of a fixed set of minibatches. It therefore identifies the bias that arises from finite step sizes, but neglects the variance in the iterates across different training runs. To expand further on this point:\n\nUnder random sampling strategies (often analyzed in previous theoretical work, but rarely used in practice), SGD exhibits variance at $O(\\epsilon)$, but the bias from finite learning rates only arises at $O(\\epsilon^2)$. It is therefore natural to focus on the variance rather than the bias in the limit of small learning rates. However under the random shuffling strategy we study in this work (which is also more common in practice), both bias and variance arise at $O(\\epsilon^2)$. We therefore anticipate that the bias will play a more important role for the random shuffling strategy than is commonly supposed, which was one of the key inspirations for our analysis. Our experiments suggest that the bias in the iterates up to $O(\\epsilon^2)$ can explain the generalization benefit of finite learning rate SGD, at least for Wide-ResNets/CIFAR-10.\n\nIntriguingly, we can construct a specific sequence of minibatches which further suppresses the variance, such that the bias at $O(\\epsilon^2)$ is unchanged but the variance arises only at $O(\\epsilon^3)$. To do this, we shuffle the dataset, perform a single epoch, then reverse the dataset and perform a second epoch, iterating through the same sequence of minibatches but in the opposite order. We then shuffle the dataset and repeat. If you inspect equations 13-15 in our analysis, you will see that reversing the sequence of the minibatches on every second epoch has the same effect as taking the expectation over all possible sequences (it replaces the sum over $k < j$ by a sum over $k \\neq j$). Under this \"reverse epoch\" strategy the path taken by the modified flow will coincide exactly with the discrete iterates of SGD (for a specific training run, at the end of every second epoch).\n\nWe will update the text to ensure that it is clear that our analysis focuses on the mean evolution. We will also describe the \"reverse epoch\" strategy above, and extend our discussion of the relative roles of bias and variance under different sampling strategies.\n\n**Additional experiments:**\nWe are currently preparing additional experiments on MNIST using a fully connected network. We hope to add these to the appendices before the end of the rebuttal period, and if they are not complete in time we will add them to the final version.\n\nWe hope that the clarifications above are helpful. Please let us know if you have any more questions or comments.\n\nBest wishes,\nThe Authors\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "5JHX7CITYyl", "original": null, "number": 3, "cdate": 1605538018653, "ddate": null, "tcdate": 1605538018653, "tmdate": 1605538018653, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "55Fkhakb3Ah", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your positive feedback. We are really glad that you enjoyed reading the paper!\nPlease let us know if you have any further questions.\n\nBest wishes,\nThe Authors\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "rq_Qr0c1Hyo", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3269/Authors|ICLR.cc/2021/Conference/Paper3269/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923839287, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Comment"}}}, {"id": "N0BP1f9P5ZW", "original": null, "number": 3, "cdate": 1604010933771, "ddate": null, "tcdate": 1604010933771, "tmdate": 1605024032873, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review", "content": {"title": "Good paper, can be improved by varying experiments", "review": "This paper studies an implicit regularization mechanism of finite learning rate SGD by introducing explicitely a regularization term, using the framework of backward analysis.\n\nThey theoretically motivate their analysis, then empirically demonstrate it on CIFAR-10 using a Wide ResNet architecture.\n\nThis extends a previous (Barrett and Dherin, preprint) analysis of GD using the same framework, but limited to full batch GD. Noticeably, this new analysis using minibatch GD highlights an additional regularization of the trace of the covariance of per-example gradients.\n\nIn sec 2, however, I think it should be made clear that the setup is slightly different from minibatch GD, even when trained for a single epoch, in that there is an expectation accross permutations of sequences of minibatches. Can you discuss this assumption a bit more?\n\nIn terms of experiments, it would be useful to include other architecture/tasks, even toyish, in order to appreciate the generality of the empirical evaluation.\n\nOverall, I think this contributes new interesting insights which are very relevant for studying minibatch GD in deep learning.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078876, "tmdate": 1606915773596, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3269/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review"}}}, {"id": "55Fkhakb3Ah", "original": null, "number": 4, "cdate": 1604279423149, "ddate": null, "tcdate": 1604279423149, "tmdate": 1605024032781, "tddate": null, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "invitation": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review", "content": {"title": "Review of On the Origin of Implicit Regularization in Stochastic Gradient Descent ", "review": "This paper analyzes the implicit regularization in SGD with finite learning rates via backward error analysis. The modified flow introduced in this paper better approximates the practical behavior of SGD as it does not require vanishing learning rates and it allows to use random shuffling in stead of i.i.d sampling. The numerical experiments validates the existence of the implicit regularization and how it affects the generalization of the model trained by SGD. The difference from SDE analysis is also discussed.\n\nReason for score:\n1. The paper is well organized. Specially, I enjoy reading section II. The tool of backward error analysis and the derivation of the implicit regularization in SGD flow are introduced clearly and concisely. The analysis is based on random shuffling instead of i.i.d sampling matches the practical use of SGD.\n2. The numerical experiments are very convincing. The consistency of SGD with larger lr and SGD with smaller lr plus explicit regularization validates the results of theoretical analysis. The numerical experiments also provide some insights into tuning hyper parameters such as learning rate and batch size.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3269/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3269/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "authorids": ["~Samuel_L_Smith1", "~Benoit_Dherin1", "~David_Barrett1", "~Soham_De2"], "authors": ["Samuel L Smith", "Benoit Dherin", "David Barrett", "Soham De"], "keywords": ["SGD", "learning rate", "batch size", "optimization", "generalization", "implicit regularization", "backward error analysis", "SDE", "stochastic differential equation", "ODE", "ordinary differential equation"], "abstract": "For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.", "one-sentence_summary": "For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "smith|on_the_origin_of_implicit_regularization_in_stochastic_gradient_descent", "pdf": "/pdf/e5f4bcf96d3ed905ac91e4ea6e3993321ecda830.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsmith2021on,\ntitle={On the Origin of Implicit Regularization in Stochastic Gradient Descent},\nauthor={Samuel L Smith and Benoit Dherin and David Barrett and Soham De},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=rq_Qr0c1Hyo}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "rq_Qr0c1Hyo", "replyto": "rq_Qr0c1Hyo", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3269/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538078876, "tmdate": 1606915773596, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3269/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3269/-/Official_Review"}}}], "count": 19}