{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028613912, "tcdate": 1490028613912, "number": 1, "id": "SyRBdFTjx", "invitation": "ICLR.cc/2017/workshop/-/paper124/acceptance", "forum": "rJNa3C4Yg", "replyto": "rJNa3C4Yg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Performance guarantees for transferring representations", "abstract": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. Our work proposes novel and general sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight sharing, which we validate with experiments.", "pdf": "/pdf/ee0119268433513002f39776ef50ccd1945a841a.pdf", "TL;DR": "We develop sufficient conditions for successfully transferring representations between tasks and present an application to weight sharing in neural networks.", "paperhash": "mcnamara|performance_guarantees_for_transferring_representations", "keywords": ["Theory", "Transfer Learning"], "conflicts": ["anu.edu.au", "data61.csiro.au", "cmu.edu"], "authors": ["Daniel McNamara", "Maria-Florina Balcan"], "authorids": ["daniel.mcnamara@anu.edu.au", "ninamf@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028614477, "id": "ICLR.cc/2017/workshop/-/paper124/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJNa3C4Yg", "replyto": "rJNa3C4Yg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028614477}}}, {"tddate": null, "tmdate": 1489513846334, "tcdate": 1489513846334, "number": 2, "id": "r1RO6jBse", "invitation": "ICLR.cc/2017/workshop/-/paper124/public/comment", "forum": "rJNa3C4Yg", "replyto": "ByVRBv7jl", "signatures": ["~Daniel_McNamara1"], "readers": ["everyone"], "writers": ["~Daniel_McNamara1"], "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for the review and the thoughtful feedback. We have made amendments to the paper which address your suggestions.\n\nWe have made a few changes to the explanations given for each of the theorems to enhance the readability and clarity of the paper. We have also made a couple of refinements to the paper's notation.\n\nWhile the function \\omega in Theorem 1 is necessarily abstract, we have added wording describing the role it plays, explaining why it is a necessary assumption and pointing the reader to the example \\omega in Theorem 2.\n\nWe have provided greater explanation in Section 4 about why assuming lower level features are more transferrable is reasonable in relevant applications.\n\nWe have tightened the language describing Theorem 1 to remove the \"vague terms\" that you mentioned."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Performance guarantees for transferring representations", "abstract": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. Our work proposes novel and general sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight sharing, which we validate with experiments.", "pdf": "/pdf/ee0119268433513002f39776ef50ccd1945a841a.pdf", "TL;DR": "We develop sufficient conditions for successfully transferring representations between tasks and present an application to weight sharing in neural networks.", "paperhash": "mcnamara|performance_guarantees_for_transferring_representations", "keywords": ["Theory", "Transfer Learning"], "conflicts": ["anu.edu.au", "data61.csiro.au", "cmu.edu"], "authors": ["Daniel McNamara", "Maria-Florina Balcan"], "authorids": ["daniel.mcnamara@anu.edu.au", "ninamf@cs.cmu.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487363260581, "tcdate": 1487363260581, "id": "ICLR.cc/2017/workshop/-/paper124/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper124/reviewers"], "reply": {"forum": "rJNa3C4Yg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487363260581}}}, {"tddate": null, "tmdate": 1489513710972, "tcdate": 1489513710972, "number": 1, "id": "rywlTsSol", "invitation": "ICLR.cc/2017/workshop/-/paper124/public/comment", "forum": "rJNa3C4Yg", "replyto": "SJWBkqmie", "signatures": ["~Daniel_McNamara1"], "readers": ["everyone"], "writers": ["~Daniel_McNamara1"], "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for the review and the thoughtful feedback. We have made amendments to the paper which address your suggestions.\n\nWe have made a few amendments to the explanations given for each of the theorems to provide additional insights about them. We have also highlighted the novelty of the work, in particular the generality of the sufficient conditions (now mentioned in the abstract) and the arguments used in the neural network example proofs (now mentioned before statement of Theorem 2). Separate to this submission, we have also written a longer paper which includes all proofs. \n\nWe have also added a sentence to the third paragraph of the introduction to provide more comparison of the pros and cons of a fixed representation vs fine-tuning."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Performance guarantees for transferring representations", "abstract": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. Our work proposes novel and general sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight sharing, which we validate with experiments.", "pdf": "/pdf/ee0119268433513002f39776ef50ccd1945a841a.pdf", "TL;DR": "We develop sufficient conditions for successfully transferring representations between tasks and present an application to weight sharing in neural networks.", "paperhash": "mcnamara|performance_guarantees_for_transferring_representations", "keywords": ["Theory", "Transfer Learning"], "conflicts": ["anu.edu.au", "data61.csiro.au", "cmu.edu"], "authors": ["Daniel McNamara", "Maria-Florina Balcan"], "authorids": ["daniel.mcnamara@anu.edu.au", "ninamf@cs.cmu.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487363260581, "tcdate": 1487363260581, "id": "ICLR.cc/2017/workshop/-/paper124/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper124/reviewers"], "reply": {"forum": "rJNa3C4Yg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487363260581}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489513275843, "tcdate": 1487363259973, "number": 124, "id": "rJNa3C4Yg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJNa3C4Yg", "signatures": ["~Daniel_McNamara1"], "readers": ["everyone"], "content": {"title": "Performance guarantees for transferring representations", "abstract": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. Our work proposes novel and general sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight sharing, which we validate with experiments.", "pdf": "/pdf/ee0119268433513002f39776ef50ccd1945a841a.pdf", "TL;DR": "We develop sufficient conditions for successfully transferring representations between tasks and present an application to weight sharing in neural networks.", "paperhash": "mcnamara|performance_guarantees_for_transferring_representations", "keywords": ["Theory", "Transfer Learning"], "conflicts": ["anu.edu.au", "data61.csiro.au", "cmu.edu"], "authors": ["Daniel McNamara", "Maria-Florina Balcan"], "authorids": ["daniel.mcnamara@anu.edu.au", "ninamf@cs.cmu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489375032756, "tcdate": 1489375032756, "number": 2, "id": "SJWBkqmie", "invitation": "ICLR.cc/2017/workshop/-/paper124/official/review", "forum": "rJNa3C4Yg", "replyto": "rJNa3C4Yg", "signatures": ["ICLR.cc/2017/workshop/paper124/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper124/AnonReviewer1"], "content": {"title": "reasonable first step for an important problem", "rating": "7: Good paper, accept", "review": "The paper provides generalization bounds for a common practice in transfer learning with deep neural nets, where the representation learned on a source task (having lot of labeled data) is transferred to a target task. It analyzes two settings: (i) when representation learned on the source is kept fixed and a new classifier for the target task is learned on top of it, (ii) when the representation is also fine-tuned for the target task. To the best of my knowledge it seems to be the first work to analyze this setting. \n\nPros:\n- considers an important problem\n- well-written paper\n\nCons:\n- hard to say if the proof techniques used are novel -- not enough details\n- doesn't give much intuition on when is fine tuning better than fixed representation", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Performance guarantees for transferring representations", "abstract": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. Our work proposes novel and general sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight sharing, which we validate with experiments.", "pdf": "/pdf/ee0119268433513002f39776ef50ccd1945a841a.pdf", "TL;DR": "We develop sufficient conditions for successfully transferring representations between tasks and present an application to weight sharing in neural networks.", "paperhash": "mcnamara|performance_guarantees_for_transferring_representations", "keywords": ["Theory", "Transfer Learning"], "conflicts": ["anu.edu.au", "data61.csiro.au", "cmu.edu"], "authors": ["Daniel McNamara", "Maria-Florina Balcan"], "authorids": ["daniel.mcnamara@anu.edu.au", "ninamf@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489375033490, "id": "ICLR.cc/2017/workshop/-/paper124/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper124/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper124/AnonReviewer2", "ICLR.cc/2017/workshop/paper124/AnonReviewer1"], "reply": {"forum": "rJNa3C4Yg", "replyto": "rJNa3C4Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper124/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper124/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489375033490}}}, {"tddate": null, "tmdate": 1489364428235, "tcdate": 1489364428235, "number": 1, "id": "ByVRBv7jl", "invitation": "ICLR.cc/2017/workshop/-/paper124/official/review", "forum": "rJNa3C4Yg", "replyto": "rJNa3C4Yg", "signatures": ["ICLR.cc/2017/workshop/paper124/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper124/AnonReviewer2"], "content": {"title": "A new theoretical angle into transfer learning", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes sufficient conditions for success of transfer learning\npros:\noriginality: To the best of my knowledge, this work is original\nsignificance: transfer learning has lead to considerable improvement in deep learning and a theoretical approach for formulating when and how it succeeds is very important and much needed\n\ncons.\nclarity: the paper is not completely well-written and in places hard to follow\nquality: overall, I like this paper due to the problem it considers and its approach, however, the paper would improve significantly with filling in the gaps mentioned below:\n* The authors provide no intuition or insight into how the bound is derived and what the different terms mean, e.g., how does function w look in practice for different datasets? what are the ways to measure or approximate it?\n* The assumptions are given without any justification of why they are needed and what are the cases that the hold. e.g., The property that is assumed in Theorem 1. Section 4: assuming that lower level features are more transferrable.\n* There are some vague terms used right before Theorem 1 that are not appropriate for a theory paper:  If w does not grow too \"quickly\", \\hat{R} is \"small\", etc\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Performance guarantees for transferring representations", "abstract": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. Our work proposes novel and general sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight sharing, which we validate with experiments.", "pdf": "/pdf/ee0119268433513002f39776ef50ccd1945a841a.pdf", "TL;DR": "We develop sufficient conditions for successfully transferring representations between tasks and present an application to weight sharing in neural networks.", "paperhash": "mcnamara|performance_guarantees_for_transferring_representations", "keywords": ["Theory", "Transfer Learning"], "conflicts": ["anu.edu.au", "data61.csiro.au", "cmu.edu"], "authors": ["Daniel McNamara", "Maria-Florina Balcan"], "authorids": ["daniel.mcnamara@anu.edu.au", "ninamf@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489375033490, "id": "ICLR.cc/2017/workshop/-/paper124/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper124/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper124/AnonReviewer2", "ICLR.cc/2017/workshop/paper124/AnonReviewer1"], "reply": {"forum": "rJNa3C4Yg", "replyto": "rJNa3C4Yg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper124/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper124/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489375033490}}}], "count": 6}