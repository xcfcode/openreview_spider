{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028643133, "tcdate": 1490028643133, "number": 1, "id": "H1jP_Fajl", "invitation": "ICLR.cc/2017/workshop/-/paper163/acceptance", "forum": "rJzabxSFg", "replyto": "rJzabxSFg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intelligent synapses for multi-task and transfer learning", "abstract": "Deep learning has led to remarkable advances when applied to problems in which the data distribution does not change over the course of learning. In stark contrast, biological neural networks exhibit continual learning, solve a diversity of tasks simultaneously, and have no clear separation between training and evaluation phase. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery that enable non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce intelligent synapses that are capable of accumulating information over time, and exploiting this information to efficiently protect old memories from being overwritten as new problems are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.", "pdf": "/pdf/c0fcbd62799e705b450ca6ef50d32ead4eab7cba.pdf", "TL;DR": "Learn sequences of tasks in a unified network by preventing important weights from changing.", "paperhash": "poole|intelligent_synapses_for_multitask_and_transfer_learning", "keywords": [], "conflicts": ["stanford.edu", "google.com"], "authors": ["Ben Poole*", "Friedemann Zenke*", "Surya Ganguli"], "authorids": ["poole@cs.stanford.edu", "fzenke@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028643693, "id": "ICLR.cc/2017/workshop/-/paper163/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJzabxSFg", "replyto": "rJzabxSFg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028643693}}}, {"tddate": null, "tmdate": 1489481205878, "tcdate": 1489481205878, "number": 2, "id": "ryCeA7Bse", "invitation": "ICLR.cc/2017/workshop/-/paper163/official/review", "forum": "rJzabxSFg", "replyto": "rJzabxSFg", "signatures": ["ICLR.cc/2017/workshop/paper163/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper163/AnonReviewer1"], "content": {"title": "Nice idea, suitable for workshop track", "rating": "7: Good paper, accept", "review": "The authors introduce a penalty term that encourages neuron stability.\nThe idea is clear and the experiments illustrate the point.\n\nThough not groundbreaking, I do like the paper and would recommend it for publication.\n\nGiven the following will be done:\n- Please discuss the exact computational cost of the method in absolute numbers.\n- An analysis of the fraction of weights receiving almost no updates on the given tasks. It would be interesting to see, if any surprising observations can be made here.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intelligent synapses for multi-task and transfer learning", "abstract": "Deep learning has led to remarkable advances when applied to problems in which the data distribution does not change over the course of learning. In stark contrast, biological neural networks exhibit continual learning, solve a diversity of tasks simultaneously, and have no clear separation between training and evaluation phase. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery that enable non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce intelligent synapses that are capable of accumulating information over time, and exploiting this information to efficiently protect old memories from being overwritten as new problems are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.", "pdf": "/pdf/c0fcbd62799e705b450ca6ef50d32ead4eab7cba.pdf", "TL;DR": "Learn sequences of tasks in a unified network by preventing important weights from changing.", "paperhash": "poole|intelligent_synapses_for_multitask_and_transfer_learning", "keywords": [], "conflicts": ["stanford.edu", "google.com"], "authors": ["Ben Poole*", "Friedemann Zenke*", "Surya Ganguli"], "authorids": ["poole@cs.stanford.edu", "fzenke@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489481206622, "id": "ICLR.cc/2017/workshop/-/paper163/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper163/AnonReviewer2", "ICLR.cc/2017/workshop/paper163/AnonReviewer1"], "reply": {"forum": "rJzabxSFg", "replyto": "rJzabxSFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper163/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489481206622}}}, {"tddate": null, "tmdate": 1489151696284, "tcdate": 1489151696284, "number": 1, "id": "ByOCI7gsx", "invitation": "ICLR.cc/2017/workshop/-/paper163/official/review", "forum": "rJzabxSFg", "replyto": "rJzabxSFg", "signatures": ["ICLR.cc/2017/workshop/paper163/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper163/AnonReviewer2"], "content": {"title": "Nice work, what about larger datasets?", "rating": "6: Marginally above acceptance threshold", "review": "This submission presents a technique which allows to train neural networks subsequently on a series of task without loosing too much performance on the tasks which have already been trained. The methods estimates the importance of synapses in training by keeping track of the gradient of the loss with respect to the synapse: large gradients indicate that the synapse is important for the task and therefore this synapse will be prevented from changing too much when training for subsequent tasks.\n\nI think the method provides a very nice approach to multi-task training and is worth presenting at ICLR. I am however missing an analysis on more state-of-the-art datasets like ImageNet or at least TinyImageNet since a lot of interesting methods fail to work (or get too expensive) on large datasets.\n\nOn first reading the paper I was surprised that the gradient would serve as a good estimate for the importance of the synapse since it should be approximately zero at the end of the optimization (for comparison, fisher information is related to the hessian matrix). But since the authors are integrating the gradient over the course of optimization, a large value should mean that the gradient used to be large in the optimization and is small at the end, which one might take as a second order statement.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intelligent synapses for multi-task and transfer learning", "abstract": "Deep learning has led to remarkable advances when applied to problems in which the data distribution does not change over the course of learning. In stark contrast, biological neural networks exhibit continual learning, solve a diversity of tasks simultaneously, and have no clear separation between training and evaluation phase. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery that enable non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce intelligent synapses that are capable of accumulating information over time, and exploiting this information to efficiently protect old memories from being overwritten as new problems are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.", "pdf": "/pdf/c0fcbd62799e705b450ca6ef50d32ead4eab7cba.pdf", "TL;DR": "Learn sequences of tasks in a unified network by preventing important weights from changing.", "paperhash": "poole|intelligent_synapses_for_multitask_and_transfer_learning", "keywords": [], "conflicts": ["stanford.edu", "google.com"], "authors": ["Ben Poole*", "Friedemann Zenke*", "Surya Ganguli"], "authorids": ["poole@cs.stanford.edu", "fzenke@stanford.edu", "sganguli@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489481206622, "id": "ICLR.cc/2017/workshop/-/paper163/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper163/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper163/AnonReviewer2", "ICLR.cc/2017/workshop/paper163/AnonReviewer1"], "reply": {"forum": "rJzabxSFg", "replyto": "rJzabxSFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper163/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper163/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489481206622}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487372644275, "tcdate": 1487368634097, "number": 163, "id": "rJzabxSFg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJzabxSFg", "signatures": ["~Ben_Poole1"], "readers": ["everyone"], "content": {"title": "Intelligent synapses for multi-task and transfer learning", "abstract": "Deep learning has led to remarkable advances when applied to problems in which the data distribution does not change over the course of learning. In stark contrast, biological neural networks exhibit continual learning, solve a diversity of tasks simultaneously, and have no clear separation between training and evaluation phase. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery that enable non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce intelligent synapses that are capable of accumulating information over time, and exploiting this information to efficiently protect old memories from being overwritten as new problems are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.", "pdf": "/pdf/c0fcbd62799e705b450ca6ef50d32ead4eab7cba.pdf", "TL;DR": "Learn sequences of tasks in a unified network by preventing important weights from changing.", "paperhash": "poole|intelligent_synapses_for_multitask_and_transfer_learning", "keywords": [], "conflicts": ["stanford.edu", "google.com"], "authors": ["Ben Poole*", "Friedemann Zenke*", "Surya Ganguli"], "authorids": ["poole@cs.stanford.edu", "fzenke@stanford.edu", "sganguli@stanford.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}