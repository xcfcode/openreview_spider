{"notes": [{"id": "0WWj8muw_rj", "original": "NMVrL82QJ5R", "number": 2908, "cdate": 1601308322573, "ddate": null, "tcdate": 1601308322573, "tmdate": 1614985774065, "tddate": null, "forum": "0WWj8muw_rj", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_Kw8Nwjm9Qh", "original": null, "number": 1, "cdate": 1610040357301, "ddate": null, "tcdate": 1610040357301, "tmdate": 1610473947030, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Dear authors,\n\nImproving the theoretical understanding of powerful algorithms is an important contribution to our field. Nevertheless, most of the reviewers are inclined to reject the paper. I somehow have to agree with them as e.g., adding more restrictive assumptions can allow deriving better bounds, but the question then is how useful this result will be to the ICLR community. I would encourage you to chose maybe another venue.\n\nThanks"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040357286, "tmdate": 1610473947012, "id": "ICLR.cc/2021/Conference/Paper2908/-/Decision"}}}, {"id": "95XxJ08yglD", "original": null, "number": 1, "cdate": 1603719877192, "ddate": null, "tcdate": 1603719877192, "tmdate": 1607255439506, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review", "content": {"title": "Reviewer #3", "review": "I will initially provide a summary of the paper and list overall strengths and weakness of the paper. Then, I present my additional comments which are related to specific expressions in the main text, proof steps in the appendix etc. I would appreciate it very much if authors could address my questions/concerns under \u201cAdditional Comments\u201d as well, since they affect my assessment and understanding of the paper; consequently my score for the paper.\n\n\n\nSummary:\n\n\u2022\tThe paper focuses on convergence of two newly-proposed versions of AdaGrad, namely AdaGrad-window and AdaGrad-truncation, for finite sum setting where each component is smooth and possibly nonconvex. \n\n\u2022\tThe authors prove convergence rate with respect to number of epochs T, where in each epoch one full pass over the data is performed with respect to well-known \u201crandom shuffling\u201d sampling strategy. \n\n\u2022\tSpecifically, AdaGrad-window is shown to achieve $\\tilde{ \\mathcal O } (T^{-1/2})$ rate of convergence, whereas AdaGrad-truncation attains $\\mathcal( T^{-1/2} )$ convergence, under component-wise smoothness and bounded gradients assumptions. Additionally, authors introduce a new condition/assumption called consistency ratio which is an essential element of their analysis.\n\n\u2022\tThe paper explains the proposed modification to AdaGrad and provide their intuition for such adjustments. Then, the main results are presented followed by a proof sketch, which demonstrates the main steps of the theoretical approach.\n\n\u2022\tIn order to evaluate the practical performance of the modified adaptive methods in a comparative fashion, two set of experiments were provided: training logistic regression model on MNIST dataset and Resnet-18 model on CIFAR-10 dataset. In these experiments; SGD, SGD with random shuffling, AdaGrad and AdaGrad-window were compared. Additionally, authors plot the behavior of their proposed condition \u201cconsistency ratio\u201d over epochs.\n\n\n\nStrengths:\n\n\u2022\tI think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.\n\n\u2022\tI have checked the proof of Theorem 1 in details and had a less detailed look at Theorems 2 and 3. I appreciate some of the technically rigorous sections of the analysis as the authors bring together analytical tools from different resources and re-prove certain results with respect to their adjustments.\n\n\u2022\tPerformance comparison in the paper is rather simple but the authors try to provide a perspective of their consistency condition through numerical evidence. It gives some rough idea about how to interpret this condition.\n\n\u2022\tMain text is written in a clear; authors highlight their modification to AdaGrad and also highlight what their new \u201cconsistency condition\u201d is. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. One of the main theorems has a proof sketch which gives an overall idea about authors\u2019 approach to proving the results. \n\n\n\nWeaknesses:\n\n\u2022\tAlthough numerically the paper provides an insight into the consistency condition, it is not verifiable ahead of time. One needs to run a simulation to get some idea about this condition, although it still wouldn\u2019t verify the correctness. Since authors did not provide any theoretical motivation for their condition, I am not fully convinced out this assumption. For instance, authors could argue about a specific problem setting in which this condition holds.\n\n\u2022\tTheorem 3 (Adagrad-truncation) sets the stepsize depends on knowledge of $r$. I couldn\u2019t figure out how it is possible to compute the value $r$ ahead of time. Therefore, I do not think this selection is practically applicable. Although I appreciate the theoretical rigor that goes into proving Theorem 3, I believe the concerns about computing $r$ weakens the importance of this result. If I am missing out some important point, I would like to kindly ask the authors to clarify it for me.\n\n\u2022\tThe related work which is listed in Table 1, within the group \u201cAdaptive Gradient Methods\u201d prove \\emph{iteration-wise} convergence rates for variants of Adam and AdaGrad, which I would call the usual practice. This paper argues about \\emph{epoch-wise} convergence. The authors claim improvement over those prior papers although the convergence rate quantifications are not based on the same grounds. All of those methods consider the more general expectation minimization setting. I would suggest the authors to make this distinction clear and highlight iteration complexities of such methods while comparing previous results with theirs. In my opinion, total complexity comparison is more important that rate comparison for the setting that this paper considers.\n\n\u2022\tAs a follow up to the previous comment, the related work could have highlighted related results in finite sum setting. Total complexity comparisons with respect to finite sum setting is also important. There exists results for finite-sum nonconvex optimization with variance reduction, e.g., Stochastic Variance Reduction for Nonconvex Optimization, 2016, Reddi et. al. I believe it is important to comparatively evaluate the results of this paper with that of such prior work.\n\n\u2022\tNumerically, authors only compare against AdaGrad and SGD. I would say this paper is a rather theory paper, but it claims rate improvements, for which I previously stated my doubts. Therefore, I would expect comparisons against other methods as well, which is of interest to ICLR community in my opinion. \n\n\u2022\tThis is a minor comment that should be easy to address. For ICLR, supplementary material is not mandatory to check, however, this is a rather theoretical paper and the correctness/clarity of proofs is important. I would say authors could have explained some of the steps of their proof in a more open way. There are some crucial expressions which were obtained without enough explanations. Please refer to my additional comments in the following part.\n\n\n\nAdditional Comments:\n\n\u2022\tI haven\u2019t seen the definition that $x_{t, m+1} = x_{t+1, 1}$ in the main text. It appears in the supplements. Could you please highlight this in the main text as it is important for indexing in the analysis?\n\n\u2022\tSecond bullet point of your contributions claim that \u201c[consistency] condition is easy to verify\u201d. I do not agree with this as I cannot see how someone could guarantee/compute the value $r$ ahead of time or even after observing any sequence of gradients. Could you please clearly define what verification means in this context?\n\n\u2022\tIn Assumption A3, I understand that $G_t e_i = g_{t,i}$ and $G_t e = \\sum_{i=1}^{m} g_{t,i}$. I believe the existing notation makes it complicated for the reader to understand the implications of this condition.\n\n\u2022\tIn the paragraph right above Section 4.2, authors state that presence of second moments, $V_{t,i}$ enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details? \n\n\u2022\tIn Corollary 1, authors state that \u201cthe computational complexity is nearly $\\tilde{ \\mathcal O (m^{5/2}nd^2\\epsilon^{-2}) }$\u201d. A similar statement exists in Corollary 2. Could you please explain what \u201cnearly\u201d means in this context?\n\n\u2022\tIn Lemma 8 in the supplements, $aa^T$ and $bb^T$ in the main expression of the lemma are rank-1 matrices. This lemma has been used in the proof of Lemma 4. As far as I understood, Lemma 8 is used in such a way that $aa^T$ or $bb^T$ correspond to something like $g_{t,j}^2 \u2013 g_{t-1, j}^2$. I am not sure if this construction fits into Lemma 8 because, for instance, the expression $g_{t,j}^2 \u2013 g_{t-1, j}^2$ is difference of two rank-1 matrices, which could have rank \\leq 2. Hence, there may not exist some vector $a$ such that $aa^T = g_{t,j}^2 \u2013 g_{t-1, j}^2$, hence Lemma 8 may not be applied. If I am mistaken in my judgment I am 100% open for a discussion with the authors. \n\n\u2022\tIn the supplements, in section \u201cA.1.7 PROOF OF MAIN THEOREM 1\u201d, in the expression following the first line, I didn\u2019t understand how you obtained the last upper bound to $\\| \\nabla f(x_{t,i}) \\|$. Could you please explain how this is obtained?\n\n\n\nScore:\n\nI would like to vote for rejecting the paper. I praise the analytically rigorous proofs for the main theorems and the use of a range of tools for proving the key lemmas. Epoch-wise analysis for stochastic methods could provide insight into behavior of algorithms, especially with respect to real-life experimental setting. However, I have some concerns:\n\n1.\tI am not convinced about the importance of consistency ratio and that it is a verifiable condition.\n\n2.\tRelated work in Table 1 has iteration-wise convergence in the general expectation-minimization setting whereas this paper considers finite sum structure with epoch-wise convergence rates. The comparison with related work is not sufficient/convincing in this perspective.\n\n3.\t(Minor) I would suggest the authors to have a more comprehensive experimental study with comparisons against multiple adaptive/stochastic optimizers. More experimental insight might be better for demonstrating consistency ratio.\n\n\n\nOverall, due to the reasons and concerns stated in my review, I vote for rejecting this paper. I am open for further discussions with the authors regarding my comments and their future clarifications.\n\n======================================= Post-Discussions ======================================= \n\nI would like to thank the authors for their clarifications. After exchanging several responses with the authors and regarding other reviews, I decide to keep my score. \n\n1. Although the authors come up with a more meaningful assumption, i.e., SGC, compared to their initial condition, I am not fully convinced about the contributions with respect to prior work: SGC assumption is a major factor in the improved rates and it is a very restrictive assumption to make in practice. \n\n2. Although this paper proposes theoretical contributions regarding adaptive gradient methods, the experiments could have been a bit more detailed. I am not sure whether the experimental setup fully displays improvements of the proposed variants of AdaGrad.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086185, "tmdate": 1606915760453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2908/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review"}}}, {"id": "joNRu0M2-Ty", "original": null, "number": 17, "cdate": 1606302524896, "ddate": null, "tcdate": 1606302524896, "tmdate": 1606302524896, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "AGCUpo_bw_w", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "further replies", "comment": "Thank you for your quick response. We upload a minor revision based on your comments on the definition of asymptotic convergence and statements of convergence rate improvement."}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "AGCUpo_bw_w", "original": null, "number": 16, "cdate": 1606301643012, "ddate": null, "tcdate": 1606301643012, "tmdate": 1606301643012, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "KlUBn0ZbINY", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "follow up comments to convergence rate discussions", "comment": "I think your definition of asymptotic convergence should be explained because from the perspective of optimization terminology, asymptotic convergence rate means something else and creates a confusion.\n\nBasically, you are relying on epoch-wise analysis, as adopted in Nguyen et al., 2020, but this leads to an additional $\\sqrt{n}$ factor in your convergence rate. I understand your point about epoch-wise convergence quantification.\n\nThank you for the quick response."}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "KlUBn0ZbINY", "original": null, "number": 15, "cdate": 1606300438264, "ddate": null, "tcdate": 1606300438264, "tmdate": 1606301389257, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "WFaelv3Z_Ie", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Thank you for your quick response", "comment": "By saying asymptotic, we meant that the results of Vaswani et al., 2019 require to take expectation over the randomness in gradients, where you point out that they used $\\mathbb{E}(X|F_n) = \\mathbb{E}(X)$ ($X$ is the stochastic gradient in iteration $n$ to my understanding) like standard analysis of SGD. Sorry if the terminology of _asymptotic_ used here is confusing. \n\nIn our analysis (which is to study the algorithm under random shuffling), the randomness comes from the partitioning of datasets. One can achieve results without taking the expectation over this randomness in random shuffling because the epoch-wise analysis can cancel out the effect of randomness. See, for example, Theorem 4 in (Nguyen et al., 2020). An intuitive understanding could be that in each epoch, we iterate through the entire dataset so that we do not need to account for this partitioning if we analyze epoch by epoch. Does this provide some answer to your question?\n\n\n\n[1] Lam M Nguyen, Quoc Tran-Dinh, Dzung T Phan, Phuong Ha Nguyen, and Marten van Dijk. A unified convergence analysis for shuffling-type gradient methods. arXiv preprint arXiv:2002.08246, 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "lcO5Vtvdi_i", "original": null, "number": 14, "cdate": 1606300037402, "ddate": null, "tcdate": 1606300037402, "tmdate": 1606300037402, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "fVIG-9B_3i", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "thank you for clarifying proof of Lemma 4", "comment": "I tried to verify this derivation for Lemma 4 and it seems correct to me. Thank you for the detailed answer."}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "WFaelv3Z_Ie", "original": null, "number": 13, "cdate": 1606298920362, "ddate": null, "tcdate": 1606298920362, "tmdate": 1606298920362, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "fogGaQY1LRs", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Answering your last part of the responses", "comment": "I think there is a misunderstanding of the results of Vaswani et al., 2019. Could you please explain why you are saying this rate is asymptotic? To my understanding, the rate is non-asymptotic as the attainment of the rate does not require any quantity to approach zero in the limit. \n\nThe proof for their Theorem 3 is pretty standard: they cannot evaluate the full expectation on its own with the set of assumptions they have as any iterate depends on the stochasticity in all the previous iterations (which is introduced through stochastic gradients). Hence, one needs to use some properties of expectation. Let $X$ be an $F$-measurable random variable and let $F_1 \\subset F_2 \\subset ... \\subset F_n \\subset F$ be a sequence of sub-sigma algebras of $F$. Then, $\\mathbb E ( X ) = \\mathbb E( \\mathbb E(X \\mid F_n) )$. Moreover, $\\mathbb E( \\mathbb E( X \\mid F_1) \\mid F_2 ) = \\mathbb E( \\mathbb E( X \\mid F_2) \\mid F_1 ) = \\mathbb E( X \\mid F_1 )$ almost surely. Their proof basically uses a combination of these properties which is standard in analysis of SGD-type methods. In fact, the first property is a special case of the second.\n\nAfter having a second look at your analysis, I got confused about the fact that you are not dealing with stochasticity of your gradients, $g_{t,i}$. The randomness in your setting is due to partitioning of the dataset into batches $[ \\mathbb B_i ]_{i = 1, ..., m}$, but you are not taking this randomness into account. I am yet to figure the reason for this out. Could you please explain to me if I am missing any point?\n\nAs always, I am open for further exchange of comments."}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "fogGaQY1LRs", "original": null, "number": 12, "cdate": 1606278287417, "ddate": null, "tcdate": 1606278287417, "tmdate": 1606278287417, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "fVIG-9B_3i", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Additional responses due to space limits", "comment": "- _You claim that Vaswani et al., 2019, shows **asymptotic** convergence rate for SGD. However, their Theorem 3 seems to prove non-asymptotic convergence of SGD with a rate of_ $O(1/T^{1/2})$ _for non-convex losses satisfying SGC._\n\nTheir theorem 3 is asymptotic where they showed $\\min_{i} \\mathbb{E} [\\|\\nabla f(w_i)\\|]\\leq \\frac{2\\rho L}{k} [f(w_0)-f^*]$. The expectation is taken with respect to the stochasticity in gradients. This expectation actually helps a lot in proving of theorem 3 as they took expectation with respect to $z_0,...,z_{t-1}$ in appendix B.2.\n\n- _I don\u2019t agree with the statement that \u201cThe key towards our convergence rate improvement is twofold: the epoch-wise analysis of random shuffling enables us to leverage the benefit of full gradients; the adaptive learning rates endow a better improvement of objective value in consecutive epochs.\u201d I would say SGC is the key assumption that enables the fast rate of $O(1/T^{1/2})$ as there are known theoretical bounds for smooth nonconvex problems which cannot be improved without further/stronger assumptions. Would the authors agree with me on this statement? If I am missing out something, please correct my mistake._\n\nThank you so much for asserting your concerns. We would say that the statement regarding our convergence rate improvement is still correct, as one can see from the overview of analysis in section 4. Undoubtedly, SGC plays a key part in this result. However, we think that this statement includes SGC as \"the adaptive learning rates endow a better improvement of objective value in consecutive epochs\" uses SGC to show a better improvement. Maybe to make this statement more clear, we should modify it into **\u201cThe key towards our convergence rate improvement is twofold: the epoch-wise analysis of random shuffling enables us to leverage the benefit of full gradients; the adaptive learning rates and the strong growth condition endow a better improvement of objective value in consecutive epochs.\u201d**\n\n[1] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over- parameterized models and an accelerated perceptron, 2019.\n\n[2] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtarik. Sgd: General analysis and improved rates, 2019.\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "fVIG-9B_3i", "original": null, "number": 11, "cdate": 1606278211868, "ddate": null, "tcdate": 1606278211868, "tmdate": 1606278211868, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "sP-18U8WXcL", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Thank you so much for your further responses ", "comment": "We will respond to your comments and answer your questions as follows:\n\n- _I think replacing your previous unverifiable condition with strong growth condition (SGC) makes your theoretical claims stronger. I must note that SGC itself is a very strong assumption to make and in most applications with real data, this condition does not hold. You explain when this condition holds in section 6, and it is evident from some of those explanations that you need strong assumptions on the data, initialization or the loss function to make sure SGC is satisfied._\n\nThank you for your recognition of our revision. As stated in the first work that defines the strong growth condition, we admit that it is a very strong assumption in that it requires the model to interpolate the data. Although this interpolation property has been observed to hold for many overparameterized models with strong expressive ability, it would still require some additional assumptions in order to satisfy SGC, e.g. weak growth condition + PL inequality. What we've shown in section 6 is that, without these kinds of additional assumptions, SGC is satisfied by two types of models (one of which SGC is satisfied by a constraint set of $x$). However, it remains unsolved that SGC can be satisfied by general models. Some partial answers to this might be the relaxations of SGC, e.g. weak growth condition [1] or expected smoothness [2].\n\n- _My comment about r was related to your previous condition which was not verifiable ahead of time. For SGC, to the best of my knowledge, you need to know the parameter r to show convergence. I don\u2019t know of any result that adapts to r._\n\nThank you for the clarification of the question. However, we are still not sure if the previous answer would adapt to this question. Our response regarding theorem 3 is that here it needs $r$ to show convergence like many other results that need $L$ (the L-smoothness constant) to show convergence. These parameters might not be given to us beforehand. But in practice, since these parameters appear in step size and we usually determine the stepsize by tuning it, it doesn't have much impact on experiments.\n\n- _I am assuming you are referring to rank-1 matrices $aa^T$ and $bb^T$  by saying \"Both $a$ and $b$ are rank-1 vectors\". Let\u2019s focus on the construction proposed in the proof of Lemma 4. You set_ $C=\\sum_{j=1}^{i+k} g_{t,j}^2+\\sum_{j=0}^{m-i-2-k} g_{t-1,m-j}^2$. _However, if you look at the expression for_ $V_{t,m}^{1/2}=\\delta I_d+\\sum_{j=1}^{m}g_{t,j}^2$, _it does not have any gradient components from previous iteration (iteration t-1) such as_ $g_{t\u22121,j}$. _Hence, I don\u2019t think your construction of $C$ is correct. In fact, for arbitrary $i$, I don\u2019t see how you would make use of Lemma 10 by representing_ $V_{t,i}^{1/2}\u2013V_{t,m}^{1/2}$ _in the form of_ $(\\delta I_d+C+aa^T)^{1/2}-(\\delta I_d+C+bb^T)$.\n\nTo answer this question, first let's reconfirm the definitions:\n\n$$ V_{t,m} = \\delta I_d+\\sum_{j=1}^{m}g_{t,j}^2,\\quad V_{t,i} = \\delta I_d+\\sum_{j=1}^{i}g_{t,j}^2+\\sum_{j=i+1}^{m}g_{t-1,j}^2 $$\n\nNext, in order to use Lemma 10, we represents $V_{t,i}^{1/2}-V_{t,m}^{1/2}$ in the form of\n$$\n\\sum_{k=0}^{m-i-1} \\big[(\\delta I_d+C_k+a_ka_k^T)^{1/2} - (\\delta I_d+C_k+b_kb_k^T)^{1/2}\\big]\n$$\ninstead of $(\\delta I_d+C+aa^T)^{1/2}-(\\delta I_d+C+bb^T)$. Therefore, we actually use Lemma 10 for **m-i times** instead of one time.\n\nThe way to construct $C_k,a_k,b_k$ so that $V_{t,i}^{1/2}-V_{t,m}^{1/2}=\\sum_{k=0}^{m-i-1} \\big[(\\delta I_d+C_k+a_ka_k^T)^{1/2} - (\\delta I_d+C_k+b_kb_k^T)^{1/2}\\big]$ is by setting\n$$\nC_k = \\sum_{j=1}^{i+k} g_{t,j}^2 + \\sum_{j=0}^{m-i-2-k}g_{t-1,m-j}^2,\\quad a_k = g_{t-1,i+k+1},\\quad b_k=g_{t,i+k+1}\n$$\nso that $-(\\delta I_d+C_k+b_kb_k^T)^{1/2}$ and $(\\delta I_d+C_{k+1}+a_{k+1}a_{k+1}^T)^{1/2}$ cancel out, $(\\delta I_d+C_{0}+a_{0}a_{0}^T)^{1/2}=V_{t,i}^{1/2}$ and $(\\delta I_d+C_{m-i-1}+b_{m-i-1}b_{m-i-1}^T)^{1/2}=V_{t,m}^{1/2}$ (note that $\\sum_{j=0}^{-1}$ is considered to be $0$).\n\nDoes this answer your question?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "sP-18U8WXcL", "original": null, "number": 10, "cdate": 1606251070411, "ddate": null, "tcdate": 1606251070411, "tmdate": 1606251070411, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "nT5eXGi5E4n", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Thank you for your responses to my initial review", "comment": "I will respond to some of the individual comments and also make some general statements regarding your modifications:\n\n-\t_We change the consistency ratio assumption to the strong growth condition, which is very similar in form but with much nicer theoretical properties. The strong growth condition can be verifiable ahead of time without simulations, and we add in proofs to show that the assumption holds for two specific types of objective functions._\n\nI think replacing your previous unverifiable condition with strong growth condition (SGC) makes your theoretical claims stronger. I must note that SGC itself is a very strong assumption to make and in most applications with real data, this condition does not hold. You explain when this condition holds in section 6, and it is evident from some of those explanations that you need strong assumptions on the data, initialization or the loss function to make sure SGC is satisfied.\n\n-\t_For the r presented in Theorem 3, we agree this makes the theory a little less elegant. However many optimizers depend on unknown constant, e.g., L-smooth constant for gradient descent, and it will not harm the practicality since we usually tuning the constant in the step size._\n\nMy comment about $r$ was related to your previous condition which was not verifiable ahead of time. For SGC, to the best of my knowledge, you need to know the parameter $r$ to show convergence. I don\u2019t know of any result that adapts to $r$.\n\n-\tThank you for your other responses for my comments under Weaknesses.\n\n-\tI would also like to thank for your responses for my comments Additional Comments. I will have a specific answer for one of those responses: _We add in an explanation of how to apply Lemma 8 (Lemma 10 now) to Lemma 4. Lemma 8 is used in the first aligned equation of proofs for Lemma 4, where in the second inequality, ..._\n\nI am assuming you are referring to rank-1 matrices $aa^T$ and $bb^T$ by saying \"Both $a$ and $b$ are rank-1 vectors\". \n\nLet\u2019s focus on the construction proposed in the proof of Lemma 4. You set $C = \\sum_{j=1}^{i+k} g_{t,j}^2 + \\sum_{j=0}^{m-i-2-k} g_{t-1,m-j}^2$. However, if you look at the expression for $V_{t,m}^{1/2} = \\delta \\mathbb I_d + \\sum_{j=1}^{m} g_{t,j}^2$, it does not have any gradient components from previous iteration (iteration t-1) such as $g_{t-1, j}$. Hence, I don\u2019t think your construction of C is correct. In fact, for arbitrary $i$, I don\u2019t see how you would make use of Lemma 10 by representing $V_{t,i}^{1/2} \u2013 V_{t,m}^{1/2}$ in the form of $(\\delta \\mathbb I_d + C + aa^T)^{1/2} \u2013 (\\delta \\mathbb I_d + C + bb^T)^{1/2}$. \n\n**Additional Comments based on the updated manuscript:**\n\n-\tYou claim that Vaswani et al., 2019, shows **asymptotic** convergence rate for SGD. However, their Theorem 3 seems to prove non-asymptotic convergence of SGD with a rate of $\\mathcal O (1 / T^{1/2})$ for non-convex losses satisfying SGC.\n\n-\tI don\u2019t agree with the statement that \u201cThe key towards our convergence rate improvement is twofold: the epoch-wise analysis of random shuffling enables us to leverage the benefit of full gradients; the adaptive learning rates endow a better improvement of objective value in consecutive epochs.\u201d I would say SGC is the key assumption that enables the fast rate of $\\mathcal O (1/  T^{1/2})$ as there are known theoretical bounds for smooth nonconvex problems which cannot be improved without further/stronger assumptions. Would the authors agree with me on this statement? If I am missing out something, please correct my mistake.\n\nI am open for further discussions to make sure we understand each other's perspectives.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "o_6GmL2DkbE", "original": null, "number": 8, "cdate": 1605882246546, "ddate": null, "tcdate": 1605882246546, "tmdate": 1605882246546, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "b0vCyC8Pb3I", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Appreciation for your quick response but several misunderstandings in your comment", "comment": "Thank you for your quick response. \n\nFor starters, we are **not** proposing a new approach. We are providing a theoretical **analysis** to gain a better understanding of **adaptive gradient methods**. Our analysis indeed gives a better convergence rate for adaptive gradient methods with random shuffling, compared with the existing rates $O(T^{-1/4})$ of adaptive gradient methods and $O(T^{-1/3})$ of random shuffling SGD.  We have stated this very clearly in the introduction as well as the first response.\n\nSecond, the convergence rate of $O(m^{5/4}T^{-1/2})$ applies to any choice of $m$. $m>1$ is where random shuffling comes into meaning. One would say this result implies that choosing $m=1$ is the best, and as we discuss in the complexity analysis, this order on $m$ might not be strict and encourage future work to improve it. Because of this, we do not conclude anything on which choice of the batch size is the best. But this nevertheless does **not** affect the main contribution, which is to **improve the convergence rate of adaptive gradient methods**.\n\nFinally, it is a well-known result that GD converges with $O(T^{-1/2})$ for non-convex optimization. It is also known that vanilla SGD converges with $O(T^{-1/4})$ and random shuffling SGD convergences with $O(n^{1/3}T^{-1/3})$, which are both worse than GD.  One can hardly say these results have no contributions if they can not compete with the simplest method. It is our goal to better understand more complex methods and we believe we have achieved that.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "b0vCyC8Pb3I", "original": null, "number": 7, "cdate": 1605873621913, "ddate": null, "tcdate": 1605873621913, "tmdate": 1605873644654, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "eq0oDax7Kbu", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Still contribution is unclear to me", "comment": "Thank you for your reply but I still do not see any benefit of your approach.\n\nLets make sure we are on the same page:\n* T - is the total number of epochs which is proportional to the total number of computations \nThus you measure the performance versus total number of computations which is the right measure to use.\n\n*In theorem 1 you show that the \"error\" behaves like,\n $O(m^{5/4}/\\sqrt{T})$ thus the optimal error is achieved when m=1 which corresponds to the case of using full gradients.\n\n*In case we use full gradients the random shuffling that is used is actually meaningless. Thus using full gradients is preferable compared to random shuffling together with minibatch Adaptive gradient method.\n\n*If we use full gradients for T epochs and the function is smooth then it is a well known result that one can obtain  O(1/\\sqrt{T}) rate even without using an adaptive scheme, and simple GD suffices in this case\n\n\nTo conclude: you show that random shuffling with adaptive method is always worse than full-gradient with adaptive method, and in the latter case you show a results similar to simple GD. Thus, I do not see any benefit neither to random shuffling nor to using adaptive methods....\n\nI therefore keep my score\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "uUk4t4Qah8Y", "original": null, "number": 4, "cdate": 1605870338366, "ddate": null, "tcdate": 1605870338366, "tmdate": 1605871910690, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "pPiF0DLYxmX", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Response to reviewer #1", "comment": "Thank you so much for the valuable comment and questions! We upload a revised version of our paper, in which the consistency ratio assumption is replaced by the strong growth assumption. These two assumptions are very similar in form but the latter has much nicer property and could answer a lot of the related questions. Please refer to the general response where we summarize the difference between the two versions.\n\nFor the technical questions, we answer them as follows:\n1. It is very similar to the strong growth condition assumption in form, where the main proof is almost intact after changing A3. The strong growth condition assumption is enforcing the norms of individual gradients to be bounded by the norm of the full gradient and has close relation with interpolation. The details are discussed in section 6.1.\n2. As following the previous response, we would answer questions in the context of the strong growth condition assumption. In our revision, we summarize the relationship between the strong growth condition and other assumptions in section 6.1.\n3. In section 6.1, we show that the assumption holds for two specific types of objective functions.\n4. Following the answer to the second question, strong convexity plus interpolation implies Assumption 3, where $r$ is closely related to the strongly convex constant and L-smooth constant. This can be seen from the proof of Lemma 6. SGD under assumption 3 and strongly convexity actually converges linearly. We discuss this in section 6.2.\n5. The step size in theorem 2 is independent of $f^*$. As for theorem 3, we agree dependencies on $f^*$ makes it a little less elegant. However many optimizers depend on unknown constant, e.g., L-smooth constant for gradient descent, and it will not harm the practicality since we usually tuning the constant in the step size.\n6. If we choose $\\eta_t = \\eta/T$, we would have a much worse convergence rate of $O(1/\\ln T)$, since the term on the left side of lemma 1 would add up to $\\ln T$.\n\nOur contribution is mainly on the theoretical analysis, as stated in the introduction. Numerically, we only compare AdaGrad and SGD (with/without shuffling) since that is the main objective of this submission and the presented experiments can provide support to the main objective. We apologize for the typos and change them in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "BnGejgEdYD2", "original": null, "number": 5, "cdate": 1605870676362, "ddate": null, "tcdate": 1605870676362, "tmdate": 1605871707108, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "UmTpW-gc7Md", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Response to reviewer #2", "comment": "Thank you so much for the sharing of thoughts and doubts! As responses to the several points in the review, we answer them in the following.\n\n- (In-depth discussions about the assumption) Indeed, as we recently discovered, the consistency condition here is very similar to the strong growth condition, which essentially needs to bound the stochastic gradients using the true gradient. It has a close relationship with other convex assumptions. Under this assumption, several asymptotic better convergences have been shown. We discuss this in section 6 of our revision.\n\n- (Doubts about memory) Thanks for pointing this out! Roughly speaking, the update is very similar to the exponential moving average since it only focuses on recent gradients. We suspect that the analysis could be applied to other methods, such as Adam, RMSprop, AMSGrad, etc.\n\n- (Comparison with GD) Yes, it seems that this would be an issue for the non-convex setting where SGD and AdaGrad can not beat GD in theory. Lowering the order on m could be potential future improvements. It would also help provide some insights into how to choose the size of mini-batches.\n\n- (Details of proof) In the proof of Lemma 6 (Lemma 8 now), the second equality is actually an inequality with $\\leq$ on the bottom row. Therefore, it doesn't violate the example. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "eBQQBZ5q7pZ", "original": null, "number": 2, "cdate": 1605868370110, "ddate": null, "tcdate": 1605868370110, "tmdate": 1605871544802, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank all the reviewers for their valuable comments, questions, and suggestions. Based on these reviews, we revise our submission into the current version.\n\nIn this part, we summarize the differences between the two versions, in an effort to answer the common hanging questions. \n1. We would like to apologize for neglecting a line of works centering on the _Strong Growth Condition_ (Schmidt & Roux, 2013; Vaswani et al., 2019; Gower et al., 2019). The strong growth condition assumption is very similar to our original consistency ratio assumption in form. $$\\frac{1}{n}\\sum_{i=1}^n ||\\nabla f_i(x)||\\leq r^2 ||\\nabla f(x)||^2\\quad vs. \\quad \\frac{1}{m}\\sum_{i=1}^m ||\\nabla f_{B_i}(x_{t,i})||\\leq r^2 ||\\nabla f(x_{t,1})||^2$$ \n   However, the strong growth condition has much nicer properties: \n   - It is algorithmic-independent in that it does not depend on $x_{t,i}$ generated by specific algorithms.\n   - It can be verifiable ahead of time without simulations since it only depends on the objective function.\n   - A wide range of objective functions can be shown to satisfy this condition.\n\n2. For reasons in 1, we change our consistency ratio assumption to the strong growth condition assumption. The main analysis is almost intact with slight modifications to Lemma 1 and Lemma 3. \n3. We discuss in detail functions satisfying the assumption and add in proofs to show that the assumption holds for two specific types of objective functions in section 6.\n4. In the introduction, we change the epoch-wise comparison to iteration-wise comparison, which is more commonly accepted as measurements in stochastic optimization. We also make clarifications about expectation minimization and finite sum minimization for the sake of fair comparisons. \n5. In experiments, since the strong growth condition is verifiable without simulations, we took out the part that tested the assumption. We point out here that the strong growth condition can be tested using experiments, but would require calculations of full gradients. Therefore, this could be a downside of the strong growth condition compared to the consistency ratio. However, since the submission focuses on theoretical results, one would prefer the strong growth condition assumption with much nicer theoretical groundings.\n\n[1] Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong growth condition, 2013.\n\n[2] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over- parameterized models and an accelerated perceptron, 2019.\n\n[3] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtarik. SGD: General analysis and improved rates, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "nT5eXGi5E4n", "original": null, "number": 3, "cdate": 1605869781184, "ddate": null, "tcdate": 1605869781184, "tmdate": 1605871123747, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "95XxJ08yglD", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Response to reviewer #3", "comment": "We first want to thank the reviewer for the detailed and valuable comment. All the suggestions are helpful and very much appreciated. We would like to point that we upload a revision that has a summary in the general response. As for the listed questions in the review, we will give corresponding responses in the following.\n\nWeakness:\n- We change the consistency ratio assumption to the strong growth condition, which is very similar in form but with much nicer theoretical properties. The strong growth condition can be verifiable ahead of time without simulations, and we add in proofs to show that the assumption holds for two specific types of objective functions.\n- For the $r$ presented in Theorem 3, we agree this makes the theory a little less elegant. However many optimizers depend on unknown constant, e.g., L-smooth constant for gradient descent, and it will not harm the practicality since we usually tuning the constant in the step size. \n- For questions regarding epoch-wise comparisons. Thanks for pointing this out! It needs a bit of clarification. In the revision, we change the epoch-wise comparison to iteration-wise comparison. We also state the distinction between expectation minimization and finite sum minimization. As a result, non-asymptotic convergences for shuffling in the finite sum minimization would introduce a dependency on sample size $n$.\n- Following the response to the previous comment, in the clarification we made about finite sum minimization vs expectation minimization, we discuss briefly how various methods can reduce variance. However, we consider SVRG to be a distinctive technique. So to make the objective of this submission clear, which is to compare adaptive gradient methods vs SGD with random shuffling, we did not add quantitative comparisons with SVRG.\n- Numerically, we only compare AdaGrad and SGD (with/without shuffling) since that is the main objective of this submission. We hope that the revised version would answer questions regarding rate improvements and that the presented experiments can provide support to the main objective. In the future, with time less limited, we would add in comparisons which could be of interest.\n\nAdditional comments:\n- Thanks for suggesting to put $x_{t+1,1}=x_{t,m+1}$ in the main text. We highlighted this in the main text where AdaGrad-type methods are introduced.\n- (2 & 3 bullet points) Please refer to the first answer in weakness. Thanks for the detailed doubts regarding the assumption. They helped a lot in our development of the revision.\n- In the large equation in section 4 with terms S1 and S2, the adaptive step size allows us to bound S1 using $\\|\\sum_{i=1}^m g_{t,i}\\|$ instead of $\\|\\sum_{i=1}^m g_{t,i}\\|^2$, which is the case for SGD ($V_{t,m}=I$).\n- We revise this in the new version. \"nearly\" is ignoring the logarithm terms.\n- We add in an explanation of how to apply Lemma 8 (Lemma 10 now) to Lemma 4. Lemma 8 is used in the first aligned equation of proofs for Lemma 4, where in the second inequality, we use Lemma 8 $m-i$ times by setting $C=\\sum_{j=1}^{i+k} g_{t,j}^2+\\sum_{j=0}^{m-i-2-k}g_{t-1,m-j}^2$ and $a=g_{t-1,i+k+1}, b = g_{t,i+k+1}$ (Both $a$ and $b$ are rank-1 vectors).\n- We add in a line of equation of how to bound $||\\nabla f(x_{t,1})||$, it is essentially using triangle inequality and bounds on $||\\nabla f(x_{t,1})-\\frac{1}{m}\\sum_{i=1}^m g_{t,i}||$ and $||\\frac{1}{m}\\sum_{i=1}^m g_{t,i}||$.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "eq0oDax7Kbu", "original": null, "number": 6, "cdate": 1605871062147, "ddate": null, "tcdate": 1605871062147, "tmdate": 1605871062147, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "AWrt-8TCi1I", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment", "content": {"title": "Response to reviewer #4", "comment": "Thanks for the review. However, we are very sorry about the comment. We think that the reviewer has missed out on quite a few major parts of our submission and would like to respond by making several clarifications about the review.\n\nFirst of all, in both versions, we measure our methods in terms of gradient evaluations. We apologize that we did not highlight this in the introduction and change this in the revision.\n\nSecondly, the reviewer pointed out that the method studied in this paper is equivalent to GD when m=1. This is not correct, since like the original AdaGrad, we would still have an adaptive step size of $\\eta (\\delta I+ g_{t,1}g_{t,1}^\\top)^{-1}$ when m=1. Furthermore, it is a general issue for the non-convex setting where SGD and AdaGrad can not beat GD in theory. What we have proved is that adaptive gradient methods can achieve the same order on $T$ as GD. Although the complexity analysis might seem that choosing $m=1$ is the best, it is again not GD.\n\nFinally, we highlight again, as written clearly in the introduction, that our goal and contribution lies in the theoretical analysis of adaptive gradient methods with random shuffling. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Paper2908/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0WWj8muw_rj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2908/Authors|ICLR.cc/2021/Conference/Paper2908/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843214, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Comment"}}}, {"id": "pPiF0DLYxmX", "original": null, "number": 2, "cdate": 1603749211893, "ddate": null, "tcdate": 1603749211893, "tmdate": 1605024106067, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review", "content": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "review": "This paper develops new stepsize rules for Adagrad with shuffling to improve the convergence rate from $O(1/T^{1/3})$ in the previous works to $\\tilde{O}(1/T^{1/2})$, which is significantly better than existing AdaGrad variants.  \nHowever, after reading up to the analysis as well as the assumptions stated in the paper, it appears that Assumption 3 seems to be artificially enforced to achieve such a fast rate. Unfortunately, the authors do not provide examples or mathematical justification to show why this assumption is reasonable. This assumption is not algorithmic independent, it depends on the sequences generated by the algorithm. Although the authors conduct some experiments to show that this assumption holds in practice, but such an example remains nonconstructive and may hold by accident. \n\nThough the proof technique seems to be new, it heavily relies on Assumption 3 to transform the squared-norm of the gradient as usually used in gradient-based methods for nonconvex problems to the norm of the gradient, making the convergence rate to be faster than known results as can be seen in the key recursive inequality (5) of Lemma 1. \n\nIn terms of technical details, the reviewer would like to raise the following questions?\n1) What should be the nature of Assumption A.3.? \n2) How should it relate to existing assumptions, e.g., strong convexity-type or bounded variance condition? \n3) Is there any example that is independent of algorithms such that this assumption can be twisted to satisfy? \n4) This rate seems to match the rate of SGD in the strongly convex case? Is there any relation between Assumption A3. and the strong convexity? If so, what is the meaning of the constant r?\n5) Can the stepsize in Theorems  2 and 3 be independent of $f^*$? \n6) Can one choose constant stepsize $\\eta_t = \\eta/T$ so that the $\\log(T)$ term in the convergence rate disappear?\n\nSince the main contribution is on the new choice of step-size/learning rate, it is very hard to assess the work from a practical point of view if one just simply tunes the learning rate in standard Adagrad to compete with the new algorithm. So, I do not see the experiment reflects much the efficiency of the new algorithm if the test is only done with NN training. Therefore, additional examples could be added to illustrate the benefit of the new variant.\n\nIn addition, the paper still contains so many typos as well as some inconsistent English expressions. Here are some of them: bouned, \nintuitions, we refer scalars that does not depend on t to constants, ormance, seems off, etc.\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086185, "tmdate": 1606915760453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2908/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review"}}}, {"id": "UmTpW-gc7Md", "original": null, "number": 3, "cdate": 1603851521558, "ddate": null, "tcdate": 1603851521558, "tmdate": 1605024105908, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review", "content": {"title": "nice results, could do with more discussion of the assumptions.", "review": "This paper shows that adaptive learning rates are beneficial for finding critical points of finite-sum optimization problems. In particular, with appropriate learning rates, a variant of adagrad can find a epsilon critical point in \\tilde O(1/epsilon^2) iterations. This improves upon previous results of either O(1/\\epsilon^3) or O(1/\\epsion^4) in various situations. The key new assumption is a \u201cconsistency condition\u201d that bounds how big individual example gradients can be when the overall gradient is small.\n\nOverall I liked this work, the result seems interesting, and potentially will inspire future work.\n\nI liked the consistency condition here, but I think the paper would be very much served by a more in-depth discussion of what this condition is saying. My own intuition here is that, roughly speaking, the standard deviation of the gradients is at most r sqrt(m) larger than the true average gradient for some constant r. \nThis seems like some kind of analog of the \u201cL*\u201d bounds in convex stochastic optimization, for which it is known that when the gradients have variance at the optimum (which also implies small loss values at the optimum in the convex case), then asymptotically faster convergence is possible. Moreover, in this case it is also known that adaptive methods like adagrad are able to achieve these rates easily. See, e.g. (https://parameterfree.com/2019/09/20/adaptive-algorithms-l-bounds-and-adagrad/).\n\nMy main qualm with the paper is that the modified algorithms appear to require O(m) memory in order to implement the windowed adagrad. This seems a bit excessive, but seems a good direction for future work. For example, it is possible that some kind of EMA would be able to remove this issue.\n\nHowever, related to this issue, it seems that the overall computational complexity of the algorithm is actually *worse* than just plain full-batch gradient descent, which would not require any memory overhead and takes O(n/epsilon^2) gradient evaluations to reach an epsilon critical point, rather than O(m^(5/2)n/epsilon^2) so it is unclear that this analysis really explains any success over full-batch gradient descent. Is it possible that the m factor is very loose in this analysis, or do I miss something here?\n\nFor Lemma 6 something seems a little off in proving the last statement. I believe the result is correct, but something is perhaps missing in the second equality, or at the very least could do with more explanation. For example, if d=1, m=2, \\delta=0 and g_1 =1 and g_2=-1, then the relevant value is zero, but the second equality would say it is sqrt(2).\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086185, "tmdate": 1606915760453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2908/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review"}}}, {"id": "AWrt-8TCi1I", "original": null, "number": 4, "cdate": 1604005797497, "ddate": null, "tcdate": 1604005797497, "tmdate": 1605024105852, "tddate": null, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "invitation": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review", "content": {"title": "No benefit over full GD", "review": "*Summary:\nThis paper investigates stochastic methods for finding an approximate stationary point for a non-convex function that can be written as a finite sum. The authors consider the combination of adaptive (Adagrad style) methods in conjunction with a random shuffling.\n\n*Significance: \nThe authors have missed a very important aspect of stochastic optimization.\nIn stochastic optimization the right way to measure progress is to present the error versus the number of stochastic gradient computations.\nThe authors present the error as a function of the number of epochs, where in each epoch we go over the whole dataset.\n\nMoreover, the suggested method does not have any benefit over full GD.\nBasically, when m=1, the method in the paper is equivalent to GD and obtains the same optimal \n$O(1/\\sqrt{T})$ rate. When $m>1$ the authors prove a rate which is worse by a factor of $O(m^{5/4})$ compared to GD. Therefore, they actually show that GD (m=1) is optimal for their algorithm and that there is no benefit to stochasticity (m > 1).\n\n\n*Summary of review:\nThe suggested method  does not show any benefit over full GD, so I don't see what is the contribution here.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2908/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2908/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Gradient Methods Can Be Provably Faster than SGD with Random Shuffling", "authorids": ["~Xunpeng_Huang1", "~Vicky_Jiaqi_Zhang2", "zhouhao.nlp@bytedance.com", "~Lei_Li11"], "authors": ["Xunpeng Huang", "Vicky Jiaqi Zhang", "Hao Zhou", "Lei Li"], "keywords": [], "abstract": "Adaptive gradient methods have been shown to outperform SGD in many tasks of training neural networks. However, the acceleration effect is yet to be explained in the non-convex setting since the best convergence rate of adaptive gradient methods is worse than that of SGD in literature. In this paper, we prove that adaptive gradient methods exhibit an $\\small\\tilde{O}(T^{-1/2})$-convergence rate for finding first-order stationary points under the strong growth condition, which improves previous best convergence results of adaptive gradient methods and random shuffling SGD by factors of $\\small O(T^{-1/4})$ and $\\small O(T^{-1/6})$, respectively. In particular, we study two variants of AdaGrad with random shuffling for finite sum minimization. Our analysis suggests that the combination of random shuffling and adaptive learning rates gives rise to better convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|adaptive_gradient_methods_can_be_provably_faster_than_sgd_with_random_shuffling", "supplementary_material": "/attachment/7aaef28a6147221a68418053bb3bd8a8f2e4b0d5.zip", "pdf": "/pdf/aa3e1d9b91089369ab6f3bbcb7c3f96c846a5d1d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=DUQAoROV9", "_bibtex": "@misc{\nhuang2021adaptive,\ntitle={Adaptive Gradient Methods Can Be Provably Faster than {\\{}SGD{\\}} with Random Shuffling},\nauthor={Xunpeng Huang and Vicky Jiaqi Zhang and Hao Zhou and Lei Li},\nyear={2021},\nurl={https://openreview.net/forum?id=0WWj8muw_rj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0WWj8muw_rj", "replyto": "0WWj8muw_rj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2908/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086185, "tmdate": 1606915760453, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2908/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2908/-/Official_Review"}}}], "count": 21}