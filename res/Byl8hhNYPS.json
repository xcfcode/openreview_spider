{"notes": [{"id": "Byl8hhNYPS", "original": "Syg_jCXMDH", "number": 189, "cdate": 1569438893564, "ddate": null, "tcdate": 1569438893564, "tmdate": 1588209556013, "tddate": null, "forum": "Byl8hhNYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "VXQUH7whHs", "original": null, "number": 1, "cdate": 1576798689756, "ddate": null, "tcdate": 1576798689756, "tmdate": 1576800945398, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper proposes using visual representations learned in a monolingual setting with image annotations into machine translation. Their approach obviates the need to have bilingual sentences aligned with image annotations, a very restricted resource. An attention layer allows the transformer to incorporate a topic-image lookup table. Their approach achieves significant improvements over strong baselines. The reviewers and the authors engaged in substantive discussions. This is a strong paper which should be included in ICLR. \n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723206, "tmdate": 1576800274646, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper189/-/Decision"}}}, {"id": "ryxpUP6RKr", "original": null, "number": 3, "cdate": 1571899220977, "ddate": null, "tcdate": 1571899220977, "tmdate": 1574565849311, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs for multimodal NMT. Their approach enables visual information to be integrated into large-scale text-only NMT. Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines.\n\nStrengths:\n- This paper is well motivated and well written. I especially like how they use external paired sentence-image data from Multi30k to learn weak pairs for sentences in machine translation.\n- Experimental results are convincing. I like how low-resource translation is included as a priority in their experiments.\n\nWeaknesses:\n- Do you have any explanations as to why the number of images, if too large, actually hurts translation performance? Is it because more images also leads to a higher chance of noisy images?\n- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.\n- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.\n- Why are there missing BLEU scores and the number of parameters in Table 1?\n\n### Post rebuttal ###\nThank you for your detailed answers to my questions.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper189/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575642544819, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper189/Reviewers"], "noninvitees": [], "tcdate": 1570237755742, "tmdate": 1575642544835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Review"}}}, {"id": "B1gYdNvCFS", "original": null, "number": 2, "cdate": 1571873904970, "ddate": null, "tcdate": 1571873904970, "tmdate": 1574307530793, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper provides an approach to use visual information to improve text only neural machine translation systems. The approach creates a \"topic word to images\" map using an existing image aligned translation corpora. Given a source sentence, the model extracts relevant images, extracts their Resnet features and fuses them with the features generated from the word sequence. The decoder uses these fused representation to generate the target sentence. Overall, I like the approach, seems like it can be easily augmented to existing NMT systems. \n\nOne of the claims of the paper was to be able to use monolingual image aligned data. However image captioning datasets are not mentioned. It would make sense to use image captioning data to create the image lookup. Also, what will be the performance of a standard image captioning system on the task ? I believe it will not be great, but I think for completeness, you should add such a baseline.\n\nMinor comments: \n1. What is M in Algorithm 1 ? \n2. First paragraph in related work is very unrelated to the current subject, please remove.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper189/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575642544819, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper189/Reviewers"], "noninvitees": [], "tcdate": 1570237755742, "tmdate": 1575642544835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Review"}}}, {"id": "rJe9E2ITYr", "original": null, "number": 1, "cdate": 1571806257838, "ddate": null, "tcdate": 1571806257838, "tmdate": 1573664834839, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #1", "review": "The authors propose to augment NMT with a grounded inventory of images.  The intuition is clear and the premise is very tempting.  The key architectural choice is to allow the transformer to use language embeddings to attend into a topic-image lookup table.  The proportion is learned to balance how much signal comes from each source.    Figure 4, attempts to investigate the importance of this sharing and its effects on performance.\n\nWhile reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.  For example, \"The old system of private arbitration courts is off the table\" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.  It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.  I trust that the authors did in fact achieve these results but I cannot figure out how or why.  This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.  \n\nIn contrast, it does make sense that Multi30K would benefit from this architecture.  As a minor note, were different feature extractors compared? The recent flurry of papers on multimodal transformers indicate that deeper resnet stacks correspond to improved downstream performance.  Is that also true in this domain?", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper189/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575642544819, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper189/Reviewers"], "noninvitees": [], "tcdate": 1570237755742, "tmdate": 1575642544835, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Review"}}}, {"id": "BylD4R7NiB", "original": null, "number": 5, "cdate": 1573301806886, "ddate": null, "tcdate": 1573301806886, "tmdate": 1573301806886, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment", "content": {"title": "Submission Update", "comment": "We thank all reviewers so much for the valuable comments on improving the quality of this work. We have updated the paper according to the feedback and our latest evaluations. The major revisions are marked in red for easy reading.\n\n1)\tWe add a discussion (Analysis 6.1) about the contribution of the lookup table for the improved results. The comparisons of different feature extractors are also included in this section. Detailed demonstrations are in Appendix A.2.\n\n2)\tWe add the discussion (Analysis 6.2) to demonstrate the effects of the number of sentence-image pairs including splitting the Multi30K and adding external MS COCO image caption datasets for comparisons.\n\n3)\tWe add the discussion of external computation time in Appendix A.1.\n\n4)\tWe add a page of more retrieved images for sentences in WMT datasets in Appendix A.4.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl8hhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper189/Authors|ICLR.cc/2020/Conference/Paper189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175045, "tmdate": 1576860550227, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment"}}}, {"id": "rJgabTX4sr", "original": null, "number": 4, "cdate": 1573301509033, "ddate": null, "tcdate": 1573301509033, "tmdate": 1573301509033, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "rJe9E2ITYr", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thanks for your insightful comments.\n\n1. How or why is the benefit.\nThis comment is insightful and we also considered about it. Intuitively, we would easily fall into the connections between each sentence and image. However, it is nearly impossible to pair sentence with images with completely the same meaning all the time. According to our investigation, we conclude that the major contribution would be more effective contextualized sentence encoding for better representation from the visual clue combination instead of single image enhancement for encoding each individual sentence or word. \n\nAccording to Distributional Hypothesis (Harris et al., 1954) which states that \u201cwords that occur in similar contexts tend to have similar meanings\u201d, we are inspired to extend the concept in multimodal world, \u201cthe sentences with similar meanings would be likely to pair with similar even the same images\u201d, where the consistent images (with similar topic) could play the role of topic or type clues for similar sentence modeling. For your example, the topic words are {private, courts, table}, which can be paired with relevant images and other sentences with the same (similar) topics will be paired with the same (similar) group of images.\n\nThis is also very similar to the idea of word embedding by taking each image as a \u201cword\u201d. Because we use the average pooled output of ResNet, each image is represented as 2400d vector. For all the 29,000 images, we have an embedding layer with size (29000, 2400). The \u201ccontent\u201d of the image is just like the embedding initialization. It indeed makes effects, but the capacity of the neural network is not up to it. In contrast, the mapping from text word to the index in the word embedding is critical. Similarly, the mapping of sentence to image in image embedding would be essential, i.e., the similar sentences (with the same topic words) tend to map the similar images. \n\nTo verify the hypothesis, we shuffle the image embeddings but keep the lookup table, to only exchange the features of each image but maintain the sentence-image mapping. Unsurprisingly, the BLEU score (EN-RO) is 33.53, which is very close to the reported one (33.78). In addition, we randomly initialize the image embedding instead of ResNet, the result is 33.28. In comparison, if we randomly retrieve unrelated images to break the lookup, the result is 32.14. These results verify the necessity of the lookup table. We have added a detailed discussion in the paper (please see Analysis 6.1). \n\nWe believe this finding would be suggestive for the future research since most previous work focused on the content of the image itself. As a different research line, we highlight the consistency among the mono-modality to bridge the gap of language and image modeling.\n\n2. Why stop words are ignored.\nAccording to the explanation above, we think the spatial relations or grammatical nuances would not be so important in this task if we take the images as topic guidance. Ignoring the stopwords can help us get rid of the disturbance of unnecessary high-frequency words (such as function words) being the topic, as the standard practice for TF-IDF topic extraction.\n\n3. Comparison of different feature extractors.\nYes. We compared with ResNet101 and ResNet152 on EN-RO. The BLEU scores are 33.63 and 33.87. It seems deeper ResNet indeed gives better results but the difference is not very significant. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl8hhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper189/Authors|ICLR.cc/2020/Conference/Paper189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175045, "tmdate": 1576860550227, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment"}}}, {"id": "r1eQFhX4ir", "original": null, "number": 3, "cdate": 1573301370894, "ddate": null, "tcdate": 1573301370894, "tmdate": 1573301370894, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "B1gYdNvCFS", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thanks for your constructive feedbacks! Please see our response below.\n\n1. About image captioning.\n\nYes. Image captioning dataset is absolutely available for creating the lookup table. As you suggest, we use MS COCO Image captioning dataset to learn a lookup table and apply it to the EN-RO translation task to do the quick evaluation. As a result, the BLEU score is (33.55), which is comparable to the current lookup table (33.78) based on Multi30K, and outperforms the Trans. (base) (32.66). \n\nRegarding the performance of the standard image captioning system, we train a caption model (Show, Attend, and Tell (Xu et al., 2015b)) with fine-tuned encoder (ResNet101) on the COCO dataset to encode the images. The result on EN-RO is 33.58.  We are a little bit uncertain if we have well understood this request because our task is text to text translation while image captioning is image to text. If not, we are glad to address further. \n\n2. About the minor comments.\n\n(1)\tThis is typo. It is Q. \n\n(2)\tYes. We will remove it following your suggestion. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl8hhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper189/Authors|ICLR.cc/2020/Conference/Paper189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175045, "tmdate": 1576860550227, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment"}}}, {"id": "rJezfnmEoB", "original": null, "number": 2, "cdate": 1573301258371, "ddate": null, "tcdate": 1573301258371, "tmdate": 1573301258371, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "ryxpUP6RKr", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks so much for your constructive feedbacks. Please see our response below.\n\n1. Influence of the number of images:\nYes. The reason might be the higher chance of noise. It would be very important to provide a group of images that share similar patterns or topics. However, too many images for a sentence would have greater chance of noise.\n\n2. Impact of paired sentence-image dataset:\nYes. We add the external MS COCO image caption training set and evaluate on the EN-RO task for quick evaluation. The BLEU scores are 33.55 and 33.71 respectively for COCO only and Multi30K+COCO. \n\nIn addition, we are also interested in the influence of the number of sentence-image pairs inspired by your suggestion. We randomly split the pairs of Multi30K into the proportion in [0.1, 0.3, 0.5, 0.7, 0.9], the corresponding BLEU scores are [33.07, 33.44, 34.01, 34.06, 33.80] respectively. These results indicate that a modest number of pairs would be beneficial.\n\n3. The extra computation:\nThe extra computation is negligible. \n\nThe time of obtaining image data for MT sentences for EN-RO dataset, for example, is approximately less than 1 minute by tensor operation in GPU. The lookup table is formed as the mapping of token (only topic words) index to image id. Then, the retrieval method is applied as the tensor indexing from the sentence token (only topic words) index to image ids, which is the same as the procedure of word embedding. The retrieved image ids are then sorted by frequency. \n\nLearning image representations takes only about 2 minutes for all the 29,000 images in Multi30K using 6G GPU memory for feature extraction and 8 threads of CPU for transforming images. The extracted features are formed as the \u201cimage embedding layer\u201d with the size of (29000, 2400) for quick accessing in neural network. \n\n4. Missing BLEU scores & the number of parameters:\nBecause those missing numbers (N/A) are not reported in the corresponding literature. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl8hhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper189/Authors|ICLR.cc/2020/Conference/Paper189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175045, "tmdate": 1576860550227, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment"}}}, {"id": "HJxJBYhb_S", "original": null, "number": 1, "cdate": 1569995062895, "ddate": null, "tcdate": 1569995062895, "tmdate": 1569995062895, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "rygYkNlb_H", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment", "content": {"comment": "Thanks for your interest and constructive comments!\n\nResponse-to-comment-1. In the introduction, we intend to indicate that visual representation has been shown beneficial by some studies. As you mentioned, we agree that it is an open question, as we discussed in Section 2 and Section 5.3. We will clarify that \u201cit is still an open question\u201d in the introduction part and add more discussions accordingly in the later version.\n\nIn previous literatures, visual information is primarily applied to the translation task over Multi30K dataset. Nevertheless, the conclusion about the benefit of the visual modality is still unclear. In this work, we propose to investigate the effectiveness in different and more universal scenarios. We are motivated to comprehensively explore and evaluate the potential of visual modality on more datasets that are in different scales. Our method relies only on image-monolingual annotations instead of the existing approach that depends on image-bilingual annotations, thus breaking the bottleneck of using visual information in NMT. Our experimental results and analysis verify the effectiveness of the visual representation. \n\nBased on the motivation above, we primarily focus on the evaluations of the stable improvements on the baselines (instead of SOTA comparisons), especially for the text-only NMT and low-resource NMT (Table 1) without manually-annotated text-image pairs. Therefore, we show the results of some public baseline models (including the transformer in Ive et al. (2019), instead of the deliberation network) for reference in Table 1-2, only to indicate that our implemented transformer models showed similar BLEU scores with other public reported transformers. As you mentioned, we will also add the SOTA performances in the revised version to show the effect of visual information in various scenarios. \n\nResponse-to-comment-2. That is the official baseline (text-only NMT) on WMT17-Multi30K 2017 test data in Elliott et al. (2017). We will make it clear by noting in the table caption to avoid confusing. In addition, we will add the result of EN-DE in Elliott and K\u00e1d\u00e1r (2017) for more comprehensive reference.\n", "title": "Response"}, "signatures": ["ICLR.cc/2020/Conference/Paper189/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl8hhNYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper189/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper189/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper189/Authors|ICLR.cc/2020/Conference/Paper189/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175045, "tmdate": 1576860550227, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper189/-/Official_Comment"}}}, {"id": "rygYkNlb_H", "original": null, "number": 1, "cdate": 1569944544831, "ddate": null, "tcdate": 1569944544831, "tmdate": 1569944544831, "tddate": null, "forum": "Byl8hhNYPS", "replyto": "Byl8hhNYPS", "invitation": "ICLR.cc/2020/Conference/Paper189/-/Public_Comment", "content": {"comment": "The introduction of your paper claims that \"Visual information has been shown beneficial in neural machine translation (NMT) (Specia et al.,2016; Elliott et al., 2017; Barrault et al., 2018).\" I think this is still an open question that is being debated in the literature, see Elliott (EMNLP 2018) and Caglayan et al. (NAACL 2019) for more details. A recent paper by Ive et al. ACL 2019 with a deliberation network claims that visual information is in fact beneficial but you did not include that model in your table of results.\n\nAlso, in Table 2, Elliott et al. (2017) is a not a system - it is a shared task overview paper. Elliott and K\u00e1d\u00e1r (2017; http://aclweb.org/anthology/I17-1014) is a system that was evaluated on English-German multimodal translation. Perhaps this is a typo in the table, or you meant a different system that is decribed in the Elliott et al. WMT 2017 overview paper.", "title": "Comment about the benefits of visual information and the system comparisons in Table 2"}, "signatures": ["~Desmond_Elliott1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Desmond_Elliott1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "pdf": "/pdf/2ac2a018a5183db8cbcf9e8e33442c42d2eb55c6.pdf", "TL;DR": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"], "paperhash": "zhang|neural_machine_translation_with_universal_visual_representation", "code": "https://github.com/cooelf/UVR-NMT", "spotlight_video": "", "slides": "", "_bibtex": "@inproceedings{\nZhang2020Neural,\ntitle={Neural Machine Translation with Universal Visual Representation},\nauthor={Zhuosheng Zhang and Kehai Chen and Rui Wang and Masao Utiyama and Eiichiro Sumita and Zuchao Li and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Byl8hhNYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cead3e517c2041e1fd4c6cd73e6654902e797734.pdf", "appendix": "", "poster": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Byl8hhNYPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504212700, "tmdate": 1576860583488, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper189/Authors", "ICLR.cc/2020/Conference/Paper189/Reviewers", "ICLR.cc/2020/Conference/Paper189/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper189/-/Public_Comment"}}}], "count": 11}