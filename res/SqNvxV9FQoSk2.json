{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362360840000, "tcdate": 1362360840000, "number": 2, "id": "oozAQe0eAnQ1w", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "SqNvxV9FQoSk2", "replyto": "SqNvxV9FQoSk2", "signatures": ["anonymous reviewer ab3b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Switched linear encoding with rectified linear autoencoders", "review": "The paper draws links between autoencoders with tied weights and rectified linear units (similar to Glorot et al AISTATS 2011), the triangle k-means and soft-thresholding of Coates et al. (AISTATS 2011 and ICML 2011), and the linear-autoencoder-like ICA learning criterion of Le et al (NIPS 2011). \r\nThe first 3 have in common that, for each example, they yield a subset of non-zero (active) hidden units, that result from a simple thresholding. And it is argued that the training objective thus restricted to that subset corresponds to that of Le et al's ICA. Many 2D and 3D graphics with Gaussian data try to convey a geometric intuition of what is going on. \r\n\r\nI find rather obvious that these methods switch on a different linear basis for each example. The specific conection highlighted with  Le et al's ICA work is more interesting, but it only applies if L1 feature sparsity regularization is employed in addition to the rectified linear activation function.\r\n\r\nAt the present stage, my impression is that this paper mainly reflect on the authors' maturing perception of links between the various methods, together with their building of an intuitive geometric understanding of how they work. But it is not yet ripe and its take home message not clear.\r\nWhile its reflections are not without basis or potential interest they are not currently sufficiently formally exposed and read like a set of loosely bundled observations. I think the paper could greatly benefit from a more streamlined central thesis and message with supporting arguments.\r\n\r\nThe main empirical finding from the small experiments in this paper seems to be that the training criterion tends to yield pairs of opposed (negated) feature vectors. What we should conclude from this is however unclear.\r\n \r\nThe graphics are too many. Several seem redundant and are not particularly enlightening for our understanding. Also the use of many Gaussian data examples seems a poor choice to highlight or analyse the switching behavior of these 'switched linear coding' techniques (what does switching buy us if a PCA can capture about all there is about the structure?)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Switched linear encoding with rectified linear autoencoders", "abstract": "Several recent results in machine learning have established formal connections between autoencoders---artificial neural network models that attempt to reproduce their inputs---and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions.", "pdf": "https://arxiv.org/abs/1301.3753", "paperhash": "johnson|switched_linear_encoding_with_rectified_linear_autoencoders", "authorids": ["leif@cs.utexas.edu", "ccor@cs.utexas.edu"], "keywords": [], "conflicts": [], "authors": ["Leif Johnson", "Craig Corcoran"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362252900000, "tcdate": 1362252900000, "number": 1, "id": "ff2dqJ6VEpR8u", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "SqNvxV9FQoSk2", "replyto": "SqNvxV9FQoSk2", "signatures": ["anonymous reviewer 5a78"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Switched linear encoding with rectified linear autoencoders", "review": "In the deep learning community there has been a recent trend in \r\nmoving away from the traditional sigmoid/tanh activation function to \r\ninject non-linearity into the model. One activation function that has \r\nbeen shown to work well in a number of cases is called Rectified \r\nLinear Unit (ReLU). \r\nBuilding on the prior research, this paper aims to provide an \r\nanalysis of what is going on while training networks using these \r\nactivation functions, and why do they work. In particular the authors \r\nprovide their analysis from the context of training a linear auto-encoder \r\nwith rectified linear units on a whitened data. They use a toy dataset in \r\n3 dimensions (gaussian and mixture of gaussian) to conduct the analysis. \r\nThey loosely test the hypothesis obtained from the toy datasets on the \r\nMNIST data.\r\n\r\nThough the paper starts with a lot of promise, unfortunately it fails to \r\ndeliver on what was promised. There is nothing in the paper (no new \r\nidea or insight) that is either not already known, or fairly straightforward \r\nto see in the case of linear auto-encoders trained using a rectified \r\nlinear thresholding unit. Furthermore there are a number of flaws in \r\nthe paper. For instance, the analysis of section 3.1 seems to be a bit \r\nmis-leading. By definition if one fixes the weight vector w to [1,0] there \r\nis no way that the sigmoid can distinguish between x's which are \r\ngreater than S for some S. However with the weight vector taking \r\narbitrary continuous values, that may not be the case. Besides, the \r\npurpose of the encoder is to learn a representation, which can best \r\nrepresent the input, and coupled with the decoder can reconstruct it. \r\nThe encoder learning an identity function (as is argued in the paper) is not \r\nof much use. Finally, the whole analysis of section 3 was based on a \r\nlinear auto-encoder, whose encoder-decoder weights were tied. However \r\nin the case of MNIST the authors show the filters learnt from an untied \r\nweight auto-encoder. There seems to be some disconnect there. \r\n\r\nIn short the paper does not offer any novel insight or idea with respect \r\n to learning representation using auto-encoders with rectified linear \r\nthresholding function. Various gaps in the analysis also makes it a not \r\nvery high quality work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Switched linear encoding with rectified linear autoencoders", "abstract": "Several recent results in machine learning have established formal connections between autoencoders---artificial neural network models that attempt to reproduce their inputs---and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions.", "pdf": "https://arxiv.org/abs/1301.3753", "paperhash": "johnson|switched_linear_encoding_with_rectified_linear_autoencoders", "authorids": ["leif@cs.utexas.edu", "ccor@cs.utexas.edu"], "keywords": [], "conflicts": [], "authors": ["Leif Johnson", "Craig Corcoran"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361946600000, "tcdate": 1361946600000, "number": 3, "id": "kH1XHWcuGjDuU", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "SqNvxV9FQoSk2", "replyto": "SqNvxV9FQoSk2", "signatures": ["anonymous reviewer 9c3f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Switched linear encoding with rectified linear autoencoders", "review": "This paper analyzes properties of rectified linear autoencoder\r\nnetworks. \r\n\r\nIn particular, the paper shows that rectified linear networks are\r\nsimilar to linear networks (ICA). The major difference is the\r\nnolinearity ('switching') that allows the decoder to select a subset\r\nof features. Such selection can be viewed as a mixture of ICA models.\r\n\r\nThe paper visualizes the hyperplanes learned for a 3D dataset and\r\nshows that the results are sensible (i.e., the learned hyperplanes\r\ncapture the components that allow the reconstruction of the data).\r\n\r\nSome comments:\r\n\r\n- On the positive side, I think that the paper makes a interesting attempt to understand properties of nonlinear networks, which is typically hard because of the nonlinearities. The choice of the activation function (rectified linear) makes such analysis possible. \r\n\r\n- I understand that the paper is mainly an analysis paper. But I feel\r\n  that it seems to miss a strong key thesis. It would be more interesting that the analysis reveals surprising/unexpected results.\r\n\r\n- The analyses do not seem particularly deep nor surprising. And I do\r\n  not find that they can advance our field in some way. I wonder if it's possible to make the analysis more constructive so that we can improve our algorithms. Or at least the analyses can reveal certain surprising properties of unsupervised algorithms.\r\n\r\n- It's unclear the motivation behind the use of rectified linear\r\n  activation function for analysis. \r\n\r\n- The paper touches a little bit on whitening. I find the section on\r\n  this topic is unsatisfying. It would be good to analyse the role of whitening in greater details here too (as claimed by abstract and introduction).\r\n\r\n- The experiments show that it's possible to learn penstrokes and\r\n  Gabor filters from natural images. But I think this is no longer\r\n  novel. And that there are very few practical implications of\r\n  this work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Switched linear encoding with rectified linear autoencoders", "abstract": "Several recent results in machine learning have established formal connections between autoencoders---artificial neural network models that attempt to reproduce their inputs---and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions.", "pdf": "https://arxiv.org/abs/1301.3753", "paperhash": "johnson|switched_linear_encoding_with_rectified_linear_autoencoders", "authorids": ["leif@cs.utexas.edu", "ccor@cs.utexas.edu"], "keywords": [], "conflicts": [], "authors": ["Leif Johnson", "Craig Corcoran"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358404200000, "tcdate": 1358404200000, "number": 3, "id": "SqNvxV9FQoSk2", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "SqNvxV9FQoSk2", "signatures": ["leif@cs.utexas.edu"], "readers": ["everyone"], "content": {"decision": "reject", "title": "Switched linear encoding with rectified linear autoencoders", "abstract": "Several recent results in machine learning have established formal connections between autoencoders---artificial neural network models that attempt to reproduce their inputs---and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions.", "pdf": "https://arxiv.org/abs/1301.3753", "paperhash": "johnson|switched_linear_encoding_with_rectified_linear_autoencoders", "authorids": ["leif@cs.utexas.edu", "ccor@cs.utexas.edu"], "keywords": [], "conflicts": [], "authors": ["Leif Johnson", "Craig Corcoran"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 4}