{"notes": [{"id": "RLRXCV6DbEJ", "original": "EDTPb7PNiaX", "number": 884, "cdate": 1601308101498, "ddate": null, "tcdate": 1601308101498, "tmdate": 1615919000322, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ivaMiRznSyK", "original": null, "number": 1, "cdate": 1610040472058, "ddate": null, "tcdate": 1610040472058, "tmdate": 1610474076163, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The paper posits that VAEs, if made sufficiently deep, are able to implement autoregressive models, and could possibly outperform them. Experimentally, the authors attempt make VAEs sufficiently deep so that they are able to outperform autoregressive models on image generation. The authors use a variety of tricks to scale the depth of the model to up to 78 stochastic layers, and achieve SOTA, or near-SOTA NLLs on a number of datasets. Furthermore, in comparison to other models (in particular the recently proposed Nouveau VAE), the models achieve these scores using far fewer parameters.\n\nAlthough the tricks are a bit ad-hoc and the novelty is a bit weak, the experimental results are quite strong and would be of interest to anyone working on VAE research. Moreover, one of the weakness of the paper, a lack of ablations, was addressed during the rebuttal. All reviewers believed that the paper should be accepted, and I see nothing in the paper or the reviews to suggest otherwise."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040472044, "tmdate": 1610474076148, "id": "ICLR.cc/2021/Conference/Paper884/-/Decision"}}}, {"id": "BGOJVXvQVXp", "original": null, "number": 8, "cdate": 1606124180813, "ddate": null, "tcdate": 1606124180813, "tmdate": 1606125658673, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "wm24F2Ek1sR", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "After the rebuttal", "comment": "Dear authors,\n\nI would like to thank you for your comments. In my opinion, the paper is very important for our field, because it shows that we can achieve amazing scores for VAEs. I would even say that this paper is a landmark for us, and now we should figure out whether we can do better (e.g., smaller architectures, better bpds) and whether we really understand what is going on in VAEs (e.g., what is the essential component of).\n\nI am completely satisfied with the rebuttal, thus, I keep my original score, namely, 8. \n\nBest."}, "signatures": ["ICLR.cc/2021/Conference/Paper884/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "iwAdDzu3bwd", "original": null, "number": 9, "cdate": 1606124306241, "ddate": null, "tcdate": 1606124306241, "tmdate": 1606124306241, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "Jx0MZ89g9Lo", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "Thank you, this improves the quality of the paper", "comment": "The paper has improved with the latest changes and this allows me to be more confident in recommending your work. \n\nLast detail:\n4. \"Latent variables are identical to observed variables\" and \"Latent variables allow for parallel generation\" remain quite unclear for the reader (how is it identical? how does it allow for parallel generation?). Maybe both text and figure could be updated to improve clarity. "}, "signatures": ["ICLR.cc/2021/Conference/Paper884/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "Jx0MZ89g9Lo", "original": null, "number": 4, "cdate": 1605752678292, "ddate": null, "tcdate": 1605752678292, "tmdate": 1605804857175, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "qnHZKFQl651", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "Thank you; updated", "comment": "Dear Reviewer 3,\n\nThank you for your careful review. We would like to respond to the issues you raised.\n\n1) Yes, that is what we meant by parallel generation. We have updated the text to explicitly state that.\n2) We meant that a similarly sized network could be trained using a similar number of GPUs; you are correct that the higher resolution layers will technically consume more resources. We clarified this in the text.\n3) We added a table to the Appendix containing the most important experimental hyperparameters.\n4) We modified our statement to read: \"Latent variables are identical copies of observed variables,\" to be more precise.\n\nThese revisions improve the clarity of our work; thank you again for pointing them out."}, "signatures": ["ICLR.cc/2021/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "gd9N19vsBh", "original": null, "number": 7, "cdate": 1605753222037, "ddate": null, "tcdate": 1605753222037, "tmdate": 1605753222037, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "DDVq9bkjgRp", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "Thank you; updated", "comment": "Dear Reviewer 4,\n\nIn figure 4, we now clarify in the text that it is generated by sampling from the posterior at low resolutions, then from the prior at higher resolutions. This means that the disappearance of features is not unusual; the prior distribution will generate features at high resolution that it thinks are likely, but this may be overriden by the ground truth features from the posterior. We apologize for not explaining how the diagram was created in our first draft.\n\nFor Table 1, Left, we expanded the description in Section 5.1 to explain exactly how blocks of variables are emitted independently of each other. If the input for the $i$th topdown block is $x_i$, we can make $K$ consecutive blocks independent by setting $x_{i+1}$, ..., $x_{i+K-1}$ all equal to $x_i$. (Normally, $x_{i+1} = x_i + f(\\mathrm{block}(x_i))$)\n\nWe use average pooling and now note this in the text. \n\nAt your suggestion, we incorporated an ablations table in the Appendix. Gradient skipping and residual initialization we found most important for stability at greater depths, and nearest neighbor upsampling mitigates some posterior collapse but is not strictly necessary for learning.\n\nBy parallel synthesis, we mean that the network can find an arrangement where some large number of variables are conditionally independent given previous latent variables. They can then be emitted simultaneously in one layer. We revised our text to make this more clear, but please let us know if there is a phrasing that would be better.\n\nWe also included all your suggested revisions for clarity. Thank you for your detailed and constructive comments.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "wm24F2Ek1sR", "original": null, "number": 6, "cdate": 1605752960702, "ddate": null, "tcdate": 1605752960702, "tmdate": 1605752960702, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "pwW9loHjo-f", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "Thank you; updated", "comment": "Dear Reviewer 1,\n\nThank you for your careful review. We introduced a line in the body of the text explaining the prior (previously the information was only available in a caption). The prior is a diagonal Gaussian distribution, similar to Maaloe et al, 2019. \n\nWith regard to batch normalization, we did not have time to fully study it, as Vahdat & Kautz (2020) was released toward the end of our experiments. Our initial experiments showed no gains from incorporating it, but our network also had too many differences with respect to Vahdat & Kautz to make any general statements about its necessity. We believe further investigating this would be valuable for future work.\n\nThank you for the reference to Lagging inference networks, and thank you for your comments, as they will improve the clarity of our work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "hgOIgATSuVy", "original": null, "number": 5, "cdate": 1605752749880, "ddate": null, "tcdate": 1605752749880, "tmdate": 1605752749880, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "5frSzxKhDB8", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "Thank you; updated", "comment": "Dear Reviewer 2,\n\nAt your suggestion, we included ablation studies in the Appendix (please see our comment above for a summary). We were not able to directly compare the techniques of Vahdat & Kautz (2020), as the paper was released after most of our experiments were complete, and the code was not yet publicly available.\n\nFor figure 4, we sample from the approx. posterior until the given resolution, then sample the rest from the prior at low temperature. This allows us to visualize what images are likely given some subset of latent variables. We added an explanation of this to the caption.\n\nThe parameter counts are only approximately similar for these networks, which we clarified. We adjust layer counts for FFHQ-1024 and also reduce the width of the final high resolution layers. We detail this in a new hyperparameters table in the Appendix.\n\nAs we noted in the comment above, we actually found that prior warmup was not required; apologies for this error in our first submission.\n\nThank you for all these comments, as we believe these changes will improve the clarity of our presentation."}, "signatures": ["ICLR.cc/2021/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "7inJZ0Eu9vE", "original": null, "number": 3, "cdate": 1605752597826, "ddate": null, "tcdate": 1605752597826, "tmdate": 1605752597826, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "cra1CWLY3U_", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "DMOL is standard", "comment": "Hi Christopher,\n\nThanks for your comment -- we are happy you were able to run the code. With regard to the output distribution, all competitive non-autoregressive models (Flow++, BIVA, NVAE) also use the DMOL layer, as does the main autoregressive model we compare against (the PixelCNN++). Most other autoregressive models use Categorical distributions, which were shown to perform equivalently to the DMOL loss in the Image Transformer work (Parmar et al, 2018). Thus, we believe that DMOL is an apples-to-apples comparison with other work.\n\nWith that said, model performance (both in ELBO and FID) will certainly be affected by using the mean squared error instead of a Gaussian output distribution[1], and between a Gaussian distribution and the DMOL. A detailed study of that is outside of the scope of our work, but we wish you luck!\n\n[1] Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "1OOy1Vy1kp", "original": null, "number": 2, "cdate": 1605752499434, "ddate": null, "tcdate": 1605752499434, "tmdate": 1605752499434, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment", "content": {"title": "Ablation studies added to Appendix", "comment": "We thank the reviewers for their detailed and careful feedback. One concern among multiple reviewers was a lack of ablation studies for the architectural components we introduced. We added these in the Appendix of the revision, and highlight the results here:\n\n- Figure 5 shows the relationship between architecture components and posterior collapse. Residual connections greatly mitigate posterior collapse, and nearest-neighbor upsampling further ameliorates posterior collapse that can occur with convolutional upsampling.\n- Table 3 shows that the residual scaling leads to fewer unstable updates across network sizes, and results in lower loss for networks with more than 30 layers.\n- Figure 6 demonstrates the effect of gradient skipping, and indicates where networks would have diverged if updates were not skipped.\n\nWe furthermore found in the process of fixing a bug in our code that the prior warmup was not necessary for training. We removed references to it in the text, and uploaded the version of the code that does not use it. The bugfix also resulted in slightly improved performance on our benchmarks. The public code release will include pretrained models that can be evaluated to confirm the results we report.\n\nWe respond to individual reviewer questions in more detail below.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper884/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper884/Authors|ICLR.cc/2021/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866122, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Comment"}}}, {"id": "cra1CWLY3U_", "original": null, "number": 1, "cdate": 1605048276607, "ddate": null, "tcdate": 1605048276607, "tmdate": 1605048550734, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Public_Comment", "content": {"title": "Output distribution", "comment": "Hi there, great paper, it's good to see VAEs making somewhat of a comeback. It's nice that you provided some code too (and it was relatively straightforward to run).\n\nI saw however that  you're actually using the discretised mixture of logistics (DMoL) for your output distribution p(x|z), rather than Gaussian (squared error). I don't know if this is a big confounding variable or not with regard to your table of results, but I think you should make it clear when this distribution is being used. I'd be curious to know for instance if this makes a big difference to the ELBO and FID. It wasn't obvious to me whether it made a big difference eye-balling samples between the two distributions, and I can say that at the very least that enabling this loss made training much slower. It may have even been the cause for some NaNs. That is probably due to an inefficient and/or unstable implementation, but it's worth knowing when training potentially very expensive models.\n\nThanks again."}, "signatures": ["~Christopher_Beckham1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Christopher_Beckham1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RLRXCV6DbEJ", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/Authors", "ICLR.cc/2021/Conference/Paper884/Reviewers", "ICLR.cc/2021/Conference/Paper884/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024977961, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper884/-/Public_Comment"}}}, {"id": "pwW9loHjo-f", "original": null, "number": 2, "cdate": 1603810055529, "ddate": null, "tcdate": 1603810055529, "tmdate": 1605024582521, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Review", "content": {"title": "A very deep and very good VAE", "review": "**GENERAL**\nThe paper claims that high quality of generated samples and SOTA bpds are achievable by VAEs if the model is deep enough (deep in terms of the number of stochastic layers). The authors explain the architecture that resemblances the U-net architecture, and explain its building blocks. Interestingly, they are able to learn VAEs with up to 78 stochastic layers, and achieve SOTA bpds on CIFAR-10, ImageNet-32, ImageNet-64, FFHQ-256 (5-bit), and setting a great result on FFHQ-1024 (8bit).\n\n**Strengths:**\nS1: The authors are capable of training VAEs with over 45 stochastic layers (up to 78).\n\nS2: The proposed architecture does not contain any extra \"tricks\", it is relatively simple. This is a great plus for the paper!\n\nS3: The presented theorems are interesting additions to this rather practical paper.\n\nS4 The experiments are well performed and the ablation studies are insightful.\n\nS5: Generated images are of very high quality! Even a reflection in a glass of a generted lady is better than samples of CIFAR10 in many papers.\n\n**Deficiencies:**\nD1: The prior is not explained in the paper! Without this information, it is hard to properly understand what kind of problems occur during training q(z|x) and p(z).\n\n**Remarks:**\nR1: The proposed heuristic method for training q(z|x) reminds of the following paper:\nHe, J., Spokoyny, D., Neubig, G., & Berg-Kirkpatrick, T. (2019). Lagging inference networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534.\nIt would be interesting to compare at the conceptual level both heuristics.\n\nR2: It seems that the authors do not use BatchNorm. Is it correct? This would be also interesting to discuss, because in the following paper:\nVahdat, A., & Kautz, J. (2020). Nvae: A deep hierarchical variational autoencoder. arXiv preprint arXiv:2007.03898.\nthe BatchNorm is indicated as an important component for achieving a deep VAE.\n\n**Questions:**\nQ1: What kind of prior was used in this paper?\n\nQ2: Is BatchNorm indeed irrelevant for VAEs?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper884/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132584, "tmdate": 1606915792577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper884/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Review"}}}, {"id": "DDVq9bkjgRp", "original": null, "number": 1, "cdate": 1603658438504, "ddate": null, "tcdate": 1603658438504, "tmdate": 1605024582463, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Review", "content": {"title": "A strong empirical contribution on hierarchical VAEs", "review": "Summary\n--------------\n\nThis paper provides evidence that \"very deep\" hierarchical VAEs can outperform autoregressive and flow-based models albeit using less parameters on image density estimation tasks.\n\nIt seems natural to think that a hierarchy of latent variables progressively compressing information would be useful for image modelling, with top latent variables capturing more abstract/general features and bottom latent variables capturing lower-level details. However, recent success of flow based and autoregressive based models such as PixelCNN seemed to invalidate the need of such hierarchy of latent variables and to \"compress\" pixel-level information. Here, the authors show that a simple hierarchical VAE architecture inspired by previously proposed ones can outperform autoregressive models if it's made sufficiently \"deep\". I think this is an important contribution. With respect to previous work, this work relates to the concurrently proposed \"Nouveau VAE\" but obtains better results with less parameters and considerably less involved customization of the architecture.  The authors report impressive results on multiple datasets generally using less parameters than competing models. Additionally, sampling from very deep VAEs is considerably cheaper than in autoregressive models.\n\nThe authors also attempt at showing that learnt latent variables implement a hierarchy of information which could be useful to have in general. This point is a bit weak and not well demonstrated in the paper.\n\nPros\n------\n\n- Strong results on multiple tasks with a method that was previously thought to have plateaued in performance\n\nCons\n-------\n\n- Originality / novelty is a bit weak\n- Clarity can be improved\n\n\nDetailed Remarks\n-------------------------\n\n- Figure 4 is not totally convincing as high-level features in the first image are not always maintained in higher-resolution realizations (sample in the first row seem to have glasses then they disappear?). Could you include more samples to back this claim? Do you think of a way of understanding whether high-level variables maintain general info (maybe by probing the posterior samples for some downstream attribute ?)\n\n- I find it hard to understand what is going on in Table 1 (left). In Section 5.1, referring to Table 1, what do you mean by \"grouping layers to output variables independently instead of conditioning on each other\" ? In Table 1, what do you mean by \"masking\" in the sentence \"with masking introduced such that the effective stochastic depth is lower\" ? I cannot find any other references to masking. \n\n- In Figure 3, what do you use as the pooling operation? (2, 2) max pooling ?\n\n- An ablation study of the proposed modifications to the architecture and training tricks would be useful, e.g. what's the most single important modification that makes the model work ? Is it the neighbour upsampling ? Is it the 1/\\sqrt(N) init of the last layer ? Is it the skipping gradient trick ?\n\n- How is Figure 4 obtained ? When you say \"The rest of the high-resolution variables can be output in parallel, largely independent of each other\", are you referring to the fact that you sample from the top 1x1 layer and then sample independently the other zs from the learnt prior e.g. p(z_4x4) ... p(z_64x64) without ancestral sampling ?\n\n- Section A.1: \"Without loss of generality, we simplify notation by assuming each vector-valued latent variable zi\nonly has one element, which we write as zi\", do you mean each latent variable is \\in R ? It'd be good to mention that\nyou assume an architecture with an auto-regressive learnable prior p_\\theta(z_i | z_<i) or refer to Eq. 2.\n\n- Section A.2: I am not sure you need this sentence: \"Without loss of generality, we simplify notation by assuming each vector-valued latent variable zi only has one element, which we write as zi\", as it seems copied from A.1.\n\nGrammar\n-------------\n- Section 5.2: \"an learned\" -> \"a learned\"\n- Section 4.1: \"the the\" -> \"the\"", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper884/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132584, "tmdate": 1606915792577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper884/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Review"}}}, {"id": "5frSzxKhDB8", "original": null, "number": 3, "cdate": 1603827445470, "ddate": null, "tcdate": 1603827445470, "tmdate": 1605024582400, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Review", "content": {"title": "Very good paper improving deep VAE performance beyond autoregressive models, ablation studies could further strengthen it", "review": "**summary** \nthe paper puts forward an idea that deep-enough VAE should perform at least as well as autoregressive models. Authors explore this in the context of image generation, and construct VAE model that is a generalisation of typical autoregressive architectures. They use several tricks to ensure stable training of very deep VAEs and show that final performance exceeds all autoregressive models. This experimentally supports their claim that very deep VAEs encompass autoregressive models.\n\n**pros**\nThe idea of perceiving VAE architectures as strictly more powerful and potentially efficient is very appealing. Given the recent work on improving deep VAE training(like Vahdat & Kautz (2020)) this paper takes another step in this direction by effectively, as it seems from the text, removing the depth limitation for training such VAEs. The tricks used to stabilise training are pretty ad hoc, but their effectiveness, showed experimentally, is important in advancing the field.\n\n\n**cons**\n* The main criticism I have is around ablation studies that justify the proposed architecture choices and training stabilisation tricks, as well as comparison to other tricks in the literature (e.g. Vahdat & Kautz (2020)). Of course the positive result speaks for itself, but the paper would be even more convincing with some details on the exploration that led to the final model.\n\n\n\n**questions**\n* it would be good to clarify in the text how exactly sampled latent variables from lower layers are decoded into the images to produce Fig. 4: is the idea to pass those latents down the top-bottom path and just not add new latents in the node \"+\" within the topdown block?\n* In Section 5.2.1, it is unclear why models with 32x32 and 1024x1024 resolutions have equal number of parameters: is this because ResNet blocks used at different resolutions share parameters?\n* Did the authors experiment with methods of slowing down the training of the prior, other then stopping it for the first half of training? It seems that exponentially averaging prior parameters might be another way of doing it, although the exponent will become another hyperparameter.\n\n**comments**\n* Further investigating the relation between using NN interpolation in upsampling and having active latents in all layers would be very useful. \n* I particularly enjoyed the perceptional shift that the paper advocates for, i.e. that VAE and autoregressive models are not competing approaches, but rather VAE is a more general one and it encompasses the latter.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper884/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132584, "tmdate": 1606915792577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper884/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Review"}}}, {"id": "qnHZKFQl651", "original": null, "number": 4, "cdate": 1603889937971, "ddate": null, "tcdate": 1603889937971, "tmdate": 1605024582341, "tddate": null, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "invitation": "ICLR.cc/2021/Conference/Paper884/-/Official_Review", "content": {"title": "A good paper: a clear idea supported by a robust set of experiments", "review": "1. Summary\nThis paper shows that deep hierarchical VAEs can outperform state-of-the-art autoregressive models on images. The authors first argue that autoregressive models are special cases of hierarchical VAEs and that hierarchical VAEs are universal approximators. They introduce a simple top-down (LVAE) architecture that scales past 70 layers. Furthermore, the model can be trained without using freebits or KL annealing -- although additional tricks are required (gradient skipping and prior warmup). They demonstrate that likelihood performance is correlated with depth and report state-of-the-art performances on multiple image datasets.\n\n2. a Strong Points\n- the contribution is clear: the idea the hierarchical models can outperform autoregressive models is clearly stated and supported by a short theoretical section and relevant experiments (likelihood vs. depth + benchmark)\n- the proposed model is simpler than existing methods and is trained using a simpler objective (no freebits/KL-warmup):\n- the authors demonstrate state-of-the-art likelihood on multiple datasets: the Very Deep VAE indeed outperforms large autoregressive models\n- the authors demonstrate that the method is scalable: proof of concept on FFHQ 256 and 1024.\n\n2. b Weak Points\n- the part on parallel generation is unclear: are you referring to each of the latents for one layer being sampled independently? (i.e. $q(\\mathbf{z}^l | \\mathbf{z}^{l-1}, \\mathbf{x}) = \\prod_d q(z^l_d | \\mathbf{z}^{l-1}, \\mathbf{x})$)\n- section 5.2.1 is unclear: \"unlike autoregressive models, this scaling does not require greater training resources\". In my opinion, using larger images does requires greater training resources because of the increased image definition.\n- Although the code is provided, the experimental protocol is not well described in the paper (learning rate, number of epochs, hidden size, ...)\n\n3. Recommendation\nI recommend accepting this paper.\n\n4. Recommendation Arguments \nThis paper presents a simple, yet solid, story. The empirical results strongly support that \"very deep VAEs can outperform autoregressive models on images\" and the authors introduced a minimal architecture allowing to do so.\n\n5. Questions to the Author\nPlease elaborate on the weak points.\n\n6. Feedback \nFigure 2: the sentence \"latent variables are observed variables\" is odd. Latent variables are unobserved by definition. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper884/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper884/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images", "authorids": ["~Rewon_Child1"], "authors": ["Rewon Child"], "keywords": ["VAE", "generative modeling", "deep learning", "likelihood-based models"], "abstract": "We present a hierarchical VAE that, for the first time, generates samples quickly $\\textit{and}$ outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.", "one-sentence_summary": "We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "child|very_deep_vaes_generalize_autoregressive_models_and_can_outperform_them_on_images", "supplementary_material": "/attachment/189f24d41236f33995f47099717f2a9707b283b4.zip", "pdf": "/pdf/e63933fc98cb52a55d96ffe8bb28d87410c6438e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nchild2021very,\ntitle={Very Deep {\\{}VAE{\\}}s Generalize Autoregressive Models and Can Outperform Them on Images},\nauthor={Rewon Child},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RLRXCV6DbEJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RLRXCV6DbEJ", "replyto": "RLRXCV6DbEJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132584, "tmdate": 1606915792577, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper884/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper884/-/Official_Review"}}}], "count": 15}