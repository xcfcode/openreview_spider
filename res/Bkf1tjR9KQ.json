{"notes": [{"id": "Bkf1tjR9KQ", "original": "r1gL1rYYFm", "number": 407, "cdate": 1538087798916, "ddate": null, "tcdate": 1538087798916, "tmdate": 1545355402991, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkx9icGgg4", "original": null, "number": 1, "cdate": 1544723105628, "ddate": null, "tcdate": 1544723105628, "tmdate": 1545354509903, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper407/Meta_Review", "content": {"metareview": "The paper describes an architecture search method which optimises multiple objectives using a genetic algorithm. All reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground-breaking to justify acceptance based on results alone.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "limited novelty"}, "signatures": ["ICLR.cc/2019/Conference/Paper407/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper407/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper407/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353228281, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper407/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper407/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper407/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353228281}}}, {"id": "rJeMDMpTT7", "original": null, "number": 4, "cdate": 1542472281650, "ddate": null, "tcdate": 1542472281650, "tmdate": 1542472281650, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": "r1xaACsOhm", "invitation": "ICLR.cc/2019/Conference/-/Paper407/Official_Comment", "content": {"title": "Arguments in favor of original work instead of just incremental", "comment": "Thanks for your time and comments. Below we pull quotes from the review followed by responses.\n\n\"The authors admit that their work is incremental and a combination of existing work. Furthermore, they admit that Dong et al. (2018) is the closest related work, however, they do not compare to them in the experimental section. The method by Dong et al. requires only 8 GPU days (Dvolver requires 50) yielding very similar results. Why this has been ignored remains unclear.\"\n\nResponse: Thanks for pointing the missing comparison with DPP-Net, we updated the paper with their results.\n          We argue that the search space of DPP-Net is too small to prove that their method is efficient. There are only 324 possible architectures with 4 layers which can be searched exhaustively.\n          DPP-Net is an extension of Progressive NAS with multi-objective optimization but their search space is different. We cannot conclude on their effectiveness in these conditions.\n\n\n\"The paper is not self-contained, important methodological aspects of the method are insufficiently described. I recommend at least to formally define the crowding distance. It would be also reasonable to define your objective functions already in Section 3 instead of mentioning them in the caption of Figure 3 and its axis labels.\"\n\nResponse: Thanks for pointing the missing information. We update the paper with more details and an appendix with algorithm details.\n          We also added the source code and checkpoints for easy reproduction.\n\n\"I think it's fair to call your approach evolutionary but you might want to discuss its relationship to beam search and in this scope discuss [A].\"\n\nResponse: [A] is a single objective method that evolves a single architecture from a simple to a more complex one.\n          It is a very different evolutionary approach than ours. We work with a population of individuals (architectures), then breed and mutate them to create new generations of architectures better than their parents.\n          It is the interactions between the individuals that is important in our method.\n\n\n\"The comparison in Table 2 is not fair. You use the swift activation function and do not report the corresponding numbers for MobileNet or Mobile NASNet. Ramachandran et al. (2017) report these (75% and 74.2% for NASNet and MobileNet).\nComparing the Dvolver architecture with ReLU activations to MobileNet does not indicate any improvements.\"\n\nResponse: First we compare NASNet and DVOLVER without swish to prove that single objective search is less efficient than multi-objective method.\n          Then, in table 3, We want to push our cells to the maximum and compare with the best results found in previous works. We added NASNet swish and showed that DVOLVER still has better accuracy than NASNet.\n          We added the best results found in state-of-the-art works for mobile architecture and found that DVOLVER is very competitive.\n          Ramachandran et al. 2017(https://arxiv.org/pdf/1710.05941.pdf), state that MobileNet can benefit from swish and the publication date implies that it is MobileNet V1.\n          Their paper states that MobileNet with RELU is at 72% (Table 8) but we could not find any reference for MobileNet V1 (https://arxiv.org/abs/1704.04861) with that accuracy. Without more details, we cannot add this values in our paper but will if we can find more details.\n          We think it is fair to compare with the best results found in previous works and it is not the subject of our paper to optimize other architectures.\n\n\n\"You mention that most previous approaches are only keeping track of the best solution while you evolve over a population. Maybe this sentence is not well written and something else is meant but now this statement is wrong.\"\n\nResponse: We remove that statement, thanks for the catch.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper407/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper407/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper407/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618765, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkf1tjR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference/Paper407/Reviewers", "ICLR.cc/2019/Conference/Paper407/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper407/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper407/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper407/Authors|ICLR.cc/2019/Conference/Paper407/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper407/Reviewers", "ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference/Paper407/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618765}}}, {"id": "rkgrzzaT67", "original": null, "number": 3, "cdate": 1542472204774, "ddate": null, "tcdate": 1542472204774, "tmdate": 1542472204774, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": "HJxO1b4Yhm", "invitation": "ICLR.cc/2019/Conference/-/Paper407/Official_Comment", "content": {"title": "Review answers and extended experimental results", "comment": "Thanks for your time and comments. Below we pull quotes from the review followed by responses.\n\n\" It lacks novelty\"\n\nResponse: Our paper proves that multi-objective search can be more efficient than single objective method with extra value relevant for mobile neural network architecture design.\n          To our knowledge, no previous work has proved it.\n\n\n\"As the paper admits this is not going to generalizing well. How good the method is if just using a single dateset?\"\n\nResponse: The main goal is to prove multi-objective search efficiency and for fast iteration, it is faster to perform the search on CIFAR-10 but it is not a requierement.\n          * We did the search on CIFAR-10 and then fully train on CIFAR-10 see Table 1.\n          * Searching on ImageNet can be done with our method but it is significantly more expensive. One of the future leads is to downscale Imagenet to make the search faster.\n\n\n\"For CIFAR-10, is this method comparable with ENAS\"\n\nResponse: ENAS uses a clever approach to reuse weights and eliminate the need to retrain from scratch.\n          ENAS is a method to improve search efficiency. Our work is more centered on multi objective search. One of our future leads, is to use weight sharing with multi-objective search. "}, "signatures": ["ICLR.cc/2019/Conference/Paper407/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper407/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper407/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618765, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkf1tjR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference/Paper407/Reviewers", "ICLR.cc/2019/Conference/Paper407/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper407/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper407/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper407/Authors|ICLR.cc/2019/Conference/Paper407/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper407/Reviewers", "ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference/Paper407/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618765}}}, {"id": "ryeNabaaa7", "original": null, "number": 2, "cdate": 1542472123927, "ddate": null, "tcdate": 1542472123927, "tmdate": 1542472123927, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": "ryeL_QZ9hm", "invitation": "ICLR.cc/2019/Conference/-/Paper407/Official_Comment", "content": {"title": "Experimental update and review answer", "comment": "Thanks for your time and comments. Below we pull quotes from the review followed by responses.\n\n\"The proposed algorithm seems highly similar to the existing multi-objective NAS algorithms, especially [\u2026] However, both aspects are of limited technical novelty.\"\n\nResponse: We argue that to prove that a search technique is efficient it has to be done on a large search space where exhaustive and random search are not tractable.\n          Moreover, Our paper prove that on a given large search space, multi-objective search can be more efficient than single objective search with extra value when designing neural architecture for mobile device.\n          To our knowledge, no previous work has stated that point because they do not search in a search space already studied with a single objective method.\n\n          Our comments on the given references:\n          * [1]: build networks incrementally with a limited set of possible operations. In theory, their search space is infinite but in practice it is quiet small.\n                 objective functions are arbitrary separated into 2 categories: expensive and cheap. We show that it is not necessary.\n          * [2]: Their method initializes the search process from a given baseline network which is a strong prior and only explore the vicinity of already defined network instead of exploring the whole search space.\n                 They do not study transferability to ImageNet.\n          * [3]: We argue that their search space is too small (See Table 4 of our paper). It is centered around efficient handcrafted architectures. For 4 layers, there are only 324 possible cell architectures of which they evaluate around 200.\n                 With small increase in computation, it is possible to perform exhaustive search.\n\n\n\" If the proposed algorithm was investigated [\u2026] to the proposed multi-objective evolution or this additional search space engineering.\"\n\nResponse: Thanks for pointing the ambiguity, we fixed the paper with more explanations: NASNet, AmoebaNet and PNAS all have additional connections in the provided checkpoints and codes by the authors. They are not described in their respective papers.\n          We did an experiment with no extra connections and compared with NASNet without additional connections and found that Dvolver outperforms NASNet.\n\n\n\"The main claimed contribution is a multi-objective evolutionary algorithm. To demonstrate its effectiveness, it would be [\u2026]  completely unaware of additional dimensions of the desired objectives.\n\"\n\nResponse: Our main goal is to compare single objective vs multi-objective search for neural network but we think your are right to ask for broader comparison and we updated the paper with results from DPP-Net (only paper with results on ImageNet).\n          We also made it clearer that when we compare with single objective method, only accuracy is to be considered.\n          One of our conclusion is that multi-objective search is still relevant even when looking for accuracy only.\n\n\"It would be informative to report the CIFAR-10 results as well.\"\n\nResponse: Again, thank you for pointing that out. We updated the paper with results for CIFAR-10.\n\n\n\"The authors did not report their training setup for ImageNet. It would be good to include those details to ensure the readers are informed should there are any additional augmentations.\"\n\nand\n\n\"It would be better to included more details on these evolution forces for reproducibility. These are also important component of the proposed algorithm\"\n\nResponse: We added more details in the paper and we provide the full source code for search and train on CIFAR-10 and also the code for train and evaluation on ImageNet (GPU and TPU) with the final checkpoints for all networks we presented on ImageNet.\n          We hope it will be enough to clear the shadow details in our setup.\n\n\n\"What\u2019s the criteria for manual selection?\"\n\nResponse: In general, interpreting the Pareto front is context sensitive. Ideally, we should train all the architectures in the Pareto front but it is very expensive.\n          Our selection process is as follow:\n          * take the cell with the best accuracy (DVOLVER-A), then take 2 more on Pareto front knees (where small drop in accuracy leads to great improvement in speed): DVOLVER-B & C.\n          * Then for all N, F and cells compute MACs and we select the few with MACs comparable with existing networks with want to compare with.\n\n          Our goal is to easy comparison with competitive architectures.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper407/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper407/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper407/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618765, "tddate": null, "super": null, "final": null, "reply": {"forum": "Bkf1tjR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference/Paper407/Reviewers", "ICLR.cc/2019/Conference/Paper407/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper407/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper407/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper407/Authors|ICLR.cc/2019/Conference/Paper407/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper407/Reviewers", "ICLR.cc/2019/Conference/Paper407/Authors", "ICLR.cc/2019/Conference/Paper407/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618765}}}, {"id": "ryeL_QZ9hm", "original": null, "number": 3, "cdate": 1541178222337, "ddate": null, "tcdate": 1541178222337, "tmdate": 1541534021855, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper407/Official_Review", "content": {"title": "Good results but limited novelty. Experimental comparison could be improved.", "review": "The paper proposes a multi-objective search algorithm that designs resource-efficient convolutional architectures. The key idea is to maintain a population of networks and to iteratively approach the Pareto front through evolution. The normal & reduction cells are searched on CIFAR-10 and then transferred to ImageNet. The resulting architectures empirically lead to better trade-offs than other baselines.\n\nPros:\nThe paper is well-written and easy to comprehend.\nResults are competitive against strong baselines such as NASNet.\nResource budgets are handled in a principled manner with multi-objective optimization. \n\nCons:\n\nMy main concerns are on the technical novelty and experimental comparison.\n\nTechnical novelty:\n\nThe proposed algorithm seems highly similar to the existing multi-objective NAS algorithms, especially the ones based on Pareto optimality [1,2,3]. In Sect 2, the authors state that the main difference from prior works such as [2] and [3] is the usage of a different and larger search space and large-scale experiments. However, both aspects are of limited technical novelty.\n\nExperimental comparison:\n\nIn sect 3.3, the authors say \u201cwe noticed that the original NASNet search space can greatly benefit from extra connections from any given block\u201d. If the proposed algorithm was investigated in an enhanced version of the NASNet space, it would be unclear whether we should attribute the reported performance to the proposed multi-objective evolution or this additional search space engineering. It would be better to report the results using the original space as well for fair comparison. \n\nThe main claimed contribution is a multi-objective evolutionary algorithm. To demonstrate its effectiveness, it would be necessary to compare against existent multi-objective NAS strategies in the literature. Most of those strategies (e.g., scalarization, weighted product method) should be straightforward to implement on top of the current search space. The current results are less convincing since the authors only compared their method against single-objective baselines (e.g. NASNet, PNAS, AmoebaNet) which are completely unaware of additional dimensions of the desired objectives. \n\nThe networks are searched on CIFAR-10 and then transferred to ImageNet. Unlike most prior works (including the ones focusing on resource-constrained NAS), the authors did not the final performance of their architecture on CIFAR-10. It would be informative to report the CIFAR-10 results as well.\n\nOther suggestions & questions:\nThe authors did not report their training setup for ImageNet. It would be good to include those details to ensure the readers are informed should there are any additional augmentations.\n\n\u201cuniform mutation and a crossover probability of 0.1\u201d (sect 4.1)\nIt would be better to included more details on these evolution forces for reproducibility. These are also important component of the proposed algorithm.\n\n\u201cWe manually select 3 architectures that we will be fully train on ImageNet in Section 4.2\u201d (sect 4.1)\nI believe this part needs more clarifications since there can be a large number of architectures on the Pareto front. What\u2019s the criteria for manual selection?\n\n[1] Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. \"Multi-objective architecture search for cnns.\" arXiv preprint arXiv:1804.09081 (2018).\n[2] Kim Ye-Hoon, Reddy Bhargava, Yun Sojung, and Seo Chanwon. NEMO: Neuro-Evolution with Multiobjective Optimization of Deep Neural Network for Speed and Accuracy. ICML\u201917 AutoML Workshop, 2017.\n[3] Dong, Jin-Dong, et al. \"DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures.\" arXiv preprint arXiv:1806.08198 (2018).", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper407/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper407/Official_Review", "cdate": 1542234468307, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper407/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335717114, "tmdate": 1552335717114, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper407/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJxO1b4Yhm", "original": null, "number": 2, "cdate": 1541124320153, "ddate": null, "tcdate": 1541124320153, "tmdate": 1541534021613, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper407/Official_Review", "content": {"title": "The results are competitive, but not too much innovation", "review": "The paper is easy to read. The authors did a job in describing the problem, concepts, and the proposed multi-objective optimization method. The computational results are on par with NASNet-A mobile. \n\nIt is good to know that we can use standard multi-objective method for neural architecture search. The implementation seems to be straightforward. The paper mainly uses existing ideas, but with some incremental improvements. It lacks novelty.  \n\nThe time reduction of this method on ImageNet comes from transfer learning by training on CIFAR-10 first. As the paper admits this is not going to generalizing well. How good the method is if just using a single dateset? For CIFAR-10, is this method comparable with ENAS(https://arxiv.org/pdf/1802.03268.pdf)?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper407/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper407/Official_Review", "cdate": 1542234468307, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper407/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335717114, "tmdate": 1552335717114, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper407/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1xaACsOhm", "original": null, "number": 1, "cdate": 1541091029448, "ddate": null, "tcdate": 1541091029448, "tmdate": 1541534021407, "tddate": null, "forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper407/Official_Review", "content": {"title": "Interesting but incremental work", "review": "The authors propose a multi-objective neural architecture search based on an evolutionary algorithm. The contradicting objective functions are optimized by ranking the candidates by Pareto-dominance, replace the bottom 50% with new candidates generated by the top 50% candidates through random mutations. The multi-objective function considers classification accuracy and an approximation of the inference speed. The method is compared to MobileNet and Mobile NASNet on ImageNet indicating an improvement with respect to search time.\n\nThe authors admit that their work is incremental and a combination of existing work. Furthermore, they admit that Dong et al. (2018) is the closest related work, however, they do not compare to them in the experimental section. The method by Dong et al. requires only 8 GPU days (Dvolver requires 50) yielding very similar results. Why this has been ignored remains unclear.\n\nThe paper is not self-contained, important methodological aspects of the method are insufficiently described. I recommend at least to formally define the crowding distance. It would be also reasonable to define your objective functions already in Section 3 instead of mentioning them in the caption of Figure 3 and its axis labels.\n\nI think it's fair to call your approach evolutionary but you might want to discuss its relationship to beam search and in this scope discuss [A].\n\nThe comparison in Table 2 is not fair. You use the swift activation function and do not report the corresponding numbers for MobileNet or Mobile NASNet. Ramachandran et al. (2017) report these (75% and 74.2% for NASNet and MobileNet).\nComparing the Dvolver architecture with ReLU activations to MobileNet does not indicate any improvements.\n\nYou mention that most previous approaches are only keeping track of the best solution while you evolve over a population. Maybe this sentence is not well written and something else is meant but now this statement is wrong.\n\n[A] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter: Simple And Efficient Architecture Search for Convolutional Neural Networks. CoRR abs/1711.04528 (2017)", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper407/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search", "abstract": "Automatic search of neural network architectures is a standing research topic. In addition to the fact that it presents a faster alternative to hand-designed architectures, it can improve their efficiency and for instance generate Convolutional Neural Networks (CNN) adapted for mobile devices. In this paper, we present a multi-objective neural architecture search method to find a family of CNN models with the best accuracy and computational resources tradeoffs, in a search space inspired by the state-of-the-art findings in neural search. Our work, called Dvolver, evolves a population of architectures and iteratively improves an approximation of the optimal Pareto front. Applying Dvolver on the model accuracy and on the number of floating points operations as objective functions, we are able to find, in only 2.5 days 1 , a set of competitive mobile models on ImageNet. Amongst these models one architecture has the same Top-1 accuracy on ImageNet as NASNet-A mobile with 8% less floating point operations and another one has a Top-1 accuracy of 75.28% on ImageNet exceeding by 0.28% the best MobileNetV2 model for the same computational resources.", "keywords": ["architecture search", "Pareto optimality", "multi-objective", "optimization", "cnn", "deep learning"], "authorids": ["guillaume.michel@netatmo.com", "mohammed-amine.alaoui@netatmo.com", "alice.lebois@netatmo.com", "amal.feriani@netatmo.com", "mehdi.felhi@netatmo.com"], "authors": ["Guillaume Michel", "Mohammed Amine Alaoui", "Alice Lebois", "Amal Feriani", "Mehdi Felhi"], "TL;DR": "Multi-objective Neural architecture search as an efficient way to find fast and accurate architecture for mobile devices.", "pdf": "/pdf/9914d02e0fdb0558096e5868bf823948591b74c9.pdf", "paperhash": "michel|dvolver_efficient_paretooptimal_neural_network_architecture_search", "_bibtex": "@misc{\nmichel2019dvolver,\ntitle={{DVOLVER}: Efficient Pareto-Optimal Neural Network Architecture Search},\nauthor={Guillaume Michel and Mohammed Amine Alaoui and Alice Lebois and Amal Feriani and Mehdi Felhi},\nyear={2019},\nurl={https://openreview.net/forum?id=Bkf1tjR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper407/Official_Review", "cdate": 1542234468307, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Bkf1tjR9KQ", "replyto": "Bkf1tjR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper407/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335717114, "tmdate": 1552335717114, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper407/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}