{"notes": [{"id": "B1xU4nAqK7", "original": "HyxAtgRctQ", "number": 1451, "cdate": 1538087981636, "ddate": null, "tcdate": 1538087981636, "tmdate": 1545355393538, "tddate": null, "forum": "B1xU4nAqK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Hyl5JGcll4", "original": null, "number": 1, "cdate": 1544753634489, "ddate": null, "tcdate": 1544753634489, "tmdate": 1545354517735, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Meta_Review", "content": {"metareview": "Strengths\n\nThe paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling)\napproach to learning the state transition function. The paper is clearly written.\n\nWeaknesses\n\nAll reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation,\nand aspects of the paper are difficult to follow or are sparse on details.\nNo revisions have been posted.\n\nSummary\n\nAll reviewers are in agreement that the paper requires significant work and that it is not ready for ICLR publication.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "incremental, limited evaluation"}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1451/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352833497, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1451/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1451/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352833497}}}, {"id": "BkevqyFqRQ", "original": null, "number": 5, "cdate": 1543307150595, "ddate": null, "tcdate": 1543307150595, "tmdate": 1543307150595, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "BJgagv2OCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "content": {"title": "Thank you", "comment": "Dear reviewer, \n\nThank you very much for your review. In response to the main criticism from all reviewers here, we have been running additional experiments on new systems and towards increasingly our method's novelty but have been unable to complete these experiments in time. We will certainty incorporate all your helpful feedback into improving a future version of this work and are grateful for the time you spent on it."}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626478, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xU4nAqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1451/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1451/Authors|ICLR.cc/2019/Conference/Paper1451/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626478}}}, {"id": "rJgyu1t9R7", "original": null, "number": 4, "cdate": 1543307111499, "ddate": null, "tcdate": 1543307111499, "tmdate": 1543307111499, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "BJx8HYl52m", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "content": {"title": "Thank you ", "comment": "Dear reviewer, \n\nThank you very much for your review. In response to the main criticism from all reviewers here, we have been running additional experiments on new systems and towards increasingly our method's novelty but have been unable to complete these experiments in time. We will certainty incorporate all your helpful feedback into improving a future version of this work and are grateful for the time you spent on it."}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626478, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xU4nAqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1451/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1451/Authors|ICLR.cc/2019/Conference/Paper1451/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626478}}}, {"id": "rkxOBkK9CQ", "original": null, "number": 3, "cdate": 1543307072161, "ddate": null, "tcdate": 1543307072161, "tmdate": 1543307072161, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "Sylir6eTh7", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "content": {"title": "Thank you", "comment": "Dear reviewer, \n\nThank you very much for your review. In response to the main criticism from all reviewers here, we have been running additional experiments on new systems and towards increasingly our method's novelty but have been unable to complete these experiments in time. We will certainty incorporate all your helpful feedback into improving a future version of this work and are grateful for the time you spent on it."}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626478, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xU4nAqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1451/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1451/Authors|ICLR.cc/2019/Conference/Paper1451/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626478}}}, {"id": "BJgagv2OCQ", "original": null, "number": 1, "cdate": 1543190260960, "ddate": null, "tcdate": 1543190260960, "tmdate": 1543190260960, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "B1lbkkIPhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "content": {"title": "Update near end of discussion phase.", "comment": "They authors did not address the concerns mentioned in my review, nor have they addressed the concerns of the other reviewers. \n\nIn this situation, I stand by my original review.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1451/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626478, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xU4nAqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1451/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1451/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1451/Authors|ICLR.cc/2019/Conference/Paper1451/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Reviewers", "ICLR.cc/2019/Conference/Paper1451/Authors", "ICLR.cc/2019/Conference/Paper1451/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626478}}}, {"id": "Sylir6eTh7", "original": null, "number": 3, "cdate": 1541373251445, "ddate": null, "tcdate": 1541373251445, "tmdate": 1541533122084, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Official_Review", "content": {"title": "Weak experimental evaluation and lack of novelty", "review": "The authors address the problem of how to use unsupervised exploration in a first phase of reinforcement learning to gather knowledge that can be transferred to new tasks to improve performance in a second task when specific reward functions are available. The authors proposed a model-based approach which uses deep neural networks as a model for the environment. The model is PETS (probabilistic ensembles with trajectory sampling), an ensemble of neural networks whose outputs parametrize predictive distributions for the next state as a function of the current state and the action applied. To collect data during the unsupervised exploration phase, they use a metric of model uncertainty computed as follows: the average over all the particles assigned to each bootstrap is computed and the variance over these computed means is the\nmetric of uncertainty. The authors validate their method on the HalfCheetah OpenAI gym environment where they consider 4 different tasks related to running forward, backward, tumbling forward and tumbling backward. The results obtained show that they outperform random and count based exploration approaches.\n\nQuality:\n\nI am concerned about the quality of the experimental evaluation of the method. The authors only consider a single environment for their experiments and artificially construct 4 relatively similar tasks. I believe this is insufficient to quantify the usefulness of the proposed method.\n\nClarity:\n\nThe paper is clearly written and easy to read.\n\nNovelty:\n\nThe proposed approach seems incremental and lacks novelty. The described method for model-based exploration consists in looking at the mean of the prediction of each neural network in the ensemble and then computing the empirical average. This approach has been used before for active learning with neural networks ensembles:\n\nKrogh, Anders, and Jesper Vedelsby. \"Neural network ensembles, cross validation, and active learning.\" Advances in neural information processing systems. 1995.\n\nThe used model, PETS, is also not novel and the proposed methodology for having first an unsupervised learning phase and then a new specific learning task is also not very innovative.\n\nSignificance:\n\nGiven the lack of a rigorous evaluation framework and the lack of novelty of the proposed methods, I believe the significance of the contribution is very low.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Official_Review", "cdate": 1542234226645, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1451/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335949735, "tmdate": 1552335949735, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJx8HYl52m", "original": null, "number": 2, "cdate": 1541175613932, "ddate": null, "tcdate": 1541175613932, "tmdate": 1541533121874, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Official_Review", "content": {"title": "An incremental work and needs more justification/clarification", "review": "The authors built upon the PETS algorithm to develop a state uncertainty-driven exploration strategy, for which the main point is to construct a reward function. The proposed algorithm was then tested on a specific domain to show some improvement. \n\nThe contribution of this paper may be limited, as it needs a specific setting, as shown in Figure 1. Furthermore, this paper is a bit difficult to follow, e.g., it was not until the 5th page to describe their algorithm. I summarize the pros and cons as follows.\n\nPros:\n- The idea to include the exploration for PETS is somewhat interesting.\nCons:\n- The paper is a bit difficult to follow. Just to list a few places:\n  1. The term \"unsupervised exploration\" was mentioned a few times in this paper. I am not sure if this is an accurate term. Is there a corresponding \"supervised exploration\" used elsewhere? \n  2. When you introduced r_t in Section 3.3, how did you use it next? Was it used in Phase II?\n  3. For the PETS (oracle) in Figure 4, why are the settings different for forward and backward tasks?\n  4. What does \"random\" mean in Figure 4?\n- The novelty of this paper is somewhat limited, as it requires a specific setting and has been applied in only one domain.\n- There are a few grammar mistakes/typos in this paper. \n  1. What is \"k\" in the equation for r_t?\n  2.  \"...we three methods...\" in Page 6.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Official_Review", "cdate": 1542234226645, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1451/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335949735, "tmdate": 1552335949735, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lbkkIPhX", "original": null, "number": 1, "cdate": 1541000920839, "ddate": null, "tcdate": 1541000920839, "tmdate": 1541533121636, "tddate": null, "forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper1451/Official_Review", "content": {"title": "Decent paper, but not very novel, sparse on details.", "review": "The paper performs model-based reinforcement learning. It makes two main contributions. First, it divides training into two phases: the unsupervised phase for learning transition dynamics and the second phase for solving a task which comes with a particular reward signal. The scope of the paper is a good fit for ICLR.\n\nThe paper is very incremental: the ideas of using an ensemble of models to quantify uncertainty, to perform unsupervised pre-training and to explore using an intrinsic reward signal have all been known for many years.\n\nThe contribution of the paper seems to be the combination of these ideas and the way in which they are applied to RL. I have the following observations / complaints about this.\n\n1. The paper is very sparse on details. There is no pseudocode for the main algorithm, and the quantity v^i_t (the epistemic variance on page 5) isn't defined anywhere. Without these things, it is difficult for me to say what the proposed algorithm is *exactly*.\n\n2. Sections 1 and 2 of the paper seem unreasonably bloated, especially given the fact that the space could have been more meaningfully used as per (1).\n\n3. The experimental section misses any kind of uncertainty estimates. If, as you say, you only had the computational resources for three runs, then you should report the results for all three. You should consider running at least one experiment for longer. This should be possible - a run of 50K steps of HalfCheetah takes about one hour on a modern 10-core PC, so this is something you should be able to do overnight.\n\n4. The exploration mechanism is a little bit of a  mystery - it isn't concretely defined anywhere except for the fact that it uses intrinsic rewards. Again, please provide pseudocode.\n\nAs the paper states now, the lack of details makes it difficult for me to accept. However, I encourage the authors to do the following:\n1. Provide pseudocode for the algorithm.\n2. Provide pseudocode for exploration mechanism (unless subsumed by (1)).\n3. Add uncertainty estimates to evaluation or at least report all runs.\n\nI am willing to re-consider my decision once these things have been done.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1451/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Exploration with Deep Model-Based Reinforcement Learning", "abstract": "Reinforcement learning (RL) often requires large numbers of trials to solve a single specific task. This is in sharp contrast to human and animal learning: humans and animals can use past experience to acquire an understanding about the world, which they can then use to perform new tasks with minimal additional learning. In this work, we study how an unsupervised exploration phase can be used to build up such prior knowledge, which can then be utilized in a second phase to perform new tasks, either directly without any additional exploration, or through minimal fine-tuning. A critical question with this approach is: what kind of knowledge should be transferred from the unsupervised phase to the goal-directed phase? We argue that model-based RL offers an appealing solution. By transferring models, which are task-agnostic, we can perform new tasks without any additional learning at all. However, this relies on having a suitable exploration method during unsupervised training, and a model-based RL method that can effectively utilize modern high-capacity parametric function classes, such as deep neural networks. We show that both challenges can be addressed by representing model-uncertainty, which can both guide exploration in the unsupervised phase and ensure that the errors in the model are not exploited by the planner in the goal-directed phase. We illustrate, on simple simulated benchmark tasks, that our method can perform various goal-directed skills on the first attempt, and can improve further with fine-tuning, exceeding the performance of alternative exploration methods.", "keywords": ["exploration", "model based reinforcement learning"], "authorids": ["kchua@berkeley.edu", "rmcallister@berkeley.edu", "roberto.calandra@berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Kurtland Chua", "Rowan McAllister", "Roberto Calandra", "Sergey Levine"], "pdf": "/pdf/175cb1413aa109f9ab1332986c8926a2441b1541.pdf", "paperhash": "chua|unsupervised_exploration_with_deep_modelbased_reinforcement_learning", "_bibtex": "@misc{\nchua2019unsupervised,\ntitle={Unsupervised Exploration with Deep Model-Based Reinforcement Learning},\nauthor={Kurtland Chua and Rowan McAllister and Roberto Calandra and Sergey Levine},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xU4nAqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1451/Official_Review", "cdate": 1542234226645, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xU4nAqK7", "replyto": "B1xU4nAqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1451/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335949735, "tmdate": 1552335949735, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1451/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}