{"notes": [{"id": "S1emOTNKvS", "original": "BkeeJy2wDS", "number": 625, "cdate": 1569439082519, "ddate": null, "tcdate": 1569439082519, "tmdate": 1577168233663, "tddate": null, "forum": "S1emOTNKvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aj1h43xJ25", "original": null, "number": 1, "cdate": 1576798701701, "ddate": null, "tcdate": 1576798701701, "tmdate": 1576800934299, "tddate": null, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Decision", "content": {"decision": "Reject", "comment": "This submission proposes a graph sparsification mechanism that can be used when training GNNs.\n\nStrengths:\n-The paper is easy to follow.\n-The proposed method is sound and effective.\n\nWeaknesses:\n-The novelty is limited.\n\nGiven the limited novelty and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722902, "tmdate": 1576800274292, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper625/-/Decision"}}}, {"id": "HyxjK2qniS", "original": null, "number": 8, "cdate": 1573854339010, "ddate": null, "tcdate": 1573854339010, "tmdate": 1573854339010, "tddate": null, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment", "content": {"title": "Appreciate your time and discussion in rebuttal", "comment": "Dear AC and reviewers,\n\nWe sincerely appreciate the valuable comments from the reviewers. The constructive suggestions indeed help us further improve the paper.\n\nMeanwhile, we feel sorry for not receiving further feedback from reviewer#2. We fully respect the reviewer's comments and have spent significant effort in addressing the concerns. In the latest draft and response posted on 11/09, we believe we have addressed the concerns from reviewer#2. If there are further questions and concerns, we are eager for discussion. However, we could not find a response or updated rating from reviewer#2. If this is the final evaluation, it is unfair for us and impairs the trust between authors and reviewers in ICLR.\n\nIn sum, we hope the decision process could cautiously consider the reviewer#2\u2019s comments, and give our paper a fair evaluation.\n\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper625/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1emOTNKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper625/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper625/Authors|ICLR.cc/2020/Conference/Paper625/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168677, "tmdate": 1576860553780, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment"}}}, {"id": "HkxPy4yhoS", "original": null, "number": 7, "cdate": 1573807070755, "ddate": null, "tcdate": 1573807070755, "tmdate": 1573807070755, "tddate": null, "forum": "S1emOTNKvS", "replyto": "ByxhW2TNsH", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment", "content": {"title": "Response to rebuttal", "comment": "Thank you for your constructive rebuttal. Due to the newly added comparison and discussion of previous work, I have increased my score.\n\nThe reasons why I still think that it is not a strong accept is that it is in the end a method very similar to previous ones (either GAT --> which, intuitively, also learns which edges to keep) and [1] which is able to truly remove (and add) edges in a discrete, non-continuous manner. "}, "signatures": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1emOTNKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper625/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper625/Authors|ICLR.cc/2020/Conference/Paper625/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168677, "tmdate": 1576860553780, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment"}}}, {"id": "B1xlsMPssr", "original": null, "number": 6, "cdate": 1573773975592, "ddate": null, "tcdate": 1573773975592, "tmdate": 1573773975592, "tddate": null, "forum": "S1emOTNKvS", "replyto": "r1xkiEt2Fr", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment", "content": {"title": "Appreciate your time and attention", "comment": "Dear Reviewer,\n\nIf you have more questions or concerns about our latest draft or response, please feel free to let us know. We are happy to discuss with you."}, "signatures": ["ICLR.cc/2020/Conference/Paper625/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1emOTNKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper625/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper625/Authors|ICLR.cc/2020/Conference/Paper625/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168677, "tmdate": 1576860553780, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment"}}}, {"id": "r1ei61PDYr", "original": null, "number": 1, "cdate": 1571413954526, "ddate": null, "tcdate": 1571413954526, "tmdate": 1573661359437, "tddate": null, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The authors propose a supervised graph sparsification technique that \"mitigates the overfitting risk by reducing the\ncomplexity of input graphs.\"\n\nThe idea is as follows: there is a sparsification network which samples subgraphs (adjacency matrices) by computing a probability distribution for each edge and drawing the existence of each edge from this distribution. The sparsified graph is then fed to a GNN that computes a classification loss. Since the authors use the Gumbel-softmax trick the method is end-to-end differentiable and the output consists of a node classifier and a graph generative model that can be used to sample sparsified graphs. \n\nThe paper covers an interesting topic and results in some good numbers on standard benchmark datasets. I also like the idea to use the Gumbel-softmax trick to make the entire model differentiable. \n\nUnfortunately, the authors miss to cite and discuss highly related work [1] (ICML 2019). In this work, the authors also maintain a graph generative model (also by parameterizing edges but with iid Bernoulli RVs), also sampling graphs from this generative model, and also using these sampled graphs to train a GNN for semi-supervised node classification. The resulting model is also end-to-end differentiable. Instead of using a Gumbel-softmax to keep the method differentiable (given that we have discrete random variables) the authors propose a novel way to compute gradients for the parameters of the Bernoulli RVs by posing the problem as a bilevel optimization problem. In [1] the graph cannot only be sparsified but also enriched with edges that might be beneficial for the classification accuracy. Indeed, it was shown that adding edges is more beneficial than removing edges. The results on Cora and Citeseer are better than the results reported by the authors in this submission. At the very least, the authors should familiarize themselves, discuss, and compare empirically to [1]. \n\nThe existence of this previous work also reduces the novelty of the proposed approach. However, if a  comparison to [1] would be added, the submission could be seen as an alternative instance of the framework presented in [1]. I would be willing to increase my score to accept, if the authors provide such a comparison in an updated version. \n\n\n[1] https://arxiv.org/abs/1903.11960\n\n\n-----\n\nI've read the rebuttal and while I don't agree necessarily with all statements made by the authors, I am happy to increase my score based on the discussion of previous work. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665074635, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper625/Reviewers"], "noninvitees": [], "tcdate": 1570237749420, "tmdate": 1575665074654, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Review"}}}, {"id": "ByxhW2TNsH", "original": null, "number": 4, "cdate": 1573342212310, "ddate": null, "tcdate": 1573342212310, "tmdate": 1573342212310, "tddate": null, "forum": "S1emOTNKvS", "replyto": "r1ei61PDYr", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment", "content": {"title": "Response to the comments from the reviewer", "comment": "Thanks for your valuable comments to our paper. We detail our response to your questions or concerns as follows.\n\nQuestion 1: What is the difference between NeuralSparse and LDS [1]?\n\nThanks for sharing this related work [1]. While NeuralSparse and LDS do share the spirit of sampling adjacency matrix for downstream graph learning tasks, NeuralSparse is significantly different from LDS in the following aspects.\n\nFirst, the assumptions behind NeuralSparse and LDS are different, pursuing different output graph samples.\n- In NeuralSparse, we assume the input graphs are complete, but not all the input graph data are relevant to the downstream tasks. Therefore, NeuralSparse samples subgraphs of input graphs. In other words, graph sampling under NeuralSparse is constrained by input graphs, and the resulting graph samples are strictly subgraphs of input graphs. \n- In LDS, the assumption is the input graphs are incomplete. Therefore, the expected graph samples are subgraphs of a complete graph with respect to given nodes. In other words, there is no guarantee that graph samples are subgraphs of input graphs in LDS.\n\nSecond, NeuralSparse is an inductive technique that is capable of handling both inductive and transductive tasks, while LDS is a transductive method that focuses on transductive tasks. \n- In NeuralSparse, the probability of an edge to be sampled is conditioned on node and edge features. In this way, a learned NeuralSparse can directly adapt to unseen testing data in an inductive setting, and the number of parameters is independent of input graph size.\n- In LDS, each possible edge is associated with a parameter. A learned LDS is expected to only work for its input training graph. Let n be the number of nodes in an input graph. The number of parameters in LDS scales quadratically with respect to n.\nAs NeuralSparse is an inductive method, it can handle both inductive and transductive tasks\n\nIn addition, NeuralSparse has better scalability. Given a fixed hyper-parameter k, NeuralSparse scales linearly with respect to the number of edges in an input graph. Meanwhile, LDS is difficult to scale with a large number of nodes, even if its underlying graph is sparse.\n\nIn the latest draft, we have added the discussion on [1], in Section 2 and the end of Section 4.\n\nQuestion 2: Empirical comparison between NeuralSparse and LDS.\n\nIn the latest draft, we have added S6 in the appendix for a detailed discussion on the empirical comparison between NeuralSparse and LDS. The following is a brief summary.\n\n[Datasets]\nAs LDS is transductive only and the tasks in Reddit, PPI, as well as Transaction datasets are inductive, we focus on Cora and Citeseer in this set of experiments.\n\n[Input graphs]\nLDS utilizes edges in an input graph to initialize edge parameters. In general, there are three possible ways to prepare input graphs.\n- Setting A: k-NN graphs [1]\n- Setting B: original input graphs\n- Setting C: edge union of k-NN graphs and original input graphs\n\n[Key observations]\nIn general, NeuralSparse and LDS achieve comparable prediction accuracy. \n- NeuralSparse performs slightly better in Setting A and Setting C\n- LDS performs slightly better in Setting B.\n\nBy comparing the original graphs and k-NN graphs in terms of the percentage of edges connecting nodes of same labels, we find k-NN graphs include significantly more irrelevant edges with respect to this task as shown below.\n+-------------+---------+-------------+\n|                 | Cora  | Citeseer  |\n+-------------+---------+-------------+\n| Original  | 82.2%| 73.1%     |\n+-------------+---------+-------------+\n| k-NN       | 53.9%| 48.4%      |\n+-------------+---------+-------------+\n\nWe conjecture that NeuralSparse is more robust to graphs with more random edges (e.g., Setting A and C), while LDS could perform better in a graph of relatively less noise (e.g., Setting B) by adding additional edges.\n\n[Robustness against random edges]\n\nTo confirm NeuralSparse is more robust against irrelevant edges, we evaluate GCN, LDS-GCN, and NeuralSparse-GCN on original graphs plus additional random edges. The detailed data preparation is described in S6.\n\nOn both Cora and Citeseer datasets, we observe similar trends. First, when we increase the amount of random edges, the accuracy of the three methods decreases. Second, starting from 200%, NeuralSparse-GCN consistently performs the best in all the cases. This suggests NeuralSparse-GCN is more robust against random (potentially irrelevant) edges, compared with the other two methods. \n\nReference\n[1] Franceschi, Luca, et al. \"Learning discrete structures for graph neural networks.\" ICML 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper625/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1emOTNKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper625/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper625/Authors|ICLR.cc/2020/Conference/Paper625/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168677, "tmdate": 1576860553780, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment"}}}, {"id": "r1xxZipViH", "original": null, "number": 3, "cdate": 1573341944343, "ddate": null, "tcdate": 1573341944343, "tmdate": 1573341944343, "tddate": null, "forum": "S1emOTNKvS", "replyto": "r1xkiEt2Fr", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment", "content": {"title": "Response to the comments from the reviewer", "comment": "Thanks for your valuable comments to our paper. Our answers to your questions/concerns are detailed as follows.\n\nQuestion 1: It is unclear why existing GCN-based approaches can not handle the cases shown in Fig. 1. Is there any evidence (either theoretical or empirical) or reference to support this argument? \n\nThanks for the suggestion. In the latest draft, we have added S7 in the appendix where we use an example to illustrate one of the scenarios where GCN could suffer sub-optimal performance by directly processing original input graphs.\n\nQuestion 2: Conventionally, graph sparsification aims to find smaller subgraphs from the input graphs that preserve the key structures. However, in Fig. 1 (b), the sparsified subgraph seems only downsampling the edges while preserving all the nodes as the original graph. The authors may want to clarify whether the sparisified subgraph has the identical size as the input graph.\n\nIn terms of node size, yes, a sparsified subgraph has the same number of nodes as its original graph does. In terms of edge size, no, for each node, it has no more than k edges, given a fixed hyper-parameter k. We have updated the definition of k-neighbor subgraphs in Section 4 to clear possible confusion.\n\nQuestion 3:  Some notations are not formally defined before using them. In Eq. 2, what do Q_\\theta and Q_\\phi denote? \n\nQ_\\theta is the approximation function for P(Y|g) and Q_\\phi is the approximation function for P(g|G). We have updated in the latest draft accordingly to clear such confusion.\n\nQuestion 4: The statement of \"trade-off between model accuracy and graph complexity by tuning the hyper-parameter k\" is vulnerable. If the overfitting exists, larger k may result in lower accuracy in the testing phase. \n\nThanks for pointing out this confusion. In the latest draft, we have updated the relevant discussion in Section 4. In particular, the quoted statement is modified as \"We are able to adjust the estimation on the amount of task-relevant graph data by tuning the hyper-parameter k.\"\n\nQuestion 5: What is the complexity of f_\\Phi()? \n\nf_\\phi() is implemented by a multi-layer perceptron (MLP) with Gumbel-Softmax. Let $d_n$ and $d_e$ be the numbers of dimensions for node and edge features, respectively. The MLP is implemented in a two-layer architecture with hidden dimensionality d_h. The complexity of the MLP is $O(d_h * (2d_n + d_e))$. Suppose the number of one-hop neighbors of a node is $\\hat{d}$. This function visits one node's one-hop neighborhood in $O(d_h * (2d_n + d_e) * \\hat{d})$.\n\nQuestion 6: The complexity (i.e., O(km)) of the proposed model is problematic. As stated at the beginning of this paper, the paper targets the graph with \"complex local neighborhood\", where each node is described by rich features and neighbors. In other words, the target graph is not sparse. In this case, the complexity of the proposed algorithm can be intractable, especially when k is large and m is close to $n^2$. \n\nWe answer this question from the following aspects.\n\n- Does NeuralSparse require or expect dense graphs? As discussed in S7, even for a graph with 1000 nodes and average degree 20, GCN still could suffer sub-optimal performance by processing irrelevant edges. Therefore, as long as there are irrelevant edges that impact the prediction performance in the downstream tasks, NeuralSparse will be effective. In sum, NeuralSparse has no assumption or expectation to the density of input graphs.\n\n- When k is large and m is close to $n^2$, is NeuralSparse intractable? First of all, k is a hyper-parameter in NeuralSparse and independent to input graph size. Given a fixed k, NeuralSparse scales linearly with respect to m. In practice, k linearly impacts computation time, and one can tune k in order to meet computation resource constraints. Second, if m is large, downstream GNNs will face the same scalability challenge. Fortunately, similar to existing GNNs, NeuralSparse also adopts independent node-centric computation, which is highly parallelizable as suggested in Section 4. This could address the scalability issue to some extent. As the scalability challenge is a common problem to existing GNNs and NeuralSparse, a comprehensive discussion of this problem could be out of our scope. In sum, we argue that the extra computation overhead from NeuralSparse is tunable and affordable, compared with the complexity in downstream GNNs."}, "signatures": ["ICLR.cc/2020/Conference/Paper625/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1emOTNKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper625/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper625/Authors|ICLR.cc/2020/Conference/Paper625/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168677, "tmdate": 1576860553780, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment"}}}, {"id": "rklYK5TEsB", "original": null, "number": 2, "cdate": 1573341825417, "ddate": null, "tcdate": 1573341825417, "tmdate": 1573341825417, "tddate": null, "forum": "S1emOTNKvS", "replyto": "ryeIwujgcB", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment", "content": {"title": "Response to the comments from the reviewer", "comment": "Thanks for your valuable comments to our paper.\n\nFor the comments on performance results in Table 2, we agree with you. As you pointed out, our empirical results suggest NeuralSparse is able to help existing GNN techniques achieve comparable or even better generalization performance by effectively reducing input graph complexity and removing graph noise. In addition, we have updated our analysis with respect to the performance results in Table 2 accordingly.\n\nFor the impact brought by a total number of trainable parameters, we have added S4 in the appendix to detail the discussion. In particular, we focus on GCN in this set of experiment, and have added a baseline called NeuralSparse-GCN-Compact which shares a similar number of trainable parameters with an original GCN. We summarize the empirical results in Table S2 as follows.\n- Both NeuralSparse-GCN-Compact and NeuralSparse-GCN consistently outperform GCN across all the datasets.\n- Compared with NeuralSparse-GCN, NeuralSparse-GCN-Compact achieves comparable prediction accuracy with smaller variance in most cases. \n\nFor the question of how the hyper-parameter k impacts validation performance, we have added S5 in the appendix for the discussion. Our observation is summarized as follows. In terms of validation, as shown in Figure S1, the validation performance increases when k ranges from 2 to 10 with more available graph data.  After k exceeds 10, the increase in validation performance slows down and turns to be saturated or dropping.  In terms of testing performance, it shares a similar trend when k ranges from 2 to 10. Meanwhile, the testing performance drops more after k exceeds 10.\n\nIn Algorithm 2, the letter for the edge set has been updated with $\\mathbb{H}$ to avoid confusion. "}, "signatures": ["ICLR.cc/2020/Conference/Paper625/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1emOTNKvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper625/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper625/Authors|ICLR.cc/2020/Conference/Paper625/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168677, "tmdate": 1576860553780, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper625/Authors", "ICLR.cc/2020/Conference/Paper625/Reviewers", "ICLR.cc/2020/Conference/Paper625/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Comment"}}}, {"id": "r1xkiEt2Fr", "original": null, "number": 2, "cdate": 1571751062644, "ddate": null, "tcdate": 1571751062644, "tmdate": 1572972572116, "tddate": null, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors argue that existing GCN-based approaches may pose non-trival overfitting risk during the training phase, especially when high-dimensional features and high-degree entities are observed in the graphs. To address the issue, the authors integrate graph sparsification with conventional graph neural nets. Experimental results show the efficacy of the proposed model in a series of benchmark datasets. In general, the paper is easy-to-follow and well-organized. My main concern is there lack some insightful discussion regarding the problem motivation and the proposed algorithm. In particular,\n(1) It is unclear why existing GCN-based approaches can not handle the cases shown in Fig. 1. Is there any evidence (either theoretical or empirical) or reference to support this argument? \n(2) The motivation example shown in Fig. 1 is confusing. Conventionally, graph sparsification aims to find smaller subgraphs from the input graphs that preserve the key structures. However, in Fig. 1 (b), the sparsified subgraph seems only downsampling the edges while preserving all the nodes as the original graph. The authors may want to clarify whether the sparisified subgraph has the identical size as the input graph.\n(3) Some notations are not formally defined before using them. In Eq. 2, what do Q_\\theta and Q_\\phi denote? \n(4) The statement of \"trade-off between model accuracy and graph complexity by tuning the hyperparameter k\" is vulnerable. If the overfitting exists, larger k may result in lower accuracy in the testing phase. \n(5) What is the complexity of f_\\Phi()?  \n(6) The complexity (i.e., f(km)) of the proposed model is problematic. As stated at the beginning of this paper, the paper targets the graph with \"complex local neighborhood\", where each node is described by rich features and neighbors. In other words, the target graph is not sparse. In this case,  the complexity of the proposed algorithm can be intractable, especially when k is large and m is close to n^2. "}, "signatures": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665074635, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper625/Reviewers"], "noninvitees": [], "tcdate": 1570237749420, "tmdate": 1575665074654, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Review"}}}, {"id": "ryeIwujgcB", "original": null, "number": 3, "cdate": 1572022366335, "ddate": null, "tcdate": 1572022366335, "tmdate": 1572972572038, "tddate": null, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "invitation": "ICLR.cc/2020/Conference/Paper625/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a trainable graph sparsification mechanism that can be used in conjunction with GNNs. This process is parameterized using a neural network and can be trained end-to-end (by using the Gumbel softmax reparameterization trick) with the loss given by the task at hand. Experimental results on node classification tasks are presented.\n\nThe paper is well written and easy to follow. I think that overall the method is sound and well executed. \n\nEmpirical results are convincing as the method consistently improves performance (compared to not applying the sparsification) over several GNNs. I feel that the claim of the improvement could be softened a bit. Please correct me if I'm reading Table 2 wrongly, with the exception of the Transaction dataset, most differences are below 3%. I am not familiar with these datasets so I cannot judge the significance of the improvement. In any case, the reduction in computation with the improved performance is a strong result.\n\nThe baselines that include unsupervised graph sparsification as a pre-processing make the results worse (with respect to not applying it) in all cases. This shows that for this problem, a task driven specification is crucial for maintaining performance. \n\nNeural Sparse model has more parameters than the version that does not use a sparsifier. Do you think that this could influence the performance? Would it be possible to compare using similar number of trainable parameters? I assume that using more parameters would not imply better performance for the baseline, but it would be good to clarify.\n\nI can understand that the performance of the model depends critically on k, and that k might vary significantly over datasets. In my view, it would be informative to include (maybe in the supplementary material) the performance variations on the corresponding validation sets as one changes k for the different datasets (as done in Figure 3 (c)).\n\nIn Algorithm 2 it would be better to use a different letter for the edge set (as currently looks like the real numbers)."}, "signatures": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper625/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust Graph Representation Learning via Neural Sparsification", "authors": ["Cheng Zheng", "Bo Zong", "Wei Cheng", "Dongjin Song", "Jingchao Ni", "Wenchao Yu", "Haifeng Chen", "Wei Wang"], "authorids": ["chengzheng@cs.ucla.edu", "bzong@nec-labs.com", "weicheng@nec-labs.com", "dsong@nec-labs.com", "jni@nec-labs.com", "yuwenchao@ucla.edu", "haifeng@nec-labs.com", "weiwang@cs.ucla.edu"], "keywords": [], "abstract": "Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "pdf": "/pdf/b0f22078a835fd027b887d742128845fc13e2b39.pdf", "paperhash": "zheng|robust_graph_representation_learning_via_neural_sparsification", "original_pdf": "/attachment/dec84586be00fe9c500624bbf8b5b4c3f2861e48.pdf", "_bibtex": "@misc{\nzheng2020robust,\ntitle={Robust Graph Representation Learning via Neural Sparsification},\nauthor={Cheng Zheng and Bo Zong and Wei Cheng and Dongjin Song and Jingchao Ni and Wenchao Yu and Haifeng Chen and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=S1emOTNKvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1emOTNKvS", "replyto": "S1emOTNKvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper625/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665074635, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper625/Reviewers"], "noninvitees": [], "tcdate": 1570237749420, "tmdate": 1575665074654, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper625/-/Official_Review"}}}], "count": 11}