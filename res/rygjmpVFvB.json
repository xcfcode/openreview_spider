{"notes": [{"id": "rygjmpVFvB", "original": "BygvKdevwH", "number": 461, "cdate": 1569439010708, "ddate": null, "tcdate": 1569439010708, "tmdate": 1583912049433, "tddate": null, "forum": "rygjmpVFvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "rIcDsAYwjb", "original": null, "number": 1, "cdate": 1576798697095, "ddate": null, "tcdate": 1576798697095, "tmdate": 1576800938616, "tddate": null, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The authors propose a way to generate unseen examples in GANs by learning the difference of two distributions for which we have access. The majority of reviewers agree on the originality and practicality of the idea.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716299, "tmdate": 1576800266412, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper461/-/Decision"}}}, {"id": "Bkg3cauHjr", "original": null, "number": 4, "cdate": 1573387667792, "ddate": null, "tcdate": 1573387667792, "tmdate": 1573701162481, "tddate": null, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment", "content": {"title": "We upload the revised manuscript .", "comment": "Thanks for all the comments!\n\nWe list all the modifications as follows.\n\n1. We add Sec. F in appendix for the ablation study on different $\\alpha$.\n2. We add Sec. G in appendix to demonstrate the sample quality of DSGAN on CelebA."}, "signatures": ["ICLR.cc/2020/Conference/Paper461/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygjmpVFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper461/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper461/Authors|ICLR.cc/2020/Conference/Paper461/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171149, "tmdate": 1576860548956, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment"}}}, {"id": "Skg49XNciH", "original": null, "number": 5, "cdate": 1573696395545, "ddate": null, "tcdate": 1573696395545, "tmdate": 1573701085867, "tddate": null, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment", "content": {"title": "We upload the revised manuscript . ", "comment": "In this revision, our modifications are listed as follows.\n\n1. We add more experiments and descriptions in Sec. G of Appendix because we trained a transferring GAN ([1]) as a strong competitor. However, compared with transferring GAN, our DSGAN still performs well. Please see Fig. 11 of Sec. G of Appendix for the new results.\n\n[1] Wang, Yaxing et al. \u201cTransferring GANs: generating images from limited data.\u201d ECCV (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper461/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygjmpVFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper461/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper461/Authors|ICLR.cc/2020/Conference/Paper461/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171149, "tmdate": 1576860548956, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment"}}}, {"id": "SyesrjuHsB", "original": null, "number": 3, "cdate": 1573387075352, "ddate": null, "tcdate": 1573387075352, "tmdate": 1573392618012, "tddate": null, "forum": "rygjmpVFvB", "replyto": "S1gKDV-0Fr", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment", "content": {"title": "Reply to review #3", "comment": "Thanks for your comments! First, we have to clarify some misunderstandings.\n\n\n>>> it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1)\n\nBadGAN has already theoretically proved that complement data are helpful for semi-supervised learning. In this paper, we demonstrate that,  using our unseen data, the proofs in badGAN still can be satisfied but in a more concise way. Therefore, compared to badGAN that requires extra PixelCNN, DSGAN saves more computational memory and is time-efficienct.\n\n\n>>> It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.\n\nIn Novelty detection, we use the reconstruction error as a criterion to determine whether an image comes from seen class or unseen class. It is expected that images from the seen classes should be reconstructed better than those reconstructed from unseen classes. However, VAE cannot force the unseen classes with high reconstructed error. So, we combine DSGAN with VAE to deal with this issue. Due to the above reason, it is expected that \"our sampled reconstruction results are not good as VAE\". Note that the seen class, car, still can be reconstructed well by our method in Fig 8 (at the last row). The quantitative results in Table 3 further validate our approach.\n\n\n>>> I would also expect more ablation studies about how to pick p_{\\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.\n\nIn fact, how to design $p_{\\hat{d}}$ depends on applications  instead of datasets, as described in Sec. 4.1 and Sec. 4.2. Please note that, in Section 5.2.1, we used the same $p_{\\bar{d}}$ for ALL datasets.\n\nWe also want to clarify the datasets used in our experiments. In semi-supervised learning, we follow our competitors to conduct experiments on MNIST, SVHN and CIFAR10. In novelty detection, our method is evaluated on CIFAR10, which is also common in this application. Furthermore, we also add additional experiments about generating complement data in CelebA, which is a more complex dataset. We can see from Fig. 10 (Appendix G) that DSGAN can create complement data for complicate images well.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper461/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygjmpVFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper461/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper461/Authors|ICLR.cc/2020/Conference/Paper461/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171149, "tmdate": 1576860548956, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment"}}}, {"id": "BkgZq5uHor", "original": null, "number": 2, "cdate": 1573386889271, "ddate": null, "tcdate": 1573386889271, "tmdate": 1573391489086, "tddate": null, "forum": "rygjmpVFvB", "replyto": "BygFHVOeqB", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment", "content": {"title": "Reply for review #2", "comment": "Thanks for your comments!\n\n>>> Experimental settings are clear, however, what makes me confused is that the construction for $p_{\\bar{d}}$ is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images. \n\nIn responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images. In this experiment, we generate the color images of size 64 $\\times$ 64. Similar to 1/7 experiments on the MNIST dataset, we let $p_{\\bar{d}}$ be the distribution of face images with glasses and without glasses, and let $p_{d}$ be images without glasses.  We sample 10000 images with glasses and 10000 images without glasses from CelebA, and we set $\\alpha$ to 0.5.  \n\n\nIn order to verify the generated image quality of DSGAN, we also train a GAN for comparison. GAN is trained with the same amount of training images (but only using face images with glasses since GAN is to learn the distribution of training data). In other words, we assume GAN can use complement data as training data directly. On the contrary, DSGAN only uses complement data indirectly (the difference between $p_{\\bar{d}}$ and $p_d$). \n\nFigure 10 in Appendix G shows generated images and FID for both methods. We can see that our DSGAN can generate images with glasses from the given $p_d$ and $p_{\\bar{d}}$, and the FID of DSGAN are comparable to that of GAN. The experiment validates that DSGAN still works well to create complement data for complicate images.\n\n\n>>> The model seems to be sensitive to the hyper-parameter $\\alpha$, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?\n\nSince the optimal $\\alpha$ of generating \"unseen\" data in DSGAN depends on the degree of overlap between $p_{\\bar{d}}$ and $p_d$, it might need to be fine-tuned for different datasets. However, in our experiments, we set $\\alpha$ to $0.8$ in most cases. \n\nTheorem 1 illustrates $\\alpha$ should be expected to be as large as possible if both network G and D have infinite capacity. Though the networks never have the infinite capacity in real applications, a general rule is to pick a large $\\alpha$ and force the complement data to be far from p_d, which is similar to the ablation studies in Sec. 5.1. According to our empirical observations, $\\alpha = 0.8$ is the good choice for all datasets. Table 11 in Sec. F of Appendix shows the experimental results of how $\\alpha$ affects the performances. We use different $\\alpha$ values in the MNIST, SVHN and CIFAR10 dataset, respectively. One can see that we achieve the best performances at $\\alpha = 0.8$.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper461/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygjmpVFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper461/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper461/Authors|ICLR.cc/2020/Conference/Paper461/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171149, "tmdate": 1576860548956, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment"}}}, {"id": "r1efiwuroH", "original": null, "number": 1, "cdate": 1573386137997, "ddate": null, "tcdate": 1573386137997, "tmdate": 1573389802557, "tddate": null, "forum": "rygjmpVFvB", "replyto": "HJe5Adneor", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment", "content": {"title": "Reply for review #1", "comment": "Thanks for your comments!\n\n>>> Only the 1/7 examples of MNIST dataset are provided in case studies. I am wondering for more complicated images, how is the performance?\n\nIn responding to this comment and the first comment of Reviewer #2, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images. In this experiment, we generate the color images of size 64 $\\times$ 64. Similar to 1/7 experiments on the MNIST dataset, we let $p_{\\bar{d}}$ be the distribution of face images with glasses and without glasses, and let $p_{d}$ be images without glasses.  We sample 10000 images with glasses and 10000 images without glasses from CelebA, and we set $\\alpha$ to 0.5.  \n\nIn order to verify the generated image quality of DSGAN, we also train a GAN for comparison. GAN is trained with the same amount of training images (but only using face images with glasses since GAN is to learn the distribution of training data). In other words, we assume GAN can use complement data as training data directly. On the contrary, DSGAN only uses complement data indirectly (the difference between $p_{\\bar{d}}$ and $p_d$). \n\nFigure 10 in Appendix G shows generated images and FID for both methods. We can see that our DSGAN can generate images with glasses from the given $p_d$ and $p_{\\bar{d}}$, and the FID of DSGAN are comparable to that of GAN. The experiment validates that DSGAN still works well to create complement data for complicate images.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper461/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygjmpVFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper461/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper461/Authors|ICLR.cc/2020/Conference/Paper461/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171149, "tmdate": 1576860548956, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper461/Authors", "ICLR.cc/2020/Conference/Paper461/Reviewers", "ICLR.cc/2020/Conference/Paper461/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Comment"}}}, {"id": "HJe5Adneor", "original": null, "number": 3, "cdate": 1573075154114, "ddate": null, "tcdate": 1573075154114, "tmdate": 1573075154114, "tddate": null, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This paper proposed the DSGAN model to generate unseen data. The intuition based on standard GAN is straightforward and makes sense. The paper is well written, especially the case studies illustrate the idea clearly. The designing of p_{\\bar{d}} also presented the limitation of this method. Two main discussed applications, semi-supervised learning and novelty detection are important in machine learning. In general, this is an interesting paper.\n\nHowever, my concern is about the experiments. As a generative model for unseen data, I would like to see the generated results, which is more convincing. Only the 1/7 examples of MNIST dataset are provided in case studies. I am wondering for more complicated images, how is the performance?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper461/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576345801971, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper461/Reviewers"], "noninvitees": [], "tcdate": 1570237751788, "tmdate": 1576345801990, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Review"}}}, {"id": "S1gKDV-0Fr", "original": null, "number": 1, "cdate": 1571849312887, "ddate": null, "tcdate": 1571849312887, "tmdate": 1572972592464, "tddate": null, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed DSGAN which learns to generate unseen data from seen data distribution p_d and its somehow \u201cbroad\u201d version p_{\\hat d} (E.g., p_d convolved with Gaussian). The \u201cunseen data\u201d is the one that appears in p_{\\hat d} but not in p_d. DSGAN is trained to generate such data. In particular, it uses samples from p_d as fake data and samples from p_{\\hat d} as the real one. \n\nAlthough the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques. The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3). It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10. I would also expect more ablation studies about how to pick p_{\\had d}, which seems to be the key of this approach, in MNIST and CIFAR10. \n\nIn terms of writing, the paper is a bit confusing in terms of motivations and notations. \n\nOverall, the method looks incremental and experimental results are mixed on small datasets so I vote for rejection. Note that I am not an expert on GAN/VAE so I put low confidence here. \t"}, "signatures": ["ICLR.cc/2020/Conference/Paper461/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576345801971, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper461/Reviewers"], "noninvitees": [], "tcdate": 1570237751788, "tmdate": 1576345801990, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Review"}}}, {"id": "BygFHVOeqB", "original": null, "number": 2, "cdate": 1572009024921, "ddate": null, "tcdate": 1572009024921, "tmdate": 1572972592423, "tddate": null, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "invitation": "ICLR.cc/2020/Conference/Paper461/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper provides an interesting application of GAN which can generate the outlier distribution of training data which forces generator to learn the distribution of the low probability density area of given data. To show the effectiveness of the method, the author intuitively shows how it works on 2-D points data as well as the reconstructed Mnist dataset. Additionally, this approach reaches a comparable performance on semi-supervised learning and novelty detection task.\n\nPaper Strengths\n\n1. The idea of this paper is novel, and the implementation of this method is easily interacted with any GAN model. Also, due to its concise structure compared to the existing method, it saves more computational memory and is time efficiency.\n\nPaper Weaknesses\n\n1. Experimental settings are clear, however, what makes me confused is that the construction for p_{\\bar{d}} is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images. \n2. The model seems to be sensitive to the hyper-parameter \\alpha, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?"}, "signatures": ["ICLR.cc/2020/Conference/Paper461/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper461/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network--Unseen Sample Generation", "authors": ["Yi Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "keywords": ["generative adversarial network", "semi-supervised learning", "novelty detection"], "TL;DR": "We proposed a novel GAN framework to generate unseen data.", "abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n", "pdf": "/pdf/61b8949ecc88a2f28518687319b880b264bbc8ec.pdf", "code": "https://drive.google.com/open?id=18aQzyPbTT7_4fdkFjxL2MLjxMLK_hCuH", "paperhash": "sung|differenceseeking_generative_adversarial_networkunseen_sample_generation", "_bibtex": "@inproceedings{\nSung2020Difference-Seeking,\ntitle={Difference-Seeking Generative Adversarial Network--Unseen Sample Generation},\nauthor={Yi Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygjmpVFvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d90628c76fe30d7a4e752bb01e6f31c65b21ab31.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygjmpVFvB", "replyto": "rygjmpVFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper461/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576345801971, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper461/Reviewers"], "noninvitees": [], "tcdate": 1570237751788, "tmdate": 1576345801990, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper461/-/Official_Review"}}}], "count": 10}