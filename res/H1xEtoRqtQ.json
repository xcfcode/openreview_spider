{"notes": [{"id": "H1xEtoRqtQ", "original": "rygwAbH9Km", "number": 436, "cdate": 1538087803959, "ddate": null, "tcdate": 1538087803959, "tmdate": 1545355414768, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xGoih6JE", "original": null, "number": 1, "cdate": 1544567706388, "ddate": null, "tcdate": 1544567706388, "tmdate": 1545354499970, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Meta_Review", "content": {"metareview": "As all the reviewers have highlighted, there is some interesting analysis in this paper on understanding which models can be easier to complete. The experiments are quite thorough, and seem reproducible. However, the biggest limitation---and the ones that is making it harder for the reviewers to come to a consensus---is the fact that the motivation seems mismatched with the provided approach. There is quite a lot of focus on security, and being robust to an adversary. Model splitting is proposed as a reasonable solution. However, the Model Completion hardness measure proposed is insufficiently justified, both in that its not clear what security guarantees it provides nor is it clear why training time was chosen over other metrics (like number of samples, as mentioned by a reviewer). If this measure had been previously proposed, and the focus of this paper was to provide empirical insight, that might be fine, but that does not appear to be the case. This mismatch is evident also in the writing in the paper. After the introduction, the paper largely reads as understanding how retrainable different architectures are under which problem settings, when replacing an entire layer, with little to no mention of security or privacy. \n\nIn summary, this paper has some interesting ideas, but an unclear focus. The proposed strategy should be better justified. Or, maybe even better for the larger ICLR audience, the provided analysis could be motivated for other settings, such as understanding convergence rates or trainability in neural networks.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "This paper provides some interesting ideas, but has a mismatch between the title and motivation and what is provided"}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper436/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353218260, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353218260}}}, {"id": "Hke4tdLNkN", "original": null, "number": 17, "cdate": 1543952507595, "ddate": null, "tcdate": 1543952507595, "tmdate": 1543952507595, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "SJgPugUVyE", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "Re: On disjoint subsets", "comment": "Thank you for the clarifications on this portion, as there are multiple ways to define the parties for this setup."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "SJgPugUVyE", "original": null, "number": 16, "cdate": 1543950446627, "ddate": null, "tcdate": 1543950446627, "tmdate": 1543950446627, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "r1lc6BNV1V", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "Re: On disjoint subsets", "comment": "Apologies for insisting on this point, but we think it's not just a point about semantics. We are worried about a misunderstanding since you stated above that one of your two major critiques of the paper (point 2) is that it is unclear whether \"the experimental tasks demonstrate a useful application or analysis of shared model governance.\"\n\nIn the example above, the first party would hold *only subset (1) of the parameters* (the first layer, *not* all parameters) and the second party would hold the remaining parameters (2) (layers 2, 3, ...). (See the third paragraph in the introduction.) So both parties hold disjoint subsets of the model, while still being able to train the model jointly by passing activations back and forth. The second party is then the party that tries to do model completion after training using their subset of the weights; in other words, the second party is attacking the security of the shared governance over the trained model.\n\nIn our experiments, the weights of both parties are on the same machine, and even in the same Tensorflow graph, we are only 'simulating' and attack.\n\nDoes this clarify the setup?\n\nFrom our discussion, it seems like the paper should make the setup more precise, and we will think about how to improve presentation. Thank you for your help!"}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "r1lc6BNV1V", "original": null, "number": 15, "cdate": 1543943618130, "ddate": null, "tcdate": 1543943618130, "tmdate": 1543944046544, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "Sygbk-VEJE", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "On disjoint subsets", "comment": "> > [...] and the actual setup that doesn't use a disjoint subset of the parameters (such as T1).\n>\n> But it does; we think this is where our misunderstanding could lie.\n>\n> Let's say the two disjoint subsets of the parameters are (1) the first layer\n> and (2) all the other layers. Suppose we only have access to (2) and not (1)\n\nI do not think that this is a misunderstanding as I am only trying\nto make a semantic point that the parties don't hold disjoint\nsubsets of the parameters, as the introduction to your paper describes.\nIn the example you are describing as I see it,\none party (the originally trained model) will hold all of the\nlearned parameters and another party (that is trying to do the completion)\nwill hold some subset of the learned parameters and\nsome other set of parameters for the part to be completed.\nThese two parties are not holding disjoint subsets of the parameters."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "Sygbk-VEJE", "original": null, "number": 14, "cdate": 1543942361282, "ddate": null, "tcdate": 1543942361282, "tmdate": 1543942437499, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "Hkl_02QmkE", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "Questions for further clarification", "comment": "Thank you for your response and for re-evaluating our paper. We appreciate that you are taking the time to engage; the paper has been significantly improved through your feedback.\n\nRegarding your point 1): What metric do you think we should have used? As stated above, we are interested in the computational cost to reach a certain performance threshold. We could either care about something other than computational costs (such as wall-clock time) or measure performance differently (as you suggested with the similarity metric). While we think both are valid point to modify, but we think that the setting we chose is a reasonable point to start at with this first paper on model completion.\n\nRegarding your point 2): Could you please elaborate on this point? We wish to know how our experiments could be more convincing or applicable to evaluate model splitting as a technique for shared model governance. The setting we chose is intended to be the most difficult (for the defender) realistic setting.\n\nThank you for explaining the similarity metric, that makes a lot of sense. We should look into evaluating this in follow-up work after this paper gets accepted.\n\n> [...] and the actual setup that doesn't use a disjoint subset of the parameters (such as T1).\n\nBut it does; we think this is where our misunderstanding could lie.\n\nLet's say the two disjoint subsets of the parameters are (1) the first layer and (2) all the other layers. Suppose we only have access to (2) and not (1) --- i.e., we are the party that holds most, but not all, of the parameters. The re-training procedure T1 then (re)initializes the parameters of the first layer (because we don't have access to them, so we can't use the original weights) and trains all of the parameters jointly: the parameters from (2) and the re-initialized parameters.\n\nIn our experiments we do not hold the disjoint subsets of the parameters on separate machines because our goal is to evaluate the difficulty of model completion. The experimental setup thus artificially removes the part of the model that needs to be completed, pretending that it is unavailable. We made sure that the weights of the missing part do not \"leak\" into the retraining procedure.\n\nDoes that make sense?\n\n> Figure 1 also does not capture the T1 re-training procedure as in part (d), layers 1,3,4 appear to stay the same even though T1 could have modified them.\n\nYes, thank you for spotting this. We will slightly adjust the colors to reflect that the layers 1,3, and 4 may be finetuned by the retraining procedure."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "Hkl_02QmkE", "original": null, "number": 12, "cdate": 1543875792503, "ddate": null, "tcdate": 1543875792503, "tmdate": 1543875792503, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "B1gdPwTdTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "Updated review", "comment": "Thanks for the clarifications and for updating some of the motivation\nparts of the paper. I think this paper has some interesting\nideas in it and I have updated my rating from a 4 to a 5\nafter reading the rebuttal. I still echo some of my original\nconcerns that 1) I am not convinced that the number\nof training iterations to reach some test accuracy is the\nright metric to use to measure the difficulty of model-completion, and\n2) I am not convinced that the experimental tasks demonstrate\na useful application or analysis of shared model governance,\nbut I am not an expert in this area.\n\nResponding to my smaller comments:\n\n> Do you have a sense how we could make this more clear in the paper?\n\nOn the motivation and positioning, I was referring to the\ndisconnection between parts of the introduction that say\n\"model splitting: distributing a deep learning model between \nmultiple parties such that each party holds a *disjoint* subset\nof the model\u2019s parameters\"\nand the actual setup that doesn't use a disjoint subset of\nthe parameters (such as T1).\nFigure 1 also does not capture the T1 re-training procedure\nas in part (d), layers 1,3,4 appear to stay the same even\nthough T1 could have modified them.\n\n> What exactly do you mean by how \u2018nicely\u2019 the missing part has been recovered?\n\nOn the similarity metric for the missing portion, I think\nit would be interesting to look at the error between the outputs\nof the original module to the outputs of the re-learned module."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "B1lby2Fjhm", "original": null, "number": 2, "cdate": 1541278681133, "ddate": null, "tcdate": 1541278681133, "tmdate": 1543875435601, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Review", "content": {"title": "Interesting idea but needs better positioning, metrics, and analysis", "review": "This paper proposes the interesting idea of analyzing how difficult\nit is to re-initialize and re-train layers in neural networks.\nThey study these techniques in the context of ImageNet classification\nand reinforcement learning in the Atari and DeepMind lab domains.\nWhile these are interesting ideas and domains to study, I have\nconcerns with the positioning and execution of the paper.\n\n[Positioning, execution and motivation]\nOn the positioning of the paper, a significant part of the introduction\nand related work section is spent arguing that this approach can be used\nfor shared model governance in contexts where homomorphic encryption\nor secure multi-party computation would instead be used.\nComparing the approaches studied in this paper to these\nsophisticated cryptographically-motivated techniques seems\nlike too much of a stretch, as the methods serve very different\npurposes and in most cases cannot even be directly compared.\n\nThe first and second paragraph discuss the vision of distributing\nthe training of models between multiple parties.\nI agree that this is a useful area to study and direction\nfor the community to go, but as the introduction of this paper\nstates, this is the most interesting when the parties have\ncontrol over logically separate components of the modeling pipeline\nand also when joint training of the components is being done,\npotentially on disjoint and private datasets.\nThe empirical results of this paper do none of this,\nas they only look at the case when a single layer is being\nreplaced.\n\nFurthermore the motivation and positioning of the paper is\nnot carried through in the empirical setup, where they\ninvestigate approaches that do training over all\nof the parameters of the model, breaking the assumption\nthat the parties should be independent and should\nnot share information.\n\n[Metrics for measuring model completeness]\nSection 3.1 defines the metric of completion hardness that is\nused throughout the rest of the paper. The metric looks at the\nnumber of iterations that re-training the model takes to\nreach the same performance as the original model.\nIt's not clear why this is an important metric and I am\nnot convinced it is the right one to use as it:\n1) does not give a notion of how nicely the missing portion\nwas recovered, just that the accuracy reached the\nsame accuracy as the original network, and\n2) methods with a very long per-iteration runtime such as\nsecond-order and sampling-based methods could be used to\nreach a good performance in a small number of iterations,\nmaking these methods appear to be very \"good\" at\ncompleting models. I don't think it is nice that this\nmetric relies on the same optimizer being used for the\noriginal model and the completed model.\n\nI think it's more interesting to study *how much* data is\nrequired to recover missing portions of the model instead\nof how many iterations are needed to recover the same performance.\nThe supervised learning experiments appear to be done\nusing the entire dataset while the RL experiments do\npresent a setting where the data is not the same.\n\n[Empirical results]\nI am also surprised by the empirical finding in Section 5.1\nthat T1 outperforms T2, since it seems like only optimizing\nthe parameters of the missing layer would be the best\napproach. I think that if a similarity metric was used\ninstead, T2 would be significantly better at finding the\nlayer that is the most similar to the layer that was removed.\n\nSome smaller comments:\n\n1. In Section 3.1, the definition of C_T does not use T explicitly\n   inside of it.\n2. In the last paragraph of Section 3.1 and first paragraph of\n   Section 3.2, N should be defined as an iteration that\n   reaches the best loss.\n3. The description of T3 does not say what method is used to\n   optimize the over-parameterized layer, is it T1 or T2?\n4. Why does T4 use T1 instead of T2?\n5. In the experimental setup, why is T2 applied with a different\n   learning rate schedule than the original training procedure?\n6. Why is T2 not shown in the AlexNet results for Figure 2?\n7. The dissimilar axes between the plots in Figure 2 and\n   Figure 3 make them difficult to compare and interpret.\n8. It's surprising that in Figure 3, the hardness of \\alpha=1.0\n   for T2 is 1.0 for everything.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Review", "cdate": 1542234462026, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335723729, "tmdate": 1552335723729, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1eUOsTtRQ", "original": null, "number": 10, "cdate": 1543261037882, "ddate": null, "tcdate": 1543261037882, "tmdate": 1543261037882, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "Updated paper to improve motivation", "comment": "To address the reviewers' feedback, we have updated the paper. It should be a lot more clear about the motivation for this work in abstract, introduction, and the paper title. The thank the reviewers for their efforts on in their reviews and hope that their criticism has been addressed appropriately."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "rylzQd6_TX", "original": null, "number": 4, "cdate": 1542146073932, "ddate": null, "tcdate": 1542146073932, "tmdate": 1542146073932, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "HkgkrCXq2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "Yes, there are numerous follow-up directions to explore", "comment": "Thank you very much for your thoughtful review!\n\n> Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?\n\nYes, this is definitely a concern, and we mention this in the final paragraph. Since we tried a variety of retraining procedures (freezing, not freezing, overparameterization, different initialization schemes) with mostly similar results, we think it is reasonable to be optimistic that there are no radically faster retraining procedures. However, we need to improve our understanding with more experiments, which we have to leave to future work. But we should also stress that at this point the security guarantees this approach provides are only empirical rather than theoretical. Depending on the application that may or may not be a dealbreaker.\n\n> More broadly, there's an additional axis to the optimization problem which is \"How much does the training scheme know about the particulars of the problem?\"\n\nThis is an excellent way to phrase our setup! We decided to strike the balance between impractical retraining procedures like setting all weights to the former values in one step (which requires information that the retraining procedure does not have access to), and making the model completion problem as easy as possible to stress-test its viability as a technique for shared model governance. Since this is (to our knowledge) the first paper on the topic, we struck a balance with what experiments can be executed with reasonable effort; e.g. we did not try to learn a dedicated optimizer. Our goal was to gain some preliminary results on the viability of shared model governance."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "SkxYOwTu67", "original": null, "number": 3, "cdate": 1542145904847, "ddate": null, "tcdate": 1542145904847, "tmdate": 1542145904847, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "B1gdPwTdTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "Regarding your smaller comments", "comment": "1. We admit that this is an awkward property of the notation. To make it technically more rigorous, we could add T as a superscript to \\theta_i everywhere, but we think this would decrease readability without adding clarity.\n2. This is a somewhat arbitrary choice we made. You are right that defining N to be the best loss would also make sense, but the final loss is what we used in our experiments. Moreover, the retraining procedure always starts with the parameters in the final step, not the best parameters. We worry that it would make the formal setup confusing if we used the loss of the best parameters, but used the final parameters for retraining. In practice the difference is minor.\n3. It is T1. We have clarified this in the paper.\n4. Because T1 has been empirically shown to be a stronger retraining procedure.\n5. We tried both and reported the one that performed better. We have clarified this in the paper.\n6. Because the results were not very strong. The data is in Appendix C.1, but also added an additional figure (Figure 7) to the appendix.\n7. This was a deliberate choice because we think that the comparison between different layers is more meaningful than the comparison between different values of \\alpha.\n8. In these runs, the model fails to recover the original performance for all layers over the course of the retraining run."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "B1gdPwTdTQ", "original": null, "number": 2, "cdate": 1542145887953, "ddate": null, "tcdate": 1542145887953, "tmdate": 1542145887953, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "B1lby2Fjhm", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "The model completion problem we study is the hardest realistic test for shared model governance via model splitting", "comment": "Thank you very much for the long and thorough review!\n\n> Comparing the approaches studied in this paper to these sophisticated cryptographically-motivated techniques seems like too much of a stretch, as the methods serve very different purposes and in most cases cannot even be directly compared.\n\nThese are indeed very different approaches with different overhead/security trade-offs. To our knowledge, the approach we are investigating (model splitting) has not been discussed before and we made some changes in the phrasing to make this more clear in the paper. Both MPC and model splitting try to solve the same problem (shared model governance), but we argue that our approach is more scalable than MPC.\n\nAs you point out, a more realistic scenario for model splitting would be one in which the parties do not share the same dataset. However, since we are interested in the security guarantees of model splitting, we study the setting that is hardest to defend against, i.e., the setting that is easiest for the adversary. Since we do not know how much data the adversary has access to, we assume they have access to everything.\n\n> Furthermore the motivation and positioning of the paper is not carried through in the empirical setup, where they investigate approaches that do training over all of the parameters of the model, breaking the assumption that the parties should be independent and should not share information.\n\nWe think this could be a misunderstanding. While in retraining procedure T1 we optimize all parameters of the model, there is always at least one entire layer of parameters that has been removed and replaced with a freshly initialized one (See Figure 1 and Section 3.1). This reflects the assumption that there is at least one layer that the adversary would at no point in time have access to, and which is held by the other party. Do you have a sense how we could make this more clear in the paper?\n\nRegarding your points about our metric for the hardness of model completion:\n\n1. In our experiments we assumed that we only care about the final test accuracy. If there are auxiliary objectives (e.g. being able to fine-tune more easily), these could be added to loss (like a regularizer) and thus feed into the MC-hardness definition. What exactly do you mean by how \u2018nicely\u2019 the missing part has been recovered?\n\n2. This is an excellent point! We wanted to compare computational costs because we are assuming this is what we care about. Computational costs are much harder to compare when you're using different optimizers. This is mentioned in the beginning of section 3.1, where we state our simplifying assumption: *\"We assume that computational cost are constant for each step, which is approximately true in our experiments.\"* We have slightly adjusted the phrasing.\n\n> I think it's more interesting to study *how much* data is required to recover missing portions of the model [...]\n\nThis is a very interesting question for the setting in which different parties have different amounts of data. We didn\u2019t study the problem from this angle because we wanted to assume the worst case (the adversary has access to all of the data) in order to stress-test model splitting as an approach for shared model governance. We have to leave the investigation of model completion under partial access to the dataset to future work.\n\n> the RL experiments do present a setting where the data is not the same.\n\nCould you please elaborate how you mean this sentence? The RL experiments train and retrain on the same environment simulator. This simulator is stochastic, so the agent will not be trained on exactly the same data, but on data drawn from the same distribution.\n\n> I am also surprised by the empirical finding in Section 5.1 that T1 outperforms T2.\n\nYour surprise is understandable. However, similar results have also been in the literature; e.g. Figures 1 and 2 in Yosinski et al. (2014) http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf.\n\n>  I think that if a similarity metric was used instead, T2 would be significantly better at finding the layer that is the most similar to the layer that was removed.\n\nCould you elaborate how you could use a similarity metric during retraining in our setup? What would you measure similarity to?"}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "rklIsV6_pQ", "original": null, "number": 1, "cdate": 1542145181884, "ddate": null, "tcdate": 1542145181884, "tmdate": 1542145181884, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "HJla2wojnm", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "content": {"title": "We made the motivation of our paper more explicit and added related work on pruning", "comment": "Thank you very much for your careful review! Let us address your concerns:\n\n1. We acknowledge that the paper should be more upfront about its motivation (e.g. mention it in the abstract). We are not aware of previous work that discusses model splitting. Our motivation was to find a technique for multi-party computation (MPC) with limited computational & communication overhead. With these constraints, model splitting is an obvious approach to try, and we didn\u2019t think to lay claim to its \"invention.\" We\u2019ve adjusted the phrasing in the paper to make this more clear.\n\nWhile model splitting is much faster as an MPC technique, its security guarantees are much weaker. The hardness of the model completion problem is at the heart of this MPC technique, and this is the reason we wanted to study it.\n\n2. Thank you for pointing us to the pruning literature. We have added a paragraph to the related work section.\n\nRegarding the two papers you referenced: Molchanov et al. (2017) evaluate different approaches for pruning trained models with the aim of making inference faster, dropping entire feature maps at a time. Figure 2 in their paper shows the distribution of importance of feature maps across layers (VGG-16). Their findings (e.g. that lower layers are more important) are compatible with ours. There are also other related papers that prune individual connections rather than entire feature maps (e.g. Changpinyo et al., https://arxiv.org/pdf/1702.06257.pdf).\n\nHan et al. (2016) improve the accuracy of models by training in three phases of which the second one adds a sparsity regularizer (how many of the connections are reduced to 0). However, they do not include an analysis on which parts of the model get pruned, making their results incomparable with ours.\n\nAll of these papers remove neurons across all layers, i.e. by dropping some but not all neurons in every layer. In contrast, in our experiments we remove entire layers at a time with all of their neurons.\n\nNevertheless, an interesting connection between our work and the pruning literature you pointed us to is the empirical evidence regarding the relative importance of different layers in the model. However, in contrast to our work the literature does not study what happens when removing an entire layer at a time.\n\n4-6: Thank you for pointing this out. We have fixed this in the paper.\n7. In Figure 2 constant values at 0.9 are due to the way learning rate schedule is updated (there's a jump at that point for all layers and they reach 0.9 in the same time). In Figure 3 the values are constant because MC-hardness is capped at 1.0 and none of the layers ever retrieve the full performance."}, "signatures": ["ICLR.cc/2019/Conference/Paper436/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609621, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xEtoRqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper436/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper436/Authors|ICLR.cc/2019/Conference/Paper436/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers", "ICLR.cc/2019/Conference/Paper436/Authors", "ICLR.cc/2019/Conference/Paper436/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609621}}}, {"id": "HJla2wojnm", "original": null, "number": 4, "cdate": 1541285812635, "ddate": null, "tcdate": 1541285812635, "tmdate": 1541533997511, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Review", "content": {"title": "Review: Problem motivation and analysis", "review": "This paper proposes and studies the \u201cmodel completion\u201d problem: given a trained network (and the data on which is was trained), if a subset of the network is reinitialized from scratch, how many retraining iterations are needed to achieve the original network accuracy (or some percentage of it)? For a variety of networks and problems in both supervised and reinforcement learning, model-completion (MC) hardness is quantified for individual network layers/sections. The experiments are the core of the paper and are generally well documented and seem reproducible.\n\nHowever, there are two issues that cloud the paper:\n\t1. The problem motivation (bounding the security of model splitting) is a bit odd. Has model splitting been proposed in the literature as a potential solution to shared model governance? Otherwise it feels like the problem setting was invented to justify the analysis in this paper: \u201cthe tail wagging the dog\u201d as the saying goes\u2026\n\t2. Model completion yet still be an interesting analytical tool for deep networks, but this requires a different evaluation. For instance, model completion provides a way to study how complicated different network layers are to learn or maybe to quantify how much of the inference task may be contained in each. (Though these concepts would need precise language and experimental evidence.) But how do these observations compare to other ways of obtaining similar observations? For instance, from the pruning literature, (Molchanov, 2017, ICLR, https://openreview.net/pdf?id=SJGCiw5gl) includes several figures detailing the statistics of individual network layers and how \u201cprunable\" are the filters in each.\n\nThis is largely an analytical paper, and I\u2019ll readily acknowledge that it is difficult to pull a clear and insightful study out of a jumble of experimental observations (and hard to review such a paper too). But the limitations of the problem motivation (point #1) and (in my opinion) the misaligned focus of the analysis (point #2), hurt the clarity and significance of this paper. For it to really be a useful tool in understanding deep learning, some additional work seems to be needed.\n\nOther notes:\n\t3. Pruning literature would be a reasonable comparison in the related work. For instance, (Han, ICLR, 2017, https://arxiv.org/abs/1607.04381) describes a dense-sparse-dense method where a (dense) model is pruned (sparse), after which the pruned connections are reinitialized and retrained (dense) leading to improved accuracy relative to the original dense model.\n\t4. Consider replacing the uncommonly used \u201cca.\u201d with \u201c~\u201d, e.g. \u201c~1000x\u201d instead of \u201cca. 1000x\u201d.\n\t5. The specifics about ImageNet in the intro to Section 3 should be moved to Section 4.\n\t6. In Section 3.2 paragraph 2, clarify if \u201closs\u201d refers to test loss as stated in the intro to Section 3.\n\t7. In Figure 2 (alpha=0.9) and Figure 3 (alpha=1.0, bottom), why are the values constant?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Review", "cdate": 1542234462026, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335723729, "tmdate": 1552335723729, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgkrCXq2Q", "original": null, "number": 1, "cdate": 1541189175222, "ddate": null, "tcdate": 1541189175222, "tmdate": 1541533997032, "tddate": null, "forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper436/Official_Review", "content": {"title": "An interesting new nugget of a problem", "review": "The authors introduce the problem of Model Completion (MC) to the machine learning community.  They provide a thorough review or related works, and convincingly argue that existing solutions to this sort of task (i.e., homomorphic encryption and multi-party computation) are not fully satisfactory in the domain of neural network learning.\n\nThe authors also provide extensive numerical experiments attempting to quantify their proposed measure of hardness-of-model-completion, MC-hardness_T(\\alpha) on a diverse set of Supervised and RL-related tasks, and they provide extensive analysis of those results.\n\nI find the paper to raise more questions than it answers (in a good way!).  The authors note that their measure depends strongly on the peculiarities of the particular (re)training scheme used.  Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?  \n\nMore broadly, there's an additional axis to the optimization problem which is \"How much does the training scheme know about the particulars of the problem?\", ranging from \"Literally has oracle access to the weights of the trained model (i.e., trivial, MC-hardness = 0 always)\" to \"knows what the architecture of the held-out-layer is and has been designed to optimize that particular network (see, e.g., learned optimizers)\" to \"knows a little bit about the problem structure, and uses hyperparameter tuned ADAM\" to \"knows nothing about the problem and picks a random* architecture to use for the held out weights, training it with SGD\".\n\nModel completion seems, morally (or at least from a security stand-point) slightly under-specified without being more careful about what information each player in this game has access to.  As it stands, it's an excellent *empirical* measure, and captures a very interesting problem, but I'd like to know how to make it even more theoretically grounded.\n\nAn excellent contribution, and I'm excited to see follow-up work.\n\n\n\n* We of course have tremendous inductive bias in how we go about designing architectures for neural networks, but hopefully you understand my point.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper436/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Scaling shared model governance via model splitting", "abstract": "Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion:  Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model\u2019s original performance?  We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent\u2019s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.  Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.", "keywords": ["deep learning", "reinforcement learning", "multi-party computation"], "authorids": ["miljanm@google.com", "leike@google.com", "atrask@google.com", "mtthss@google.com", "legg@google.com", "pushmeet@google.com"], "authors": ["Miljan Martic", "Jan Leike", "Andrew Trask", "Matteo Hessel", "Shane Legg", "Pushmeet Kohli"], "TL;DR": "We study empirically how hard it is to recover missing parts of trained models", "pdf": "/pdf/b4f9ffc6c76e2178857321ca25fbf5b15440ad4c.pdf", "paperhash": "martic|scaling_shared_model_governance_via_model_splitting", "_bibtex": "@misc{\nmartic2019scaling,\ntitle={Scaling shared model governance via model splitting},\nauthor={Miljan Martic and Jan Leike and Andrew Trask and Matteo Hessel and Shane Legg and Pushmeet Kohli},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xEtoRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper436/Official_Review", "cdate": 1542234462026, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xEtoRqtQ", "replyto": "H1xEtoRqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper436/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335723729, "tmdate": 1552335723729, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper436/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}