{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124469289, "tcdate": 1518458332399, "number": 181, "cdate": 1518458332399, "id": "rJNJIU1vf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJNJIU1vf", "signatures": ["~Jiaming_Song1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Multi-Agent Generative Adversarial Imitation Learning", "abstract": "We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competitive agents.", "paperhash": "song|multiagent_generative_adversarial_imitation_learning", "keywords": [], "_bibtex": "@misc{\n  song2018multi-agent,\n  title={Multi-Agent Generative Adversarial Imitation Learning},\n  author={Jiaming Song and Hongyu Ren and Dorsa Sadigh and Stefano Ermon},\n  year={2018},\n  url={https://openreview.net/forum?id=rJNJIU1vf}\n}", "authorids": ["jiaming.tsong@gmail.com", "rhy@pku.edu.cn", "dorsa@cs.stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "TL;DR": "We perform Inverse RL in general-sum Markov games with a new Multi-agent Actor Critic algorithm.", "pdf": "/pdf/4ec1895ca7ee132d1bb5a4c34247edda172012dc.pdf"}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1521603062224, "tcdate": 1521603062224, "number": 2, "cdate": 1521603062224, "id": "SkAxML19G", "invitation": "ICLR.cc/2018/Workshop/-/Paper181/Public_Comment", "forum": "rJNJIU1vf", "replyto": "S1FKCQQKz", "signatures": ["~Jiaming_Song1"], "readers": ["everyone"], "writers": ["~Jiaming_Song1"], "content": {"title": "Some Clarifications", "comment": "Thank you for your comment! To clarify some of your concerns:\n\n- One challenge to inverse reinforcement learning is the ill-defined nature of the problem. Even in single agent settings there are multiple solutions to the IRL problem (for example, set zero reward everywhere). This is even worse in the multi-agent case because there could be several Nash equilibria. If we do not make any assumptions to simplify the problem, it is highly possible to learn many rewards that explain the demonstrated behavior. If we merely learn a new set of policies from any set of rewards, we may land into another Nash equilibrium that do not reflect the expert policies."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Agent Generative Adversarial Imitation Learning", "abstract": "We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competitive agents.", "paperhash": "song|multiagent_generative_adversarial_imitation_learning", "keywords": [], "_bibtex": "@misc{\n  song2018multi-agent,\n  title={Multi-Agent Generative Adversarial Imitation Learning},\n  author={Jiaming Song and Hongyu Ren and Dorsa Sadigh and Stefano Ermon},\n  year={2018},\n  url={https://openreview.net/forum?id=rJNJIU1vf}\n}", "authorids": ["jiaming.tsong@gmail.com", "rhy@pku.edu.cn", "dorsa@cs.stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "TL;DR": "We perform Inverse RL in general-sum Markov games with a new Multi-agent Actor Critic algorithm.", "pdf": "/pdf/4ec1895ca7ee132d1bb5a4c34247edda172012dc.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625380, "id": "ICLR.cc/2018/Workshop/-/Paper181/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper181/Reviewers"], "reply": {"replyto": null, "forum": "rJNJIU1vf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625380}}}, {"tddate": null, "ddate": null, "tmdate": 1521602478550, "tcdate": 1521602478550, "number": 1, "cdate": 1521602478550, "id": "HyL2k8kcz", "invitation": "ICLR.cc/2018/Workshop/-/Paper181/Public_Comment", "forum": "rJNJIU1vf", "replyto": "Bydr7_FFG", "signatures": ["~Jiaming_Song1"], "readers": ["everyone"], "writers": ["~Jiaming_Song1"], "content": {"title": "Some Clarifications", "comment": "Thank you for your comment! We thought about most of your concerns as well, but chose to omit the details due to limited space. To clarify:\n\n- One interesting notion in the multi-agent case is that there might not be optimal policy, and we need to replace this notion with equilibrium concepts such as Nash equilibrium. For certain reward structures, multiple NE would exist; this would require us to rethink the notion of inverse reinforcement learning in the multi-agent settings. We did not mention this line of reasoning (and how we obtain MAGAIL from here) due to space constraints.\n\n- Our proposed multi-agent actor critic with K-FAC did not consider other agents as merely part of the environment. The critic considered the observations and actions of other agents, similar to MADDPG. We find that merely providing such external data is able to train efficiently as opposed to using/inferring other agent's policies as in MADDPG.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Agent Generative Adversarial Imitation Learning", "abstract": "We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competitive agents.", "paperhash": "song|multiagent_generative_adversarial_imitation_learning", "keywords": [], "_bibtex": "@misc{\n  song2018multi-agent,\n  title={Multi-Agent Generative Adversarial Imitation Learning},\n  author={Jiaming Song and Hongyu Ren and Dorsa Sadigh and Stefano Ermon},\n  year={2018},\n  url={https://openreview.net/forum?id=rJNJIU1vf}\n}", "authorids": ["jiaming.tsong@gmail.com", "rhy@pku.edu.cn", "dorsa@cs.stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "TL;DR": "We perform Inverse RL in general-sum Markov games with a new Multi-agent Actor Critic algorithm.", "pdf": "/pdf/4ec1895ca7ee132d1bb5a4c34247edda172012dc.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625380, "id": "ICLR.cc/2018/Workshop/-/Paper181/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper181/Reviewers"], "reply": {"replyto": null, "forum": "rJNJIU1vf", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625380}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582641349, "tcdate": 1520807553541, "number": 1, "cdate": 1520807553541, "id": "S1FKCQQKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper181/Official_Review", "forum": "rJNJIU1vf", "replyto": "rJNJIU1vf", "signatures": ["ICLR.cc/2018/Workshop/Paper181/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper181/AnonReviewer3"], "content": {"title": "Limited to obvious cases", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose to apply generative adversarial imitation learning to multi player Markov games. \n\nMy opinion is that the studied cases are the most simple cases in multi-agent problems. Rewards are either the same for every agents (collaborative) or opposite (zero-sum) or agents are totally independent. Therefore there is little challenge for imitation learning as in the two first cases, there is only one reward function to learn and in the third, each agent can be imitated independently from the others. \n\nAs the authors say, the computation of the advantage function assumes that other agents are just part of the environment which makes the problem much simpler. \n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Agent Generative Adversarial Imitation Learning", "abstract": "We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competitive agents.", "paperhash": "song|multiagent_generative_adversarial_imitation_learning", "keywords": [], "_bibtex": "@misc{\n  song2018multi-agent,\n  title={Multi-Agent Generative Adversarial Imitation Learning},\n  author={Jiaming Song and Hongyu Ren and Dorsa Sadigh and Stefano Ermon},\n  year={2018},\n  url={https://openreview.net/forum?id=rJNJIU1vf}\n}", "authorids": ["jiaming.tsong@gmail.com", "rhy@pku.edu.cn", "dorsa@cs.stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "TL;DR": "We perform Inverse RL in general-sum Markov games with a new Multi-agent Actor Critic algorithm.", "pdf": "/pdf/4ec1895ca7ee132d1bb5a4c34247edda172012dc.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582641167, "id": "ICLR.cc/2018/Workshop/-/Paper181/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper181/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper181/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper181/AnonReviewer1"], "reply": {"forum": "rJNJIU1vf", "replyto": "rJNJIU1vf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper181/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper181/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582641167}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582589960, "tcdate": 1521218368277, "number": 2, "cdate": 1521218368277, "id": "Bydr7_FFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper181/Official_Review", "forum": "rJNJIU1vf", "replyto": "rJNJIU1vf", "signatures": ["ICLR.cc/2018/Workshop/Paper181/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper181/AnonReviewer1"], "content": {"title": "Good paper overall, but seems like an incremental extension of the GAIL algorithm (Ho & Ermon, 2016)", "rating": "6: Marginally above acceptance threshold", "review": "The paper considers the problem of learning from demonstrations to act in a multi-agent Markov game. The proposed method is based on the GAIL algorithm, where a discriminator is trained to distinguish between expert trajectories and learned trajectories, which are generated by a generator network that tries to fool the discriminator. The proposed algorithm, named MAGAIL, extends GAIL to multi-agent systems. It seems like the extension consists in training all the agents centrally, and executing the learned policies in a decentralized fashion. Each agent considers the other agents as part of the environment while training. An advantage function is considered to tackle the high variance resulting from the non-stationary nature of the environment. Experiments are performed on several tasks, with varying levels of competition vs cooperation. The results indicate that the MAGAIL achieves a better performance than a simple Behavioral Cloning (BC).\nI am concerned about the technical contribution of this work. The proposed algorithms seems like a straightforward extension of the GAIL. The authors should study the implications of considering other agents as part of the environment, which could hinder the learning process. It is also strange to consider such an assumption in the zero-sum case. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Agent Generative Adversarial Imitation Learning", "abstract": "We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competitive agents.", "paperhash": "song|multiagent_generative_adversarial_imitation_learning", "keywords": [], "_bibtex": "@misc{\n  song2018multi-agent,\n  title={Multi-Agent Generative Adversarial Imitation Learning},\n  author={Jiaming Song and Hongyu Ren and Dorsa Sadigh and Stefano Ermon},\n  year={2018},\n  url={https://openreview.net/forum?id=rJNJIU1vf}\n}", "authorids": ["jiaming.tsong@gmail.com", "rhy@pku.edu.cn", "dorsa@cs.stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "TL;DR": "We perform Inverse RL in general-sum Markov games with a new Multi-agent Actor Critic algorithm.", "pdf": "/pdf/4ec1895ca7ee132d1bb5a4c34247edda172012dc.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582641167, "id": "ICLR.cc/2018/Workshop/-/Paper181/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper181/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper181/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper181/AnonReviewer1"], "reply": {"forum": "rJNJIU1vf", "replyto": "rJNJIU1vf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper181/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper181/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582641167}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573570133, "tcdate": 1521573570133, "number": 119, "cdate": 1521573569790, "id": "rk5TCARKf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJNJIU1vf", "replyto": "rJNJIU1vf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Agent Generative Adversarial Imitation Learning", "abstract": "We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competitive agents.", "paperhash": "song|multiagent_generative_adversarial_imitation_learning", "keywords": [], "_bibtex": "@misc{\n  song2018multi-agent,\n  title={Multi-Agent Generative Adversarial Imitation Learning},\n  author={Jiaming Song and Hongyu Ren and Dorsa Sadigh and Stefano Ermon},\n  year={2018},\n  url={https://openreview.net/forum?id=rJNJIU1vf}\n}", "authorids": ["jiaming.tsong@gmail.com", "rhy@pku.edu.cn", "dorsa@cs.stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Jiaming Song", "Hongyu Ren", "Dorsa Sadigh", "Stefano Ermon"], "TL;DR": "We perform Inverse RL in general-sum Markov games with a new Multi-agent Actor Critic algorithm.", "pdf": "/pdf/4ec1895ca7ee132d1bb5a4c34247edda172012dc.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 6}