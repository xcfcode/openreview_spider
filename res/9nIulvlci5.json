{"notes": [{"id": "9nIulvlci5", "original": "W3Kto-_Pzp", "number": 3347, "cdate": 1601308371305, "ddate": null, "tcdate": 1601308371305, "tmdate": 1614985770600, "tddate": null, "forum": "9nIulvlci5", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5mC9GK8CLXn", "original": null, "number": 1, "cdate": 1610040361846, "ddate": null, "tcdate": 1610040361846, "tmdate": 1610473952053, "tddate": null, "forum": "9nIulvlci5", "replyto": "9nIulvlci5", "invitation": "ICLR.cc/2021/Conference/Paper3347/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Techniques are introduced for improving representation learning\ncapabilities of neural networks, and the result is interpreted in\nterms of random projections.\n\nIn further discussion, even the reviewer with the highest grade said\nthat the paper does not yet have enough clarity to address the\nreviewers' comments. Particularly important would be to isolate the\ncausal impact of the proposed components in the final result. But also\nseveral technical details would need to be clarified including\ncomparing to simple l2 regularization and precise implications of Fig\n1.\n\nPositive aspects: The problem of learning representations and\ndecorrelation is of course important. The authors have imagination,\nand the authors are encouraged to improve the ideas by taking the\nreviewer feedback into account.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9nIulvlci5", "replyto": "9nIulvlci5", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040361830, "tmdate": 1610473952034, "id": "ICLR.cc/2021/Conference/Paper3347/-/Decision"}}}, {"id": "qP-Hjb9iFjq", "original": null, "number": 2, "cdate": 1603992279621, "ddate": null, "tcdate": 1603992279621, "tmdate": 1606759063809, "tddate": null, "forum": "9nIulvlci5", "replyto": "9nIulvlci5", "invitation": "ICLR.cc/2021/Conference/Paper3347/-/Official_Review", "content": {"title": "on the benefits of orthogonal weight matrices", "review": "The paper studies the usage of the representations developed in the last layer of a neural network as a way to measure the similarity between input patterns. The fundamental idea revolves around the concept of orthogonal weight matrices, to decorrelate the activations of the neurons, and which would definitely enrich the internal representations developed by the neurons in the last hidden layer. In the paper, it is suggested to regularize during training to maintain the orthogonality of the weight matrices. Moreover, a variant of Batch Normalization is proposed. The method proposed in the paper is then experimentally applied and evaluated on two benchmark datasets (MNIST and  Henan Renmin).\n\nPROs:\n- The study of intrinsic richness in neural network architectures related to the shape of the weight matrices, in this case orthogonality, is of great appeal.\n\nCONs:\n- In the experimental part I could not see (even in the appendix and in the code) any consideration made in relation to the hyper-parameters of the neural networks, and of the used learning algorithms. The choices made in the paper (and reported in the appendix), e.g. on the learning rate, on the number of epochs, on the regularizer coefficients, seems rather arbitrary and would be better selected on a validation set.\n- Being (at least in my view) the paper based on suggesting and using orthogonality as a design choice for the weight matrices in a neural network, the novelty content seems rather limited\n- Personally, I found the manuscript a bit hard to follow in some parts, limiting the overall readability. For example, the notation is first introduced in the appendix (and I suggest to introduce it at least before section 3.1). Moreover, I suggest to re-phrase some overstating sentences, e.g. on page 3 where it is reported that \" [...] there is usually a significant information loss on the last hidden layer, which makes it impossible to apply this representation as is. We have identified the main causes of this: a strong correlation of neurons and an insufficient size of the last hidden layer. [...]\". While the intuition is clear here, I could not see in the paper a clear evidence of the fact that the information loss in the last hidden layer is caused by the correlation of small-sized layers.\n\n-- EDIT: I am thankful to the authors for their insightful answer to my concerns, and for the though work in reviewing the manuscript. At this stage, I would keep my score (also in light of the other comments), but I hope that the authors find the suggestions from all the reviewers useful to re-present a more consolidated work soon.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3347/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9nIulvlci5", "replyto": "9nIulvlci5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077583, "tmdate": 1606915761728, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3347/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3347/-/Official_Review"}}}, {"id": "CImp5Utuq0v", "original": null, "number": 6, "cdate": 1606234612248, "ddate": null, "tcdate": 1606234612248, "tmdate": 1606234612248, "tddate": null, "forum": "9nIulvlci5", "replyto": "AXISW-U9RxY", "invitation": "ICLR.cc/2021/Conference/Paper3347/-/Official_Comment", "content": {"title": "Response to the reviewer", "comment": "Thank you very much for take a time to review our article.\n\nWe conducted much more experiments, tried to research how hyperparameters  influence on correlations.  Unfortunately we haven't more time to research this interesting question.\nWe suggest that orthogonal regularization influence more than dropout referring to statement 1 from our work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9nIulvlci5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3347/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3347/Authors|ICLR.cc/2021/Conference/Paper3347/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838541, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3347/-/Official_Comment"}}}, {"id": "dE64I-6G7Q-", "original": null, "number": 5, "cdate": 1606233587947, "ddate": null, "tcdate": 1606233587947, "tmdate": 1606233875731, "tddate": null, "forum": "9nIulvlci5", "replyto": "qP-Hjb9iFjq", "invitation": "ICLR.cc/2021/Conference/Paper3347/-/Official_Comment", "content": {"title": "Response", "comment": ">>In the experimental part I could not see (even in the appendix and in the code) any consideration made in relation to the hyper-parameters of the neural networks, and of the used learning algorithms. The choices made in the paper (and reported in the appendix), e.g. on the learning rate, on the number of epochs, on the regularizer coefficients, seems rather arbitrary and would be better selected on a validation set.\n\n \n\nThank you for your comment. We tried to improve our work and expanded our experiments, in particular we made a grid of hyperparameters and set early stopping for epochs. Our experiments continue to show that our approach performs better than other models.\n>>Being (at least in my view) the paper based on suggesting and using orthogonality as a design choice for the weight matrices in a neural network, the novelty content seems rather limited\n\n \n\nThe use of orthogonal matrices is not a suggestion for solving the problem, but a consequence of allowing to reduce the correlation of output neurons for fully connected layers.\n\n \n\n>>I suggest to re-phrase some overstating sentences, e.g. on page 3 where it is reported that \" [...] there is usually a significant information loss on the last hidden layer, which makes it impossible to apply this representation as is. We have identified the main causes of this: a strong correlation of neurons and an insufficient size of the last hidden layer. [...]\". While the intuition is clear here, I could not see in the paper a clear evidence of the fact that the information loss in the last hidden layer is caused by the correlation of small-sized layers.\n\n \n\nThanks. We have rephrased this passage and added an experiment that shows that the size of the last hidden layer and the correlation of neurons affect the quality of the representation."}, "signatures": ["ICLR.cc/2021/Conference/Paper3347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9nIulvlci5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3347/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3347/Authors|ICLR.cc/2021/Conference/Paper3347/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838541, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3347/-/Official_Comment"}}}, {"id": "zQuphEW0kwT", "original": null, "number": 3, "cdate": 1605542227899, "ddate": null, "tcdate": 1605542227899, "tmdate": 1605542227899, "tddate": null, "forum": "9nIulvlci5", "replyto": "PBBk9cCtxFR", "invitation": "ICLR.cc/2021/Conference/Paper3347/-/Official_Comment", "content": {"title": "Some questions to the reviewer", "comment": "Thank you very much for the reviewing our paper. We will be glad to discuss all of unclear moments. \n\n1. \"Is the paper trying...?\"\nIn this work, we are interested in studying the issue of implicit learning of representations for obtaining hidden classes. Can the neural networks store information about subcategories if we don\u2019t explicitly train them to do this?\n    \nA solution to this problem could allow us to use only one model for different tasks without fine-tuning, which significantly reduces time for developing and supporting of several models. \n\n2. You said that why bother using neural networks for comparing similarity between datasets?\n    \n    However, in our work, we don't compare similarity between datasets. We have the same source and target domains but different tasks and we don\u2019t have labeled data in the target domain.\n    \n    Could you explain please, what did you mean?\n    \n3. About the example with pre-trained VGGNet and ResNet34 on ImageNet dataset.\n    \n    You said, \"... 1000 512-D vectors of ResNet34 are correlated with each other\" and \"... 1000 4096-D vectors of VGGNet are orthogonal\". \n    \n    Could you please clarify which vectors you are considering?\n    \n    And in this regard, we don't understand how the vectors of weight matrix might be correlated and how you compare such properties as correlation and orthogonal together. Furthermore, in this work, we operate with the output of the last hidden layer.\n\n    Could you explain please, what did you mean? Unfortunately, at the moment we don't understand how your argument is related with our work.\n    \n4. About $l_2$ -regularization.\n    \n    First of all, considering the solution of ridge regression $w = (X^TX +  aI)^{-1}X^Ty$ we see that the adding of $l_2$-regularization doesn't moves covariance matrix to identity matrix. It is needed to obtain not singular matrix. And, consequentely, this allow us to compute the inverse matrix. The second, in model A only $l_2$-regularization is used and how can be seen from Fig.3 and Fig.5 the features are highly correlated. Your statement contradicts our experimental data.\n    \n5. \"The example presented in Fig 1. is not a valid...\"\n    \n    You said that from classification perspective, one can say that, in Fig. 1a, the model has observed enough amount of information so that the uncertainty of the two neurons' behaviours is drastically reduced, while Fig. 2b still presents a large amount of uncertainty.\n    \n    However, in our work we say a similar thing (\"... This phenomenon may occur due to the fact that the neural network does not retain more information than is necessary to solve a particular task\", page 3). And example in Fig. 1 shows that, a model whose neurons are correlated (Fig 1a) only retain the information for classification. The model (Fig. 1b) can not only solve the same problem, but also distinguish between input objects within the same class. This follows from the fact that we have retained more uncertainty, and therefore we get more information from observation.\n    \n    We agree with your statement, but it is not entirely clear to us why it contradicts our statements.\n    \n6. \"The tanh activation function is already bounded...\"\n    \n    In our work we don't rely on bounded mean and variance. We require the certain properties that are mentioned on the page 5 in the paragraph \"Providing the necessary moments of neurons after activation\".\n    \n    Could you please clarify your point of view?\n    \n7. About the experiments on the MNIST dataset.\n    \n    How it was mentioned above, our goal is to obtain representations by implicit approach.\n    We believe that comparisons should be made within methods that are not explicitly trained to represent data.\n\nAt this point, several of your arguments remain unclear to us. We would be glad to continue discussing our work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3347/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9nIulvlci5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3347/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3347/Authors|ICLR.cc/2021/Conference/Paper3347/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838541, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3347/-/Official_Comment"}}}, {"id": "AXISW-U9RxY", "original": null, "number": 1, "cdate": 1603775467076, "ddate": null, "tcdate": 1603775467076, "tmdate": 1605024018304, "tddate": null, "forum": "9nIulvlci5", "replyto": "9nIulvlci5", "invitation": "ICLR.cc/2021/Conference/Paper3347/-/Official_Review", "content": {"title": "Neural decorrelation for dimensionality reduction?", "review": "I love the problem of neural information decorrelation which is so widespread in neural systems precisely for the reasons that the authors point out to detect novel information.\nDespite the paper is nicely written, it\u2019s perhaps not best organized, because I had to go back and forth to find out the main author\u2019s contributions. I think the main contribution of the authors is combining random projection initialization, training with orthonormality regularization (as proposed by others) and drop out. The authors also claim that they can reduce the dimensionality by applying Lemma 1.\n\nThe results in Fig. 3 illustrate how the model achieves decent to batch normalization but with a flatter distribution of correlations, which I agree with the authors is the goal.\n\nThe paper could benefit from a small summary at the introduction that lists with clarity the components of the model in a succinct manner. \n\nThe three models compared should be explained better in the experimental section. So, people can read the paper faster.\n\nI\u2019d be interesting to know what component on the model is more important and for what. Is it the regularization, is it the random projection initialization used for faster convergence and information compression? Is it the dropout necessary to ensure a flatter distribution of correlations? An experimental section illustrating the impact on the model performance with each of these components would help us learn more.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3347/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9nIulvlci5", "replyto": "9nIulvlci5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077583, "tmdate": 1606915761728, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3347/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3347/-/Official_Review"}}}, {"id": "PBBk9cCtxFR", "original": null, "number": 3, "cdate": 1604701508265, "ddate": null, "tcdate": 1604701508265, "tmdate": 1605024017811, "tddate": null, "forum": "9nIulvlci5", "replyto": "9nIulvlci5", "invitation": "ICLR.cc/2021/Conference/Paper3347/-/Official_Review", "content": {"title": "too many handwavy claims...", "review": "The paper proposed two methods for reducing the correlation of neurons within a layer so that the effective dimension of the last layer can be reduced as well, and the two methods include enforcing orthogonality on weight matrices, and batchnorm after activation functions, There are many issues regarding the claims and the experiments presented in the paper. \n\n1. Is the paper trying to propose a novel way of measuring the similarity between inputs or to improve the vector representations generated from the last layer to be more reflective and indicative in terms of the fine-grained structure of the data? Those two topics seemed intertwined with each other, and also it seemed that the authors are using one as the supporting argument for the other. \n\nFor the first one, the authors claimed that the reasons for the information loss from bottom to the top layer in a neural networks include (1) correlation of neurons, and (2) insufficient width of the last hidden layer. \n\nLet's assume that both reasons are valid, then why bother using neural networks for comparing similarity between datasets? One can certainly consider kernel methods for comparison, and specifically kernel two-sample tests with characteristic or universal kernels. \n\nIMO, I don't agree with the reasons listed by the authors. In fact, let's take VGGNet and ResNet34 pretrained on ImageNet for comparison. The dimension of the top layer of VGGNet is 4096, while it is 512 for ResNet34, and as known, ImageNet classification task has 1000 different labels. It means that those 1000 512-D vectors learnt in the top layer in ResNet34 are correlated with each other to some degree, while those 1000 4096-D vectors from VGGNet could be orthogonal to each other. However, as shown, ResNet34 generalises better than VGGNet not only on ImageNet tasks, but also on other vision-related tasks. In this comparison, clearly the architecture matters more than the dimension of the last layer and the correlation of neurons.\n\nFor the second one, let's consider a linear system, ridge regression in particular. By adding a certain level of l2 regularisation on the parameter vector or matrix, it effectively moves the covariance matrix of the input data close to an identity matrix, which improves the orthogonality of the covariance matrix. It seems that adding l2 regularisation is a simpler way than ones proposed in this paper to reduce the correlation of neurons. \n\n2. The example presented in Fig 1. is not a valid piece of evidence for the claims made in this paper.\n\nFrom the perspective of information theory, the definition of information is the thing/observation that reduces the uncertainty. It heavily depends on the subject of the stufy. Therefore, from classification perspective, one can say that, in Fig. 1a, the model has observed enough amount of information so that the uncertainty of the two neurons' behaviours is drastically reduced, while Fig. 2b still presents a large amount of uncertainty. The paper referred to the term 'information' many times without consolidating specific occasions. \n\n3. The activation function applied in the experiments is tanh, which squashes values to be in between -1 and 1, and the authors proposed to apply BatchNorm after tanh activation function. I don't see how the batchnorm after tanh is helpful in anyways as both mean and variance of the neurons are bounded already.\n\n4. The comparison conducted on the MNIST dataset seemed missing some fundimental vectorisation and quantisation methods, including both learning-based or non-learning-based. If the goal was to find a low-D vector representation of data, one can consider many approaches without using neural networks. This comment goes back to my first point in a way that I am not super clear about what the paper is trying to solve. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3347/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3347/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Random Projection: From the Initial Task To the Input Similarity Problem", "authorids": ["~Alan_Savushkin1", "nikita.benkovich@kaspersky.com", "dmitry.s.golubev@kaspersky.com"], "authors": ["Alan Savushkin", "Nikita Benkovich", "Dmitry Golubev"], "keywords": [], "abstract": "The data representation plays an important role in evaluating similarity between objects. In this paper, we propose a novel approach for implicit data representation to evaluate similarity of input data using a trained neural network. In contrast to the previous approach, which uses gradients for representation, we utilize only the outputs from the last hidden layer of a neural network and do not use a backward step. The proposed technique explicitly takes into account the initial task and significantly reduces the size of the vector representation, as well as the computation time. Generally, a neural network obtains representations related only to the problem being solved, which makes the last hidden layer representation useless for input similarity task.\nIn this paper, we consider two reasons for the decline in the quality of representations: correlation between neurons and insufficient size of the last hidden layer. To reduce the correlation between neurons we use orthogonal weight initialization for each layer and modify the loss function to ensure orthogonality of the weights during training. Moreover, we show that activation functions can potentially increase correlation. To solve this problem, we apply modified Batch-Normalization with Dropout. Using orthogonal weight matrices allow us to consider such neural networks as an application of the Random Projection method and get a lower bound estimate for the size of the last hidden layer. We perform experiments on MNIST and physical examination datasets. In both experiments, initially, we split a set of labels into two disjoint subsets to train a neural network for binary classification problem, and then use this model to measure similarity between input data and define hidden classes. We also cluster the inputs to evaluate how well objects from the same hidden class are grouped together. Our experimental results show that the proposed approach achieves competitive results on the input similarity task while reducing both computation time and the size of the input representation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "savushkin|neural_random_projection_from_the_initial_task_to_the_input_similarity_problem", "one-sentence_summary": "A neural network from the Random Projection perspective", "supplementary_material": "/attachment/c6a5e905d0153c85b325170778c05234d8b5a600.zip", "pdf": "/pdf/8c462fcb2dd146516c4c9c7cb36b2267988cbee1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pjx7Q8EiBu", "_bibtex": "@misc{\nsavushkin2021neural,\ntitle={Neural Random Projection: From the Initial Task To the Input Similarity Problem},\nauthor={Alan Savushkin and Nikita Benkovich and Dmitry Golubev},\nyear={2021},\nurl={https://openreview.net/forum?id=9nIulvlci5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9nIulvlci5", "replyto": "9nIulvlci5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3347/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077583, "tmdate": 1606915761728, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3347/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3347/-/Official_Review"}}}], "count": 8}