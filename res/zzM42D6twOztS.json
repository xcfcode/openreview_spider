{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392658680000, "tcdate": 1392658680000, "number": 4, "id": "FFW7YqOZd2FC0", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zzM42D6twOztS", "replyto": "zzM42D6twOztS", "signatures": ["Mathias Berglund"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revised version of the paper has now been published. Thank you for all the helpful comments.\r\n\r\nAs an additional comment, please note that we are not measuring the variance of the average of the estimates obtained with M independent chains (i.e. we use a minibatch size of 1), since the variance of estimates obtained with averaging (i.e. using a minibatch size of M>1) is easy to compute from the case of a minibatch size of 1, given that the different estimates are independently sampled."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence", "decision": "submitted, no decision", "abstract": "Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different variances for stochastic gradient estimates of individual samples. In this paper we show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the sum of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the finding that CD can be used with smaller minibatches or higher learning rates than PCD.", "pdf": "https://arxiv.org/abs/1312.6002", "paperhash": "berglund|stochastic_gradient_estimate_variance_in_contrastive_divergence_and_persistent_contrastive_divergence", "keywords": [], "conflicts": [], "authors": ["Mathias Berglund", "Tapani Raiko"], "authorids": ["m.p.e.berglund@gmail.com", "tapani.raiko@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392068280000, "tcdate": 1392068280000, "number": 3, "id": "zzq5dAvF5ndg4", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zzM42D6twOztS", "replyto": "zzM42D6twOztS", "signatures": ["Mathias Berglund"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Reply to both reviewers:\r\n\r\nThank you for the extensive and helpful comments. As the bias of CD is quite well documented while the variance of PCD vs. CD is less so, the paper intentionally does not focus on the bias. We should however make this more clear in the introduction.\r\n\r\nAlthough it would be interesting to study the bias / variance trade-off in all the training settings in the paper, we still saw that there was value in documenting the variance also stand-alone in order to give sense of the magnitude of the differences in variance between PCD and CD. We would therefore still argue that documenting only the variance has value, although we agree that it would be meaningful to explore the bias/variance trade-off in a more extended discussion on the topic.\r\n\r\nWe will submit a revised version of the paper based on the comments as soon as possible.\r\n\r\n\r\nReply to Anonymous 9c34:\r\n\r\nThank you for the references, we should mention iterate averaging as a method to alleviate the high variance for PCD. Thank you also for the second PCD reference.\r\n\r\n\r\nReply to Anonymous 11c9:\r\n\r\nThank you for the comments. We fear that the reviewer 'Anonymous 11c9' is dubious about the experimental setting in Figure 2 due to a misunderstanding, but we hope to clear things up in our response (see below), and we hope to make that part much clearer in the next revision.\r\n\r\nRegarding the request for clarification, the Figures 1 and 2 show quite different results, which we realize can be misleading and gives an impression of a large asymmetry between CD and PCD. In Figure 1, the x-axis depicts the number of CD steps for the *same* data point, where only the number of steps in the CD sampling increases. Therefore, following one of the lines in Figure 1 along the x-axis, we are comparing the same update starting from the same data point, but with a longer and longer chain for the negative phase. The figure is hence what would be expected from a typical figure comparing different values of k for CD-k.\r\n\r\nHowever, in Figure 2, we are looking at the variance of the *mean* (or sum, see below) of subsequent estimates. Differing from Figure 1, in Figure 2 we are summing up subsequent estimates along the x-axis, which means that the further we go along the x-axis, the more estimates we have included. The reason for doing so is that the high variance of PCD is hypothesized to stem from subsequent negative phase estimates to be dependent.\r\n\r\nPlease also note that in Figure 2, the PCD variance is divided by the variance of the sum of k estimates using \u201cexact\u201d sampling \u2013 which means that the figure is identical to taking the mean of subsequent gradient estimates and compare them to the mean of estimates with \u201cexact\u201d sampling.  Therefore, for a chain that mixes well, this relative variance should not increase with summing more steps during training. This we can also see in Figure 2, where the variance for MNIST and CIFAR in the beginning of the training are very close to \u201cexact\u201d sampling when summing 1-20 subsequent steps (the horizontal lines in Figure 2). However, we realize that the text would be  clearer if we used the word mean instead of sum, and we will revise the text accordingly. The pseudocode for Figure 2 is presented below.\r\n\r\nRegarding the baseline, we ran M >> 1 independent chains for 1000 steps, i.e. exactly aimed at running a large number of independent chains until convergence. Although we have not tried to validate whether 1000 steps is enough, we have simply assumed that 1000 steps is enough for approximate convergence.\r\n\r\nRegarding evaluating PCD variance on a model trained via CD we agree that the most reliable results would be obtained if we trained the model with e.g. enhanced gradient and parallel tempering instead of CD.\r\n\r\nRegarding the I-CD experiments, the 10 gradient estimates were run by initializing the Markov chain from a random training sample (which was different in all the 10 runs), but also different from the training sample used for the positive phase. Although we agree that the result of higher I-CD variance compared to CD is trivial, we still found the magnitude of variance of I-CD relevant to display. If for instance the variance of I-CD was very similar to that of CD (but much less than the \u201cexact\u201d estimate), the low variance of CD could be explained by the fact that we run the chain very few steps from *any* data point. However, we agree that the text should be changed to state this more clearly.\r\n\r\nThank you also for the clarity comments, as you assumed, they were both indeed errors in the text.\r\n\r\n\r\nPseudocode for Figure 2:\r\n\r\ndo for each data point in data set {\r\n    use the data point for positive phase\r\n    run negative particle sampling for 1000 steps from random data point\r\n    initialize gradient_sum to zero\r\n       \r\n    do 20 times {\r\n        calculate gradient estimate using current positive and negative particle\r\n        add gradient estimate to gradient_sum\r\n        store sufficient statistics of the gradient_sum\r\n        pick new random data point for positive phase\r\n        run the negative particle chain one step forward (independent of positive phase)\r\n        }\r\n}\r\n\r\ndo for each data point in data set {\r\n    use the data point for positive phase\r\n    run negative particle sampling for 1000 steps from random data point\r\n    initialize gradient_sum_exact to zero\r\n       \r\n    do 20 times {\r\n        calculate gradient estimate using current positive and negative particle\r\n        add gradient estimate to gradient_sum_exact\r\n        store sufficient statistics of the gradient_sum_exact\r\n        pick new random data point for positive phase\r\n        run negative particle sampling for 1000 steps from random data point\r\n        }\r\n}\r\n\r\ncompute the sum of componentwise variances from the statistics of the gradient_sum for each of the 20 steps separately\r\ncompute the sum of componentwise variances from the statistics of the gradient_sum_exact for each of the 20 steps separately\r\ndivide the first sum above with the second sum above for each of the 20 steps separately"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence", "decision": "submitted, no decision", "abstract": "Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different variances for stochastic gradient estimates of individual samples. In this paper we show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the sum of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the finding that CD can be used with smaller minibatches or higher learning rates than PCD.", "pdf": "https://arxiv.org/abs/1312.6002", "paperhash": "berglund|stochastic_gradient_estimate_variance_in_contrastive_divergence_and_persistent_contrastive_divergence", "keywords": [], "conflicts": [], "authors": ["Mathias Berglund", "Tapani Raiko"], "authorids": ["m.p.e.berglund@gmail.com", "tapani.raiko@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391972400000, "tcdate": 1391972400000, "number": 2, "id": "FsLVFk86XIY5D", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zzM42D6twOztS", "replyto": "zzM42D6twOztS", "signatures": ["anonymous reviewer 11c9"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence", "review": "This paper provides an empirical evaluation of the variance of maximum likelihood gradient estimators for RBMs, comparing Contrastive Divergence to Persistent CD. The results confirm a well known belief that PCD suffers from higher-variance, than the biased CD gradient. While the result may not be surprising, I believe the authors are correct in stating that the issue had not properly been investigated in the literature. It is unfortunate however that the authors avoid the much more important question of trade-off between bias and variance. Before making a final judgement on the paper however, I would ask that the authors clarify the following potential major issue. Other more general feedback for improving the paper follows.\r\n\r\n\r\nRequest for Clarification:\r\n\r\nWhy the asymmetry between the estimation of CD-k vs PCD-k gradients ? The use of PCD-k is highly unusual. If the goal was to study variance as a function of the ergodicity of a single Markov chain, then PCD-k gradients should have been computed with a single training example (for the positive phase) and computing the negative phase gradient by *averaging* over the k-steps of the negative phase chain.\r\n\r\nCould the authors clarify (through pseudocode) how the gradients and their variance are computed for the experiments of Figure 2?\r\n\r\nDue to the loss of ergodicity of the Markov chain, the effective number of samples used to estimate the model expectation should indeed be larger at 10 epochs, than at 500 epochs. It is thus predictable that variance of the gradient estimates would increase during training. However, this is for a fixed value of k. I find very strange that the variance would *increase* with k (at a fixed point of training). I am left wondering if this is an artefact of the experimental protocol: the authors seem to be computing the variance of the *sum* of k-gradient estimates. This quantity will indeed grow with k, and will do so linearly if the estimates at each k are assumed to be independent. The linearity of the curves in Fig.2 gives some weight to this hypothesis.\r\n\r\n\r\nOther general feedback:\r\n\r\n* One area of concern is that the paper evaluates PCD in a regime which is not commonly used in practice: i.e.  estimating the negative phase expectation via the correlated samples of a single Markov chain. I worry that some readers may conclude that PCD is not viable, due to its excessively large variance. For this reason, I think the paper would benefit from repeating the experiments but averaging over M independent chains.\r\n\r\n* A perhaps more appropriate baseline, would be to run M >> 1 independent Markov chains to convergence and average the resulting gradient estimates. This might not change much, but the above would yield a better estimate of the ML gradient than CD-1000.\r\n\r\n* Evaluating the variance of PCD gradients on a model trained via CD may be problematic. The mixing issues of PCD can be exacerbated when run on a CD-trained model, where the energy has only been fit locally around training data (Desjardins, 2010). While I do not expect the conclusions to change, I would be interested in seeing the same results on a PCD-k trained model.\r\n\r\n* RE: I-CD experiments. 'This supports the hypothesis that the low variance of CD [stems from] the negative particle [being] sampled from the positive particle, and not from that the negative particle is sampled only a limited number of steps from an arbitrary data point'.\r\n\r\nI am not sure that the experiment allows you to draw this conclusion. When computing the 10 gradient estimates (for each training example) did you initialize the Markov chain from a random (but fixed throughout the 10 gradient evaluations) training example ? Otherwise, I believe the conclusion is rather uninteresting and doesn't shed light on the 'importance' of initializing the negative chain from the positive phase training data.  In CD-training, the only variance stems from the trajectory taken by the (short) Markov chain from a fixed starting point. In I-CD, there are two sources of variance: (1) the trajectory of the chain, and (2) the starting point of the chain. If the chain is initialized randomly for the 10 gradient evaluations, then this will undoubtedly increase the variance of the estimator (but with lower bias).\r\n\r\n\r\nClarity:\r\n\r\n* In I-CD, the 'negative particle is sampled from a random positive particle' ? I would make explicit that you initialize the chain of I-CD from a random training example. In Section 4, 'arbitrary data point' left me wondering if you were instead initializing the chain from an independent pseudo-sample of the model (using i.e. a uniform distribution or a factorial approximation to p(v)).\r\n\r\n* 'Conversely, the variance of the mean of subsequent variance estimates using PCD is significantly higher' ? Did the authors mean 'the variance of the mean of subsequent gradient estimates' ? Otherwise, please consider rephrasing."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence", "decision": "submitted, no decision", "abstract": "Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different variances for stochastic gradient estimates of individual samples. In this paper we show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the sum of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the finding that CD can be used with smaller minibatches or higher learning rates than PCD.", "pdf": "https://arxiv.org/abs/1312.6002", "paperhash": "berglund|stochastic_gradient_estimate_variance_in_contrastive_divergence_and_persistent_contrastive_divergence", "keywords": [], "conflicts": [], "authors": ["Mathias Berglund", "Tapani Raiko"], "authorids": ["m.p.e.berglund@gmail.com", "tapani.raiko@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391848860000, "tcdate": 1391848860000, "number": 1, "id": "adiPdjpKvR56T", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zzM42D6twOztS", "replyto": "zzM42D6twOztS", "signatures": ["anonymous reviewer 9c34"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence", "review": "This paper presents an empirical study of the variance in gradient estimates between contrastive divergence (CD) and persistent contrastive divergence (PCD). It is well known that PCD tends to be less stable than CD, requiring a larger learning rate and larger mini-batch sizes. The paper does a fairly good job of empirically verifying this phenomenon on several image datasets, and most of the results are consistent with expectations. The observation that the variance increases toward the end of learning is an interesting and not entirely obvious finding.\r\n\r\nOne issue though is that the paper seems to miss a crucial part of the story: CD learning enjoys a low variance at the cost of an increase in bias. It is easy to construct a gradient estimate that exhibits zero variance, however practically speaking this would not be very useful. What is more interesting is the trade-off between bias and variance. For example, PCD exhibits significant variance on the silhouettes dataset. Does this mean that it requires an impractically small learning rate?\r\n\r\nIt has been shown in the past that the technique of iterate averaging can be used to remove much of the variance in PCD learning, but that it does not work nearly as well when applied to CD [1]. The fact that PCD is asymptotically unbiased, but exhibits high variance compared to CD supports these results.\r\n \r\n[2] should be cited for PCD as well.\r\n\r\nReferences:\r\n[1] Kevin Swersky,  Bo Chen, Benjamin Marlin, and Nando de Freitas, \u201cA Tutorial on Stochastic Approximation Algorithms for Training Restricted Boltzmann Machines and Deep Belief Nets,\u201d Information Theory and Applications Workshop, 2010.\r\n\r\n[2] Laurent Younes, \u201cParametric inference for imperfectly observed Gibbsian \ufb01elds,\u201d Probability Theory and Related Fields, vol. 82, no. 4, pp. 625\u2013645, 1989."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence", "decision": "submitted, no decision", "abstract": "Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different variances for stochastic gradient estimates of individual samples. In this paper we show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the sum of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the finding that CD can be used with smaller minibatches or higher learning rates than PCD.", "pdf": "https://arxiv.org/abs/1312.6002", "paperhash": "berglund|stochastic_gradient_estimate_variance_in_contrastive_divergence_and_persistent_contrastive_divergence", "keywords": [], "conflicts": [], "authors": ["Mathias Berglund", "Tapani Raiko"], "authorids": ["m.p.e.berglund@gmail.com", "tapani.raiko@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387822020000, "tcdate": 1387822020000, "number": 11, "id": "zzM42D6twOztS", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "zzM42D6twOztS", "signatures": ["m.p.e.berglund@gmail.com"], "readers": ["everyone"], "content": {"title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence", "decision": "submitted, no decision", "abstract": "Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different variances for stochastic gradient estimates of individual samples. In this paper we show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the sum of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the finding that CD can be used with smaller minibatches or higher learning rates than PCD.", "pdf": "https://arxiv.org/abs/1312.6002", "paperhash": "berglund|stochastic_gradient_estimate_variance_in_contrastive_divergence_and_persistent_contrastive_divergence", "keywords": [], "conflicts": [], "authors": ["Mathias Berglund", "Tapani Raiko"], "authorids": ["m.p.e.berglund@gmail.com", "tapani.raiko@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 5}