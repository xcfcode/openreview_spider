{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730183845, "tcdate": 1509083102367, "number": 257, "cdate": 1518730183836, "id": "SJLy_SxC-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SJLy_SxC-", "original": "SJIkOBeRb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Log-DenseNet: How to Sparsify a DenseNet", "abstract": "Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, \\logdenses are easier to scale than DenseNets, and no longer require careful GPU memory management. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.", "pdf": "/pdf/fd11ad29bc2400441cde591b9bbd502b8ef7c86b.pdf", "TL;DR": "We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.", "paperhash": "hu|logdensenet_how_to_sparsify_a_densenet", "_bibtex": "@misc{\nhu2018logdensenet,\ntitle={Log-DenseNet: How to Sparsify a DenseNet},\nauthor={Hanzhang Hu and Debadeepta Dey and Allie Del Giorno and Martial Hebert and J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJLy_SxC-},\n}", "keywords": ["DenseNet", "sparse shortcut connections", "network architecture", "scene parsing", "image classification"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Allie Del Giorno", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "adelgior@ri.cmu.edu", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260084283, "tcdate": 1517249916183, "number": 610, "cdate": 1517249916161, "id": "BkVKH1arf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "SJLy_SxC-", "replyto": "SJLy_SxC-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper presents an empirical study into sparse connectivity patterns for DenseNets.\n\nWhilst sparse connectivity is potentially interesting, the paper does not make a strong argument for such sparse connectivity patterns: in particular, the results on ImageNet suggest that sparse connectivity performs substantially worse than full connectivity (at the same FLOPS-level, Log-DenseNet obtains ~2.5% lower accuracy than baseline DenseNet models, and the best Log-DenseNet is ~4% worse than the best DenseNet). On CamVid, both network architectures appear to perform on par.\n\nThe paper motivates the model architecture by the high memory consumption of DenseNets but, frankly, that is a very weak motivation: DenseNets are actually very memory-efficient if implemented correctly (https://arxiv.org/pdf/1707.06990.pdf). The fact that such implementations are not well-supported by TensorFlow/PyTorch is a shortcoming of those deep-learning frameworks, not in DenseNets. (In fact, the memory management features that deep-learning frameworks have implemented to make residual networks memory-efficient (for instance, caching GPU memory allocation in PyTorch) are far more complex than the \"thousand lines of C++\" currently needed to implement a DenseNet correctly.) Such issues will likely be resolved relatively soon by better implementations, and are hardly a good motivation for a different network architecture."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log-DenseNet: How to Sparsify a DenseNet", "abstract": "Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, \\logdenses are easier to scale than DenseNets, and no longer require careful GPU memory management. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.", "pdf": "/pdf/fd11ad29bc2400441cde591b9bbd502b8ef7c86b.pdf", "TL;DR": "We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.", "paperhash": "hu|logdensenet_how_to_sparsify_a_densenet", "_bibtex": "@misc{\nhu2018logdensenet,\ntitle={Log-DenseNet: How to Sparsify a DenseNet},\nauthor={Hanzhang Hu and Debadeepta Dey and Allie Del Giorno and Martial Hebert and J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJLy_SxC-},\n}", "keywords": ["DenseNet", "sparse shortcut connections", "network architecture", "scene parsing", "image classification"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Allie Del Giorno", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "adelgior@ri.cmu.edu", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642421010, "tcdate": 1511709301421, "number": 1, "cdate": 1511709301421, "id": "BJaucU_gM", "invitation": "ICLR.cc/2018/Conference/-/Paper257/Official_Review", "forum": "SJLy_SxC-", "replyto": "SJLy_SxC-", "signatures": ["ICLR.cc/2018/Conference/Paper257/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "This paper investigates how to impose layer-wise connections in DenseNets most efficiently. The authors propose a connection-pattern, which connects layer i to layer i-2^k, k=0,1,2... The authors also propose maximum backpropgation distance (MBD) for measuring the fluency of gradient flow in the network, and justify the Log-DenseNet's advantage in this framework. Empirically, the author demonstrates the effectiveness of Log-DenseNet by comparing it with two other intuitive connection patterns on CIFAR datasets. Log-DenseNet also improves on FC-DenseNet, where the connection budget is the bottleneck because the feature maps are of high resolutions.\n\n\nStrengths:\n1. Generally, DenseNet is memory-hungry if the connection is dense, and it is worth studying how to sparsify a DenseNet. By showing the improvements on FC-DenseNet, Log-DenseNet demonstrates good potential on tasks which require upsampling of feature maps. \n2. The ablation experiments are well-designed and the visualizations of connectivity pattern are clear.\n\nWeakness:\n1. Adding a comparison with Log-DenseNet and vanilla DenseNet in the Table 2 experiment would make the paper stronger. Also, the NearestHalfAndLog pattern is not used in any latter visual recognition experiments, so I think it's better to just compare LogDenseNet with the two baselines instead. Despite there are CIFAR experiments on Log-DenseNet in latter sections, including results here would be easier to follow.\n2. I would like to see the a comparison with the DenseNet-BC in the segmentation and CIFAR classification tasks, which uses 1x1 conv layers to reduce the number of channels. It should be interesting to study whether it is possible to further sparsify DenseNet-BC, as it has much higher efficiency.\n3. The improvement of efficiency on classifications task is not that significant.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log-DenseNet: How to Sparsify a DenseNet", "abstract": "Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, \\logdenses are easier to scale than DenseNets, and no longer require careful GPU memory management. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.", "pdf": "/pdf/fd11ad29bc2400441cde591b9bbd502b8ef7c86b.pdf", "TL;DR": "We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.", "paperhash": "hu|logdensenet_how_to_sparsify_a_densenet", "_bibtex": "@misc{\nhu2018logdensenet,\ntitle={Log-DenseNet: How to Sparsify a DenseNet},\nauthor={Hanzhang Hu and Debadeepta Dey and Allie Del Giorno and Martial Hebert and J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJLy_SxC-},\n}", "keywords": ["DenseNet", "sparse shortcut connections", "network architecture", "scene parsing", "image classification"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Allie Del Giorno", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "adelgior@ri.cmu.edu", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642420261, "id": "ICLR.cc/2018/Conference/-/Paper257/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper257/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper257/AnonReviewer2", "ICLR.cc/2018/Conference/Paper257/AnonReviewer1", "ICLR.cc/2018/Conference/Paper257/AnonReviewer3"], "reply": {"forum": "SJLy_SxC-", "replyto": "SJLy_SxC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper257/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642420261}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642420963, "tcdate": 1511807860239, "number": 2, "cdate": 1511807860239, "id": "H12ujAYlG", "invitation": "ICLR.cc/2018/Conference/-/Paper257/Official_Review", "forum": "SJLy_SxC-", "replyto": "SJLy_SxC-", "signatures": ["ICLR.cc/2018/Conference/Paper257/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting idea to sparsify skip connections in DenseNets, well executed experiments", "rating": "6: Marginally above acceptance threshold", "review": "This paper introduces a new connectivity pattern for DenseNets, which encourages short distances among layers during backpropagation and gracefully scales to wider and deeper architectures. Experiments are performed to analyze the importance of the skip connections\u2019 place in the context of image classification. Then, results are reported for both image classification and semantic segmentation tasks.\n\nThe clarity of the presentation could be improved. The main contribution of the paper is a network design that places skip connections to minimize the distances between layers, increasing the distance from 1 to 1 + log L when compared to traditional DenseNets. This design principle allows to mitigate the memory required to train DenseNets, which is critical for applications such as semantic segmentation where the input resolution has to be recovered.\n\nExperiments seem well executed; the authors consider several sparse connectivity patterns for DenseNets and provide empirical evidence highlighting the advantages of having a short maximum backpropagation distance (MBD). Moreover, they provide an analysis on the trade-off between the performance of a network and its computational cost.\n\nAlthough literature review is quite extensive, [a] might be relevant to discuss in the Network Compression section.\n[a] https://arxiv.org/pdf/1412.6550.pdf\n\nIt is not clear why Log-DenseNets would be easier to implement than DenseNets, as mentioned in the abstract. Could the authors clarify that?\n\nIn Tables 1-2-3, it would be good to add the results for Log-DenseNet V2. Adding the MBD of each model in the tables would also be beneficial.\n\nIn Table 3, what does \u201cnan\u201d accuracy mean? (DeepLab-LFOV)\n\nFinally, the authors might want to review the citep/cite use in the manuscript. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log-DenseNet: How to Sparsify a DenseNet", "abstract": "Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, \\logdenses are easier to scale than DenseNets, and no longer require careful GPU memory management. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.", "pdf": "/pdf/fd11ad29bc2400441cde591b9bbd502b8ef7c86b.pdf", "TL;DR": "We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.", "paperhash": "hu|logdensenet_how_to_sparsify_a_densenet", "_bibtex": "@misc{\nhu2018logdensenet,\ntitle={Log-DenseNet: How to Sparsify a DenseNet},\nauthor={Hanzhang Hu and Debadeepta Dey and Allie Del Giorno and Martial Hebert and J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJLy_SxC-},\n}", "keywords": ["DenseNet", "sparse shortcut connections", "network architecture", "scene parsing", "image classification"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Allie Del Giorno", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "adelgior@ri.cmu.edu", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642420261, "id": "ICLR.cc/2018/Conference/-/Paper257/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper257/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper257/AnonReviewer2", "ICLR.cc/2018/Conference/Paper257/AnonReviewer1", "ICLR.cc/2018/Conference/Paper257/AnonReviewer3"], "reply": {"forum": "SJLy_SxC-", "replyto": "SJLy_SxC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper257/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642420261}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642420281, "tcdate": 1511808744041, "number": 3, "cdate": 1511808744041, "id": "Hkxey1cxM", "invitation": "ICLR.cc/2018/Conference/-/Paper257/Official_Review", "forum": "SJLy_SxC-", "replyto": "SJLy_SxC-", "signatures": ["ICLR.cc/2018/Conference/Paper257/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Nice idea, the presentation could be improved.", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a nice idea of sparsification of skip connections in DenseNets. The authors decide to use a principle for sparsification that would minimize the distance among layers during the backpropagation. \n\nThe presentation of the paper could be improved. The paper presents an elegant and simple idea in a dense and complex way making the paper difficult to follow. E. g., Fig 1 d is discussed in Appendix and not in the main body of the paper, thus, it could be moved to Appendix section.\n\nTable 1 and 3 presents the results only for LogDenseNet V1, would it be possible to add results for V2 that have different MBD. Also, the budget for the skip connections is defined as log(i) in Table 1 and Table 2 has the budget of log(i/2), would it be possible to add the total number of skip connections to the tables? It would be interesting to compare the total number of skip connections in Jegou et. al. to LogDenseNet V1 in Table 3.\n\nOther issues:\n- Table 3, has an accuracy of nan. What does it mean? Not available or not a number? \n- L is used as the depth, however, in table 1 it appears as short for Log-DenseNetV1. Would it be possible to use another letter here?\n- \u201c\u2026, we make x_i also take the input from x_{i/4}, x_{i/8}, x_{i/16}\u2026\u201d. Shouldn\u2019t x_{1/2} be used too?\n- I\u2019m not sure I understand the reasons behind blurred image in Fig 2 at \u00bd. It is mentioned that \u201cit and its feature are at low resolution\u201d. Could the authors comment on that?\n- Abstract: \u201c\u2026 Log-DenseNets are easier than DenseNet to implement and to scale.\u201d It is not clear why would LogDenseNets be easier to implement. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log-DenseNet: How to Sparsify a DenseNet", "abstract": "Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, \\logdenses are easier to scale than DenseNets, and no longer require careful GPU memory management. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.", "pdf": "/pdf/fd11ad29bc2400441cde591b9bbd502b8ef7c86b.pdf", "TL;DR": "We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.", "paperhash": "hu|logdensenet_how_to_sparsify_a_densenet", "_bibtex": "@misc{\nhu2018logdensenet,\ntitle={Log-DenseNet: How to Sparsify a DenseNet},\nauthor={Hanzhang Hu and Debadeepta Dey and Allie Del Giorno and Martial Hebert and J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJLy_SxC-},\n}", "keywords": ["DenseNet", "sparse shortcut connections", "network architecture", "scene parsing", "image classification"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Allie Del Giorno", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "adelgior@ri.cmu.edu", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642420261, "id": "ICLR.cc/2018/Conference/-/Paper257/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper257/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper257/AnonReviewer2", "ICLR.cc/2018/Conference/Paper257/AnonReviewer1", "ICLR.cc/2018/Conference/Paper257/AnonReviewer3"], "reply": {"forum": "SJLy_SxC-", "replyto": "SJLy_SxC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper257/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642420261}}}], "count": 5}