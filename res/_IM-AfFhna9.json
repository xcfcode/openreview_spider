{"notes": [{"id": "_IM-AfFhna9", "original": "ayVB9wD4dU", "number": 3595, "cdate": 1601308399650, "ddate": null, "tcdate": 1601308399650, "tmdate": 1615962400957, "tddate": null, "forum": "_IM-AfFhna9", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1CJ3_lw7_p", "original": null, "number": 1, "cdate": 1610040405306, "ddate": null, "tcdate": 1610040405306, "tmdate": 1610474001805, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Three of four reviewers are in favour of accepting the paper. Some reviewers raised valid criticism regarding the derivations, interpretation of the mathematical analysis and experimental results. So clearly some aspects of the paper could and should be clarified in accordance with the points raised by the reviewers. However, all in all the paper contains enough contributions to warrant publication.   "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040405292, "tmdate": 1610474001789, "id": "ICLR.cc/2021/Conference/Paper3595/-/Decision"}}}, {"id": "99ISK-NZTtS", "original": null, "number": 18, "cdate": 1606226291800, "ddate": null, "tcdate": 1606226291800, "tmdate": 1606226291800, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "cPZsnEKtGll", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Changes to the abstract and introduction", "comment": "Thank you for your response. As you suggested, we have now modified the Abstract and the Introduction in the latest version of the paper. This is to make the importance of the synergy between GVCL and FiLM layers more obvious. For example, the last paragraph of the Introduction now explicitly says that experiments are included with GVCL+FiLM layers, and we added a sentence, \u201cIn Section 5.4 we show that FiLM layers provide a disproportionate improvement to variational methods, confirming our hypothesis in Section 3.\u201d"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "OWtvoM6mn6", "original": null, "number": 15, "cdate": 1606136380045, "ddate": null, "tcdate": 1606136380045, "tmdate": 1606226170259, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "T2QHQTPRtFG", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "23/11 General Response", "comment": "In the latest revision, we made some minor changes to some figures and section 2.2, which we outline here:\n\n1. Added a paragraph clarifying how predictions are made with GVCL at the end of section 2.2\n2. Updated outdated figures in appendix K which describe the easy-CHASY benchmark\n3. Fixed an error in the newly added plots from the last revision in appendix J where the joint training lines were plotted wrong\n4. fixed a problem with the SGD, Separate, SGD-Frozen, and Joint (MAP) training on CHASY datasets where early stopping was performed on slightly the wrong criteria. This affects table 1 and figure 2ab (in the backwards and forwards transfer metrics by ~0.2% and the plot of the joint training line). The change is very minor and does not affect our analysis or conclusions\n\n24/11 Small revision:\n1. Modified the abstract and introduction so that it is more clear that the improvements are from both GVCL and FiLM layers"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "cPZsnEKtGll", "original": null, "number": 17, "cdate": 1606189339776, "ddate": null, "tcdate": 1606189339776, "tmdate": 1606189339776, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "eqdDibo5AVM", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Thank you for your response", "comment": "Thank you for the response with clarifications. So the improvement comes from both GVCL and FiLM layers, rather than the GVCL framework alone. As a result, I think it is essential to highlight this in the main text, or even in the title. Otherwise, it gives one impression that GVCL is the solution. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "n2JbrFs8a8", "original": null, "number": 3, "cdate": 1603972280707, "ddate": null, "tcdate": 1603972280707, "tmdate": 1606140736612, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "This work considers online variational Bayesian approaches to continual learning. The authors propose a beta-ELBO objective which they claim interpolates between Gaussian variational inference (beta = 1) and Laplace\u2019s approximation (beta = 0).\nFurthermore, the authors propose task-specific, non-probabilistic (point estimation) FiLM layers that apply an element-wise transformation to the activations.\n\nTheory / Contribution:\nThe two contributions seem quite orthogonal to each other and each of them is rather minor in novelty. \nIt is obvious that using beta=0 leads to MAP estimates from which Laplace\u2019s approximation can be computed. However, I am quite confused what exactly the authors do here and there could be a major mistake:\nFrom the paper, I am not sure if the authors a) compute Laplace\u2019s approximation in the end, at the resulting mean of q, for any beta value? As far as I understand, the authors instead b) only optimise the variance through the beta-ELBO. \nHowever, in this case, the resulting approximation would *not* identical to Laplace\u2019s approximation!\nI need clarification what the authors are doing here.\nConsider the case of beta=0, the covariance will be the dirac distribution as the authors note in Sec. 2.2 or the supplementary material. The authors then go on and write the optimal covariance matrix for which the derivative of the beta-ELBO is zero.\nYou have first postulated that the covariance is zero, in order to be able to pull out the expectation, and then you again allow for a non-zero beta-elbo-minimizing covariance. This would be a contraction. This makes me guess you do compute Laplace\u2019s approximation instead. But then it is not discussed how you deal with beta>0. \n\n\nRelated work:\nThe related work section is rather short mentioning only very few related approaches. More effort is required here.\n\n\nExperiments:\nThe experimental evaluation is thorough and seems promising. Although I am wondering why e.g. Fig. 2 does not include VCL and EWC. Figure 8 in the supplementary material probably has some legends mixed up, or the explanations that small beta values cause locally measured locally are wrong? For Fig. a), the largest beta=10 seems to be a good approximation and also the most local. In case of Fig. B) and C) it is unclear / subjective (from visually inspecting the likelihood function) which is the best approximation. In A), beta=0.1 is the least local approximation, in B) beta=10 and in C) beta=1. I cannot follow the intuition provided here.\n\n\nSummary:\nI am sceptical about the correctness regarding the equivalence between VI and Laplace\u2019s approximation; the exact approach proposed in the paper is unclear and may be based on a contradiction. In case I have a misunderstanding here, I hope the authors will point this out and update the manuscript. \n\n\nUpdate after Rebuttal:\nThe authors provided clarifications and improved the manuscript. \nIn particular, the authors now detail the two special cases (beta=0, beta=1) and how it relates to EWC and VCL. \nI am no longer sceptical that the claims regarding the equivalence to EWC in case of beta=0 is correct. \nBased on this, I changed my evaluation and now suggest acceptance. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072996, "tmdate": 1606915774079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review"}}}, {"id": "JilFwoZpdq", "original": null, "number": 16, "cdate": 1606136779393, "ddate": null, "tcdate": 1606136779393, "tmdate": 1606136779393, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "guy52WMx3W3", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "New revision with clarifications", "comment": "We have now updated the paper to include the following discussion of the predictive distribution at the end of section 2.2:\n\nWhen performing inference with GVCL at test time, we use samples from the unmodified $q(\\theta)$ distribution. This means that when $\\beta = 1$, we recover the VCL predictive, and as $\\beta \\to 0$, the posterior collapses as described earlier, meaning that the weight samples are effectively deterministic. This is in line with the inference procedure given by Online EWC and its variants.\nIn practice, we use values of $\\beta = 0.05 - 0.2$ in Section 5, meaning that some uncertainty is retained, but not all. We can increase the uncertainty at inference time by using an additional tempering step, which we describe, along with further generalizations in Appendix D.\n\nIf you have any suggestions, we are happy to modify the paragraph, or add more details in one of the appendices. \nThe other changes in the latest revision are given in the latest general response."}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "WeUPiP0IZ4u", "original": null, "number": 14, "cdate": 1606106608459, "ddate": null, "tcdate": 1606106608459, "tmdate": 1606106608459, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "6mbxJeIjP5u", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Thank you for your response", "comment": "Thank you for your efforts to address my comments and to improve the paper.\n\n1.\nI find the link to cold posteriors interesting, it would explain the choices more consistently from an inferential point of view. This analysis can also be deepened somewhat in future work, as the importance of 'cold' posteriors seems under-explored in the context of continual learning.\n\n2.\nThank you for clarifying that. I suspected FiLM layers would also work well with VCL itself in this case, I am glad this gets confirmed, as it may inform users running VCL systems that structure akin to FiLM layers may immediately help improve their systems. It is also quite interesting that GVCL outperforms VCL when using FiLM layers. I think this is overall a valuable addition to the paper and makes the case for this 'second' idea in the manuscript more succinctly.\n\n3.\nI would be presumptuous to suggest a title and I exclude this from my evaluation of the paper of course, but my mental hash function for this manuscript stored it under 'unifying VCL and EWC' rather than 'generalizing VCL' as the suggested changes are not strictly advances in the field of approximate inference.  I don't know if my hash function is useful enough to the authors for considering titles that would more directly inform the reader about the content, it is anecdotal.\n\nOverall, I remain convinced this is a strong paper and continue to suggest acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "guy52WMx3W3", "original": null, "number": 13, "cdate": 1605799689211, "ddate": null, "tcdate": 1605799689211, "tmdate": 1605799689211, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "FmPNw7vhNn", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "review adjust scores", "comment": "Thanks again for further clarifications. \nI will read the paper again (after the revision) and update my scores. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "FmPNw7vhNn", "original": null, "number": 12, "cdate": 1605711974467, "ddate": null, "tcdate": 1605711974467, "tmdate": 1605711974467, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "P1GY-NN9Cup", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "GVCL Posterior", "comment": "Thanks again for your response.\n\n1. In the next revision, we will add a discussion of how predictions are performed in Section 2.2 noting that the VCL predictive is recovered when $\\beta=1$ and the Online EWC predictive is recovered when $\\beta \\to 0$ as the weight uncertainty disappears. In the paper, we typically use values of $\\beta$ between 0.2 and 0.05, so there is still some uncertainty retained.\n\n2. It is not clear to us whether Ritter et al. do use weight uncertainties when making predictions e.g. in section 2, paragraph 1 (https://papers.nips.cc/paper/2018/file/f31b20466ae89669f9741e047487eb37-Paper.pdf),\nthe paper says their aim is to find a MAP estimate to the posterior over all datasets. This would imply that they do not predict with uncertainty for *all* of the algorithms, including e.g. the one called \"Online Laplace\", even though this might be confusing. Furthermore, they describe EWC as \u201capproximat[ing] the posterior .. with a Gaussian\u201d (section 3 paragraph 2), but EWC also does not use weight uncertainty either, casting doubt on whether they use uncertainty too.  Moreover, the only (non-official) implementation we have found also only uses deterministic weights (https://github.com/hannakb/KFA).\n\nIndeed, generally, if you use Laplace's approximation for Bayesian neural networks with Monte Carlo sampling for forming the predictive, you get very poor results (see e.g. https://arxiv.org/abs/1906.11537). So you have to either temper the posterior by a large amount (e.g. by removing all uncertainty) or use the linearisation approximation discussed in the above paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "P1GY-NN9Cup", "original": null, "number": 11, "cdate": 1605649751357, "ddate": null, "tcdate": 1605649751357, "tmdate": 1605649751357, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "gwCeps9NgkF", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "posterior approximation", "comment": "Thanks for the clarifications!\n\nLaplace propagation and Online Structured Laplace Approximations (Ritter et. al) do compute the Hessians though. \nI understand now that you do not need to compute the Hessians for learning continually because of the cancellation. However for the posterior predictive distribution, computing the \"right\" covariance would be necessary. \nI would have expected that the variational resulting distribution from optimizing the ELBO is a posterior approximation, which is the case for VCL and for Laplace's approximation, but for GVCL this is arguably not the case.\nI think this should be discussed. \n\nI suppose, we could do a Laplace's approximation in GVCL as well, if we compute the Hessian at mu, ignoring the learnt variance (which does not correspond to either variational or Laplace approximation). "}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "gwCeps9NgkF", "original": null, "number": 10, "cdate": 1605626993554, "ddate": null, "tcdate": 1605626993554, "tmdate": 1605626993554, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "2m9VQMwSyc", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Clarification of misunderstanding with Online-EWC", "comment": "Thank you for your quick response, and for your time.\n\nYou are correct when you state that the resulting posterior is not identical to that recovered using Laplace\u2019s approximation: the means are the same, but the covariance is near-zero. Therefore the approximate posterior over the weights has no uncertainty. This is in fact exactly how Online EWC performs predictions (Schwarz et al., 2018, \u201cProgress & Compress:  A scalable framework for continual learning\u201d, Section 4). That is, predictions at test-time are made with deterministic weights (no uncertainty), and the Hessian / the link to Laplace\u2019s approximation is only used for updating this mean parameter value.\n\nIf Laplace\u2019s approximation is used to form a non-deterministic posterior which is used to make predictions, then that is a slightly different algorithm to Online EWC, which we agree does not fall under the GVCL family.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "2m9VQMwSyc", "original": null, "number": 9, "cdate": 1605619437314, "ddate": null, "tcdate": 1605619437314, "tmdate": 1605619437314, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "1CT7Zvg20nN", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Thanks for clarification, one more potential misunderstanding to be clarified", "comment": "Thanks for the clarification. Fig. 8 now makes sense to me. \n\nRegarding 2), I am still trying to understand whether or not the variance resulting from optimizing the beta-ELBO is the same as the variance you would obtain when computing the Hessian at mu_t. \n\nLets start again with i) beta=0 (exactly zero) and then again ii) beta --> 0 (close to zero).\nIn i), it is clear that we will always obtain just the MLE estimate (not even MAP) with 0 covariance in every step, as we will always completely ignore the KL term. Now I don't see how for ii) the covariance suddenly jumps to the correct covariance for an infinitesimal change. \n\nLets try to get there through your results in App. C.\nStep 1) dL/dSigma = 0: We obtain an optimal (local minimum with derivative zero) precision matrix in (9) under the condition that beta is close to zero. I also agree with the resulting recursion of the precision. \nNote: So far, we have not concluded whether or not this optimal precision matrix is the same as we would get from Laplace's approximation. It is just whatever we get from optimizing the beta-ELBO, and in case of beta=0, precision will be inf. \nStep 2) Eq. (10) now looks at the ELBO when we use the previous posterior resulting from optimizing the beta-ELBO.\nNow, I agree that if beta is only *close* to zero, it will cancel (for the Hessian). \nAnd I also agree that the beta-ELBO then looks like the optim. from Laplace propagation. \nBut that is only for optimizing mu! \nSo I agree also that each iteration (time-step) t should in theory find the same mu_t as we would for Laplace Propagation. \nHowever, my understanding is that we will have a too sharp posterior at t-1 and then because we down-weight the KL term through beta, it will act as if it had the correct variance. So in the limiting case, we have a dirac distribution and it is weighted with zero, canceling completely. \n\nIn other words, the resulting posterior approximation at every iteration is not identical to Laplace's approximation, but the mean is. This means that posterior predictives will not use the right posterior (although we could apply Laplace's approximation just for posterior predictives while doing continual learning as proposed).\n\nDo you think there is still a misunderstanding or would you agree? "}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "eqdDibo5AVM", "original": null, "number": 8, "cdate": 1605614452748, "ddate": null, "tcdate": 1605614452748, "tmdate": 1605614452748, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "Tmnu6w2n6QB", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Reviewer 3 Response", "comment": "Many thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. It is kind of weird to start from the Bayesian framework and then go back to the non-Bayesian perspective.\n\nWe did not intend to claim that our approach is entirely Bayesian, and our resulting algorithm is not. Rather, we considered this work to be Bayesian inspired: we looked at the Bayesian approach (VCL) and made careful adjustments to fix key issues affecting it. Please also see the general response, point 2 for more discussion of these points.\n\nIn the latest version of the paper, we re-interpreted $\\lambda$ so that it falls into the general theme of the paper as tempering different parts of the ELBO in light of the cold-posterior effect. Note that tempering is not Bayesian, but rather a commonly-used workaround for dealing with the poor performance of Bayesian methods.\n\n2. Moreover, as described in Sec. 2.3, the resultant GVCL when $\\beta \\to 0$ is actually different from the previous online EWC algorithm\n\nWe assume you are referring to the difference in $\\tilde{\\lambda}$ and $\\lambda$. These two approaches differ only in their treatment of the prior variance. In the original EWC paper, and the paper proposing Online-EWC, it is unclear what the treatment of the prior covariance should be, hence this difference arises due to an ambiguity in the original algorithm. Furthermore, note that with increasingly small $\\beta$, the difference between $\\tilde{\\lambda}$ and $\\lambda$ vanishes, as the prior term becomes negligible compared to the Hessian.\n\nWe have changed the text to make this clearer in Section 2.3.\n\n3. GVCL ... should perform at least the same as VCL and online EWC.\n\nAs we mentioned previously, GVCL performs worse than Online EWC sometimes due to optimization issues with convergence as $\\beta \\to 0$. However, note that GVCL-F always outperforms VCL-F and EWC-F as expected. We also note that there is a benefit to having a unifying framework that encompasses a range of existing approaches allowing them to be better understood.\n\nWe have now added a toy example of GVCL in a toy 2d regression dataset showing the convergent behaviour of GVCL to Online EWC. In this toy example, it takes 10 times longer to achieve convergence for very small values of $\\beta$ (1e-4) compared to $\\beta = 1$. Note that these values of $\\beta$ are smaller than we had in our neural network examples, and given that the toy example has only 3 parameters yet still took a very long time to optimize, it is likely that we cannot practically reach the Online EWC limit on a neural network of even modest size. \n\n4. Regarding the results of the GVCL and GVCL-F, it seems that the improvement mainly comes from the FiLM layers\n\nThis is not a correct interpretation of the results -- the improvement comes from both GVCL and FiLM layers, with significant contributions coming from both. We show this explicitly in the new Section 5.4, which as you suggested, looks at the performance gains from adding FiLM layers to GVCL, VCL and Online EWC. It shows for example:\n\nSplit CIFAR: moving from VCL to GVCL is 25% gain, adding FiLM layers is a further 11% gain\nMixed Vision: moving from VCL to GVCL is 24% gain, adding FiLM layers is a further 30% gain\n\nAlso note adding FiLM layers to Online EWC only gives 0.1% and 7.7% improvement on these datasets respectively. See Table 2 and General Response point 3 for additional information.\n\nSo we see that FiLM layers alone provide little benefit to Online EWC, and a key insight in this paper is how FiLM layers interact with variational methods to fix the pruning issue. Unlike HAT, because of the interaction with the prior, we do not need more complex training procedures to learn the FiLM parameters. Note that we did not include HAT + FiLM layers since it already has per-task channel-wise gating layers. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "6mbxJeIjP5u", "original": null, "number": 4, "cdate": 1605613776977, "ddate": null, "tcdate": 1605613776977, "tmdate": 1605614368077, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "bHD9wQgOkEX", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Reviewer 4 Response", "comment": "Many thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. I found the introduction of the reweighting terms in Sec. 2.3 to be ad hoc and not particularly well justified...I think the authors should dig deeper here for better justifications for such choices\u2026\n\nWe agree that the writing in Section 2.3 could be improved and have re-written this section connecting the introduction of the parameter $\\lambda$ to the literature on cold posteriors. Please also see the general response, point 2 for more information.\n\n2. \u201dAdditionally, the film layers work great, but I maybe missed if they are the main attraction powering performance or if it is the combination with the new ELBO. Would film layers with VCL do equally well?\u201d\n\nWe have added Section 5.4, which shows the relative performance gain from adding FiLM layers to VCL and Online EWC. We see that GVCL and VCL both see large performance gains while Online EWC only receives marginal gains. This suggests that FiLM layers are particularly synergistic with VI based methods. Additionally, GVCL+FiLM outperforms VCL+FiLM.\n\n3. The title is somewhat misleading\n\nWe selected the title as the paper generalizes several existing continual learning algorithms under a single variational framework, related by the choice of Q distribution class and choices related to the tempering of distributions. We are happy to consider alternative titles and would be open to hear your suggestions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "RscEZOMhMdA", "original": null, "number": 7, "cdate": 1605614160127, "ddate": null, "tcdate": 1605614160127, "tmdate": 1605614160127, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "fgM2BnHqLp", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Reviewer 1 Response", "comment": "Many thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. It would be interesting to have VCL and Online EWC added to Figures 2 and 3.\n\nWe purposefully included only the best algorithms in Figures 2 and 3 so that the y-axis covers an appropriate range. VCL and Online EWC perform relatively poorly, so we left them out so that it is easy to compare the performance of the better algorithms. We have added additional figures to Appendix K which show these plots, along with the performance of additional algorithms.\n\n2. Why is GVCL significantly worse than baselines for split-mnist (Figure 2c)?\n\nThe baselines in Figure 2c are potentially confusing, and it is unfair to compare vanilla GVCL to them. HAT and GVCL-F store task-specific parameters, and hence have growing memory demands with number of tasks. Therefore the natural comparison for HAT is GVCL-F, and not GVCL (note that Joint-MAP is not a continual learning algorithm). It appears that this is much more important for Split-MNIST than the CHASY benchmarks.\n\n3. Why is split-mnist omitted from Figure 3?\n\nMany of the tasks all effectively are at 100% performance, so including it does not provide much useful information. However, we have included the plot in Appendix K.\n\n4. The supplementary material contains some analysis on the effect and sensitivity of the value of $\\beta$ on the performance of the algorithm. This should be extended and presented in the main paper.\n\nWe have added more explanation in the main text (end of Section 2.2). We have also now added a small toy example showing the convergent behaviour of GVCL to Online EWC in Appendix B, which includes a range of $\\beta$ values. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "1CT7Zvg20nN", "original": null, "number": 6, "cdate": 1605614054511, "ddate": null, "tcdate": 1605614054511, "tmdate": 1605614054511, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "JAminNFC9sk", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Reviewer 2 Response Part 2", "comment": "5. Figure 8 in the supplementary material probably has some legends mixed up, or the explanations that small beta values cause locally measured locally are wrong\n\nThe legends are correct, and there is likely some misunderstanding here. We clarify that by \u201clocal\u201d we refer to the immediate vicinity around a point, i.e. if we zoomed in very close to a point. We also reiterate that a \u201cgood\u201d and a \u201clocal\u201d approximation are not to be conflated, particularly in the toy examples presented.\n\nFor Figures 8b and 8c, there is a cusp at the mode of the distribution. Therefore, a local approximation would approximate the function immediately surrounding that cusp, and ignore the regions far away. We see that this is exactly what $\\beta = 0.1$ (very small $\\beta$) does: it has a very sharp curve. Similarly, for Figure 1, if zoomed in very close to the mode of (a), we would notice that the true distribution is nearly flat. This means that a local fit would match that flatness, and ignore the fact that it begins to curve further away from the mode. $\\beta = 0.1$  does exactly this. It appears to be a bad fit because of the scale of our graphs, but if we zoomed in very closely to the mode, it would appear to be the best fit, because it is the most local fit.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "JAminNFC9sk", "original": null, "number": 5, "cdate": 1605613992691, "ddate": null, "tcdate": 1605613992691, "tmdate": 1605613992691, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "n2JbrFs8a8", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "Reviewer 2 Response", "comment": "Many thanks for your comments. They have helped us improve the paper. We respond to your questions below.\n\n1. The two contributions seem quite orthogonal to each other and each of them is rather minor in novelty.\n\nWe disagree that these contributions are minor. In this paper, we show that several continual learning algorithms all arise from a single unifying framework, and certain choices and hyperparameters in each algorithm all arise by making different choices in the tempering of posteriors, priors, and likelihoods. For example:\n\n* VCL occurs when no tempering is performed\n* Online-EWC, Online-Structured Laplace and SOLA are instances of GVCL where $\\beta \\to 0$, and the choice of $Q$ distribution is changed\n* $\\lambda$ arises by tempering the posterior and prior using the same temperature in the KL-divergence\n* Online-EWC\u2019s $\\gamma$ arises by tempering the posterior and prior using different temperatures in the KL-divergence\n\nA unifying algorithm immediately opens the door for different choices of these parameters, and lets us understand the relationship between and broader context of these algorithms. Naturally, it means that improvements and innovations in one of these algorithms can readily be applied to others and paves the way for rapid and systematic progress.\n\nOur second contribution, the usage of FiLM layers, addresses a key limitation in variational methods, which is particularly problematic in the continual learning setting. While this contribution is orthogonal to GVCL, it is particularly synergistic. What differs between our version of FiLM layers and other similar algorithms, such as HAT, is its synergy with variational methods. Because of the pruning effect and the prior, no special algorithm is needed to fit these FiLM layers, and the resulting gain for variational methods is over 10%, compared to merely 2% for non-variational methods. We have added a new section in the revised version of the paper to make this clear (Section 5.4).\n\n2. I am not sure if the authors a) compute Laplace\u2019s approximation in the end, at the resulting mean of q, for any beta value?\n\nThis is a misunderstanding. We have responded to this point in the general response (point 1), but add further clarification below.\n\nTo be clear, we never compute Laplace\u2019s approximation directly. We update $\\Sigma$ using the $\\beta$-ELBO and show that this recovers a version of Laplace\u2019s approximation in a limiting case. In the derivation of this result, we did not assume $\\beta = 0$, but rather assume that $\\beta$ is very close to zero as we take the limit. When this is done, there is a cancellation in the $\\beta$-ELBO whereby the beta-dependence in the previous posterior cancels with the beta term in the $\\beta$-ELBO, giving rise to the EWC regularisation (see equations 10 and 11 in Appendix C).\n\nIn our derivation, we did not assume the covariance was zero, but we assumed it was $\\textit{near}$-zero, so we can still apply normal arithmetic to it. \n\nWe agree that in Appendix C, the statement that \u201cq approaches a delta function\u201d was rather imprecise. To amend this, we have now added a proof that moving from Equation 8 to 9 is valid for small $\\beta$. This is included in Appendix C.1.\n\n3. Related work: The related work section is rather short mentioning only very few related approaches. More effort is required here.\n\nNote that many related works are mentioned previously in the text (e.g. 16 unique texts in sections 2 and 3), not only in the Related Works section. Also, we have a longer related work section in Appendix I, which we were unable to include in the main text due to space constraints. As we now have additional space (from gaining an additional page), we are happy to expand this section in the main text. Please let us know of the specific references you have in mind.\n\n4. I am wondering why e.g. Fig. 2 does not include VCL and EWC\n\nWe have included the requested plots in Appendix J. We only included the best performing algorithms in figure 2 since VCL and EWC performance lies well below the range of the graph. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "T2QHQTPRtFG", "original": null, "number": 3, "cdate": 1605613549737, "ddate": null, "tcdate": 1605613549737, "tmdate": 1605613549737, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "blmc-57Q2C", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "General Response Part 2", "comment": "Here we note the changes we have made to the revised version of the paper:\n\n1. A derivation of the quadratic term multiplier based on tempering the posterior and prior in Section 2.3.\n2. Section 5.4 (and Table 2), which includes performance gains from adding FiLM layers to EWC and VCL.\n3. Appendix B, which empirically shows the convergence of GVCL to Online EWC in a toy example and highlights the difficulty of 4. achieving this limit in practice. \n5. Appendix C.1, which includes a proof that the delta-function argument used in Equation 8 to 9 is valid. \n6. Appendix C.3, which shows how Online-EWC\u2019s $\\gamma$ arises by tempering the posterior and prior in the KL-divergence by slightly different temperatures\n7. Additional results of EWC + FiLM and VCL + FiLM added to the full result tables in Appendix J, as well as additional figures showing each algorithm\u2019s performance on each task (similar to Figures 2 and 3).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "blmc-57Q2C", "original": null, "number": 2, "cdate": 1605613454329, "ddate": null, "tcdate": 1605613454329, "tmdate": 1605613490353, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank all the reviews for their thoughtful criticisms and suggestions. Here, we address the main points raised by reviewers and outline the changes we have made to the paper in response. More specific points are addressed in the response to each of the individual reviewers.\n\n\n1. Reviewer 2 has concerns about the mathematical limit that connects generalized variational inference to Laplace\u2019s approximation \n\nThe reviewer has misunderstood the theory in our paper associated with the limit $\\beta \\to 0$. We briefly outline the key result: \n\n* Consider running GVCL with a common $\\beta$ value used across all tasks\n* Now take the limit of this procedure as $\\beta$ tends to zero\n* In this case, all the approximate posteriors (qs) limit to deltas around the MAP estimate\n* The inverse variances (precisions) of these approximate posteriors tend to sums of the Hessians at the MAP value scaled by 1/$\\beta$\n*Critically, the objective functions for each task become equal to online EWC due to a cancellation of the terms involving beta \n\nWe have improved the discussion of this limit by adding more detail to Appendix C and C.1, in particular we justify the argument that for small $\\beta$ the expectation in Equation 8 becomes approximately the Hessian at the mean (Equation 9).\n\n\n2. Reviewers 3 and 4 are worried that the introduction of $\\lambda$ -- which is necessary to recover Online EWC in a general way -- is not theoretically-well justified\n\nIt is true that, from a Bayesian perspective, it is not straightforward to justify the introduction of the parameter $\\lambda$. In the revised version of the paper, we add a theoretical explanation for reweighting the quadratic term with $\\lambda$ as well (Section 2.3).\n\nIn the re-written Section 2.3, we show that $\\lambda$ arises if we make use of tempering, as has been proposed in the context of cold posteriors (Wenzel et al. 2020). Specifically, at each step we temper the previous posterior before applying variational inference. We believe that this new interpretation sheds light on the relationship between the effectiveness of $\\lambda$ and cold posteriors.\n\nWe would also like to reiterate that our final algorithm cannot be strictly considered \u201cBayesian,\u201d nor do we claim it to be. Rather, there is a general trend in the Bayesian Deep Learning community whereby Bayesian methods are used to develop new algorithmic approaches to deep learning problems and then relaxations of these approaches are considered, with additional parameters, that perform better empirically than the pure-Bayesian method. EWC was developed using this approach (and indeed this resulted in the same $\\lambda$ parameter being introduced without rigorous justification) and more recent work has followed this example (Kirkpatrick et al. 2016, Ritter et al. 2018, Osawa et al. 2019 , Pan et al. 2020, Wenzel et al. 2020,  Higgins et al 2017, Alemi et al. 2017). This class of approaches has been called \u2018Bayesian Inspired\u2019 and we see the current work as belonging to this pragmatic vein.\n\nIn this paper we take the more strictly Bayesian VCL algorithm, and improve it by addressing the main shortcomings of variational Bayesian methods. Namely, we address the poor data fit problem by considering tempered likelihoods with $\\beta$ and $\\lambda$, and fix the pruning issue using FiLM layers.\n\n\n3. Reviewers 3 and 4 question \u201cwhether the performance gains are from FiLM Layers or GVCL\u201d\n\nWe have added results on all benchmarks with Online EWC + FiLM layers, and VCL + FiLM layers in section 5.4. These results show that (i) FiLM layers provide a significant benefit to the variational algorithms (VCL and GVCL), while not so much for Online EWC, (ii) GVCL + FiLM outperforms all competing algorithms, with both innovations contributing to the improved performance.\n\nTo summarise, in the revised Section 5.4 in Table 2 and Appendix J we see:\n* VCL+FiLM >> VCL and GVCL+FiLM >> GVCL, while EWC + FiLM $\\approx$ EWC.\n* GVCL+FiLM > GVCL > VCL+FiLM >> VCL \n\n\n4. AnonReviewers 1 and 2 have some questions about the omission of certain baseline algorithms from figures\n\nFor Figures 2 and 3, we only included the GVCL, GVCL-F, and the top performing baseline algorithm. This was done to keep the figure uncluttered, and to keep the y-axis in a reasonable range as these baseline algorithms perform poorly. However, as requested by reviewers, we have now added some extra figures in Appendix J. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_IM-AfFhna9", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3595/Authors|ICLR.cc/2021/Conference/Paper3595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Comment"}}}, {"id": "Tmnu6w2n6QB", "original": null, "number": 1, "cdate": 1603523460306, "ddate": null, "tcdate": 1603523460306, "tmdate": 1605023971351, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review", "content": {"title": "An interesting perspective but lack of preciseness and kind of unclear.", "review": "This paper proposed a generalized variational continual learning (GVCL) framework using the \\beta - ELBO, and then combined with FiLM layers. The idea is interesting but there is a lack of preciseness. The pros and cons are as follows. \n\nPros: \n1. The proposed GVCL proposed a different and interesting perspective on the online EWC, viewed as a special case of \\beta \\to 0;\n2. FiLM layers are introduced to combine with GVCL, which lead to significant improvement in the performance;\n3. Various experiments are performed, showing some level of advantages. \n\nCons:\n1. The new perspective that online EWC could be viewed as a special case of the GVCL framework is lacking preciseness. First of all, as described in Sec. 2.3, the result of the \\beta-ELBO, even with \\beta \\to 0, does not lead to the key hyper parameter \\lambda in online EWC. To compensate this, the authors introduce a modified KL divergence to make them similar. However, it is not justified, from a unified Bayesian or some other theoretical perspective , why the previous \\beta-ELBO needs to be modified. It is kind of wired to start from the Bayesian  framework and then go back to the non-Bayesian perspective to design a Bayesian algorithm to improve the performance, and then claim that the previous non-Bayesian algorithm is a special case of the unified Bayesian framework. Moreover, as described in Sec. 2.3, the  resultant GVCL when \\beta \\to 0 is actually different from the previous online EWC algorithm. As a result, strictly speaking, it is not approperiate to claim that the online EWC could be recovered as a limiting case. \n\n2. If it is true that the proposed GVCL is a generalization of VCL and Online EWC, which allows interpolation between the two, then it is expected and reasonable that the GVCL alone (without additional FiLM layers) should perform at least the same as VCL and online EWC. Otherwise, the statement is not true and there is no advantage of the proposed GVCL framework . However, as shown in experimental results, e.g., Table 1, GVCL alone performs worse than Online EWC in large datasets, which is really wired. The authors also acknowledged this point and claimed that this is due to the difficulty in optimizing GVCL with small \\beta. It would be better to make such statement more precise because this is really important point for this paper. Otherwise, it implies that the so-called interpolation between VCL and online EWC has no additional advantage. \n\n3. Regarding the results of the GVCL and GVCL-F, it seems that the improvement mainly comes from the FiLM layers, rather than the GVCL framework itself.  To make this more clear and for a more fair comparison, it is highly suggested to compare other methods (online EWC, VCL, HAT, etc) with FiLM layers. Otherwise, the current improvement of the performance is unclear. In addition, the improvement of GVCL-F over the baseline is not consistent. \n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072996, "tmdate": 1606915774079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review"}}}, {"id": "fgM2BnHqLp", "original": null, "number": 2, "cdate": 1603617206978, "ddate": null, "tcdate": 1603617206978, "tmdate": 1605023971275, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review", "content": {"title": "GVCL", "review": "This paper proposes Generalized Variational Continual Learning (GVCL). It is shown that Online EWC and VCL are special cases of GVCL, along with other theoretical contributions. Further, GVCL is augmented with FiLM to alleviate weaknesses of VCL and GVCL. GVCL and GVCL-F are applied to a number of continual learning tasks and demonstrate competitive performance. \n\nAlthough GVCL and GVCL-F do not outperform baselines, particularly in hard settings (split-mnist and mixed vision), GVCL is an original and excellent contribution. The paper is clear and well-written, the proposed algorithm is theoretically motivated and analysed, experiments are comprehensive, demonstrating the empirical performance of GVCL. \n\nI have the following comments:\n- It would be interesting to have VCL and Online EWC added to Figures 2 and 3.\n- Why is GVFL significantly worse than baselines for split-mnist (Figure 2c)?\n- Why is split-mnist omitted from Figure 3?\n- The supplementary material contains some analysis on the effect and sensitivity of the value of $\\beta$ on the performance of the algorithm. This should be extended and presented in the main paper.\n\nMinor:\n- \"the node is *effective* shut off\" -> effectively", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072996, "tmdate": 1606915774079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review"}}}, {"id": "bHD9wQgOkEX", "original": null, "number": 4, "cdate": 1604299316786, "ddate": null, "tcdate": 1604299316786, "tmdate": 1605023971151, "tddate": null, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "invitation": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review", "content": {"title": "Interesting take on Unifying VCL and EWC", "review": "The authors propose Generalized VCL in this paper, which consists of multiple ideas: first, the authors introduce a beta-Elbo, which facilitates downweighting the KL-term of VCL. If beta taken to the limit towards zero, the authors show that the beta-elbo recovers the online EWC learning criterion, which draws an interesting link between VCL and EWC.\nThe authors also discuss reweighting terms to introduce a parameter lambda as in EWC, which they incorporate via a lambda-kl divergence term.\nFinally, furnished with this learning objective that interpolates between VCL and EWC, the authors propose to combine the learning objective with the architectural choice of Film layers, which they show facilitate overcoming the pruning behavior that their method inherits from VCL by offering ways to prune nodes without injecting noise into the network.\n\nExperiments are broad on multiple interesting datasets and quite clearly show that their proposed combined model performs best.\n\nPositives:\nThe paper draws an interesting unification between EWC and VCL, and in fact also other related works, as subtle modifications in a regularizer. This by itself is an interesting contribution. The fact that the authors study the interplay of their learning arlgorithm with architectural biases, i.e. overcoming early pruning via film layers, is also a valuable idea that I find not just interesting in itself, but also stylistically valuable as an approach to studying  deep learning. While the Film layers per se also appear somewhat ad hoc, their empirical benefits -particuarly when paired with the lambda-elbo, are impressive and well put together.\n\nCriticisms:\nWhile I really enjoy the derivation of the beta-elbo in the zero limit, I found the introduction of the reweighting terms in Sec. 2.3 to be ad hoc and not particularly well justified. It feels as if it is reverse engineered to match the desired criterion from EWC. I think the authors should dig deeper here for better justifications for such choices, as they did a good job having a mathematically interesting framework to derive earlier.\n\nAdditionally, the film layers work great, but I maybe missed if they are the main attraction powering performance or if it is the combination with the new ELBO. Would film layers with VCL do equally well? This is empirically confusing, it would be great to get some more help to understand the relative merits of each components here and clarify more how these pieces fit together empirically. I do enjoy the appendix discussing this qualitatively, but I would like to understand it quantitatively better, as theoretically film layers plus VCL (without this paper's innovations) should also benefit similarly.\n\nOne additional criticism is that the title is somewhat misleading, as it does not generalize VCL to broader settings, but rather collapses it towards the limit beta towards zero. The title raised hopes for a richer variational treatment rather than a unification to EWC and an architecture change. The authors might want to consider tweaking the title to sth that is closer to the paper's actual contributions.\n\n\nOverall:\nThis paper takes an interesting approach towards adding to the EWC and VCL literature by unifying them and offering an architectural fix for a key problem in these scenarios. While the contributions are mixed and not consistently derived from clear modeling assumptions, their interplay is well studied and highly relevant to the understanding and improvement of practical continual learning. I also want to again applaud the authors for studying and explaining the interplay of pruning and film layers, I enjoyed reading the supplementary information on this. I wish more papers that discover methods that perform well empirically would study the interplays of algorithm and architecture similarly to expose interesting effects.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3595/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3595/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Variational Continual Learning", "authorids": ["~Noel_Loo1", "~Siddharth_Swaroop2", "~Richard_E_Turner1"], "authors": ["Noel Loo", "Siddharth Swaroop", "Richard E Turner"], "keywords": [], "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "loo|generalized_variational_continual_learning", "one-sentence_summary": "We generalize VCL and Online-EWC and combine with task-specific FiLM layers", "supplementary_material": "/attachment/9c96be58dfb85976780da76306fed422b28aa22c.zip", "pdf": "/pdf/75e7423995a4eb4d239591596556bd1ac05f5e63.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nloo2021generalized,\ntitle={Generalized Variational Continual Learning},\nauthor={Noel Loo and Siddharth Swaroop and Richard E Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=_IM-AfFhna9}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_IM-AfFhna9", "replyto": "_IM-AfFhna9", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538072996, "tmdate": 1606915774079, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3595/-/Official_Review"}}}], "count": 23}