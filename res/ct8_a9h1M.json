{"notes": [{"id": "ct8_a9h1M", "original": "B1Jw_rTXPOF", "number": 1270, "cdate": 1601308142061, "ddate": null, "tcdate": 1601308142061, "tmdate": 1615057051367, "tddate": null, "forum": "ct8_a9h1M", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module", "authorids": ["~XINJIE_FAN2", "~Shujian_Zhang1", "korawat.tanwisuth@utexas.edu", "~Xiaoning_Qian2", "~Mingyuan_Zhou1"], "authors": ["XINJIE FAN", "Shujian Zhang", "Korawat Tanwisuth", "Xiaoning Qian", "Mingyuan Zhou"], "keywords": ["Efficient Inference Methods", "Probabilistic Methods", "Supervised Deep Networks"], "abstract": "Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.", "one-sentence_summary": "We propose contextual dropout as a scalable sample-dependent dropout method, which makes the dropout probabilities depend on the input covariates of each data sample.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|contextual_dropout_an_efficient_sampledependent_dropout_module", "supplementary_material": "/attachment/b62879adb3a79ceaf96ad6170d2f7dbb3673cfb2.zip", "pdf": "/pdf/e60af0056960e7c5e82ee3d15ad9c6dd3577788d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021contextual,\ntitle={Contextual Dropout: An Efficient Sample-Dependent Dropout Module},\nauthor={XINJIE FAN and Shujian Zhang and Korawat Tanwisuth and Xiaoning Qian and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ct8_a9h1M}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "JjNxacG9vkV", "original": null, "number": 1, "cdate": 1610040487760, "ddate": null, "tcdate": 1610040487760, "tmdate": 1610474093365, "tddate": null, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "invitation": "ICLR.cc/2021/Conference/Paper1270/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes an input-dependent dropout strategy, using variational inference to infer the rates.  While the idea is a fairly straightforward variant of recent probabilistic dropout methods, the paper demonstrates consistent improvements across several types of NN layers (dense, convolutional, and attention) in large-scale experiments (e.g. ImageNet).  The reviewers unanimously agreed on accepting the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module", "authorids": ["~XINJIE_FAN2", "~Shujian_Zhang1", "korawat.tanwisuth@utexas.edu", "~Xiaoning_Qian2", "~Mingyuan_Zhou1"], "authors": ["XINJIE FAN", "Shujian Zhang", "Korawat Tanwisuth", "Xiaoning Qian", "Mingyuan Zhou"], "keywords": ["Efficient Inference Methods", "Probabilistic Methods", "Supervised Deep Networks"], "abstract": "Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.", "one-sentence_summary": "We propose contextual dropout as a scalable sample-dependent dropout method, which makes the dropout probabilities depend on the input covariates of each data sample.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|contextual_dropout_an_efficient_sampledependent_dropout_module", "supplementary_material": "/attachment/b62879adb3a79ceaf96ad6170d2f7dbb3673cfb2.zip", "pdf": "/pdf/e60af0056960e7c5e82ee3d15ad9c6dd3577788d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021contextual,\ntitle={Contextual Dropout: An Efficient Sample-Dependent Dropout Module},\nauthor={XINJIE FAN and Shujian Zhang and Korawat Tanwisuth and Xiaoning Qian and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ct8_a9h1M}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040487747, "tmdate": 1610474093349, "id": "ICLR.cc/2021/Conference/Paper1270/-/Decision"}}}, {"id": "sCemgA99YN1", "original": null, "number": 2, "cdate": 1603950456755, "ddate": null, "tcdate": 1603950456755, "tmdate": 1607053615644, "tddate": null, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "invitation": "ICLR.cc/2021/Conference/Paper1270/-/Official_Review", "content": {"title": "Small but consistent improvement in experimental results.", "review": "Summary: This paper proposes a way of estimating data-dependent dropout probabilities. This is done in each layer using a small auxiliary neural network which takes the data (on which dropout is going to be applied) as input and outputs dropout probabilities, which are sampled and multiplied into the data.\n\nPros:\n- The proposed model consistently leads to improvements in accuracy and uncertainty estimation over standard dropout for multiple network types and tasks (including CNNs and Transformers).\n- The experimental results are thorough and include relevant p-values and error bars.\n- The method is sound and well explained.\n\nCons:\n- The gains in accuracy and uncertainty estimation for most tasks are small.\n- A concern with learning the dropout probabilities on the training set is that the optimal value of dropout is zero, since that would remove regularization, thereby minimizing the training loss. It is not clear how this is avoided in the proposed approach. One thing that could prevent the dropout rates from going to zero is the prior ((\\ \\eta \\\\). However, this is also learned. So it would be good to explain what prevents the dropout rates from becoming zero.\n- Conflating of 2 different effects : There are at least 2 aspects of the proposed model that could be beneficial : (1) Increase in model capacity from having a multiplicative gating interaction in the network (in expectation, the states are gated by \\\\(\\sigma(\\alpha^l)\\\\)), and (2) a decrease in model capacity (regularization) due to dropout noise.   An ablation analysis can help tease apart the benefits coming from these two sources.  This could be done, for example, by removing stochastic sampling (so that only the gating aspect remains) and optionally adding a regular dropout layer on the gated data. The increase in model capacity due to gating (aspect (1)) could partly explain why dropout rates do not become zero.\n\nOverall, the model leads to consistent gains in performance with relatively extra low computational cost which makes this a good contribution. However, the significance is limited because the results are only moderately better.\n\nPost rebuttal\nThe authors addressed the concerns around having a deterministic gating only baseline. I will increase my score to 7", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1270/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1270/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module", "authorids": ["~XINJIE_FAN2", "~Shujian_Zhang1", "korawat.tanwisuth@utexas.edu", "~Xiaoning_Qian2", "~Mingyuan_Zhou1"], "authors": ["XINJIE FAN", "Shujian Zhang", "Korawat Tanwisuth", "Xiaoning Qian", "Mingyuan Zhou"], "keywords": ["Efficient Inference Methods", "Probabilistic Methods", "Supervised Deep Networks"], "abstract": "Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.", "one-sentence_summary": "We propose contextual dropout as a scalable sample-dependent dropout method, which makes the dropout probabilities depend on the input covariates of each data sample.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|contextual_dropout_an_efficient_sampledependent_dropout_module", "supplementary_material": "/attachment/b62879adb3a79ceaf96ad6170d2f7dbb3673cfb2.zip", "pdf": "/pdf/e60af0056960e7c5e82ee3d15ad9c6dd3577788d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021contextual,\ntitle={Contextual Dropout: An Efficient Sample-Dependent Dropout Module},\nauthor={XINJIE FAN and Shujian Zhang and Korawat Tanwisuth and Xiaoning Qian and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ct8_a9h1M}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1270/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122552, "tmdate": 1606915796878, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1270/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1270/-/Official_Review"}}}, {"id": "uO0GEL-hnhp", "original": null, "number": 1, "cdate": 1603345041773, "ddate": null, "tcdate": 1603345041773, "tmdate": 1606778145796, "tddate": null, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "invitation": "ICLR.cc/2021/Conference/Paper1270/-/Official_Review", "content": {"title": "Sample-dependent dropout for prediction and confidence improvements", "review": "Sample-dependent dropout for prediction and confidence improvements\n\n**Quality:**\n\n_Pros._ The authors assume that the sample-dependent dropout rate is necessary to improve model performance in aspects of prediction and confidence. In the Bayesian framework, they propose a contextual dropout module that is carefully designed considering computational efficiency since it is prone to increase model capacity and computational cost.\n\n_Cons._ Figures and tables are intentionally resized to fit the smaller areas. Some text is hard to read. Please increase the font size of the text in the figures for a printed paper.\n\n**Clarity:**\n\n_Pros._ The manuscript is well-written and straightforward to understand the proposed method. \n\n_Cons._ In Section 2.3, why do we have a multi-dimensional tensor for a fully-connected layer, instead of two-dimensional U^l? (in \"Contextual dropout module for fully-connected layers\" paragraph.)\n\n**Originality:**\n\n_Pros._ It explores the context-aware dropout method, while the other dropout-related works do not, with a few extra parameters.\nIncluding the ImageNet and the VQA experiments is helpful to illustrate the applicability of the proposed method.\nThey show that the contextual dropout can successfully apply to attention networks.\n\n_Cons._ The sample-dependent dropout has a resemblance to the squeeze-and-excitation module (the authors also mentioned in Section 2.3). In the related work section, more discussion on it would be helpful to recognize its novelty.\n\nSample-dependent dropout inevitably needs a sub-network to model a conditional distribution q of z for a given x. For this matter, it would be fair to compare with the squeeze-and-excitation module or self-attention models (with a small hidden dimension for parsimony; with the weights from sigmoid function). \n\nIn the proposed work, if the output of a sigmoid function is used for the real-valued weight (without the ARM estimator), instead of binary-sampled weight, does it significantly underperforms? \n\n**What expected in rebuttal:**\n\n(1) I would like to know that the ARM estimator in the proposed work is a critical factor, where the real-valued output of a sigmoid function is readily used for the dropout mask (as a weight). One could see this work as a variant of self-attention networks instead of a new dropout technique. In this viewpoint, the authors should defend with more persuasive logic (and experiments if possible) for their originality. \n\n(2) For the VQA experiment, MCAN (Yu et al., 2019) gets 67.2 in the original paper with a standard dropout module with the same setting as described in the Appendix. To be fair, this report should be included. Then, the improvement from a standard dropout is reduced to 0.22 (Bernoulli contextual dropout gets 67.42 with extra parameters.)\n\n**Minor comments:**\nFootnote 1 has extra space before the period.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1270/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1270/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module", "authorids": ["~XINJIE_FAN2", "~Shujian_Zhang1", "korawat.tanwisuth@utexas.edu", "~Xiaoning_Qian2", "~Mingyuan_Zhou1"], "authors": ["XINJIE FAN", "Shujian Zhang", "Korawat Tanwisuth", "Xiaoning Qian", "Mingyuan Zhou"], "keywords": ["Efficient Inference Methods", "Probabilistic Methods", "Supervised Deep Networks"], "abstract": "Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.", "one-sentence_summary": "We propose contextual dropout as a scalable sample-dependent dropout method, which makes the dropout probabilities depend on the input covariates of each data sample.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|contextual_dropout_an_efficient_sampledependent_dropout_module", "supplementary_material": "/attachment/b62879adb3a79ceaf96ad6170d2f7dbb3673cfb2.zip", "pdf": "/pdf/e60af0056960e7c5e82ee3d15ad9c6dd3577788d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021contextual,\ntitle={Contextual Dropout: An Efficient Sample-Dependent Dropout Module},\nauthor={XINJIE FAN and Shujian Zhang and Korawat Tanwisuth and Xiaoning Qian and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ct8_a9h1M}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1270/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122552, "tmdate": 1606915796878, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1270/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1270/-/Official_Review"}}}, {"id": "V_Gn0UVV29p", "original": null, "number": 5, "cdate": 1605710312586, "ddate": null, "tcdate": 1605710312586, "tmdate": 1605710312586, "tddate": null, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "invitation": "ICLR.cc/2021/Conference/Paper1270/-/Official_Comment", "content": {"title": "Authors: Thank you for response / Reviewers: Please update", "comment": "Thank you, authors, for your responses.\n\nReviewers, please read the responses and update your reviews by stating that your concerns have been addressed or by providing further rebuttal."}, "signatures": ["ICLR.cc/2021/Conference/Paper1270/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1270/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module", "authorids": ["~XINJIE_FAN2", "~Shujian_Zhang1", "korawat.tanwisuth@utexas.edu", "~Xiaoning_Qian2", "~Mingyuan_Zhou1"], "authors": ["XINJIE FAN", "Shujian Zhang", "Korawat Tanwisuth", "Xiaoning Qian", "Mingyuan Zhou"], "keywords": ["Efficient Inference Methods", "Probabilistic Methods", "Supervised Deep Networks"], "abstract": "Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.", "one-sentence_summary": "We propose contextual dropout as a scalable sample-dependent dropout method, which makes the dropout probabilities depend on the input covariates of each data sample.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|contextual_dropout_an_efficient_sampledependent_dropout_module", "supplementary_material": "/attachment/b62879adb3a79ceaf96ad6170d2f7dbb3673cfb2.zip", "pdf": "/pdf/e60af0056960e7c5e82ee3d15ad9c6dd3577788d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021contextual,\ntitle={Contextual Dropout: An Efficient Sample-Dependent Dropout Module},\nauthor={XINJIE FAN and Shujian Zhang and Korawat Tanwisuth and Xiaoning Qian and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ct8_a9h1M}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ct8_a9h1M", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1270/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1270/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1270/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1270/Authors|ICLR.cc/2021/Conference/Paper1270/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1270/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861653, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1270/-/Official_Comment"}}}, {"id": "gSjUB7GE9ar", "original": null, "number": 3, "cdate": 1604049813370, "ddate": null, "tcdate": 1604049813370, "tmdate": 1605024486403, "tddate": null, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "invitation": "ICLR.cc/2021/Conference/Paper1270/-/Official_Review", "content": {"title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module ", "review": "##########################################################################\n\nSummary:\n\n \nThe paper proposes contextual dropout as a sample-dependent dropout module, which can be applied to different models at the expense of marginal memory and computational overhead. The authors chose to focus on Visual Question Answering and Image classification tasks.  The results in the paper show the contextual dropbox can improve accuracy on ImageNet and VQA2.0 datasets. \n\n\n##########################################################################\n\nReasons for score: \n\n \nI vote for accepting. I like the idea of data-dependent dropbox and the authors showing its applicability to various neural network layers like fully connected, convolutional, and attention layers. My major concern is that it seems that the improvement seems marginal in most cases.  The clarity of the paper can also be improved. Hopefully, the authors can address my concern in the rebuttal period. \n\n \n##########################################################################\n\nPros: \n\n \n1. The paper develops a variant of dropout regularization which was shown to be effective in training deep neural nets in the past several years. \n \n2. The proposed modified version is novel and capture data dependency more naturally relying on aleatoric uncertainty to model the uncertainty on y given x ( less explored). The computational and memory overhead is also insignificant. It is also compatible with both Bernoulli dropout and Gaussian dropout. \n\n3.  Contextual dropout outperform alternative dropout variants and the gain is a little higher when noise is introduced \n\n\n \n##########################################################################\n\nCons: \n\n \n1. complexity an additional parameters even if not many might make it hard to tune. More studies on the hyper-parameter selection are needed especially based on the level of noise and the parameter \n\n2. More experiments on a recurrent neural network model would be helpful to evaluate the generality of contextual dropout.  For example, applying contextual dropout on LSTMS / Transformer models in language tasks or vision&language tasks like image captioning. \n\n3. contrast/comparison to Drop connect[R1], which was proposed as a generalization of dropout [R1].\n\n[R1] Regularization of Neural Networks using DropConnect\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, Rob Fergus ; Proceedings of the 30th International Conference on Machine Learning, PMLR 28(3):1058-1066, 2013\n\n4. \n\n minor\n-----------\nFig5 font is not so clear and colors can be better\nwhy gamma is between 8 and 16\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1270/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1270/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Contextual Dropout: An Efficient Sample-Dependent Dropout Module", "authorids": ["~XINJIE_FAN2", "~Shujian_Zhang1", "korawat.tanwisuth@utexas.edu", "~Xiaoning_Qian2", "~Mingyuan_Zhou1"], "authors": ["XINJIE FAN", "Shujian Zhang", "Korawat Tanwisuth", "Xiaoning Qian", "Mingyuan Zhou"], "keywords": ["Efficient Inference Methods", "Probabilistic Methods", "Supervised Deep Networks"], "abstract": "Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.", "one-sentence_summary": "We propose contextual dropout as a scalable sample-dependent dropout method, which makes the dropout probabilities depend on the input covariates of each data sample.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|contextual_dropout_an_efficient_sampledependent_dropout_module", "supplementary_material": "/attachment/b62879adb3a79ceaf96ad6170d2f7dbb3673cfb2.zip", "pdf": "/pdf/e60af0056960e7c5e82ee3d15ad9c6dd3577788d.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021contextual,\ntitle={Contextual Dropout: An Efficient Sample-Dependent Dropout Module},\nauthor={XINJIE FAN and Shujian Zhang and Korawat Tanwisuth and Xiaoning Qian and Mingyuan Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ct8_a9h1M}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ct8_a9h1M", "replyto": "ct8_a9h1M", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1270/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122552, "tmdate": 1606915796878, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1270/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1270/-/Official_Review"}}}], "count": 6}