{"notes": [{"id": "HkxYzANYDB", "original": "HJlbByruwB", "number": 1008, "cdate": 1569439248909, "ddate": null, "tcdate": 1569439248909, "tmdate": 1583912039942, "tddate": null, "forum": "HkxYzANYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "6O_zaNzS4F", "original": null, "number": 1, "cdate": 1576798712123, "ddate": null, "tcdate": 1576798712123, "tmdate": 1576800924275, "tddate": null, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The reviewers are unanimous in their opinion that this paper offers a novel approach to causal learning.  I concur.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706013, "tmdate": 1576800253934, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Decision"}}}, {"id": "rylwb13YiH", "original": null, "number": 4, "cdate": 1573662462930, "ddate": null, "tcdate": 1573662462930, "tmdate": 1573662462930, "tddate": null, "forum": "HkxYzANYDB", "replyto": "BJesxs8dYS", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thanks a lot for your helpful comments and suggestions about our manuscript. We address your specific concerns and questions below.\n\n1. Multiple choice versus fine-grained physical reasoning.\n\nEven though the tasks in CLEVRER are grounded to multiple choices on natural language inputs, identification and prediction of collision events require precise prediction of the motion trajectories, since otherwise the moving objects will miss each other. In this sense, fine-grained physical reasoning is an implicit requirement of our task. This is also shown by the fact that having a dynamics model for trajectory prediction leads to stronger results than other baselines.\n\nAlso, CLEVRER provides detailed annotations of the object motion trajectories for diagnostics and further benchmarking, based on which new evaluation metrics such as trajectory accuracy can be easily added to the dataset. \n\n2. Generation of the causal events.\n\nThe causal events in the dataset are created by a recursive process as stated in section 3.1. To further specify the details, we start with simulating the motion of one single object, and then add another object whose initial position / velocity is set such that it will lead to a collision between the two objects. We repeat this process to add more objects to collide with either or both of the outgoing objects from previous collisions. In this way, a causal relation between the collision events is established. The videos are rendered from the final simulation outputs with annotated causal structure.\n\n3. Answer distribution and question biases.\n\nThe answer distribution of descriptive questions can be found in supplementary material A (figure 6), which indicates a very well-balanced distribution. The number of correct choices versus incorrect choices in the multiple-choice questions is 1:1. As shown in Table 2, the random baseline has ~50% per-option accuracy, which suggests the number of correct and incorrect choices are balanced. \n\nWe carefully designed the question generation process to minimize biases at various levels. As also mentioned in the general response #1, for each video and question type, we sample the questions to balance the answer space. Furthermore, resampling the questions also reduces the language bias, as the answers might be statistically correlated with the length of the questions or co-occur with certain word combinations. We apply rejection sampling to maintain an uncorrelated joint distribution between the questions and answers over the entire dataset. The result of the LSTM model shows a small performance gain over the random baselines, which suggest the language bias is effectively removed. Finally, for the causal events, the motion trajectories and collision targets are fully randomized and independent to the visual attributes (color, shape, material) during the generation process of the causal graph as described above. As a result, undesired correlations are prevented and the questions have small bias.\n\n4. Model details and paper length.\n\nThanks, we will follow your suggestion to move more key details of the model from the supplementary materials to the main text.\n\n5. Extension to real-life scenarios.\n\nAs also mentioned in the general response #2, our approach of incorporating an object-centric dynamics model for physical reasoning has similar applications in robotics planning and manipulation [Janner et al. 2018] [Veerapaneni et al. 2019]. We also agree that combining the design principles of CLEVRER with GQA [Hudson and Manning 2019] is a great way of further extending our task to complex real-world scenarios. We hope to note that the tasks in CLEVRER still remain challenging to current approaches, and a stronger model on the dataset will contribute towards causal reasoning in more complex real environments.\n\n6. Further clarifications on the term \u201cneuro-symbolic\u201d\n\nThe word \u201csymbolic\u201d in our framework corresponds to symbolic program execution, during which the inferred question logic (program) is explicitly applied to the extracted motion and event traces. This process is operated on symbolic representations. In a more general sense, \u201cneuro-symbolic\u201d stands for using neural network for pattern recognition and symbolic operations for logic reasoning, which combines the best of two approaches. \n\n- Janner, Michael, et al. \"Reasoning about physical interactions with object-oriented prediction and planning.\" ICLR (2019).\n- Veerapaneni, Rishi, et al. \"Entity Abstraction in Visual Model-Based Reinforcement Learning.\" CoRL (2019).\n- Hudson, Drew A., and Christopher D. Manning. \"GQA: A new dataset for real-world visual reasoning and compositional question answering.\" CVPR (2019). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxYzANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1008/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1008/Authors|ICLR.cc/2020/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162703, "tmdate": 1576860546980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment"}}}, {"id": "Syx9iRiYsr", "original": null, "number": 3, "cdate": 1573662370285, "ddate": null, "tcdate": 1573662370285, "tmdate": 1573662370285, "tddate": null, "forum": "HkxYzANYDB", "replyto": "H1lez5_nFr", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thanks a lot for your helpful comments and suggestions about our manuscript. We address your specific concerns and questions below.\n\n1. Overcoming ill-posed and degenerate questions\n\nWe provide further details of the question generation process in our general response #1. The ill-posed and degenerate questions in CLEVR is caused by the high order logic applied to the object attributes (i.e. \u201cWhat is the color of the sphere left to the cylinder right to the green sphere?\u201d). However in CLEVRER, the logic traces of the questions span over multiple domains including object attributes, temporal order, event attributes, etc (i.e. \u201cWhat is the color of the sphere that first collides with the cylinder?\u201d). We intentionally avoid applying high-order logic within the same domain. This effectively removes the degeneracy and prevents ill-posed questions.\n\n2. Features for baseline evaluation.\n\nThanks a lot for the suggestion on improving the baselines. Following your suggestion, we conducted experiments on the CNN-based methods by replacing the pool5 feature by both the 14 x 14 and the I3D feature maps. We also replaced LSTM with a ConvLSTM to preserve spatial information. The results are summarized below:\n\n- CNN (14 x 14) + ConvLSTM:\nDescriptive: 58%\nExplanatory (per-opt / per-Q): 63.4% / 16.8%\nPredictive (per-opt / per-Q): 56.3% / 29.8%\nCounterfactual (per-opt / per-Q): 63.1% / 13.6%\n\n- I3D + ConvLSTM:\nDescriptive: 62%\nExplanatory (per-opt / per-Q): 62.2% / 23.3%\nPredictive (per-opt / per-Q): 51.8% / 36.4%\nCounterfactual (per-opt / per-Q): 58.5% / 11.3%\n\nThese methods still do not show strong performance on the causal reasoning tasks (explanatory, predictive, and counterfactual), which is consistent to our observation that incorporating a dynamics model is essential to the task.\n\n3. Model noise from Mask R-CNN misdetections.\n\nOur video frame parser is inspired by the scene parser from [Yi et al. 2018], which achieves strong performance on the CLEVR dataset. We use their open-sourced code (https://github.com/kexinyi/ns-vqa) for scene parsing on the same visual domain (CLEVRER uses the same renderer as CLEVR). In practice, Mask-RCNN performs well and misdetections are rare. On the other hand, since our neuro-symbolic framework is disentangled and operates on grounded representations, the errors caused by the misdetections are transparent and can be diagnosed.\n\nReferences:\n- Yi, Kexin, et al. \"Neural-Symbolic VQA: Disentangling reasoning from vision and language understanding.\" NeurIPS (2018).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxYzANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1008/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1008/Authors|ICLR.cc/2020/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162703, "tmdate": 1576860546980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment"}}}, {"id": "rkgwUAsKiS", "original": null, "number": 2, "cdate": 1573662287065, "ddate": null, "tcdate": 1573662287065, "tmdate": 1573662287065, "tddate": null, "forum": "HkxYzANYDB", "replyto": "SJl5uAax5B", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks a lot for your helpful comments and suggestions about our manuscript. We address your specific concerns and questions below.\n\n1. Details of question generation.\n\nAs also mentioned in the general response #1, questions in CLEVRER are generated by a multi-step procedure. For each question type, a pre-defined logic template is chosen. The logic template can be further populated by attributes that are specifically associated with the video context (i.e. the color, material, shape that identifies the object to be queried). For a given video and template, we first generate the set of all possible questions by exhaustively iterate through all possible attributes. Then we sample from the list of questions to balance answer distribution and minimize language biases. \n\n2. Generalize to real world setting.\n\nAs also discussed in the general response #2, our approach of incorporating an object-centric dynamics model for physical reasoning has similar applications in robotics planning and manipulation [Janner et al. 2018] [Veerapaneni et al. 2019]. Furthermore, incorporating temporal and causal annotations to real videos similar to the GQA dataset [Hudson and Manning 2019] is also an important direction to pursue. We also hope to note that, even within the restricted domain, the causal reasoning tasks still remain challenging for current approaches. Further studies on these tasks under a controlled environment will contribute towards building a model for real-world causal reasoning.\n\nReferences:\n- Janner, Michael, et al. \"Reasoning about physical interactions with object-oriented prediction and planning.\" ICLR (2019).\n- Veerapaneni, Rishi, et al. \"Entity Abstraction in Visual Model-Based Reinforcement Learning.\" CoRL (2019).\n- Hudson, Drew A., and Christopher D. Manning. \"GQA: A new dataset for real-world visual reasoning and compositional question answering.\" CVPR (2019). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxYzANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1008/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1008/Authors|ICLR.cc/2020/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162703, "tmdate": 1576860546980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment"}}}, {"id": "rJgTxAiKoS", "original": null, "number": 1, "cdate": 1573662196855, "ddate": null, "tcdate": 1573662196855, "tmdate": 1573662196855, "tddate": null, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank all reviewers for their constructive comments and suggestions to improve the strength and clarity of this paper. Here we address some common concerns raised by the reviewers. We will update the text of the paper to further incorporate these suggestions and fix the typos.\n\n1. Details of question generation (R1, R2, R3)\n\nQuestions in CLEVRER are generated by a multi-step procedure. For each question type, a pre-defined logic template is chosen. The logic template can be further populated by attributes that are specifically associated with the video context (i.e. the color, material, shape that identifies the object to be queried). For a given video and template, we first generate the set of all possible questions by exhaustively iterate through all possible attributes. Then we sample from the list of questions to balance answer distribution and minimize language biases. \n\n2. Relation to real-world tasks and applications (R1, R3)\n\nOur approach of incorporating an object-centric dynamics model in physical reasoning is related to applications in robotic planning and manipulation. For example, [Janner et al. 2018] and [Veerapaneni et al. 2019] have demonstrated using object-centric dynamics model for planning in real-world manipulation tasks.\n\nThere are still several steps to take towards causal reasoning in complex and noisy real environments. Along this path, CLEVRER is designed to serve as an initial step that focuses on the temporal and causal relations under a constrained visual context. The environment also supports adding richer and more realistic visual and physical entities, such as object texture, size, elasticity, friction, etc. As suggested by Reviewer 1, a video dataset similar to GQA [Hudson and Manning 2019] would be a very nice goal to aim for as the next step. \n\nWe also emphasize that the tasks in CLEVRER remain challenging in its current form. As shown in Table 2, current deep learning models do not perform well on CLEVRER, especially for counterfactual tasks: the best baseline (without a dynamics model) achieves a per-option accuracy of 63.5%, while NS-DR (with a dynamics model) achieves 74.1%. A stronger model on CLEVRER will contribute toward causal reasoning in more complex real environments.\n\nReferences:\n- Janner, Michael, et al. \"Reasoning about physical interactions with object-oriented prediction and planning.\" ICLR (2019).\n- Veerapaneni, Rishi, et al. \"Entity Abstraction in Visual Model-Based Reinforcement Learning.\" CoRL (2019).\n- Hudson, Drew A., and Christopher D. Manning. \"GQA: A new dataset for real-world visual reasoning and compositional question answering.\" CVPR (2019). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxYzANYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1008/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1008/Authors|ICLR.cc/2020/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504162703, "tmdate": 1576860546980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1008/Authors", "ICLR.cc/2020/Conference/Paper1008/Reviewers", "ICLR.cc/2020/Conference/Paper1008/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Official_Comment"}}}, {"id": "BJesxs8dYS", "original": null, "number": 1, "cdate": 1571478259325, "ddate": null, "tcdate": 1571478259325, "tmdate": 1572972524134, "tddate": null, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "A new benchmark is presented, which requires to reason on spatio-temporal data (videos) and incorporates physics (videos of physical dynamics), language (questions are formulated through language) and causality. The benchmark builds on the well-known CLEVR benchmark and adds several interesting contributions, in particular reasoning over time.\n\nCompared to other benchmarks proposed on reasoning on physical dynamics, this benchmark adds to the language component (which is presented in classical VQA benchmarks), and an interesting counterfactual component known from the causal inference literature. This is fairly new for the computer vision and statistical machine learning literature (a few papers on counterfactual reasoning exist), but in contrast to the causal inference literature, here the do-operator is observable during training: we do have access to do-interventions and even the outcome during training and therefore we can even learn counterfactuals using supervised learning. This statement is not meant as criticism, as learning counterfactuals without supervision from high-dimensional input like images seems to be currently out of a reach, or at least has not yet been demonstrated up to my knowledge.\n\nOn the downside, compared to other physical reasoning benchmarks, the answers here are multiple choice instead of regressions of the future motion, which requires more fine-grained reasoning. This will guide solutions to a certain type and will also favor solutions, requiring the network to detect certain binary concepts and combine them instead of regressing complex functions. It also favors solutions of the type presented in the paper.\n\nAs for other benchmarks, the dataset is quite large (20 000 videos) and, given its synthetic nature, is accompanied by functional programs. As for CLEVR, the simulations have been performed by a physics engine and then separately rendered with Blender to maximize visual quality. The result is a very interesting benchmark, and I have no doubt that it will be very useful for us (the learning reasoning community).\n\nAnother positive point is the number of baselines tested on the benchmark, among which we can find strong papers on VQA/VQA2\n\nI have a couple of questions:\n\nHow is the causal graph created? Are the experiments rendered and outcomes examined, creating the causal graph for the counterfactual answers, or are the experiments selected with a given outcome already decided?\n\nThe distribution of question types has been provided, but how about the biases? Distributions of the answers would have been helpful. How did the authors avoid biases during construction of the dataset, in particular in the counterfactual case (see remarks on the causal graph).\n\nThe paper also comes with a method for solving, which is very similar to existing methods on learning through functional programs. The method itself is unfortunately described only very briefly and the reader needs to look it up almost entirely in the appendix of the paper, which is surprising, as the paper is only 8 pages long instead of 10.\n\nAs for CLEVR, one of the downsides of this type of benchmark is the synthetic nature of the images and the limited range of different objects in the scene. Of course this comes with the advantage of being able to study compositional reasoning in detail, as a scene graph can be calculated easily (and is available during simulation). However, it also makes reasoning through functional programs much easier, as the proposed filters are limited in number and can strong respond to the small number of shapes and colors available in the data. I have strong doubts that this kind of approach extends to real life scenarios.\n\nFor VQA type of scenarios, GQA is a nice compromise between natural looking images and the availability of scene graphs and the restriction of questions to compositional reasoning. The optimal choice would be a similar compromise for spatio-temporal data, but of course this would be a huge effort and it would be up to impossible to have access to counterfactuals.\n\nLast point, and this question is not restricted to this paper, as the name came up elsewhere, why is the model called neuro-symbolic reasoning? While it could be argued that the questions require a sort of \u201csymbolic reasoning\u201d, I am not sure that the reasoning method itself is symbolic even partially. Other than selecting functional programs out of a discrete set, the reasoning itself is connectionist and performed with graph networks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1008/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1008/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916755365, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1008/Reviewers"], "noninvitees": [], "tcdate": 1570237743734, "tmdate": 1575916755380, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Official_Review"}}}, {"id": "H1lez5_nFr", "original": null, "number": 2, "cdate": 1571748359723, "ddate": null, "tcdate": 1571748359723, "tmdate": 1572972524101, "tddate": null, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the temporal and causal structures in videos. Specifically, the authors first introduce a new dataset called CLEVRER drawing motivation from CLEVR, a well-known visual reasoning dataset. They further evaluate a set of state-of-the-art methods on the newly introduced dataset to confirm their initial beliefs of the challenges posed by causal reasoning. Based on empirical clues, they also suggest neural-symbolic based framework for causal reasoning.\nOn the whole, I think this is a good paper and it addresses one of the most challenging and exciting tasks in visual reasoning. The introduction of the CLEVRER dataset is likely valuable for the community to facilitate research in this area. I have some concerns as follows:\n(1) As the fact that questions are generated algorithmically, I believe many of them are either ill-posed or degenerate similar to what described in the CLEVR. Did you manage to filter out those questions? Please provide more details on the question generation process.\n(2) I have a doubt on the reported results in table 2 as you extract visual features not fairly between all methods. Outputs of pool5 feature basically kill all spatial information while 14x14 feature map used as input of MAC, IEP and TbD-net keep the spatial information. I also not sure if the temporal attention is a better option than vectorizing the whole video features (Some pooling layers might be helpful) before feeding into those methods. It would be fairer to evaluate the state of the art methods on the CLEVRER more carefully.\n(3) As the number of objects in CLEVRER is limited, using class labels from Mask R-CNN may introduce noises to the model due to incorrect detections. The authors may need to explain more clearly at this point in the implementation details.\n(4) Have you tried to incorporate flow features, for example, C3D/I3D feature?\n\nMinor comments:\nThere are some typos in the paper in both main paper and supplementary document -causal vs. casual. Please fix these."}, "signatures": ["ICLR.cc/2020/Conference/Paper1008/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1008/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916755365, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1008/Reviewers"], "noninvitees": [], "tcdate": 1570237743734, "tmdate": 1575916755380, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Official_Review"}}}, {"id": "SJl5uAax5B", "original": null, "number": 3, "cdate": 1572032114467, "ddate": null, "tcdate": 1572032114467, "tmdate": 1572972524062, "tddate": null, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "invitation": "ICLR.cc/2020/Conference/Paper1008/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Authors propose a new Dataset CLEVRER, a simulated video dataset involving interaction between objects. It is discussed, the existing state-of-the-art models for visual question answering, doesn\u2019t capture the causal structure between the objects and their claim is supported by their experiments.  Authors also proposed a model which captures the dynamics of the objects involved in the video, through experiments they have shown their model performs better than the existing models. \n\nThe CLEVRER dataset is designed to test a model capability to answer the queries involving causal relationship between the objects involved in the video. \n\nDetails.\nThis work begins with a well motivated problem by pointing out the drawback in existing VQA(Visual Question Answering)  models, that existing works focus on visual and input language patterns to answer the queries and doesn\u2019t tackle the task involving causal structure.  It  explores the current literature around the problem related to visual question answering(both real world data and simulated data). Through experiments they have shown the existing state-of-the-art work on visual question answering doesn\u2019t perform well on the dataset CLEVRER. \n\nTo prove the incompetence of existing models to capture causal structure, authors designed an artificial dataset with questions which can be answered only when the model is capable of capturing the causal structure between the objects.  \n\nThe process involving the creation of CLEVRER dataset is well explained. But, it is unclear how the questions are generated. \n\nThrough experiments authors revealed the drawbacks of existing models on capturing the dynamics between the objects and  proposed a model which is said to be inspired by previous VQA[1] model. An important modification by incorporating neural dynamics predictor module to the existing model is key, and also achieves good performance on the dataset. \n\nComments:\n\n- The paper is well written, but it is unclear how the questions are generated during dataset creation process.\n- The main contribution of the paper is to show the incompetence of existing models to capture dynamics. Which is a form of analysis.\n- It is shown that,  learning dynamics of the objects the model can achieve better performance.\n- This dataset is created in more restricted environment like height of the objects should be same. How can this be generalized to a more real world setting ?\n\nSome minor issues:\nIn few places causal is misspelled as casual(page 2,8).\nEquation 2, in the appendix the subscripts are not proper. \n\n[1] Yi, Kexin, et al. \"Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.\" Advances in Neural Information Processing Systems. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1008/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1008/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kyi@g.harvard.edu", "ganchuang1990@gmail.com", "liyunzhu@mit.edu", "pushmeet@google.com", "jiajunwu@mit.edu", "torralba@mit.edu", "jbt@mit.edu"], "title": "CLEVRER: Collision Events for Video Representation and Reasoning", "authors": ["Kexin Yi*", "Chuang Gan*", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "pdf": "/pdf/5c16bdbb2bb062be7c809cb259b10585a45dec5b.pdf", "TL;DR": "We present a diagnostic dataset for systematic study of temporal and casual reasoning in videos. ", "abstract": "The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes four types of question:  descriptive (e.g., \u2018what color\u2019), explanatory (\u2018what\u2019s responsible for\u2019), predictive (\u2018what will happen next\u2019), and counterfactual (\u2018what if\u2019).  We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations. ", "keywords": ["Neuro-symbolic", "Reasoning"], "paperhash": "yi|clevrer_collision_events_for_video_representation_and_reasoning", "code": "http://clevrer.csail.mit.edu/", "_bibtex": "@inproceedings{\nYi*2020CLEVRER:,\ntitle={CLEVRER: Collision Events for Video Representation and Reasoning},\nauthor={Kexin Yi* and Chuang Gan* and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B. Tenenbaum},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxYzANYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1a880a42da32cdc43cfc8f4d5fdb95c1c1236450.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxYzANYDB", "replyto": "HkxYzANYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916755365, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1008/Reviewers"], "noninvitees": [], "tcdate": 1570237743734, "tmdate": 1575916755380, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1008/-/Official_Review"}}}], "count": 9}