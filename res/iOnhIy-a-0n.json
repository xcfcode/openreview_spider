{"notes": [{"id": "iOnhIy-a-0n", "original": "1TEBsimeWD7", "number": 2336, "cdate": 1601308257578, "ddate": null, "tcdate": 1601308257578, "tmdate": 1615997234914, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "PxzShiCY6Te", "original": null, "number": 1, "cdate": 1614959828960, "ddate": null, "tcdate": 1614959828960, "tmdate": 1614959828960, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "wODx4Ja00uS", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Comment", "content": {"title": "Energy variance reduction accelerates the exponential convergence", "comment": "We appreciate your suggestions.\n\nWe tried to train ResNet20 on CIFAR100 based on VR-SGHMC with variance-reduced gradients and the default learning rate and found that it performs even worse than the vanilla SGHMC. We suspect that the learning rate is well-tuned for SGHMC but sub-optimal for VR-SGHMC. To improve the performance, VR-SGHMC requires the additional tuning of learning rates, while ours does not.\n\nAs to the optimization speed, we tried to run reSGHMC and cycSGHMC on ResNet32 with the same time budget as VR-reSGHMC and obtained around 76.9\\%$\\pm$0.3\\% and 77\\%$\\pm$0.2\\% accuracy, respectively, which is still not better than 77.4\\%$\\pm$0.3\\% based on our method. This shows the potential of energy variance reduction in improving the optimization speed. We conjecture that the reason why energy variance reduction works well is it not only reduces the discretization error in the swaps (and accepts larger swapping intensity) but also accelerates the exponential convergence."}, "signatures": ["~Wei_Deng2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Wei_Deng2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iOnhIy-a-0n", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2336/Authors|ICLR.cc/2021/Conference/Paper2336/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649465393, "tmdate": 1610649465393, "id": "ICLR.cc/2021/Conference/Paper2336/-/Comment"}}}, {"id": "wODx4Ja00uS", "original": null, "number": 1, "cdate": 1610040493446, "ddate": null, "tcdate": 1610040493446, "tmdate": 1610474099545, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This work aims at doing Bayesian inference via Langevin dynamics with data subsampling. This builds on previous work with \"replica exchange\" where parallel chains are run at different temperatures and can be swapped to encourage moving between modes. The main technical novelty here is a scheme to reduce variance. This is done in the style of SGRD by periodically computing the gradient on all data and then using those values as control variates. This is shown to reduce variance.\n\nReviewers generally felt that this represented a sensible combination of known ideas aimed at an important and timely problem with sufficient empirical evaluation. There was consensus the paper was clearly written. I concur that even if the combination is \"expected\" to work, the presence of guarantees for performance represent sufficient technical novelty. I particularly applaud the fact that the paper does not over-claim and generously gives credit to related work. This is helpful to the reader and encourages the flow of ideas. For these reasons I recommend acceptance of the paper.\n\nIn reading the paper, I had a couple questions about the experiments:\n\n1. It's not obvious to me from the experiments how specific the method is to the replica exchange setting. The main control variate idea appears to be applicable without replica exchange. I would very much like to see a \"VR-SGHMC\" row in Table 1 unless there is a good reason that this cannot be done. It would be very beneficial to understand the contributions of these different algorithmic components.\n\n2. The CIFAR experiments directly test variance. That's fine, the paper is aimed at reducing variance, after all. However, I would like to see more tests of the follow-on improvements in optimization speed. It has been my experience that improvements in variance sometimes produce surprisingly small improvements in optimization speed. My intuition for this is that reduced variance mostly helps by making it possible to use a larger step-size without the same penalty in the stationary dist. In practice, the step-size typically ends up being imperfect, meaning that changes in variance have small changes.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040493431, "tmdate": 1610474099528, "id": "ICLR.cc/2021/Conference/Paper2336/-/Decision"}}}, {"id": "Z2Fy7TcLbz", "original": null, "number": 6, "cdate": 1605582692360, "ddate": null, "tcdate": 1605582692360, "tmdate": 1605583708783, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "c_35NgVa005", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment", "content": {"title": "Hyper-parameters, multiple temperatures, and others", "comment": "We appreciate the detailed and valuable comments.\n\nQ1: How much tweaking do these parameters such as $F, m, n, \\eta, \\gamma, \\tau$ require?\n\n$F$ is an important hyperparameter and the tuning directly affects the empirical swapping rates. We would like to study the extension of a more user-friendly replica exchange algorithm directly based on the target swapping rates in the future.\n\nThe other hyperparameters don't require much tuning. The hyperparameter settings of $n, \\eta$ and $\\tau^{(1)}$ can be naturally imported from SGD and SGHMC. $\\gamma$ is a smoothing factor and can be fixed at $0.3$ by default. The update frequency $m$ can be chosen in the order of batch size $n$ and we can try $\\tau^{(2)}$ in $\\{3\\tau^{(1)}, 10\\tau^{(1)}, 30\\tau^{(1)}, 100 \\tau^{(1)}\\}$. \n\nQ2: Consider more than two temperatures?\n\nYes, it is natural to study the population version with many temperatures. However, the naive extension may lead to little communication between the chains at the two ends of temperatures, which hence deteriorates the performance. To solve this issue and improve the efficiency of parallel tempering, we may consider optimizing the temperature intervals [1], adapting the number of sweeps to the canonical autocorrelation time [2], and exploiting the even-odd swapping scheme to optimize the round trip rate [3].\n\nWe leave the systematic study of population parallel tempering with the minimal possible hyperparameters in the future.\n\n\n[1] Katzgraber, etc. Feedback-optimized Parallel Tempering Monte Carlo. J. Stat. Mech. 2006\n\n[2] Bittner, etc. Make Life Simple: Unleash the Full Power of the Parallel Tempering Algorithm. Phy. Rev. Let. 2008.\n\n[3] Syed, etc.  Non-Reversible Parallel Tempering: A Scalable Highly Parallel MCMC scheme. arXiv:1905.02939v2, 2019.\n\nQ3: Define acceleration (relative to what?) and what do you mean by \"exponentially faster\" on page 5, compared to reSGLD? \n\nThe acceleration means to accelerate the exponential convergence of the continuous-time process of the standard replica exchange SGLD. We have rephrased the words in the revised paper to make the statement clear and rigorous.\n\nQ4: What is \"the crude estimate\"? The upper bound provided by Gronwall's inequality?\n\nYes.\n\nQ5: \"The underlying Dirichlet form ...\" is a bit obscure and what is the $\\cal E$\n\nWe have included discussions of the Dirichlet form in the preliminary section and also linked it to Theorem 1.\n\nQ6: Page 3: The sign of $c^{\\star}$ is not correct and the denominator should be $\\mathrm{Var}(B|\\widehat \\beta^{(h)})$. Page 4: The update rule for $\\tilde \\sigma_k^2$ is unclear.\n\nThanks for pointing out the typos. We have corrected them in the revised paper. The code implementation was not affected.\n\nQ7: Page 4, Algorithm 1: How to set $\\gamma$? \n\n$\\gamma$ is a smoothing factor to filter out high-frequency noise. A larger $\\gamma$ captures the trend better but is less robust. Empirically, we can set $\\gamma=0.3$ by default.\n\nQ8: Page 6: Why does Theorem 3 imply a much larger swapping rate?\n\nWe acknowledge that a much larger swapping rate should better be stated after Lemma 2. We have rephrased the comments in the revision.\n\nQ9: Page 6: You set $\\tau^{(1)}=10$ to avoid peaky modes, but it would be interesting to see the performance of VR-reSGLD for a sharply peaked posterior (high-dimensional parameter spaces and a large number of data).\n\nExtension of $\\tau^{(1)}$ to a different value such as 1 is straightforward. As shown in section G in the appendix of the revised paper, a similar conclusion still holds. However, the posterior may not be always peaked in high-dimensional big data problems. For example, as indicated by some discussions [1,2,3], sharply peaked optima may generalize worse than wide optima. \n\n\n[1] Keskar, etc. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ICLR'17.\n\n[2] Chaudhari, etc. Entropy-SGD: Biasing Gradient Descent into Wide Valleys. ICLR'17.\n\n[3] Li, etc. Visualizing the Loss Landscape of Neural Nets. NeurIPS'18.\n\nQ10: Page 7: Why do you anneal $\\tau^{(1)}$ in the CIFAR experiment?\n\nWe annealed $\\tau^{(1)}$ because it could provide roughly a 0.3\\% improvement on the predictions of CIFAR100. In the code we attached in the supplementary file, we tried a slightly new strategy and only annealed the temperature during the warm-up period, the performance was almost the same as before and no annealing was conducted during the Bayesian-model-averaging period. We will give a comprehensive study of this strategy in the next revision.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iOnhIy-a-0n", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2336/Authors|ICLR.cc/2021/Conference/Paper2336/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849591, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment"}}}, {"id": "_-e7_fOqZpo", "original": null, "number": 5, "cdate": 1605580472606, "ddate": null, "tcdate": 1605580472606, "tmdate": 1605583648168, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "xx78WKR2Rzl", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment", "content": {"title": "The role of the thining factor T and why different values of F are used", "comment": "We appreciate the valuable comments.\n\nQ1: What is the role of the thinning factor T? Is it involved in the convergence given by Theorem 3?\n\nThe main purpose of a thinning factor is to avoid a cumbersome system and extensive computations in Bayesian model averaging and the reduced computations yield an improved statistical efficiency [1]. It is not directly related to the convergence guarantee given by Theorem 3.\n\n[1] Owen. Statistically efficient thinning of a Markov chain sampler. Technical Report. 2015.\n\nQ2: For the synthetic experiments, why different values of F are used (1 for VR-reSGLD and 100 for reSGLD)?\n\nIt is ideal to set F=1 if there are enough swaps. However, such a choice leads to no swaps for the standard reSGLD in the synthetic experiment, hence a larger $F$ is applied to reSGLD and only leads to several swaps. By contrast, VR-reSLD doesn't have this issue due to the significant reduction of variance and setting F=1 still yields sufficiently many swaps.\n\nQ3: Thm 1: E is not defined in the main text.\n\nWe have included discussions of the Dirichlet form in the preliminary section and also linked it to Theorem 1."}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iOnhIy-a-0n", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2336/Authors|ICLR.cc/2021/Conference/Paper2336/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849591, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment"}}}, {"id": "f-S3Yf8FhJz", "original": null, "number": 4, "cdate": 1605579604065, "ddate": null, "tcdate": 1605579604065, "tmdate": 1605583178596, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "5R9M62gbVow", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment", "content": {"title": "Software implementation and novelty", "comment": "We appreciate the valuable comments.\n\nQ1: I would highly appreciate that the authors would create and share a minimal implementation.\n\nWe have attached the code for section 5.2 in the supplementary file. We will release all of them in the final version. \n\nQ2: Limited novelty in variance reduction.\n\nWe acknowledge the main variance reduction method is quite standard, which, however, suffers from an insufficient reduction of variance when the learning rate is large and the correlation is weak. To handle this issue, we further proposed the adaptive variance-reduced replica exchange SGLD in Algorithm 2 in the appendix to adaptively estimate the unknown optimal correlation. As a result, we obtained around 40% improvement in variance reduction in the early stage. \n\nWe leave the study of more powerful variance reduction techniques, such as multiple reference points/ control variates [1], lightweight SAGA in the future.\n\n[1] Dongruo Zhou, etc. Stochastic Nested Variance Reduction for Nonconvex Optimization. JMLR'19.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iOnhIy-a-0n", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2336/Authors|ICLR.cc/2021/Conference/Paper2336/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849591, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment"}}}, {"id": "Lbt6UaQMsKo", "original": null, "number": 3, "cdate": 1605578519255, "ddate": null, "tcdate": 1605578519255, "tmdate": 1605583022290, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "xmyKe1d0JQW", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment", "content": {"title": "Setting m in the order of  n gives a good empirical performance", "comment": "We appreciate the valuable comments.\n\nQ1: How to tune $m$?\n\nThe key to tuning $m$ is to balance between acceleration and cost. For example, choosing $m$ in the order of $n$ gives a good empirical performance on CIFAR10 and CIFAR100 datasets. Further reducing $m$ may yield a diminishing marginal utility with a high cost in gradient evaluation. \n\nQ2: Does $m=1$ work in variance reduction?\n\nYes. In such a case, variance reduction works perfectly because the exact energy is used in each iteration. Nevertheless, the computational cost may be unacceptable. \n\nQ3: Why Johnson and Zhang [1] suggest a sufficiently large $m$?\n\nWe focus on different theoretical aspects. [1] aims to prove a geometric convergence in convex optimization for a large value of $m$; our work concentrates on the variance reduction of energy estimators to propose more effective swaps to accelerate the exponential convergence of the continuous-time dynamics in non-convex learning. Similar theoretical results that a smaller $m$ yields a smaller variance have been obtained in Theorem 3 [2].\n\nTo be more specific, Theorem 1 of [1] only works in a manner of every $m$ iterations. The second inequality in page 5 [1] is essentially $\\mathop{\\mathbb{E}}[P(w_{k+m})-P(w_{\\star})]\\leq \\alpha \\mathop{\\mathbb{E}}[\\left(P(w_{k})-P(w_{\\star})\\right)]$. In the extreme case $m=\\infty$, the theorem only guarantees that the output after an infinite number of iterations is better than the initial point.\n\n[1] Johnson and Zhang. Accelerating Stochastic Gradient Descent using Predictive Variance reduction. NIPS'13.\n\n[2] Dubey, etc. Variance Reduction in Stochastic Gradient Langevin Dynamics. NIPS'16.\n\nQ4: In Sec5.2, the author conjectured that the effect of variance reduction becomes significant because the learning rate is decreased. I think the proposed variance reduction is significantly affected by the step size and using the constant might not be a good idea.\n\nOur experimental result agrees with the theoretical analysis in Lemma 1, which says that a smaller learning rate leads to a better variance reduction given the same update frequency $m$ and batch size $n$. Using a constant $m$ is quite standard in variance reduction, although we acknowledge the potential benefit of an adaptive $m$ in improving the performance of the proposed algorithm. We would like to study the extension of an adaptive $m$ in the future."}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "iOnhIy-a-0n", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2336/Authors|ICLR.cc/2021/Conference/Paper2336/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849591, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Comment"}}}, {"id": "c_35NgVa005", "original": null, "number": 1, "cdate": 1603712985151, "ddate": null, "tcdate": 1603712985151, "tmdate": 1605024235319, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review", "content": {"title": "Paper could be accepted after a thorough review. ", "review": "# Summary\n\nThe paper presents VR-reSGLD, a method to accelerate replica exchange stochastic gradient Langevin diffusion (reSGLD), which has been proposed recently to tackle non-convex learning problems. reSGLD suffers from two major sources of error resulting in low swapping rates: minibatch noise and the discretization error of Langevin diffusion. The idea of the paper is to use control variates to reduce the variance of energy estimators and thereby improve the swapping rate (which should lead to an accelerated convergence). Unlike previous modifications of SGLD, the variance reduction proposed in the paper aims at improving the energy estimators rather than the gradient estimators. The paper presents non-asymptotic results backing the intended acceleration of the Markov jump process. Numerical experiments illustrate the performance gain achieved by VR-reSGLD. These tests include a one-dimensional example (learning the component mean of a bimodal mixture of Gaussians) and Bayesian training of DNNs based on CIFAR imaging data. \n\n# Assessment\n\nOverall, I think the paper could be accepted in principle, but requires a thorough revision. \n\nreSGLD has been proposed quite recently (ICML 2020); and the use of variance reduction (VR) techniques is common to account for minibatch noise. So the combination of reSGLD and VR seems like an obvious idea. However, the current paper adds more than just a minor modification of reSGLD in that it provides interesting theoretical results and illustrates the validity of these results by numerical experiments. \n\n## Pros\n\n1. Convergence in 2-Wasserstein distance, no asymptotic normality assumed.\n\n2. Significant acceleration, tighter discretization error.\n\n3. Improved uncertainty quantification.\n\n## Cons\n\n1. The theoretical part of the paper is hard to follow for a non-expert.\n\n2. There are quite a number of algorithmic parameters (e.g. $m$, $n$, $\\eta$, $\\gamma$, $F$, etc.) that seem to require some tweaking. \n\n3. The improvement presented on imaging data (Table 1) seems to be rather marginal. \n\n# Comments and Questions:\n\n- How much tweaking do parameters such as $F$ etc. require? Can you provide some intuition on how to set these parameters? \n\n- Have you considered learning rates $\\eta_k$ that depend on the temperature?\n\n- Have you considered using more than two temperatures? \n\n- Please define \"acceleration\" (relative to what?)\n\n- Page 2: \"To avoid the crude estimate, ...\" is unclear. What is \"the crude estimate\"? The upper bound provided by Gronwall's inequality?\n\n- Page 3: The sentence starting with \"The underlying Dirichlet form ...\" is a bit obscure. Please try to clarify. \n\n- Page 3: Regarding the constant $c$ achieving minimum variance: Is the sign of $c^\\star$ correct? It seems that the denominator should be $\\text{Var}(B|\\hat\\beta^{(h)})$ rather than $\\text{Var}(B|\\beta^{(h)})$. \n\n- Page 4, Algorithm 1: How to set $\\gamma$? The update rule for $\\tilde \\sigma_k^2$ is unclear to me. What is $\\sigma_k^2$? Should this be $\\sigma^2$?\n\n- Page 5, Theorem 1: What do you mean by \"exponentially faster\", compared to reSGLD? What are the $\\mathcal{E}$s? \n\n- Page 6: Your comment after Theorem 3 (\"This theorem implies ...\"). Why does Theorem 3 imply a much larger swapping rate?\n\n- Page 6: You set $\\tau^{(1)}=10$ \"to avoid peaky modes\", but it would be interesting to see the performance of VR-reSGLD for a sharply peaked posterior (which is more realistic when thinking about high-dimensional parameter spaces and a large number of data...)\n\n- Page 7: Why do you anneal $\\tau^{(1)}$ in the CIFAR experiment?\n\n# Minor:\n\n## Typos\n\n- Replace \"Gr\u00f6wall\" with \"Gr\u00f6nwall\" or \"Gronwall\"\n\n## Grammar and wording\n\n* In \"obtain the state-of-the-art results\" delete \"the\"\n\n* Try to improve: \"the noisy energy estimators in mini-batch settings render the naive implementation a large bias...\"\n\n* Page 2: \"it may cause the process to stuck in ...\"\n\n* There a probably more problematic phrases. Please improve the quality of writing. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098699, "tmdate": 1606915798371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2336/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review"}}}, {"id": "xx78WKR2Rzl", "original": null, "number": 2, "cdate": 1603814062369, "ddate": null, "tcdate": 1603814062369, "tmdate": 1605024235253, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review", "content": {"title": "Nice variant of reSGLD using Variance Reduction on energy estimators", "review": "The authors propose a variant of the Replica Exchange Stochastic Gradient Langevin Dynamics (reSGLD) for non log-concave sampling by using a variant reduction technique on the estimation of the swapping rate. Assuming that the log-density is a finite sum. the authors apply classical variance reduction techniques to the energy estimator necessay to compute the swapping rate. They show that applying such technique yields a higher swapping frequency and faster convergent rate of both the continuous time SDE and its dicretization scheme. Finally, the authors perform numerical experiments on both synthetic and real world data, and show that VR indeed reduces the variance of energy estimator by several orders of magnitude, hence inducing faster convergence.\n\nIn the algorithm, what is the role of the thining factor T? Is it involved in the convergence guarantee given by Theorem 3?\n\nFor the synthetic experiments, can you explain this choice for different values of F (1 for VR-reSGLD and 100 for reSGLD)? I can believe that variance reduction lkeads to a different bias/variance trade-off, but it would be good to explain whythese values of F were used.\n\nThm 1: Operator E is not defined in the main text (only in the appendix).", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098699, "tmdate": 1606915798371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2336/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review"}}}, {"id": "xmyKe1d0JQW", "original": null, "number": 4, "cdate": 1603933295873, "ddate": null, "tcdate": 1603933295873, "tmdate": 1605024235192, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review", "content": {"title": "Proposed the control variates based variance reduction technique for replica SG-MCMC. The analysis seems new.", "review": "##  Summary of the paper\nThis paper extends the replica stochastic gradient MCMC by incorporating the new variance reduction technique. The author analyzed the non-asymptotic theoretical behavior of the proposed method, which shows the proposed method provides a better energy landscape.\n\n## Strong and weak points of the paper\n### Strong points\n- Although the applied variance reduction technique is not new, the analysis of the swapping rate in Lemma 2 seems novel compared to the past replica-exchange MCMC work.\n- Provided an asymptotic analysis for the variance reduction in Theorem 4.\n\n### Weak points \n- Proposed variance reduction is not new, I think. It is very similar to the standard control variates methods, which had been extensively applied into MCMCs and other machine learning tasks.\n\n## Rating\n- Clarity: Well written, easy to read, although I did not check the proofs in detail.\n- Correctness: I did not check all the proof in detail.\n- Novelty: The idea seems not novel but the swapping rate analysis seems new and interesting.\n\n## Comments and Questions\n- Q) All the experimental results and theoretical analysis suggest that using a smaller $m$ is better. But how can we tune this $m$? Does this variance reduction work even I use $m=1$?  But I also think that sufficiently large $m$ is required to estimate the control covariate appropriately as the work of  Rie Johnson and Tong Zhang suggest in their paper. Does this intuition wrong?\n\n- Q) In Sec5.2, as for Fig.3 c and d, the effect of variance reduction becomes significant after the sufficient epochs and the author conjectured that this is because that the learning rate is decreased. I think that this suggests that the proposed variance reduction is significantly affected by the step size which changes during the exploration stage in SG-MCMCs. And thus I thought that using the constant $m$ during the exploration stage might not be a good idea.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098699, "tmdate": 1606915798371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2336/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review"}}}, {"id": "5R9M62gbVow", "original": null, "number": 3, "cdate": 1603898723302, "ddate": null, "tcdate": 1603898723302, "tmdate": 1605024235132, "tddate": null, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "invitation": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review", "content": {"title": "Interesting, well written, limited originality", "review": "Accelerating convergence of replica exchange stochastic gradient mcmc via variance reduction\n\nSummary:\n\nThe paper presents a variance reduction technique to achieve more efficient swaps in replica exchange stochastic gradient Langevin dynamics MCMC. The paper provides detailed analysis of the method as well as empirical evaluation on some standard deep learning tasks.\n\nPositive:\n\n1. Overall I would say that the paper is well written and and it is fairly easy to follow the presentation and details in the derivations.\n2. The topic is very timely and the method appears to be very useful. As an attractive method for minibatched Bayesian inference, stochastic gradient Langevin Dynamics samplers are of high interest, but tuning the algorithm can be somewhat finicky in my experience. Replica exchange is sometimes extremely useful, and finding good defaults for these types of methods is important.\n3. Experimental validation is reasonable (although a bit limited) and the methods chosen for comparison are resonable.\n4. A comprehensive set of appendices are included to provide further details. Although I did not go through the appendices in detail, I find it appealing that further information is provided for readers wishing to apply these methods in practice.\n\nNegative:\n\n1. The authors do not provide a reference software implementation. This makes it more difficult for readers to verify the results and might limit the impact of the paper. I would highly appreciate that the authors would create and share a minimal implementation.\n2. The novelty / originality is limited: A well known type of variance reduction applied in a new way/context where it makes perfect sense though.\n\nRecommendation:\n\nGood paper. Accept.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2336/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2336/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction", "authorids": ["~Wei_Deng1", "qif@usc.edu", "georgios.karagiannis@durham.ac.uk", "~Guang_Lin1", "~Faming_Liang1"], "authors": ["Wei Deng", "Qi Feng", "Georgios P. Karagiannis", "Guang Lin", "Faming Liang"], "keywords": ["variance reduction", "replica exchange", "parallel tempering", "stochastic gradient Langevin dynamics", "uncertainty quantification", "change of measure", "generalized Girsanov theorem", "Dirichlet form", "Markov jump process"], "abstract": "Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein ($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.", "one-sentence_summary": "We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|accelerating_convergence_of_replica_exchange_stochastic_gradient_mcmc_via_variance_reduction", "pdf": "/pdf/c077f043fe1bbbb4d720b3fb0fbe7afe580d8374.pdf", "supplementary_material": "/attachment/fad607e4dd1e8e3de579ac68522cfce35c96204d.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021accelerating,\ntitle={Accelerating Convergence of Replica Exchange Stochastic Gradient {\\{}MCMC{\\}} via Variance Reduction},\nauthor={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=iOnhIy-a-0n}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "iOnhIy-a-0n", "replyto": "iOnhIy-a-0n", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2336/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538098699, "tmdate": 1606915798371, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2336/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2336/-/Official_Review"}}}], "count": 11}