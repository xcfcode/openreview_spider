{"notes": [{"id": "IhUeMfEmexK", "original": "bWs6nVY3Qhqw", "number": 1145, "cdate": 1601308128698, "ddate": null, "tcdate": 1601308128698, "tmdate": 1614985736592, "tddate": null, "forum": "IhUeMfEmexK", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8V8JZzJyQ4V", "original": null, "number": 1, "cdate": 1610040401711, "ddate": null, "tcdate": 1610040401711, "tmdate": 1610473997785, "tddate": null, "forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "invitation": "ICLR.cc/2021/Conference/Paper1145/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents a knowledge distillation method for face recognition, by inheriting the teacher\u2019s classifier as the student\u2019s classifier and optimizing the student model with advanced loss functions. It received comments from three reviewers: 1 rated \u201cOk but not good enough - rejection\u201d, 1 rated \u201cMarginally below\u201d and 1 rated \u201cMarginally above\u201d. The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition. During the rebuttal, the authors made efforts to response to all reviewers\u2019 comments. However, the rating were not changed. The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work. Therefore, this paper can not be accepted at its current state.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "tags": [], "invitation": {"reply": {"forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040401697, "tmdate": 1610473997769, "id": "ICLR.cc/2021/Conference/Paper1145/-/Decision"}}}, {"id": "5hxxw4iuUK4", "original": null, "number": 4, "cdate": 1605679576404, "ddate": null, "tcdate": 1605679576404, "tmdate": 1605684698794, "tddate": null, "forum": "IhUeMfEmexK", "replyto": "X4hUCAzpu9", "invitation": "ICLR.cc/2021/Conference/Paper1145/-/Official_Comment", "content": {"title": "Thanks for your careful and valuable comments. We will explain your concerns point by point. ", "comment": "Thanks for your careful and valuable comments. We will explain your concerns point by point. \n\n**Q1:**\nThe novelty is limited. Directly inheriting the teacher\u2019s classifier is a common strategy in the face recognition community, which can be found in Ref.1. \n\n**A1:**\nThank you very much for pointing out the related work Ref. 1 that we missed. In Ref. 1, they directly copy and fix the weights of the margin inner-product layer of the teacher model to the student model to train the student model combined with distillation loss. And the motivation of Ref. 1 is the student model can be trained with better pre-defined inter-class information from the teacher model. \n \nDespite the similarity of coping the weight from teacher model, we summaries the following novel contributions that make our work differ from Ref. 1 and existing works:\n1. We analyze the shortcomings of existing knowledge distillation methods. Specifically, the existing methods target optimizing the proxy task rather than the target task; and they cannot conveniently integrate with advanced large margin constraints to further lift performance. These valuable analyses and observations are not found in Ref. 1 and other existing works.\n2. Strong motivation and the physical explanation of the proposed ProxylessKD is well explained in our work. Fig 1 and corresponding analysis explained why ProxylessKD can achieve better performance than the existing methods that optimize the proxy task. Such in-depth analysis and strong physical explanation are novel and cannot be found in Ref. 1 and other existing works. We believe these novel findings and the proposed solution are valuable to the face recognition community and will inspire researchers in related fields. \n3. Solid experiments are designed and conducted to justify the importance of directly optimize the final task rather than the proxy task when doing knowledge distillation. And the properties of ProxylessKD about using different margin-based loss function and hyper-parameters are well examined.(See. Sec 4.2) These detailed analyses about ProxylessKD cannot be found in Ref. 1 and other existing works.\nWe believe the above important differences and novel contributions make our work differs from Ref. 1 and existing works. Thank you for point out the missing related work, we have added it to the related work with more detailed discussions.\n\n**Q2:**\nThe experiment lacks comparison with the general knowledge distillation methods (Ref.2) in image classification and the specific used methods (Ref.3) in face recognition. \n\n**A2:**\nIn face recognition systems, embeddings extracted from different models are required to be mapped into the same embedding space, so that similarity comparison can be conducted across different devices and models in different versions. However, general KD methods only force the student to mimic the teacher\u2019s behavior and thus cannot ensure the embedding space alignment. Ref.2 and Ref.3 belong to the above category and thus cannot meet the requirement for similarity comparison across different models for real-world face recognition systems, and thus not compared."}, "signatures": ["ICLR.cc/2021/Conference/Paper1145/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IhUeMfEmexK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1145/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1145/Authors|ICLR.cc/2021/Conference/Paper1145/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863133, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1145/-/Official_Comment"}}}, {"id": "ACNWC_aYWzk", "original": null, "number": 2, "cdate": 1605621439537, "ddate": null, "tcdate": 1605621439537, "tmdate": 1605681175073, "tddate": null, "forum": "IhUeMfEmexK", "replyto": "MrSB61DPjh", "invitation": "ICLR.cc/2021/Conference/Paper1145/-/Official_Comment", "content": {"title": "Thanks for your careful and valuable comments. We will explain your concerns point by point.", "comment": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\n**Q1:**\nwhy the experiments make the setting of templates using teacher model to extract feature, while the query using student model to extract feature? \n\n**A1:**\nWe've designed two experiment settings:\n1. single model mode: both using student model to extract feature\n2. multiple model mode: templates using teacher model to extract feature, while the query using student model to extract feature\n\nThe first setting is a common setting for evaluating the face recognition of the student model; The second setting aims to check if the student's embedding space is perfectly aligned with the teacher's embedding space, which is very important for making querying across templates extracted between different models possible.\n\nNote that: querying across templates extracted between different models is very important in the practical application of face recognition, for two reasons: 1) ensure feature consistency between model updates; 2) the embeddings extracted from different devices (e.g. smart security camera, mobile phone, CPU Server, GPU Server) with different models can be directly compared.\n\n**Q2:**\nWould the comparison of using student model for extracting both template and query feature be possible?\n\n**A2:**\nIt\u2019s sure. We also use the student model to extract both template and query features to illustrate the performance in Table 5, 6, 7 of the paper. E.g. \u201cProxylessKD-s\u201d. However, we mainly focus on the setting that the template features are extracted by teacher model and the query features are extracted by student model, which is more meaningful for practical application.\n\n**Q3:**\nIt can provide a direct comparison to other methods, i.e. ArcFace trained using ResNet18 compared to the student model with ResNet18.\n\n**A3:**\nIn Table 5,6,7, \u201cStudent\u201d is the \u201cArcFace trained using ResNet18\u201d and \u201cProxylessKD-s\u201d can be viewed as \u201cthe student model with ResNet18\u201d.  The experiments also show that our proposed ProxylessKD is an effective distillation method.\n \n**Q4:**\nCurrent experiments lack the comparison to the state-of-the-art methods, i.e. ArcFace and CosFace.\n\n**A4:**\nIn Table 5,6,7, \u201cStudent\u201d is the \u201cArcFace trained using the same backbone, which is significantly lower than our proposed method. Besides, models trained directly using Arcface (or CosFace) cannot ensure the embedding spaces are well aligned, and thus querying across templates extracted from different devices and models is impossible.\n\n**Q5:**\nthe ablation is emphasizing on the ProxylessKD combining with different losses. It does not consider the knowledge distillation itself. For example, when ProxylessKD is combined with the proxy task, i.e., feature distillation loss, how would it perform compared to only ProxylessKD? \n\n**A5:**\nMulti-task learning (i.e. combine proxy task and proxyless task) is an interesting topic to explore but not the core of our work. We agree that it would interesting to see if L2KD will give a further performance boost for ProxylessKD. Thanks for the suggestion, we are running this experiment and will add it.\n\n**Q6:**\nMeanwhile, in many KD papers, there are also intermediate layer feature distillation, would it harm the overall performance under this paper's setting? It needs sufficient analysis to justify the authors' choice of only applying the teacher model's classifier as distillation. \n\n**A6:**\nThe main contribution of this work is to point out the drawbacks of optimizing the proxy task rather than the target task when transferring knowledge from teacher models and present an inspiring method that can directly optimize the target task. We justify our claims under the most commonly adopted experiment settings and focusing on the face recognition task. We do not consider intermediate layer feature distillation, because: 1) it is rarely used for face recognition; 2) we couldn't find a strong baseline to follow. 3) embedding space cannot be well aligned for querying across different models\n\n**Q7:**\nIn ablation, how would the number of teachers influence the student performance?\nMeanwhile, how would the network architecture influence the student performance? i.e., fixing the teachers to be the same, while varying student architecture with multiple hypothesis, i.e., ResNet, AttentionNet, DenseNet? It is good to know what specific architecture is favored under the authors' proposed framework. \n\n**A7:**\nOur work does not focus on multi-model ensembling and thus does not have ablation studies to evaluate how would the number of teachers influence student performance. We would suggest the readers to refer multi-model ensembling related works for more information.\nThe design of loss function is usually orthogonal student architectures based on previous works. Since our proposed method is not designed for specific CNN architecture, we adopt the most commonly used ResNet without loss of generality. Thanks for the suggestion."}, "signatures": ["ICLR.cc/2021/Conference/Paper1145/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IhUeMfEmexK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1145/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1145/Authors|ICLR.cc/2021/Conference/Paper1145/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863133, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1145/-/Official_Comment"}}}, {"id": "KdcAdWPBwu", "original": null, "number": 3, "cdate": 1605621976689, "ddate": null, "tcdate": 1605621976689, "tmdate": 1605679417825, "tddate": null, "forum": "IhUeMfEmexK", "replyto": "gp9LY45dkw1", "invitation": "ICLR.cc/2021/Conference/Paper1145/-/Official_Comment", "content": {"title": " Thanks for your careful and valuable comments. We will explain your concerns point by point. ", "comment": "Thanks for your careful and valuable comments. We will explain your concerns point by point.  \n**Q1:**\nHowever, I still have some concerns. First, ProxylessKD makes an assumption that the subjects of the dataset for training both models are somehow overlapping, while L2KD does not have such limitation. So more analysis and detailed discussions on the pros and cons of ProxylessKD and L2KD are needed. \n\n**A1:**\nCompared to the L2KD method, our method indeed requires overlapping datasets. But, ProxylessKD considers not only minimizing the intra-class but also maximizing the inter-class distance. However, the L2KD only considers minimizing the intra-class distance. Meanwhile, our ProxylessKD can benefit from large margin constraints (e.g. Cosface loss and Arcface loss) from which L2KD does not benefit. The experiments also show that our proposed ProxylessKD is more effective than L2KD.\n \n**Q2:**\nSecond, ProxylessKD can be interpreted as initializing classifier of student model by the classifier of teacher model. It would be interesting to see how performance changes with more layers of student model inherited from teacher model. For example, the last two layers, the last three layers. \n\n**A2:**\nBecause the weight of the teacher's classifier can be approximately viewed as the center of each class and the goal of this paper is to align embedding space between the teacher model and the student model, our ProxylessKD directly inherits the classifier of the teacher model to optimize task. We agree that changing more layers of the student model inherited from the teacher model would be an interesting direction to explore, and we will leave it to future works.\n \n**Q3:**\nThird, experimental comparisons with more advanced KD methods are necessary, e.g.[1], [2], [3] etcs. Currently the only comparison with other method is a self-implemented L2KD, which couldn't comprehensively show the effectiveness of the proposed method. \n[1] Park, Wonpyo, et al. \"Relational knowledge distillation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. \n[2] Peng, Baoyun, et al. \"Correlation congruence for knowledge distillation.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n[3] Karlekar, Jayashree, et al. \"Deep face recognition model compression via knowledge transfer and distillation.\" arXiv preprint arXiv:1906.00619 (2019).\n \n**A3:**\nWe adopt L2KD as the baseline because it can ensure the student model's embedding space is well aligned with the teacher model's. This is critical for similarity comparison between embeddings across different students model. In the practical application of face recognition, embeddings extracted from different models are required to be in the same embedding space, so that similarity comparison can be conducted. The advanced KD methods only force the student to mimic the teacher\u2019s behavior and do not ensure the embedding space alignment. Thank you for the suggestion and we agree it would be easier for the reader to understand if we add the suggested experiments.\n "}, "signatures": ["ICLR.cc/2021/Conference/Paper1145/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IhUeMfEmexK", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1145/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1145/Authors|ICLR.cc/2021/Conference/Paper1145/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863133, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1145/-/Official_Comment"}}}, {"id": "gp9LY45dkw1", "original": null, "number": 1, "cdate": 1603860244434, "ddate": null, "tcdate": 1603860244434, "tmdate": 1605024519228, "tddate": null, "forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "invitation": "ICLR.cc/2021/Conference/Paper1145/-/Official_Review", "content": {"title": "Good idea, but need more experiments", "review": "This paper proposes ProxylessKD method from a novel perspective of knowledge distillation. Instead of minimizing the outputs of teacher and student models, ProxylessKD adopts a shared classifier for two models. The shared classifier yields better aligned embedding space, so the embeddings from teacher and student models are comparable. Since the optimization objective for student model is learning discriminative embeddings,  the face recognition performance is improved compared to the vanilla KL counterpart.\n\nHowever, I still have some concerns.\nFirst, ProxylessKD makes an assumption that the subjects of the dataset for training both models are somehow overlapping, while L2KD does not have such limitation. So more analysis and detailed discussions on the pros and cons of ProxylessKD and L2KD are needed.\n\nSecond, ProxylessKD can be interpreted as initializing classifier of student model by the classifier of teacher model. It would be interesting to see how performance changes with more layers of student model inherited from teacher model. For example, the last two layers, the last three layers.\n\nThird, experimental comparisons with more advanced KD methods are necessary, e.g.[1], [2], [3] etcs. Currently the only comparison with other method is a self-implemented L2KD, which couldn't comprehensively show the effectiveness of the proposed method.\n[1] Park, Wonpyo, et al. \"Relational knowledge distillation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Peng, Baoyun, et al. \"Correlation congruence for knowledge distillation.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n[3] Karlekar, Jayashree, et al. \"Deep face recognition model compression via knowledge transfer and distillation.\" arXiv preprint arXiv:1906.00619 (2019).\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1145/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1145/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125750, "tmdate": 1606915773096, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1145/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1145/-/Official_Review"}}}, {"id": "MrSB61DPjh", "original": null, "number": 3, "cdate": 1603920847928, "ddate": null, "tcdate": 1603920847928, "tmdate": 1605024519134, "tddate": null, "forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "invitation": "ICLR.cc/2021/Conference/Paper1145/-/Official_Review", "content": {"title": "The paper proposes a novel knowledge distillation (KD) framework of directly using teacher model's classifier to distill the student feature learning. There are several empirical results showing the method's effectiveness.", "review": "This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself.\n\nThe idea of using teacher model\u2019s classifier to directly reshape the student model\u2019s feature representation is somewhat novel. It considers the situation of single teacher model and multiple teacher models. The teacher ensemble is achieved by concatenating features from each of the teacher model and then conducting dimension reduction using PCA. The methodology illustration is simple yet clear. There are multiple experiments on major face recognition datasets and demonstrate superior performance against baselines such as L2KD-s.\n\nRegarding the concerns, I am listing them into bullets.\n\n1. why the experiments make the setting of templates using teacher model to extract feature, while the query using student model to extract feature?\n\nWould the comparison of using student model for extracting both template and query feature be possible? It can provide a direct comparison to other methods, i.e. ArcFace trained using ResNet18 compared to the student model with ResNet18. \n\nCurrent experiments lack the comparison to the state-of-the-art methods, i.e. ArcFace and CosFace.\n\n2. the ablation is emphasizing on the ProxylessKD combining with different losses. It does not consider the knowledge distillation itself. For example, when ProxylessKD is combined with the proxy task, i.e., feature distillation loss, how would it perform compared to only ProxylessKD? Meanwhile, in many KD papers, there are also intermediate layer feature distillation, would it harm the overall performance under this paper's setting? It needs sufficient analysis to justify the authors' choice of only applying the teacher model's classifier as distillation.\n\n3. In ablation, how would the number of teachers influence the student performance? Meanwhile, how would the network architecture influence the student performance? i.e., fixing the teachers to be the same, while varying student architecture with multiple hypothesis, i.e., ResNet, AttentionNet, DenseNet? It is good to know what specific architecture is favored under the authors' proposed framework.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1145/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1145/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125750, "tmdate": 1606915773096, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1145/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1145/-/Official_Review"}}}, {"id": "X4hUCAzpu9", "original": null, "number": 2, "cdate": 1603890971204, "ddate": null, "tcdate": 1603890971204, "tmdate": 1605024519063, "tddate": null, "forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "invitation": "ICLR.cc/2021/Conference/Paper1145/-/Official_Review", "content": {"title": "This work has simple idea and is easy to implement. However, novelty is somewhat limited and more comparison shoud be provided.", "review": "The paper proposes a knowledge distillation method for face recognition, which inherits the teacher\u2019s classifier as the student\u2019s classifier and then optimizes the student model with advanced loss functions.  The paper demonstrates using an ensemble of teacher models can boost the performance of knowledge distillation.\n\nStrength:\n- The proposed method is simple and easy to implement. \n- The experimental results demonstrate the effectiveness of the technique.\n- The paper is well organized and well written.\n\nWeakness:\n- The novelty is limited. Directly inheriting the teacher\u2019s classifier is a common strategy in the face recognition community, which can be found in Ref.1.\n- The experiment lacks comparison with the general knowledge distillation methods (Ref.2) in image classification and the specific used methods (Ref.3) in face recognition.\n\n\u30101\u3011Deng, Jiankang, and Guo, Jia et al. Lightweight Face Recognition Challenge. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops\n\u30102\u3011Hinton, G., Vinyals, O., Dean., J.: Distilling the knowledge in a neural network. In: arXiv preprint arXiv:1503.02531 (2015)\n\u30103\u3011Xiaobo Wang and Tianyu Fu et al. Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition. ECCV2020\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1145/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1145/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition", "authorids": ["~Weidong_Shi2", "~Guanghui_Ren1", "~Yunpeng_Chen1", "~Shuicheng_Yan2"], "authors": ["Weidong Shi", "Guanghui Ren", "Yunpeng Chen", "Shuicheng Yan"], "keywords": ["inherited classifier", "embedding space alignment", "face recognition", "knowledge distillation"], "abstract": "Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher\u2019s behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as the large margin constraint (e.g. margin-based softmax).  We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition.  We conduct extensive experiments on standard face recognition benchmarks,  \nand the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.", "one-sentence_summary": "We proposed an inherited classifier knowledge distillation to enhance the feature space alignment between the student model and teacher model, which aims to improve the performance in some retrieval targets, e.g., face recognition.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "shi|proxylesskd_direct_knowledge_distillation_with_inherited_classifier_for_face_recognition", "pdf": "/pdf/bae4334c3eff9c14b3f94428791e2ae950a2d9b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Y1EwJfG_aX", "_bibtex": "@misc{\nshi2021proxylesskd,\ntitle={Proxyless{\\{}KD{\\}}: Direct Knowledge Distillation with Inherited Classifier for Face Recognition},\nauthor={Weidong Shi and Guanghui Ren and Yunpeng Chen and Shuicheng Yan},\nyear={2021},\nurl={https://openreview.net/forum?id=IhUeMfEmexK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IhUeMfEmexK", "replyto": "IhUeMfEmexK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1145/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125750, "tmdate": 1606915773096, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1145/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1145/-/Official_Review"}}}], "count": 8}