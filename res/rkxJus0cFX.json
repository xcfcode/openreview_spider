{"notes": [{"id": "rkxJus0cFX", "original": "BJgO3UGFFm", "number": 320, "cdate": 1538087783508, "ddate": null, "tcdate": 1538087783508, "tmdate": 1545355429765, "tddate": null, "forum": "rkxJus0cFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJxgsfuNl4", "original": null, "number": 1, "cdate": 1545007768321, "ddate": null, "tcdate": 1545007768321, "tmdate": 1545354486530, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Meta_Review", "content": {"metareview": "This paper proposed Residual Gradient Compression as a promising approach to reduce the synchronization cost of gradients in a distributed settings. It provides a useful approach that works for a number of models. The reviewers have a consensus that the quality is below acceptance standard due to practicality of experiments and lack of contribution.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "limited contribution"}, "signatures": ["ICLR.cc/2019/Conference/Paper320/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper320/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353257166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper320/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper320/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper320/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353257166}}}, {"id": "HJxplXBIJN", "original": null, "number": 5, "cdate": 1544078069279, "ddate": null, "tcdate": 1544078069279, "tmdate": 1544078069279, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rJe6slB81N", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "content": {"title": "You are right!", "comment": "Thanks for your reimplementation.\nBoth of your findings are right. I will correct them."}, "signatures": ["ICLR.cc/2019/Conference/Paper320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622871, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxJus0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper320/Authors|ICLR.cc/2019/Conference/Paper320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622871}}}, {"id": "rJe6slB81N", "original": null, "number": 2, "cdate": 1544077477292, "ddate": null, "tcdate": 1544077477292, "tmdate": 1544077477292, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Public_Comment", "content": {"comment": "I have implemented algorithm 2 and 3.\nIn algorithm 2, I think line 6's condition should be nnz < k. Because increasing threshold will only make nnz even bigger.\nAlso in algorithm 3, line 10 and 12, I think ratio is the value that should be assigned to r and l respectively, or else this doesn't make any sense for a binary search.\nPlease confirm if my findings are correct or not. Thanks.", "title": "Errors in algorithm 2 and 3?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311867629, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rkxJus0cFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311867629}}}, {"id": "BylHXcGWCm", "original": null, "number": 2, "cdate": 1542691356757, "ddate": null, "tcdate": 1542691356757, "tmdate": 1542692174800, "tddate": null, "forum": "rkxJus0cFX", "replyto": "r1eSJT-a27", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "content": {"title": " Response", "comment": "Thank you for your comments.\nRedSync performs better with the worse network.  Actually, platforms used to test our implement, a supercomputer, and a multi-GPU server, are equipped with the relative good inter-connected network. If testing on worse network fabric, like Ethernet and Wifi, ResNet will also gain a performance boost.\nLSTM is traditionally scaled with model parallelism. However, as we mentioned, data parallel is the easiest way to scale out with limited modifications of original serial code.  Part of work in (Lin et al 2018) also involves LSTMs."}, "signatures": ["ICLR.cc/2019/Conference/Paper320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622871, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxJus0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper320/Authors|ICLR.cc/2019/Conference/Paper320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622871}}}, {"id": "rkeBP9fZRQ", "original": null, "number": 4, "cdate": 1542691421115, "ddate": null, "tcdate": 1542691421115, "tmdate": 1542691453761, "tddate": null, "forum": "rkxJus0cFX", "replyto": "Bkgkpnntn7", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your sincere comments.\nWe add one sentence in the paper to clear that our contributions lie in system perspective rather than information theory perspective.\nWe also have reorganized the figures and make them more clear."}, "signatures": ["ICLR.cc/2019/Conference/Paper320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622871, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxJus0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper320/Authors|ICLR.cc/2019/Conference/Paper320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622871}}}, {"id": "Byx1rqMWCm", "original": null, "number": 3, "cdate": 1542691383291, "ddate": null, "tcdate": 1542691383291, "tmdate": 1542691383291, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rklNHAOc2X", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your comments.\nAdmittedly, dirty works we did overshadows our main contributions.\nI believe the value of this paper for ICLR is that it is one of few works considers gradient sparsification from the perspective of real system implementation. We would like to share with our peers some of our experiences, although looks not so remarkable.\n1.The fast top-0.1 method on GPU.\n2.Using allgather for sparse allreduce.\n3.Details for parallel Local Gradient Clipping.\nThank you for your advice. Considering the limitation of space, a systematic tuning will be left as our future work.\nMy draft may not be very well-written due to limited time. We have polished it and fix most of the typos."}, "signatures": ["ICLR.cc/2019/Conference/Paper320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622871, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxJus0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper320/Authors|ICLR.cc/2019/Conference/Paper320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622871}}}, {"id": "r1eSJT-a27", "original": null, "number": 3, "cdate": 1541377245090, "ddate": null, "tcdate": 1541377245090, "tmdate": 1541534095882, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Review", "content": {"title": "Good analysis and provides empirical value of gradient compression", "review": "Paper focuses on Residual Gradient Compression (RGC) as a promising approach to reducing the synchronization cost of gradients in a distributed settings. Prior approaches focus on the theoretical value of good compression rates without looking into the overall cost of the changes. This paper introduces RedSync that builds on the existing approaches by picking the most appropriate ones that reduce the overall cost for gradient reduction without unduly focusing on the compression rate.\nThe paper does this by providing an analysis of the cost of RGC and also the limitations in scaling as the bandwidth required grows with the number of nodes. It also highlights the value of applying different algorithms in this process for compression and the benefits and issues with each.\n\nPros:\n- Useful analysis that will help direct research in this area\n- Shows that this approach works for models that have a high communication to computation ratio\n- Provides a useful approach that works for a number of models\n\nCons:\n- Positive experimental results are on models that are typically not used in practice e.g. AlexNet and VGG16\n- Speedups shown on LSTMs don't see worthwhile to scale, and in practice a model-parallelism approach may scale better\n\nCorrections:\n- Typo in notes for Table 1 last sentence RCG => RGC\n- Typo in first sentence in section 3.2: RedSycn => RedSync\n- Section 3.3, #2 last sentence: maybe overdrafts => overshadows ?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper320/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Review", "cdate": 1542234488271, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335697875, "tmdate": 1552335697875, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rklNHAOc2X", "original": null, "number": 2, "cdate": 1541209659811, "ddate": null, "tcdate": 1541209659811, "tmdate": 1541534095671, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Review", "content": {"title": "RedSync should implement a more systematic approach for optimization. ", "review": "This paper introduces a set of implementation optimizations for minimizing communication overhead and thereby reducing the training time in distributed settings. The method relies on existing gradient compression and pruning techniques and is tested on synchronous/data-parallel settings. \n\nThe contribution and impact of the paper is unclear. The authors claim implementation innovations that show true performance gains of gradient compression techniques. But again it is unclear what those innovations are and how they can be reused for accelerating training for a new model.\n\nThe authors did perform an extensive set of experiments and while the method works well for some models and batch sizes, it doesn't work well for some other models. What would make the paper much more compelling would be if it came up with ways to systematically explore the relationship between training batch size, model parameter size, communication/computation/decompression ratos, and based on these properties, it can come up with best strategies to accelerate distributed data parallel training for any new model. \n\nThe paper needs to be polished as it has multiple typos. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper320/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Review", "cdate": 1542234488271, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335697875, "tmdate": 1552335697875, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkgkpnntn7", "original": null, "number": 1, "cdate": 1541160118661, "ddate": null, "tcdate": 1541160118661, "tmdate": 1541534095466, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Review", "content": {"title": "Good implementation optimizations in a important practical problem, but relatively incremental contribution", "review": "Quality and clarity:\nThe paper proposes an approach to reduce the communication bandwidth and overhead in distributed deep learning. The approach leverages on previous work (mainly the residual gradient compression (RGC) algorithm), and proposes several implementation optimizations. From what I can read, it is the basic RGC algorithm that is used, but with some clever optimization to improve the performance of it. \n\nThe quality of the paper is good, it is well-written and easy to read. The evaluation of the proposed approach is well done, using several diverse datasets and models, and executed on two different parallel systems. However, the reasons why RGC and qRGC sometimes have better accuracy than SGD needs to be analyzed and explained. \n\nOriginality and significance:\nThe originality of the paper is relatively low (optimization of an existing algorithm) and the contributions are incremental. However, the paper addresses an important practical problem in distributed learning, and thus can have a significant practical impact on how distributed deep learning systems are implemented.\n\nPros:\n* Addresses an important issue. \n* Good performance.\n* Good evaluation on two different systems. \n\nCons:\n* Limited contribution. Although I like implementation papers (very important), I think the contribution is to low for ICLR.\n\nMinor:\n* In general, the figures are hard to read (the main problem is to small text)\n* Compression in the title is slightly misleading, since it's mainly selection that is done (top-0.1% gradients). Although the values are packed in a data structure for transmission, it's not compression in a information theory perspective.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper320/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Review", "cdate": 1542234488271, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335697875, "tmdate": 1552335697875, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygdFMCGnQ", "original": null, "number": 1, "cdate": 1540706943526, "ddate": null, "tcdate": 1540706943526, "tmdate": 1540706943526, "tddate": null, "forum": "rkxJus0cFX", "replyto": "S1e1qb1-hm", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "content": {"title": "Clarifications on technical contribution.", "comment": "Thank you for reading our paper. \nThe Gradient Compression idea was first proposed in 2014. Its ultimate goal is to accelerate the performance of data parallel training in real practice. A set of work including (Lin et al 2018) is devoted to solving the convergence problem of the algorithm. Based on their efforts, our work is devoted to solving the performance problem of the algorithm. Some of our innovations are critical to the successful application of this algorithm, which is a big concern for the industry. More importantly, we pointed out that some algorithmic improvements are not equal to system performance improvements.\nYou may think the contribution of our work lies in the implementation part. However, \"implementation issues, parallelization, software platforms, hardware\u201d are indeed included in the relevant topics of this conference (ICLR 2019).\nThanks again for your comments."}, "signatures": ["ICLR.cc/2019/Conference/Paper320/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622871, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxJus0cFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper320/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper320/Authors|ICLR.cc/2019/Conference/Paper320/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622871}}}, {"id": "S1e1qb1-hm", "original": null, "number": 1, "cdate": 1540579718986, "ddate": null, "tcdate": 1540579718986, "tmdate": 1540579718986, "tddate": null, "forum": "rkxJus0cFX", "replyto": "rkxJus0cFX", "invitation": "ICLR.cc/2019/Conference/-/Paper320/Public_Comment", "content": {"comment": "Hi there,\n     In this paper, you claim that you design a cost-efficient method for communication, but the core of the algorithms is already shown in Lin et al 2018 ICLR. So from the technical perspective, I didn't see anything new here. \n     Then you combine the encoding technique which is well established in the past years, and nothing new in this paper.\n     If you say the technical contribution is only for the implementation part, and then I think it is too weak for the contribution, or you can call it programming skills in real practice.\n     Thanks for reading your paper. ", "title": "where is technical contribution"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper320/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RedSync : Reducing Synchronization Traffic for Distributed Deep Learning", "abstract": "Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes.  Since the synchronization of the local models or gradients can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently.  Among several recent proposed compression algorithms, \nResidual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still preserve accuracy. However, the literature on compressing deep networks focuses almost exclusively on achieving good compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that achieves significant training time improvement in real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including a supercomputer and a multi-card server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement.", "keywords": ["Data parallel", "Deep Learning", "Multiple GPU system", "Communication Compression", "Sparsification", "Quantization"], "authorids": ["fang_jiarui@163.com", "rainfarmer@gmail.com"], "authors": ["Jiarui Fang", "Cho-Jui Hsieh"], "TL;DR": "We proposed an implementation to accelerate DNN data parallel training by reducing communication bandwidth requirement.", "pdf": "/pdf/86315f9a2f989e5fd7a4e26fed9f84dc9ab08e0b.pdf", "paperhash": "fang|redsync_reducing_synchronization_traffic_for_distributed_deep_learning", "_bibtex": "@misc{\nfang2019redsync,\ntitle={RedSync : Reducing Synchronization Traffic for Distributed Deep Learning},\nauthor={Jiarui Fang and Cho-Jui Hsieh},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxJus0cFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper320/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311867629, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rkxJus0cFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper320/Authors", "ICLR.cc/2019/Conference/Paper320/Reviewers", "ICLR.cc/2019/Conference/Paper320/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311867629}}}], "count": 12}