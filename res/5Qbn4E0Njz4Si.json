{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363334160000, "tcdate": 1363334160000, "number": 1, "id": "-B7o-Yy0XjB0_", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "5Qbn4E0Njz4Si", "replyto": "Oel6vaaN-neNQ", "signatures": ["Hyun-Ah Song"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "- Points on the con that experimental results are not great:\r\nWhen we refer to Figure 2 in the paper, the proposed hierarchical feature extraction method results in much better classification and reconstruction performance, especially for small number of features.\r\nIt can be interpreted that, by taking hierarchical stages in reducing dimensions, our proposed method successfully finds more meaningful and helpful features in aspect of representing the data, compared to reducing dimensions at one step."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Data Representation Model - Multi-layer NMF", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Understanding and representing the underlying structure of feature hierarchies present in complex data in intuitively understandable manner is an important issue. In this paper, we propose a data representation model that demonstrates hierarchical feature learning using NMF with sparsity constraint. We stack simple unit algorithm into several layers to take step-by-step approach in learning. By utilizing NMF as unit algorithm, our proposed network provides intuitive understanding of the learning process. It is able to demonstrate hierarchical feature development process and also discover and represent feature hierarchies in the complex data in intuitively understandable manner. We apply hierarchical multi-layer NMF to image data and document data to demonstrate feature hierarchies present in the complex data. Furthermore, we analyze the reconstruction and classification abilities of our proposed network and prove that hierarchical feature learning approach excels performance of standard shallow network. By providing underlying feature hierarchies in complex real-world data sets, our proposed network is expected to help machines develop intelligence based on the learned relationship between concepts, and at the same time, perform better with the small number of features provided for data representation.", "pdf": "https://arxiv.org/abs/1301.6316", "paperhash": "song|hierarchical_data_representation_model_multilayer_nmf", "keywords": [], "conflicts": [], "authors": ["Hyun-Ah Song", "Soo-Young Lee"], "authorids": ["hi.hyunah@gmail.com", "longlivelee@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363255380000, "tcdate": 1363255380000, "number": 2, "id": "CC-TCptvxlrvi", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "5Qbn4E0Njz4Si", "replyto": "Oel6vaaN-neNQ", "signatures": ["Hyun-Ah Song"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "- Details on the specifics of the method:\r\nSorry for the insufficient explanations on the method. We had to fit into 3 page limit.. We added detailed explanation of the method and computation in Appendix.\r\n \r\n- Hierarchies by topic models [A,B]:\r\nThanks for the recommendation! In this paper, we focused on the general property of the hierarchical learning of the proposed network, regardless of types of data set (whether it is document set or image set, etc). This is the reason why we did not compare our result with any of other topic model result. However, we think it is meaningful to carefully observe how it works in different types of data sets in more detail. As furtherwork, we would like to look into more details on the function of our proposed network in terms of document dataset application by comparing the result with [A,B].\r\n \r\n- On Figure 2, the x-axis represents the number of features, and also, the dimensions provided for data representation in H. If we increase the number of features provided for learning, the network learns features separately so that it can come up with more exact reconstruction of the original data. (For example, if we restrict the number of feature to one, the network has to cram the essential parts necessary for data representation into one, and it is hard to represent exactly what we want using just one feature or building block. However, if we provide sufficient number of features, network learns several essential parts separately, which means more number of more accurate building blocks and it will be easier to represent data by making use of the necessary features or building blocks, which may lead to more accurate reconstruction of the data) This is the reason why reconstruction error decreases with respect to increasing number of features.\r\n \r\n- MNIST dataset difference between shallow and deep network:\r\nSorry for the small image! Instead of showing whole 0-9 MNIST digit reconstruction, we enlarged and focused on the image to a few example of digits that show clear difference between the shallow and deep network.\r\n \r\n- What is achieved by the research?\r\nThere are mainly two contributions of our work: By taking step-by-step approach in learning of features using NMFs, 1. we discovered the relationships between low level features and high level features, and intuitively demonstrated class hierarchies present in the data, and additionally, 2. we learned more meaningful features which lead to better distributed data representation, which results in better classification and reconstruction performance (provided insufficient dimensions for data representation).\r\nBy extending NMF into several layers, we proposed a way to discover intuitive concept hierarchies by learning relationships between features, regardless of types of data set (while topic models are focused on revealing concept hierarchies of document set only, our proposed network can handle any types non-negative data sets.). With the experiments of comparison with the shallow network, we also proved that taking step-by-step approach in learning benefits feature learning as well. (this will be supporting evidence of further application of the proposed network)\r\n \r\n- Remainder: The revised version will be available at  Fri, 15 Mar 2013 00:00:00 GMT."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Data Representation Model - Multi-layer NMF", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Understanding and representing the underlying structure of feature hierarchies present in complex data in intuitively understandable manner is an important issue. In this paper, we propose a data representation model that demonstrates hierarchical feature learning using NMF with sparsity constraint. We stack simple unit algorithm into several layers to take step-by-step approach in learning. By utilizing NMF as unit algorithm, our proposed network provides intuitive understanding of the learning process. It is able to demonstrate hierarchical feature development process and also discover and represent feature hierarchies in the complex data in intuitively understandable manner. We apply hierarchical multi-layer NMF to image data and document data to demonstrate feature hierarchies present in the complex data. Furthermore, we analyze the reconstruction and classification abilities of our proposed network and prove that hierarchical feature learning approach excels performance of standard shallow network. By providing underlying feature hierarchies in complex real-world data sets, our proposed network is expected to help machines develop intelligence based on the learned relationship between concepts, and at the same time, perform better with the small number of features provided for data representation.", "pdf": "https://arxiv.org/abs/1301.6316", "paperhash": "song|hierarchical_data_representation_model_multilayer_nmf", "keywords": [], "conflicts": [], "authors": ["Hyun-Ah Song", "Soo-Young Lee"], "authorids": ["hi.hyunah@gmail.com", "longlivelee@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363255140000, "tcdate": 1363255140000, "number": 1, "id": "APRX62OnXa6nY", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "5Qbn4E0Njz4Si", "replyto": "ZIE1IP5KlJTK-", "signatures": ["Hyun-Ah Song"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "- Description of proposed architecture:\r\nSorry about insufficient description of the proposed architecture! We had to fit all of the content into 3 pages.. We added more details on the architecture of our network, which includes actual computations involved in implementing the network in Appendix.\r\n\r\n- Comparison with other baselines:\r\nIn this paper, we concentrated on proving our hypothesis that extending NMF into several layers will discover feature hierarchies present in the data, and provide better and meaningful features, compared to the standard shallow one (self-comparison). Since we wanted to observe the behavioral change when extended into several layers, we solely compared the result before and after stacking layers. We regarded comparing the performance with other baselines or layer-wise feature learners as not so important because other baselines do not provide us intuitive demonstration of the feature hierarchies. However, like your comment, we think that it may be meaningful to compare with the other feature learners and see if our proposed network can function as a simple feature extraction algorithm (without considering discovery of feature hierarchies).\r\n\r\n- Experiments and visualizations do not sufficiently demonstrate the claim that NMF-based feature hierarchies are easier to interpret:\r\nThrough our proposed research, we wanted to prove that hierarchical learning with NMFs can present intuitive feature hierarchies by learning feature relationships across the layers.\r\nWe think our proposed network provides meaningful feature hierarchies compared to other networks (not necessarily easier interpretation) because:\r\na) compared to other feature learning networks that does not restrict the sign of the data, our proposed network intuitively represents learned features with the non-negativity property.\r\nb) while shallow feature learning networks that is able to demonstrate features intuitively (ex. by restricting non-negativity constraint, NMF or other topic models),  it learns relationships of features which develop into hierarchies.\r\nc) although some recent topic models provides topic hierarchies in intuitive manner, the application is restricted to only document data. However, our proposed network can be applied to any types of data with non-negative signs, not just documents (it can be used to learn underlying feature hierarchies present in image, as well.)\r\nWith the experimental results in this paper, we are aware of the fact that interpretation of sub-class topics may not seem clear; we showed how words in the first layer features differ slightly from each other in terms of content, but develop into the same broad topic class. However, we believe that this has shown a good signal of potential for development. In order to reinforce our claim and strongly support the function of the proposed network, we would like to look for a text document set that provides ground-truth label of sub-categories as well.\r\n\r\n- We included abstracts in the revised version, and corrected the notes you made above! Thanks!\r\n\r\n- Remainder: The revised version will be available at  Fri, 15 Mar 2013 00:00:00 GMT."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Data Representation Model - Multi-layer NMF", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Understanding and representing the underlying structure of feature hierarchies present in complex data in intuitively understandable manner is an important issue. In this paper, we propose a data representation model that demonstrates hierarchical feature learning using NMF with sparsity constraint. We stack simple unit algorithm into several layers to take step-by-step approach in learning. By utilizing NMF as unit algorithm, our proposed network provides intuitive understanding of the learning process. It is able to demonstrate hierarchical feature development process and also discover and represent feature hierarchies in the complex data in intuitively understandable manner. We apply hierarchical multi-layer NMF to image data and document data to demonstrate feature hierarchies present in the complex data. Furthermore, we analyze the reconstruction and classification abilities of our proposed network and prove that hierarchical feature learning approach excels performance of standard shallow network. By providing underlying feature hierarchies in complex real-world data sets, our proposed network is expected to help machines develop intelligence based on the learned relationship between concepts, and at the same time, perform better with the small number of features provided for data representation.", "pdf": "https://arxiv.org/abs/1301.6316", "paperhash": "song|hierarchical_data_representation_model_multilayer_nmf", "keywords": [], "conflicts": [], "authors": ["Hyun-Ah Song", "Soo-Young Lee"], "authorids": ["hi.hyunah@gmail.com", "longlivelee@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362279120000, "tcdate": 1362279120000, "number": 2, "id": "Oel6vaaN-neNQ", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "5Qbn4E0Njz4Si", "replyto": "5Qbn4E0Njz4Si", "signatures": ["anonymous reviewer 7984"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Hierarchical Data Representation Model - Multi-layer NMF", "review": "The paper proposes to stack NMF models on top of each other. At each level, a non-linear function of normalized decomposition coefficients is used and decomposed using another NMF.\r\n\r\nThis is essentially an instance of a deep belief network, where the unsupervised learning part is done using NMF, which, to the best of my knowledge had not been done before.\r\n\r\nThe new method is then applied to document data where a hierarchy of topics seems to be discovered. Applications are also shown on reconstructing digits.\r\n\r\nThe extended abstract however does not give many details on all the specifics of the method.\r\n\r\nComments:\r\n-It would have been nice (a) to relate the hierachy to existing topic models [A,B], and (b) to see more topics.\r\n-On Figure 2, why are reconstruction errors decreasing with the number of features? \r\n-On the digits, the differences between shallow and deep networks are not clear.\r\n\r\n[A] D. Blei, T. Griffiths, and M. Jordan.   The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies.   Journal of the ACM, 57:2 1\u201330, 2010.  \r\n\r\n[B] R. Jenatton, J. Mairal, G. Obozinski, F. Bach. Proximal Methods for Hierarchical Sparse Coding. Journal of Machine Learning Research, 12, 2297-2334, 2011.\r\n\r\nPros:\r\n-Interesting idea of stacking NMFs.\r\n\r\nCons:\r\n-Experimental results are interesting but not great. What is exactly achieved is not clear."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Data Representation Model - Multi-layer NMF", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Understanding and representing the underlying structure of feature hierarchies present in complex data in intuitively understandable manner is an important issue. In this paper, we propose a data representation model that demonstrates hierarchical feature learning using NMF with sparsity constraint. We stack simple unit algorithm into several layers to take step-by-step approach in learning. By utilizing NMF as unit algorithm, our proposed network provides intuitive understanding of the learning process. It is able to demonstrate hierarchical feature development process and also discover and represent feature hierarchies in the complex data in intuitively understandable manner. We apply hierarchical multi-layer NMF to image data and document data to demonstrate feature hierarchies present in the complex data. Furthermore, we analyze the reconstruction and classification abilities of our proposed network and prove that hierarchical feature learning approach excels performance of standard shallow network. By providing underlying feature hierarchies in complex real-world data sets, our proposed network is expected to help machines develop intelligence based on the learned relationship between concepts, and at the same time, perform better with the small number of features provided for data representation.", "pdf": "https://arxiv.org/abs/1301.6316", "paperhash": "song|hierarchical_data_representation_model_multilayer_nmf", "keywords": [], "conflicts": [], "authors": ["Hyun-Ah Song", "Soo-Young Lee"], "authorids": ["hi.hyunah@gmail.com", "longlivelee@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362127980000, "tcdate": 1362127980000, "number": 1, "id": "ZIE1IP5KlJTK-", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "5Qbn4E0Njz4Si", "replyto": "5Qbn4E0Njz4Si", "signatures": ["anonymous reviewer d1c1"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Hierarchical Data Representation Model - Multi-layer NMF", "review": "This paper proposes a multilayer architecture based upon stacking non-negative matrix factorization modules and fine-tuning the entire architecture with reconstruction error. Experiments on text classification and MNIST reconstruction demonstrate the approach.\r\n\r\nDuring layer-wise initialization of the multilayer architecture NMF is performed to obtain a low-rank approximation to the input. The output of an NMF linear transform passes through a nonlinearity to form the input to the subsequent layer. These nonlinear outputs of a layer are K = f(H) where f(.) is a nonlinear function and H are linear responses of the input. During joint network training, a squared reconstruction error objective is used. Decoding the final hidden layer representation back into the input space is performed with explicit inversions of the nonlinear function f(.). Overall, the notation and description of the multi-layer architecture (section 3) is quite unclear. It would be difficult to implement the proposed architecture based only upon this description\r\n\r\nExperiments on Reuters text classification and MNIST primarily focus on reconstruction error and visualizing similarities discovered by the model. The text similarities are interesting, but showing a single learned concept does not sufficiently demonstrate the model's ability to learn interesting structure. MNIST visualizations are again interesting, but the lack of MNIST classification results is strange given the popularity of the dataset. Finally, no experiments compare to other models e.g. simple sparse auto-encoders to serve as a baseline for the proposed algorithm.\r\n\r\n\r\n\r\nNotes:\r\n-The abstract should be included as part of the paper\r\n- Matlab notation is paragraph 2 of section 2 is a bit strange. Standard linear algebra notation (e.g. I instead of eye) is more clear in this case\r\n- 'smoothen' -> smooth or apply smoothing to\r\n\r\nSummary:\r\n- A stacking architecture based upon NMF is interesting\r\n- The proposed architecture is not described well. Others would have difficulty replicating the model.\r\n- Experiments do not compare to sufficient baselines or other layer-wise feature learners.\r\n- Experiments and visualizations do not sufficiently demonstrate the claim that NMF-based feature hierarchies are easier to interpret"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Data Representation Model - Multi-layer NMF", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Understanding and representing the underlying structure of feature hierarchies present in complex data in intuitively understandable manner is an important issue. In this paper, we propose a data representation model that demonstrates hierarchical feature learning using NMF with sparsity constraint. We stack simple unit algorithm into several layers to take step-by-step approach in learning. By utilizing NMF as unit algorithm, our proposed network provides intuitive understanding of the learning process. It is able to demonstrate hierarchical feature development process and also discover and represent feature hierarchies in the complex data in intuitively understandable manner. We apply hierarchical multi-layer NMF to image data and document data to demonstrate feature hierarchies present in the complex data. Furthermore, we analyze the reconstruction and classification abilities of our proposed network and prove that hierarchical feature learning approach excels performance of standard shallow network. By providing underlying feature hierarchies in complex real-world data sets, our proposed network is expected to help machines develop intelligence based on the learned relationship between concepts, and at the same time, perform better with the small number of features provided for data representation.", "pdf": "https://arxiv.org/abs/1301.6316", "paperhash": "song|hierarchical_data_representation_model_multilayer_nmf", "keywords": [], "conflicts": [], "authors": ["Hyun-Ah Song", "Soo-Young Lee"], "authorids": ["hi.hyunah@gmail.com", "longlivelee@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1359440100000, "tcdate": 1359440100000, "number": 38, "id": "5Qbn4E0Njz4Si", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "5Qbn4E0Njz4Si", "signatures": ["hi.hyunah@gmail.com"], "readers": ["everyone"], "content": {"title": "Hierarchical Data Representation Model - Multi-layer NMF", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Understanding and representing the underlying structure of feature hierarchies present in complex data in intuitively understandable manner is an important issue. In this paper, we propose a data representation model that demonstrates hierarchical feature learning using NMF with sparsity constraint. We stack simple unit algorithm into several layers to take step-by-step approach in learning. By utilizing NMF as unit algorithm, our proposed network provides intuitive understanding of the learning process. It is able to demonstrate hierarchical feature development process and also discover and represent feature hierarchies in the complex data in intuitively understandable manner. We apply hierarchical multi-layer NMF to image data and document data to demonstrate feature hierarchies present in the complex data. Furthermore, we analyze the reconstruction and classification abilities of our proposed network and prove that hierarchical feature learning approach excels performance of standard shallow network. By providing underlying feature hierarchies in complex real-world data sets, our proposed network is expected to help machines develop intelligence based on the learned relationship between concepts, and at the same time, perform better with the small number of features provided for data representation.", "pdf": "https://arxiv.org/abs/1301.6316", "paperhash": "song|hierarchical_data_representation_model_multilayer_nmf", "keywords": [], "conflicts": [], "authors": ["Hyun-Ah Song", "Soo-Young Lee"], "authorids": ["hi.hyunah@gmail.com", "longlivelee@gmail.com"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 6}