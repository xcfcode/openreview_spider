{"notes": [{"id": "foNTMJHXHXC", "original": "hDkMD9BD6ja", "number": 268, "cdate": 1601308037897, "ddate": null, "tcdate": 1601308037897, "tmdate": 1614985621388, "tddate": null, "forum": "foNTMJHXHXC", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "0bx5FsGtRJ_", "original": null, "number": 1, "cdate": 1610040538071, "ddate": null, "tcdate": 1610040538071, "tmdate": 1610474148210, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper is proposing Risk Extrapolation (REX) as a domain generalization algorithm. Authors extends the distributionally robust learning to affine mixture of distributions from convex mixture. Authors later uses variances instead of this extension and demonstrate various empirical and theoretical properties. The paper is reviewed by four expert reviewers and the reviewers did not reach to a consensus. Hence, I also read the paper in detailed and reviewed it. In summary, reviewers argue the following:\n\n- R#2: Main argument is the lack of justification of the claim \"Rex could deal with both covariate and concept shift together\". Authors try to address this in their response. Moreover, reviewer also argues in the private discussion that manuscript is not updated and authors did not address any of the issues during the discussion period.\n- R#3: Argues that (similar to R#2), dealing with covariate shift is not explained properly. Reviewer is not persuaded that REX results in invariant prediction.\n- R#1 and R#4: Largely positive about the paper. In the mean time, argue that organization of the paper is lacking and some of the material in the supplement is relevant and should be moved to the main text. R#1 decreases their score due to the lack of re-organization during the discussion.\n\nThe value of the paper is clear to me, the joint treatment of minimax perspective, domain generalization and invariances is definitely interesting and valuable. Hence, the paper has merit to be published. However, the presentation is lacking significantly.  The main contribution of the paper lies in Table 1 but the invariant prediction property is not justified at all in the main text. Hence, Table 1 is not justified properly. Authors discuss Thm 1&2 in their response but they both are in the supplement. From reading only the main text, confusion of the reviewers are well justified. ICLR guidelines clearly states that \"...Note that reviewers are encouraged, but not required to review supplementary material during the review process...\" It is authors' responsibility to make the main paper self contained. Even more worrisome is the fact that authors dismiss this concern in their response to R#1 which eventually leads to R#1 decreasing their score. Hence, I decided to reject the paper since the presentation is subpar and authors did not persuaded reviewers that they can fix this presentation issue by the camera-ready deadline. On the other hand, I think the paper can be really influential if it was written clearly. I suggest authors to revise the claims more precisely, extended the discussion on the claims and move the theorems to the main paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040538057, "tmdate": 1610474148192, "id": "ICLR.cc/2021/Conference/Paper268/-/Decision"}}}, {"id": "RiTpWcDI1QY", "original": null, "number": 1, "cdate": 1603477541339, "ddate": null, "tcdate": 1603477541339, "tmdate": 1606769958338, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Review", "content": {"title": "A significant contribution to the field of domain adaptation and transfer learning.", "review": "Summary:\nThis paper addresses the problem of distributional shift in transfer learning from multiple training domains. The authors propose Risk Extrapolation (REx), which is a novel approach for out-of-distribution generalization when the new test domain for which we do not even have the covariate matrix. Thorough empirical experiments show that REx significantly outperforms state-of-the-art.\n\nPros:\n- This is a highly quality paper with strong theoretical and empirical results. \n- The paper is clearly written and easy to understand.\n- Based on the thorough literature review, this idea of this work is original.\n- The results of this work are highly significant and of interest to the domain adaptation and transfer learning community.\n\nCons:\n- Although I understand the page limit, most of the major parts of the paper (especially the theoretical aspects) can be found in the appendix. As a person who is not very familiar with the literature on distributional shift from multiple domains, I did appreciate having this thorough overview; however, the detailed discussions of the contributions of the paper might be overlooked if (when) located in the appendix. Specifically, there is  only half a page on introducing the proposed methods for REx  in Sec. 3.1.\n\nMinor comment(s):\n- Reference \u201cPeter B\u00fchlmann. Invariance, causality and robustness, 2018a.\u201d is duplicated.\n\n############################################################\n\nPost-Rebuttal:\n\nAfter reviewing the concerns raised by the other reviewers, and the responses provided by the authors, I have decided to adjust my scores.\n\nMoreover, I was disappointed that the authors did not use the extra one page to move some material from the appendix to the main text in order to elaborate on the proposed method.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146825, "tmdate": 1606915810603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper268/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Review"}}}, {"id": "33Pu1nPfpXV", "original": null, "number": 20, "cdate": 1606281660904, "ddate": null, "tcdate": 1606281660904, "tmdate": 1606281660904, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "Cil_NxIe1N", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "Regarding Williamson and Menon", "comment": "We agree with your assessment of this related work, and will update the paper accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "2pgmh7zO2Tx", "original": null, "number": 19, "cdate": 1605999222539, "ddate": null, "tcdate": 1605999222539, "tmdate": 1605999222539, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "Gg93gBHV0ip", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "Relationship between Invariant Prediction and Causal Discovery", "comment": "We agree with your list, except on point 4.  It is unclear to whether REx or IRM is more robust to change in P(Y|X).  And we believe IRM should be viewed as a method for invariant prediction, which is only equivalent to discovering the causes of Y.  The only clear qualitative difference between REx and IRM that we are aware of, in terms of how they generalize OOD is the one we focus on: REx promotes robustness to covariate shift, but as a consequence can fail on heteroskedastic data.\n\nWe define invariant prediction as in Koyama and Yamaguchi (2020).  Invariant prediction can *require* causal discovery (in the sense of identifying the causes of Y, rather than identifying the entire causal of the data, e.g. which elements of X cause each other); this has been proven in Peters et al. (2015) for the case where X contains the parents of Y, and the perturbation set is generated via certain interventions on X.  For more general cases, we believe the relationship is still not entirely clear to the research community (although this area has received significant attention this year, and so there may be some new works we are not familiar with).  However, it is the case that sometimes an invariant predictor can use non-causal information; see top of page 5 of Koyama and Yamaguchi (2020) for an example.\n\nWe hope this helps clarify things, and are happy to continue discussing."}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "Gg93gBHV0ip", "original": null, "number": 18, "cdate": 1605979110577, "ddate": null, "tcdate": 1605979110577, "tmdate": 1605979110577, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "aPdWLn5Jfc", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "About invariant prediction", "comment": "Thanks for the timely reply.\n\n**To:** \"Referencing our abstract, like REx, DRO assumes \"that variation across training domains is representative of the variation we might encounter at test time\", but only REx assumes \"that shifts at test time may be more extreme in magnitude\".\"\n\n**Re:** \n\n1. Group DRO is robust to change in P(Y|X). \n\n2. REx is more robust than group DRO because REx considers more extreme shifts in P(Y|X).\n\n3. You experimentally show that REx can discover invariant predictor on Colored MNIST.\n\n4. IRM aims to use causal features to derive invariant preditor. This implies that comparing to REx, IRM is more robust to change in P(Y|X).\n\n5. REx is robust to changes in P(X) while IRM cannot. \n\nAm I right by saying that? What is the relationship between invariant prediction and causal discover? \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "aPdWLn5Jfc", "original": null, "number": 17, "cdate": 1605772400477, "ddate": null, "tcdate": 1605772400477, "tmdate": 1605772454627, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "V_exJcN0tsc", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "Why DRO fails to learn an invariant predictor", "comment": ">  Could you explain why group DRO cannot discover an invariant predictor?\n\nCertainly!\nA more precise statement would be: \"DRO typically does not learn an invariant predictor\" (even when REx does). First, let's establish that this is in fact the case, using Colored MNIST as an example.\n\nConsider Figure 2 (left), and to simplify things, let's restrict ourselves to considering the case where $\\beta=0$ (no REx) or $\\beta=10000$ (REx).  An invariant predictor in CMNIST must ignore color, and training with REx leads the model to ignore color, achieving ~70% accuracy.  However, color is a more predictive feature than shape in *both* CMNIST training domains, and thus the worst-case optimal predictor across training domains (which is what DRO seeks) would use color, and achieve ~80% accuracy.  Thus DRO will not learn an invariant predictor in CMNIST.\n\nNonetheless, you are correct that DRO can induce robustness to $P(Y|X)$.  However, in the case of CMNIST, $P(Y=0 | \\mathrm{color=red}) \\in \\{0.1, 0.2\\}$ for the training distributions.  Since DRO only considers *convex* combinations of training distributions, it will only be worst-case optimal for distributions with $P(Y=0 | \\mathrm{color=red}) \\in [0.1, 0.2]$.  Unfortunately (for DRO), since the test distribution has $P(Y=0 | \\mathrm{color=red}) = 0.9$, DRO will not generalize to the CMNIST test set.\n\nReferencing our abstract, like REx, DRO assumes \"that variation across training domains is representative of the variation we might encounter at test time\", but only REx assumes \"that *shifts at test time may be more extreme in magnitude*\".\n\nPlease let us know if you'd like us to elaborate on any of these points.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "V_exJcN0tsc", "original": null, "number": 16, "cdate": 1605757594286, "ddate": null, "tcdate": 1605757594286, "tmdate": 1605757594286, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "vSPayxqzYzT", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "Official Blind Review #3", "comment": "Thank you for the response.\n\n**To:** \"Regarding invariant prediction in particular, robustness to spurious features is a special case of robustness to change in $P(X,Y)$ , and so REx can provide robustness to spurious features if the training domains indicate their spuriousness (note that IRM also requires evidence of spuriousness, in the same form of differences in the conditional $P(Y|X)$  across domains/environments). This leads to invariant prediction when all spurious features can be identified as such.\n\n**RE:** As far as I understand, the invariant prediction of REx comes from the robustness to change in $P(Y|X).$ The group DRO is also robust to the distributional change.  Could you explain why group DRO cannot discover an invariant predictor? "}, "signatures": ["ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "2prkohJ3hE-", "original": null, "number": 15, "cdate": 1605726256669, "ddate": null, "tcdate": 1605726256669, "tmdate": 1605726256669, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "RJ4crRpVKl1", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "RE (2)", "comment": "All of our experiments with CMNIST (and variants) already use this evaluation procedure; we will make sure this is clear in revision.\n\nWe plan to include results using this form of evaluation for DomainBed experiments, as well.\nA fair comparison with other methods will require rerunning them, since the DomainBed paper/repo does *not* use this evaluation procedure. \nDue to limited resources, we plan to compare only ERM and REx. \nWe hope to finish these experiments by the end of the discussion phase and include them in the revision, but may not have sufficient resources."}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "vSPayxqzYzT", "original": null, "number": 14, "cdate": 1605577039732, "ddate": null, "tcdate": 1605577039732, "tmdate": 1605577039732, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "RJ4crRpVKl1", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "RE (1)", "comment": "Thanks for continuing the discussion!\n\n1) We're sorry our explanations for how REx can perform invariant prediction were not clear enough.  \nYou are correct in observing that REx can provide robustness to arbitrary directions of change in $P(Y|X)$ (or more generally, $P(X,Y)$, see Figure 1), provided training domains illustrate such shifts.  Thus REx has the potential to provide robustness in a wider range of circumstances than methods which focus on invariant prediction, such as IRM.  The greater flexibility of REx is also a selling point of our work, which we highlight be demonstrating that REx can provide robustness to covariate shift.\n\nRegarding invariant prediction in particular, robustness to spurious features is a special case of robustness to change in $P(X,Y)$, and so REx can provide robustness to spurious features if the training domains indicate their spuriousness (note that IRM also requires evidence of spuriousness, in the same form of differences in the conditional $P(Y|X)$ across domains/environments).  This leads to invariant prediction when all spurious features can be identified as such.\n\nWe also prove that REx can perform causal discover under suitable conditions in Appendix E; the homoskedasticity assumption of E.1.2 can also be replaced by a \"no covariate shift\" assumption (we'll mention this in the revision).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "Cil_NxIe1N", "original": null, "number": 13, "cdate": 1605576048033, "ddate": null, "tcdate": 1605576048033, "tmdate": 1605576048033, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "76OUF4kNz-1", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "Response", "comment": "Thanks for the review, and the questions and pointers.\n\nSome responses, ordered as in your review:\n* We agree that the variance-based penalty can be related to the DRO version, and plan to include this in our revision.\n* Thanks for mentioning the Rosenfeld, Ravikumar, Risteski paper.  We haven't had time to look at it yet, but are interested in discussing it further.  Could you perhaps elaborate a bit, e.g. characterize these negative results and/or point to specific parts of the paper?\n* We'll take a closer look at Williamson and Menon and get back to you on this point.  \n* In fact, minimax does *not* equalize risk across different domains.  Figure 3 (right) provides a demonstration: beta=0 gives better minimax (training) risk, but *worse* test risk. This is because paying more attention to color reduces risk on *both* training domains, but also *increases* the difference between training risks, and the risk on the *test* domain. This example should help explain out intuition for enforcing *exact* equality, as well as why DRO fails to do so, and thus cannot serve as a method for invariant prediction. Please let us know if any of this is still unclear.\n* We'll move some of the results such as statements of theorems to the main text, and would love more suggestions as to which material from the appendix would be most valuable to include in the main text.\n* No, ERM is sensitive to the number of examples from different domains; see the Remark on page 18.  In particular, we can consider the case where $1-\\epsilon$ of the data comes from the observational distribution (with no interventions), and $\\epsilon$ of the data comes from other domains.  Then the ERM objective is easily dominated by the loss on the observational distribution. \n* Yes, the assumptions for E.1.2 are indeed strong, although I'm not sure exactly what you mean by \"pointwise\" homoskedasticity.  The homoskedasticity assumption could be replaced with a \"no covariate shift\" assumption.  Note that IRM doesn't require either assumption, but also (unlike REx) fails to provide robustness to covariate shift in the limited data regime.\nIt is possible in principle to control for the effects of covariate shift in the infinite data/capacity (\"realizable\") case using variant of REx, but we were advised in a previous review cycle to remove this content, which is still somewhat a work-in-progress.  Ultimately, we seek a method of invariant prediction that provides robustness to covariate shift in the limited data/capacity case, which does not require such strong assumptions; it seems like such a method would need to distinguish between inputs x that have high loss due to 1) inherent noise vs. 2) underfitting.  \n* Thanks for the minor comments; we'll implement the suggestions in our revision (except we might keep Figure 3, space permitting)."}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "VXiIiC0NBM", "original": null, "number": 12, "cdate": 1605574461637, "ddate": null, "tcdate": 1605574461637, "tmdate": 1605574461637, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "TZNTpU6k5-_", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "RE: more detailed comments", "comment": "* Thanks for noticing; we'll fix it!\n* We're not sure we understand this point.  To clarify, our use of the term \"invariant predictor\" is based on Koyama & Yamaguchi (2020).  It refers to a representation $\\phi$ for which the *true* $P(Y|\\phi)$ is invariant across environments.\nWe would welcome better terminology, since $\\phi$ is in fact a representation, not a predictor, but the term \"invariant representation\" is already taken.  In any case, this notion seems different from what you are describing, and is meant to define this concept, not to characterize what IRM does. \n* Thanks for noticing; we'll fix it!\n* We believe it is accurate, but are open to being corrected!  Pan et al. (2010) certainly predates Ganin et al. (2015) [3], and we believe Pan et al. introduce this idea.  Note that Ganin et al. themselves claim that Pan et al. \"seek an explicit feature space transformation that would map source distribution into the target ones\". "}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "HQ51LuwsqHW", "original": null, "number": 11, "cdate": 1605574078011, "ddate": null, "tcdate": 1605574078011, "tmdate": 1605574078011, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "TZNTpU6k5-_", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "RE: \"misleading discussion\"", "comment": "We'll respond to each each of your 4 bullet points in order.\nBriefly, we agree that the 2nd was misleading (and will fix it!), but not the others.\n\n* We do not claim that IRM is the first method for invariant prediction. Our claim, verbatim, is: \"The first method for invariant prediction *to be compatible with modern deep learning problems and techniques* is Invariant Risk Minimization (IRM)\".\nWe've added the emphasis, since this was left our in your original review, and significantly changes the meaning of the statement.\nWe consider the following to be substantial and significant differences between IRM and ICP (already discussed by Arjovsky et al. 2019): 1) ICP assumes a linear model.  2) ICP relies on statistical hypothesis testing 3) ICP assumes that elements of X are either causal or non-causal, whereas IRM allows them to contain a mix of causal and non-causal elements.  These are the factors which make IRM, but not ICP, compatible with modern deep learning.  The nonlinear extension of ICP (Heinze-Deml et al. 2018) drops the linearity assumption, but it otherwise similar to ICP.\nIf you disagree with any of this, please explain?\nOr if you believe that ICP is \"compatible with modern deep learning problems and techniques\", even given these differences, can you please say why?\n* Thank you for pointing this out.  We will look into the works you mention and revise our submission accordingly.  \n* Far from misleading, we consider this a clear, precise, and correct statement about the differences of these methods.\nThe next sentence describes how this difference can be an advantage for IRM over REx, so we are not saying this to try and make ourselves look good!  However, we disagree that \"only the conditional mean matters.\" For instance, in risk-sensitive situations, we might like to match the true output distribution, not just its mean.\n* This paper seems to be about fairness, which doesn't seem relevant, since our goal is not fairness.  Indeed, we mention the fairness literature in related work to note the similarity of the methods *and* the difference of the goals.  Regarding theorem 3.3: At a glance, this looks like this theorem is a slight restatement of Theorem 4.1 of Zhao et al. 2019, which we already cite.  Can you please elaborate on why you think it is necessary to cite and discuss this work and this result?"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "S0eQv_PFlgA", "original": null, "number": 10, "cdate": 1605573431604, "ddate": null, "tcdate": 1605573431604, "tmdate": 1605573431604, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "TZNTpU6k5-_", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "RE: negative probabilities", "comment": "We're not sure we understand your criticism regarding negative probabilities; can you please elaborate in light of our comment below, if this doesn't clear it up?\n\nQuasiprobabilities are simply signed measures that integrate to 1, and allow negative probability to be assigned to some examples. \u00a0As we illustrate in Figure 3, negative weights $\\lambda$ do not *necessarily* lead to negative probabilities (in which case we REx can be considered exactly as an instance of DRO). \u00a0We acknowledge that negative probabilities may be a limitation for REx, and discuss this in more detail in Appendix E.2\n\nNonetheless, a central point of our work is that extrapolating beyond convex combinations provides new generalization powers (2nd line of page 2). \u00a0One reason this is significant is because Arjovsky et al. (2019) motivated IRM by showing inadequacies of the DRO approach in such setting. Our work shows that a simple generalization of DRO can work as well as IRM, or even better. \u00a0Given it's effectiveness, we consider the strangeness of this approach a strength: this is what sets us apart from the vast existing literature on robust optimization / DRO."}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "F7Mf3gab1A", "original": null, "number": 9, "cdate": 1605572726884, "ddate": null, "tcdate": 1605572726884, "tmdate": 1605572726884, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "TZNTpU6k5-_", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "RE: main claim", "comment": "Thank you for your review.\n\nAs I understand it, you were not convinced that REx can provide robustness by encouraging both: 1) robustness to covariate shift and 2) invariant prediction.\nThis is indeed a central claim of our paper, which we believe is well supported experimentally and conceptually.  \n\nResponding to your statements in more detail:\n\n> Although the argument that IRM is not robust to covariate shift in the feature space is true, out-of-domain generalization is not the original goal of IRM either. \n\nWe're not sure we've understood this argument.  Can you clarify what you mean by this? Our understanding is that out-of-*distribution* (OOD) generalization is the goal of IRM. \nWould you agree?  Did you mean something different by out-of-*domain* generalization?\n\nAs support the claim that IRM *is* focused on OOD generalization, we offer this quote from the abstract of Arjovsky et al. 2019:\n\"we leverage tools from causation to develop the mathematics of spurious and invariant correlations, in order to alleviate the excessive reliance of machine learning systems on data biases, allowing them to generalize to new test distributions.\"\nWe also refer to Arjovsky's thesis (see: https://cs.nyu.edu/dynamic/reports/?type=PhD), titled \"Out of Distribution Generalization in Machine Learning\", which is simply a somewhat expanded version of Arjovsky et al., 2019.  \n\nIn this thesis they also note that IRM may not help in the \"realizable\" case, where X is already an invariant predictor.  In this case, P(Y|X) is fixed by assumption, and so only covariate shift occurs.  Their discussion of this setting indicates their recognition of covariate shift as an important problem in OOD generalization.  Arjovsky's thesis recognizes this as an apparent limitation of IRM, while providing some arguments for optimism that invariant prediction could help even in this realizable case.\n\n>  My main concern is that it is not clear to me why Rex could deal with both covariate and concept shift together, as from the optimization formulation in Eq. (6), neither invariant predictor nor invariant representation is enforced. In particular, no theoretical analysis is given to justify this claim.\n\n* First, we want to note that our experiments provide a demonstration of REx dealing with both covariate and concept shift.\n* Our work also describes and illustrates conceptually why equalizing risks (i.e. \"flattening the risk plane\") could be expected to provide robustness to the kinds of shifts encountered at training time.  In the case of MM-REx this also follows directly from the definition and the linearity of the risk in the P(X,Y).\n* Appendix C.1/2 also provides simple examples illustrating how REx can encourage robustness to both kinds of shift.\n* In terms of theory, Appendix E.1 proves that REx can perform causal discovery.  This has a close correspondance with invariant prediction, as discussed in Arjovsky et al. (2019), although as far as we know, the details of the relationship between these two has not been fully elaborated.\n\nWe recognize that many of these important results we refer to are in the Appendix, and plan to emphasize these results more clearly in the main text in our revision.  We also welcome suggestions regarding which content would be most helpful to move to the main text.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "RJ4crRpVKl1", "original": null, "number": 7, "cdate": 1605238231776, "ddate": null, "tcdate": 1605238231776, "tmdate": 1605238231776, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "qAkAGs7eq3t", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "Official Blind Review #3", "comment": "Thank you for the response. You have addressed some of my concerns. Overall, this paper is well written. My reasons for penalizing score is as follows\uff1a\n\n**1 [My major concern]: This paper reviews various previous work but does not provide a clear comparison from them.**\n\nLet us focus on three methods: group DRO, IRM and REx.  Throughout this paper, you consider IRM as the main competitor and show that REx is sensitivity to covariate shift as an important qualitative difference from IRM that can help improve results. So the foundation of this work is that REx can do invariant prediction. \n\nIRM explains the invariant prediction from the view of causality. However, REx is an extension of the group distributional robustness. In your response, you use \" robustness to spurious features\" to interpret the invariant prediction. This is imprecise. REx is robust to the conditional distribution $P(Y|X)$ rather than spurious features. Notice that, in Table 1, group DRO cannot do invariant prediction. So I think the first task of this work is to explain: How REx discovers invariant prediction by introducing negative weights.\n\n**2 About the evaluation procedure**\n\nFigure 2 gives a good example. In your response, you say: \"The unseen domain is used as a proxy for how well the model would generalize in practice to unseen domains.\" We should note that, for the experiments on Colored MNIST, it is possible to achieve 0.9 test accuracy on the test domain with $P(Y=0|color=red)=0.9$ while the test accuracy on two training domains is just 0.1 and 0.2. This is not the OOD generalization in Eq.(1).   \n\nThe experiments should respond to the original OOD problem (1).  For VLCS and PACS,  all four domains mimic the unseen $\\mathcal{F}$. Then three domains are observed: two for training and one for validation.  Then the OOD generalization is to generalize to all four domains.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "Ra9BDwHABC", "original": null, "number": 6, "cdate": 1605213705057, "ddate": null, "tcdate": 1605213705057, "tmdate": 1605213705057, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "RiTpWcDI1QY", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "On splitting content between main text and appendix", "comment": "Thanks for your review.\n\nWe found it hard to decide which material to leave out of the main text.\nUltimately, we decided to focus on communicating the key ideas and intuitions of the paper with maximal clarity in the main text.\nWe'd love any more specific suggestions for which content you'd like to see cut from or added to the main text.\nIn particular, other reviewers suggested cutting Algorithm 1 and Figure 3, and including theoretical results from Appendix E.  What do you think of those suggestions?"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "bN4yW9qqqRk", "original": null, "number": 5, "cdate": 1605213289957, "ddate": null, "tcdate": 1605213289957, "tmdate": 1605213469808, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "5kN2X-tfv3q", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "RE comments and questions", "comment": "Thank you for these detailed comments and questions.\nThey identify some important points which we hope to clarify and address here and in our revision.\n\n1. In general, REx is meant to help provide robustness to *whatever* forms of distributional shift are observed in training domains.  In particular, this could include robustness to covariate shift and/or robustness to spurious features (i.e. invariant prediction).  IRM is the obvious point of comparison for invariant prediction; we emphasize REx's sensitivity to covariate shift as an important qualitative difference from IRM that can help improve results.\nWe show experimentally that REx can solve invariant prediction tasks (CMNIST (4.1) and our Reinforcement Learning experiments (4.4)).  We also prove that REx discovers an invariant predictor in somewhat restricted settings, in Appendix E.  \n\n2. The main novelty is that previous methods for invariant prediction (e.g. IRM) do not provide this robustness to covariate shift.  Our experiments show that this leads IRM to underperform in settings that involve both covariate shift and spurious features.  Robustness to covariate shift is important in practice because it is very common and there is no reason we expect it *not* to co-occur with spurious features.  \nThus if a method for invariant prediction cannot perform well in the presence of covariate shift (which our experiments indicate is true of IRM), then that method may fail to perform well in realistic settings.\n\n3. \nWe apologize for the confusion, and will clarify this in revision.\nEquation 1 serves two roles.\nFirst, it is introduced in a general form as the OOD problem definition, where F is typically unknown.\nSecond, it is used as an objective function, replacing the true, unknown F with some proxy.  In DRO, this proxy is the set of training domains.  In MM-REx, it is the set of extrapolated domains (see eqn5 and Appendix E.2).  In IRM, F is considered as domains corresponding to \"valid interventions\" (see bottom of page 10 and definition 7 of Arjovsky et al. 2019); this definition is similar to that of ICP (Peters et al. 2015) which considers the set of domains corresponding to interventions that preserve the causal mechanism of Y.\nIn Appendix E, we show a connection between equalizing risks (as in REx) and robustness to interventions.\n\n4. We agree this is not true in general.  Some conditions are given in Appendix E.  We believe our experiments also suggest that REx can be a good approach to discovering causal structure in practice.\n\n5. We follow the evaluation procedure of Gulrajani & Lopez-Paz 2020, which we consider a methodologically sound and meaningful evaluation (the focus of their paper is on proper methodology for OOD generalization).  As mentioned above, we don't know the true F.  The unseen domain is used as a proxy for how well the model would generalize in practice to unseen domains.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "qAkAGs7eq3t", "original": null, "number": 4, "cdate": 1605211178878, "ddate": null, "tcdate": 1605211178878, "tmdate": 1605211178878, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "5kN2X-tfv3q", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment", "content": {"title": "Please clarify argument for rejection.", "comment": "It looks to us like this is the main reason given for rejecting our submission:\n\"This paper reviews various previous work but does not provide a clear comparison from them.\"\nCan you please elaborate on this critique?\nAnd/or explain what, if not this, are the main reasons for recommending rejection?\n\n\nTo clarify our position:\n- We consider the methods listed in Table 1 to be our primary points of comparison.\n- This table is meant to provide a summary of the qualitative advantages of REx over these alternatives, i.e. a clear comparison.\n- Arjovsky et al. 2019 already demonstrated IRM's advantages over the other methods listed.\n- Our work focuses on comparing REx with IRM, and especially the advantage REx has in handling covariate shift.  We believe our experiments (4.1,4.2,4.4), and mathematical analyses (C.2) provide a clear comparison.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "foNTMJHXHXC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper268/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper268/Authors|ICLR.cc/2021/Conference/Paper268/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923872852, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Comment"}}}, {"id": "76OUF4kNz-1", "original": null, "number": 3, "cdate": 1603846797040, "ddate": null, "tcdate": 1603846797040, "tmdate": 1605024727288, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Review", "content": {"title": "An interesting approach, some comments clarifying the motivation and conditions for success", "review": "Overview:\n\nThe authors propose Risk Extrapolation (Rex) which is an invariance-based approach to domain generalization. The main idea is to go from worst-casing over a convex combination of domains to an affine set of domains (MM-Rex) or to penalize the variance (V-Rex). Evaluations compare to IRM and ERM and show uniform improvements.\n\nPositives:\n\nThe paper has a well written and motivated introduction, and there's substantial added expository material in the supplement. I would maybe tone down on the amount of bolded text. \n\nThe arguments are pretty well-thought-out, including some discussion of the differences between causal recovery, invariant prediction, and domain generalization. \n\nThe method itself seems simple (in a good way), though I have some questions below. I also wonder if the variance version of the objective can be tied to DRO based approaches via the fact that DRO on a chi-squared perturbation ball is equivalent to variance regularization of the risk. \n\nThe method seems to work well overall compared to IRM, and although it only matches ERM in the domain generalization benchmark, I think this is still a decent result given that most methods underperformed ERM.\n\nNegatives:\n\nThis is possibly a comment that refers to a paper that's too recent, so not addressing this comment won't affect my rating of the paper, but it seems worthwhile from a scientific perspective to address how recent negative theory results (Rosenfeld, Ravikumar, Risteski 2020) about IRM (and REx) affect the intro framing. In particular, I'd like to see the claim in the intro about how REx can extrapolate can be reconciled with the claims in the other paper that IRM and REx succeed under the same conditions as ERM. \n\nAs a note: I also don't think Williamson and Menon suggest the use of variance of risks. They explicitly state that the R_{sd} risk aggregator is not a fairness risk measure. The risks fulfilling the axioms in that paper are coherent risk measures, and I believe they will all fall under the category of RI methods through duality arguments.\n\nI can believe that minimizing the max risk is a reasonable thing to do, but it seems like an added leap of faith to want to actively increase the risk of the other domains since minimax risk would naturally equalize the different domains (if it's possible to achieve equal risk). It would be nice to see a clearer justification for this behavior.\n\nHaving looked at the supplement, there's substantial and good material there, and I would maybe suggest that the authors cut something like figure 3 and algorithm 1 to bring back some more intuition and motivation about REx, maybe some result from section E.\n\nHaving looked at the Thm 1 result, wouldn't 3 interventions on every variable also recover the true beta with ERM? This result also depends very heavily on fixed noise across environments. I do think this is a neat result though, and it might be useful to use this to give additional intuition about REx in the main text.\n\nDoes E.1.2 require pointwise homoskedasticity ? that seems wildly strong for a general result... In classification, I think this means that the map from x \u2192 y has to be deterministic and in general, for any log-probably loss, this says that the entropies have to be identical everywhere. \n\nMinor: \n\nMight be worth re-stating what the methods are in the experiment section (RI, for example, is defined pretty early on).\n\nDRO is usually expanded to distributionally robust optimization, not domain robust optimization.\n\nFigure 3 seems unnecessary.\n\nIs Epsilon_j in the statement of Theorem 1 a typo? is there a typo in the subscript of beta_j = beta_{0,j} for all j?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146825, "tmdate": 1606915810603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper268/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Review"}}}, {"id": "5kN2X-tfv3q", "original": null, "number": 2, "cdate": 1603685496867, "ddate": null, "tcdate": 1603685496867, "tmdate": 1605024727223, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "This paper studies the out-of-distribution (OOD) generalisation problem via risk extrapolation (REx). The authors propose two methods, MM-REx and V-REx, and empirically show that REx can recover the causal mechanisms on Colored MNIST, while also providing some robustness to covariate shift. The authors deal with the relationship between robustness, invariance and causality carefully, and provide experimental evidence beyond Colored MNIST. \n\nI vote for weakly rejecting. This paper reviews various previous work but does not provide a clear comparison from them.\n\nI have some comments and questions as follows:\n1. Contribution 1 and Table 1 state that REx is suitable for invariant prediction. In the experiments, the authors also take IRM as the competitor. So I think the main objective of REx is the invariant prediction, rather than covariate shift. However, the authors emphasize that REx can deal with covariate shift. Please explain more on how REx discovers the invariant prediction.\n2. When considering the invariant prediction, we aim to discover a stable conditional distribution $P(Y|X)$ or an invariant conditional mean $E[Y|X].$ However, the covariate shift refers to the changes in the distribution of X. What is the novelty of assuming covariate shift here? Please also figure out the importance of considering covariate shift under the invariant prediction.\n3. What is the expression (1) in Page 1? If (1) is the risk function of the OOD generalization problem, $\\mathcal{F}$ is unseen and should be the same throughout this paper. At the end of Page 1, you say: \"Our method minimax Risk Extrapolation (MM-REx) is an extension of DRO where $\\mathcal{F}$ instead contains affine combinations of training risks, see Figure 1.\" Does MM-REx solve a different OOD problem? Please figure out the definitions of $\\mathcal{F}$ of  DRO, MM-REx and IRM respectively.\n4. Please explain Contribution 3. In general, the equality of risks is not a sufficient condition of the causality. \n5. In the VLCS and PACS experiments, the evaluation is incorrect. In Section 4.3, the task is to train on three domains and generalize to the fourth one at test time. However, this test accuracy is not worst-case performance.  According to (1), the problem is to generalize to all four domains at test time and to find out the worst domain. Then Table 3 should report the average of the worst-domain accuracy.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146825, "tmdate": 1606915810603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper268/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Review"}}}, {"id": "TZNTpU6k5-_", "original": null, "number": 4, "cdate": 1603945534557, "ddate": null, "tcdate": 1603945534557, "tmdate": 1605024727159, "tddate": null, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "invitation": "ICLR.cc/2021/Conference/Paper268/-/Official_Review", "content": {"title": "Not convinced by the main claim of the paper", "review": "This manuscript studies the problem of domain generalization and proposes a method, dubbed Rex, for this purpose. The main movitation of this work over the invariant risk minimization (IRM) [1] paradigm is that IRM is not robust to covariate shift, while the authors claim that Rex can deal with both covariate shift and concept shift together. Although the argument that IRM is not robust to covariate shift in the feature space is true, out-of-domain generalization is not the original goal of IRM either. My main concern is that it is not clear to me why Rex could deal with both covariate and concept shift together, as from the optimization formulation in Eq. (6), neither invariant predictor nor invariant representation is enforced. In particular, no theoretical analysis is given to justify this claim. \n\nPerhaps what adds more confusion to me is that, what's the meaning of negative probability since the authors allow the combination weight \\lambda of different domains to be negative? As the authors have already pointed out on page 5 (Probabilities vs Risks), the risk is a linear functional of the joint distribution over X and Y, hence using affine combination in the risk functions from different domains directly translate to allowing the use of negative coefficient for probabilities. This is quite strange, since the mixture distribution is only a convex combination, not affine combination.\n\nFurthermore, I also found some of the discussions in the related work section misleading: \n-   \"The first method for invariant prediction ... is IRM\". This is not correct. As the authors have already realized, ICP [2] has been proposed in 2015, and the definition of IRM is essentially the same as ICP. \n\n-   The discussions about invariant representations on page 4 are not accurate. Only P_e(\\phi) (but not P_e(\\phi | Y)) is called invariant representations, and only this one can fail domain adaption if the marginal label distributions differ. The C-ADA does not try to find invariant P_e(\\phi | Y). Instead, it tries to find invariant P_e(\\phi x \\hat{Y}), where \\hat{Y} is the classifier output, and this is precisely the reason why C-ADA also fails under different label distributions. See more discussions in [4]. In fact, it has recently been shown in [4] that matching P_e(\\phi | Y) provably works for domain adaptation, see Theorem 3.1 of [4]. \n\n-   \"Also, unlike Rex, IRM seeks to match E[Y | \\phi], not the full P(Y | \\phi)\". Again, this discussion is misleading. For the purpose of out of domain generalization, the learner does not need to match the full distributions P(Y | \\phi). Only the conditional mean matters. See Theorem 4.1 of [5] as well as Theorem 3.3 of [6] in the context of fairness.\n\n-   About the discussion on fairness of equalizing risk across groups. In fact a sufficient condition for this goal has been proved in Theorem 3.3 of [6]. Given the close relationship between these two problems, I feel it's necessary to cite and have a discussion of this result here as well. \n\n\nMore detailed comments:\n-   The reference of David et al., 2010 should be Ben-David et al. 2010 on page 2\n\n-   I think the naming of invariant prediction on page 2 is not very accurate. To be precise, as long as the same hypothesis (classifier) is used over different domains, this amounts to be an invariant prediction rule. Instead, what IRM enforces is the invariant OPTIMAL predictor, i.e., invariant conditional means.  \n\n-   Eq. (3) is not correct: the second term is a weighted combination of samples from different domains, where the larger the sample size from a domain the more weight it has in the combination. However, the right most term has a wrong weight for a domain. The correct one should be |D_e| / \\sum_e |D_e|. \n\n-   On page 4, the citation of Pan et al. 2010 is not accurate. Invariant representations for domain adaptation is first proposed by [3].\n\n\n\n[1]     Invariant Risk Minimization\n[2]     Causal inference using invariant prediction: identification and confidence intervals\n[3]     Unsupervised Domain Adaptation by Backpropagation\n[4]     Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift\n[5]     On Learning Invariant Representation for Domain Adaptation\n[6]     Inherent Tradeoffs in Learning Fair Representations\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper268/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper268/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)", "authorids": ["~David_Krueger1", "~Ethan_Caballero1", "~Joern-Henrik_Jacobsen1", "~Amy_Zhang1", "~Jonathan_Binas1", "~R\u00e9mi_LE_PRIOL1", "~Dinghuai_Zhang1", "~Aaron_Courville3"], "authors": ["David Krueger", "Ethan Caballero", "Joern-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "R\u00e9mi LE PRIOL", "Dinghuai Zhang", "Aaron Courville"], "keywords": ["out of distribution", "domain generalization", "invariant risk minimization", "robust optimization", "invariant causal prediction", "spurious features", "generalization"], "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model\u2019s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "krueger|outofdistribution_generalization_via_risk_extrapolation_rex", "one-sentence_summary": "Risk extrapolation is a simple robust optimization approache for Out-of-Distribution (OOD) generalization and invariant prediction.", "pdf": "/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=n0igWqZZgr", "_bibtex": "@misc{\nkrueger2021outofdistribution,\ntitle={Out-of-Distribution Generalization via Risk Extrapolation ({\\{}RE{\\}}x)},\nauthor={David Krueger and Ethan Caballero and Joern-Henrik Jacobsen and Amy Zhang and Jonathan Binas and R{\\'e}mi LE PRIOL and Dinghuai Zhang and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=foNTMJHXHXC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "foNTMJHXHXC", "replyto": "foNTMJHXHXC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper268/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538146825, "tmdate": 1606915810603, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper268/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper268/-/Official_Review"}}}], "count": 22}