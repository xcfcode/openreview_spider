{"notes": [{"id": "HkxDheHFDr", "original": "HJeJDe-YDS", "number": 2540, "cdate": 1569439918974, "ddate": null, "tcdate": 1569439918974, "tmdate": 1577168223516, "tddate": null, "forum": "HkxDheHFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "LAVAE: Disentangling Location and Appearance", "authors": ["Andrea Dittadi", "Ole Winther"], "authorids": ["adit@dtu.dk", "olwi@dtu.dk"], "keywords": ["structured scene representations", "compositional representations", "generative models", "unsupervised learning"], "TL;DR": "Generative model that learns structured, interpretable, object-based representations of visual scenes, disentangling object location and appearance.", "abstract": "We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "pdf": "/pdf/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "paperhash": "dittadi|lavae_disentangling_location_and_appearance", "original_pdf": "/attachment/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "_bibtex": "@misc{\ndittadi2020lavae,\ntitle={{\\{}LAVAE{\\}}: Disentangling Location and Appearance},\nauthor={Andrea Dittadi and Ole Winther},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxDheHFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Cz0iRDKaaz", "original": null, "number": 1, "cdate": 1576798751635, "ddate": null, "tcdate": 1576798751635, "tmdate": 1576800884040, "tddate": null, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2540/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents a VAE approach where the model learns representation while disentangling the location and appearance information. The reviewers found issues with the experimental evaluation of the paper, and have given many useful feedback. None of the reviewers were willing to change their score during the discussion period. with the current score, the paper does not make the cut for ICLR, and I recommend to reject this paper. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LAVAE: Disentangling Location and Appearance", "authors": ["Andrea Dittadi", "Ole Winther"], "authorids": ["adit@dtu.dk", "olwi@dtu.dk"], "keywords": ["structured scene representations", "compositional representations", "generative models", "unsupervised learning"], "TL;DR": "Generative model that learns structured, interpretable, object-based representations of visual scenes, disentangling object location and appearance.", "abstract": "We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "pdf": "/pdf/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "paperhash": "dittadi|lavae_disentangling_location_and_appearance", "original_pdf": "/attachment/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "_bibtex": "@misc{\ndittadi2020lavae,\ntitle={{\\{}LAVAE{\\}}: Disentangling Location and Appearance},\nauthor={Andrea Dittadi and Ole Winther},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxDheHFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724593, "tmdate": 1576800276260, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2540/-/Decision"}}}, {"id": "B1xFs1anFH", "original": null, "number": 1, "cdate": 1571766176880, "ddate": null, "tcdate": 1571766176880, "tmdate": 1574449642373, "tddate": null, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2540/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper introduces a compositional generative model of images, where the image is described by a variable number of latent variables. Moreover, the latent variables are disentangled, in the sense that they represent different parts of the scene, and where appearance and location are described separately. While the generative model is very similar to AIR [1], the central claim of the paper is that the proposed inference scheme can generalize to a much higher number of objects than seen during training, which is demonstrated empirically, and with which AIR struggles. Better generalization is achieved by removing the recurrent core of AIR and replacing it with a fully-convolutional inference network, which predicts appearance vectors at every location in the feature map. The appearance vectors are then stochastically sampled without replacement according to a location distribution. Unlike AIR, this approach does not model the object scale.\n\nI recommend REJECTing this paper. While the improved generalization performance is a useful property, it has been achieved previously and in a very similar fashion by SPAIR [2], which follows a similar model design. SPAIR still uses an RNN, while the proposed approach does not, but this is only a small simplification and does not warrant publication at a top tier conference. There are no other contributions in this paper. Additionally, on the one hand, the experimental evaluation is insufficient: the proposed approach is compared only against a fully-convolutional VAE, while very similar models like AIR [1], SPAIR [2], SuPAIR [3] are not considered. On the other hand, the experimental section focuses on the disentanglement of representations, which is a) evident from the model construction and b) achieved in all previous models. \n\nThe paper is clearly written, and the presented generative model is interesting. Having said that, the problem that this paper addresses is mostly solved in [2] and [4]; also both [2] and [4] scale the general approach introduced in [1] to much higher number of objects (in the hundreds) and more general settings (real images, atari games).\n\n[1] Eslami et. al., \u201cAttend, Infer, Repeat:...\u201d, NIPS 2016.\n[2] Crawford and Pineau, \u201cSpatially Invariant Unsupervised Object Detection with Convolutional Neural Networks\u201d, AAAI 2019.\n[3] Stelzner et. al., \u201cFaster Attend-Infer-Repeat with Tractable Probabilistic Models\u201d, ICML 2019.\n[4] Jiang et. al., \u201cScalable Object-Oriented Sequential Generative Models\u201d, arXiv 2019.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LAVAE: Disentangling Location and Appearance", "authors": ["Andrea Dittadi", "Ole Winther"], "authorids": ["adit@dtu.dk", "olwi@dtu.dk"], "keywords": ["structured scene representations", "compositional representations", "generative models", "unsupervised learning"], "TL;DR": "Generative model that learns structured, interpretable, object-based representations of visual scenes, disentangling object location and appearance.", "abstract": "We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "pdf": "/pdf/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "paperhash": "dittadi|lavae_disentangling_location_and_appearance", "original_pdf": "/attachment/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "_bibtex": "@misc{\ndittadi2020lavae,\ntitle={{\\{}LAVAE{\\}}: Disentangling Location and Appearance},\nauthor={Andrea Dittadi and Ole Winther},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxDheHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575420307441, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2540/Reviewers"], "noninvitees": [], "tcdate": 1570237721399, "tmdate": 1575420307456, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2540/-/Official_Review"}}}, {"id": "SygdvztCKB", "original": null, "number": 3, "cdate": 1571881568102, "ddate": null, "tcdate": 1571881568102, "tmdate": 1574376680801, "tddate": null, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2540/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "\nAfter reading reviews and comments I have decided to confirm the initial rating.\n\n===================\n\nThe work presents an approach to encode latent representations of objects such that there are separate and disentangled representations for location and appearance of objects in a scene.  The work presents impressive qualitative results which shows the practical use of the proposal on multi-mnist and multi-dSprites. \n\nWhile the use of inference networks proposing positions for the network as a means of improving the disentanglement is clever and seems novel, though not unlike inference sub-networks which are well-known in conditional generation, the evaluation is not up to a standard I can endorse, resulting in a recommendation to reject. \n\nDespite the interesting qualitative results, I will have to quote the work in saying, \u201cAll methods cited here are likelihood based so they can and should be compared in terms of test log likelihood. We leave this for future work.\u201d.  Indeed the cited works should have been evaluated against, especially Greff et al. 2019, Nash et al, 2017, and Eslami et al, 2016, which are all very similar.  As written it\u2019s impossible to tell whether this work actually improves over the state of the art, we only have the constructed baseline (which as a community we all know clearly would not have worked). \n\nA figure showing the relevant submodules of the network architecture and what they do in relation to the overall method would be helpful to understand the pipeline and how the inference network relates to the whole.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LAVAE: Disentangling Location and Appearance", "authors": ["Andrea Dittadi", "Ole Winther"], "authorids": ["adit@dtu.dk", "olwi@dtu.dk"], "keywords": ["structured scene representations", "compositional representations", "generative models", "unsupervised learning"], "TL;DR": "Generative model that learns structured, interpretable, object-based representations of visual scenes, disentangling object location and appearance.", "abstract": "We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "pdf": "/pdf/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "paperhash": "dittadi|lavae_disentangling_location_and_appearance", "original_pdf": "/attachment/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "_bibtex": "@misc{\ndittadi2020lavae,\ntitle={{\\{}LAVAE{\\}}: Disentangling Location and Appearance},\nauthor={Andrea Dittadi and Ole Winther},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxDheHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575420307441, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2540/Reviewers"], "noninvitees": [], "tcdate": 1570237721399, "tmdate": 1575420307456, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2540/-/Official_Review"}}}, {"id": "HyeSe_lhoS", "original": null, "number": 2, "cdate": 1573812205219, "ddate": null, "tcdate": 1573812205219, "tmdate": 1573812205219, "tddate": null, "forum": "HkxDheHFDr", "replyto": "HJeeQSlnsS", "invitation": "ICLR.cc/2020/Conference/Paper2540/-/Official_Comment", "content": {"title": "I'm keeping my score", "comment": "Thanks for the response. I appreciate that you are going to compare your method with other baselines. I cannot take your word for it when evaluating the paper, though, and it has no influence on my score. \n\nOther remarks:\n* I do realize that SCALOR was made available only after the ICLR deadline. I cited it because it solves a very similar problem.\n* Even if the other works do not report marginal likelihoods or ELBOs, I don't think that merely reporting them does constitute a big contribution. Moreover, Sequential AIR of Kosiorek et. al. does report ELBOs and estimates of the log-ikelihood for AIR.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LAVAE: Disentangling Location and Appearance", "authors": ["Andrea Dittadi", "Ole Winther"], "authorids": ["adit@dtu.dk", "olwi@dtu.dk"], "keywords": ["structured scene representations", "compositional representations", "generative models", "unsupervised learning"], "TL;DR": "Generative model that learns structured, interpretable, object-based representations of visual scenes, disentangling object location and appearance.", "abstract": "We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "pdf": "/pdf/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "paperhash": "dittadi|lavae_disentangling_location_and_appearance", "original_pdf": "/attachment/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "_bibtex": "@misc{\ndittadi2020lavae,\ntitle={{\\{}LAVAE{\\}}: Disentangling Location and Appearance},\nauthor={Andrea Dittadi and Ole Winther},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxDheHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxDheHFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2540/Authors", "ICLR.cc/2020/Conference/Paper2540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2540/Reviewers", "ICLR.cc/2020/Conference/Paper2540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2540/Authors|ICLR.cc/2020/Conference/Paper2540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139841, "tmdate": 1576860557807, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2540/Authors", "ICLR.cc/2020/Conference/Paper2540/Reviewers", "ICLR.cc/2020/Conference/Paper2540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2540/-/Official_Comment"}}}, {"id": "HJeeQSlnsS", "original": null, "number": 1, "cdate": 1573811479846, "ddate": null, "tcdate": 1573811479846, "tmdate": 1573811479846, "tddate": null, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2540/-/Official_Comment", "content": {"title": "Response to reviewers", "comment": "We thank the reviewers for their thorough and helpful comments. The reviewers agree that our approach is interesting and qualitative results are impressive, but also deem the experimental evaluation insufficient, and suggest that an experimental comparison with related methods is needed. We agree with these general points, and we plan to extend the experimental section and compare our results with methods such as AIR [1], SPAIR [2], and SuPAIR [3] in the future.\n\nHowever, we would like to make a few remarks:\n\n* SCALOR [4] is a concurrent submission to ICLR, and was uploaded to arxiv after the deadline (October 6).\n\n* The papers cited by the reviewers are not really concerned about the generative aspect of the models, and they do not show generated samples or report log-likelihood scores. Comparison with previous methods is mostly qualitative and not necessarily very thorough. For example, in the AIR paper the ELBO is only shown in plots and not explicitly reported, and estimates of the marginal log-likelihood are not mentioned. The SuPAIR paper reports the ELBO for SuPAIR and AIR, but because of the choice of data likelihood, these scores are not comparable, as also stated by the authors. The performance evaluation of SPAIR is entirely focused on counting accuracy and related downstream tasks, overlooking typical generative modeling metrics.\n\n* Real-world examples are rarely considered as they present a different set of difficulties. SCALOR is tested on crowd tracking from surveillance cameras, but the fact that qualitative results on that data set are good is debatable (predicted frames after only 1-2 time steps).\n\n* \"Has been achieved previously and in a very similar fashion by SPAIR\": there are in fact different mechanisms involved, and experimental results (which are now missing) should show to what extent SPAIR and our model can solve the problem under consideration.\n\n* MNIST digits are rescaled so that more of them can fit in an image, as done in related works. They are binarized so the data can be modeled as a set of independent Bernoulli variables -- a common approach in generative modeling on simple data sets.\n\n* We use the definition of disentanglement used for example in [5] and [6].\n\n\n\n[1] Eslami et al., \u201cAttend, Infer, Repeat:...\u201d, NIPS 2016.\n[2] Crawford and Pineau, \u201cSpatially Invariant Unsupervised Object Detection with Convolutional Neural Networks\u201d, AAAI 2019.\n[3] Stelzner et al., \u201cFaster Attend-Infer-Repeat with Tractable Probabilistic Models\u201d, ICML 2019.\n[4] Jiang et al., \u201cScalable Object-Oriented Sequential Generative Models\u201d, arXiv 2019.\n[5] Higgins et al., \"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\", ICLR 2017.\n[6] Locatello et al., \"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\", ICML 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LAVAE: Disentangling Location and Appearance", "authors": ["Andrea Dittadi", "Ole Winther"], "authorids": ["adit@dtu.dk", "olwi@dtu.dk"], "keywords": ["structured scene representations", "compositional representations", "generative models", "unsupervised learning"], "TL;DR": "Generative model that learns structured, interpretable, object-based representations of visual scenes, disentangling object location and appearance.", "abstract": "We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "pdf": "/pdf/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "paperhash": "dittadi|lavae_disentangling_location_and_appearance", "original_pdf": "/attachment/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "_bibtex": "@misc{\ndittadi2020lavae,\ntitle={{\\{}LAVAE{\\}}: Disentangling Location and Appearance},\nauthor={Andrea Dittadi and Ole Winther},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxDheHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxDheHFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2540/Authors", "ICLR.cc/2020/Conference/Paper2540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2540/Reviewers", "ICLR.cc/2020/Conference/Paper2540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2540/Authors|ICLR.cc/2020/Conference/Paper2540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139841, "tmdate": 1576860557807, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2540/Authors", "ICLR.cc/2020/Conference/Paper2540/Reviewers", "ICLR.cc/2020/Conference/Paper2540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2540/-/Official_Comment"}}}, {"id": "H1xyP11TYH", "original": null, "number": 2, "cdate": 1571774295312, "ddate": null, "tcdate": 1571774295312, "tmdate": 1572972325179, "tddate": null, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2540/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a probabilistic generative model for identifying the location, type and colour of images placed within a grid. The paper is in general well written and presents a large number of visual images demonstrating that the approach works. The main concerns with the paper are as follows:\n\n1) The implementation details for the work are relegated to an appendix. As this is a core concept for the work one would expect this to be presented in the main body of the work.\n\n2) Although there are a large number of visual images there is little in the way of analytical analysis of the work. As a particular concern the authors claim that figure 3 \u2018prove\u2019 that objects are disentangled. From Maths 101 I remember it being drummed into us that \u2018proof by example is not a proof\u2019.\n\n3) The are parts of the process which are not explained for example, why is the following conducted? \u2018The digits are first rescaled from their original size (28 \u00d7 28) to 15 \u00d7 15 by bilinear interpolation, and finally binarized by rounding.\u2019\n\n4) I would also like to know why this is called disentangling as the whole process prevents the \u2018icons\u2019 from overlapping. Disentangle normally refers to things which are at least overlapping. There are examples of works in this area.\n\n5) The example cases are simple. One would expect at least one real-world example."}, "signatures": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2540/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LAVAE: Disentangling Location and Appearance", "authors": ["Andrea Dittadi", "Ole Winther"], "authorids": ["adit@dtu.dk", "olwi@dtu.dk"], "keywords": ["structured scene representations", "compositional representations", "generative models", "unsupervised learning"], "TL;DR": "Generative model that learns structured, interpretable, object-based representations of visual scenes, disentangling object location and appearance.", "abstract": "We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "pdf": "/pdf/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "paperhash": "dittadi|lavae_disentangling_location_and_appearance", "original_pdf": "/attachment/8884701df9ac56a05d74581f7eb58816ba9484d6.pdf", "_bibtex": "@misc{\ndittadi2020lavae,\ntitle={{\\{}LAVAE{\\}}: Disentangling Location and Appearance},\nauthor={Andrea Dittadi and Ole Winther},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxDheHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxDheHFDr", "replyto": "HkxDheHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575420307441, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2540/Reviewers"], "noninvitees": [], "tcdate": 1570237721399, "tmdate": 1575420307456, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2540/-/Official_Review"}}}], "count": 7}