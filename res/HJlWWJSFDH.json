{"notes": [{"id": "HJlWWJSFDH", "original": "SJl5F_idwS", "number": 1533, "cdate": 1569439481227, "ddate": null, "tcdate": 1569439481227, "tmdate": 1583912032268, "tddate": null, "forum": "HJlWWJSFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "9vWLSWYvh1", "original": null, "number": 1, "cdate": 1576798725818, "ddate": null, "tcdate": 1576798725818, "tmdate": 1576800910682, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "All three reviewers are consistently positive on this paper. Thus an accept is recommended.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726676, "tmdate": 1576800278863, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Decision"}}}, {"id": "rJgKcidXir", "original": null, "number": 5, "cdate": 1573256081480, "ddate": null, "tcdate": 1573256081480, "tmdate": 1573256081480, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "BJeDySijKr", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment", "content": {"title": "Re: Official Blind Review #3", "comment": "We thank the reviewer for acknowledging the novelty of our work and for noting that our experiments are thorough.\n\nThank you for pointing out a related preprint by Z. Hu et al. [arXiv:1905.13728]. We note the work by Z. Hu et al. was developed independently and concurrently to our work here, and we were not aware of it at the time of writing our paper. We shall cite the preprint and include a discussion in our paper. \n\nBriefly, the key difference between our work and that of Hu et al. is that Hu et al. consider a more restrictive setting where graphs are completely unlabeled (i.e., graphs have no node features). Hu et al. then focus on extracting generic graph properties of unlabeled graphs by pre-training on randomly-generated graphs. While the approach is interesting, the limitation of such an approach is that it improves performance only marginally over ordinary supervised classification of the original attributed graphs. This is because it is hard for random unlabeled graphs to capture domain-specific knowledge that is useful for a specific application. Moreover, in practice, graphs tend to have labels together with rich node and edge attributes, but Hu et al.\u2019s approach cannot naturally leverage such attribute information, which then results in limited gains. \n\nIn principle, we could compare our approach against Hu et al., however, right now, this would be extremely challenging because of the following reasons. (1) We cannot find a public implementation of Hu et al.\u2019s approach for reliable comparison. (2) Reimplementing their method requires knowledge of many specific implementational details and design choices (feature extraction, graph generation, etc.), which are not discussed in their preprint. (3) Finally, their pre-trained GNN operates on unlabeled graphs, and so it cannot be directly applied to our datasets of labeled graphs.\n\nLastly, in contrast to Hu et al., our work focuses on important real-world domains, where one wants to pre-train GNNs by utilizing the abundant graph, node, and edge attributes. Importantly, our approach is able to learn a domain-specific data distribution that is useful for downstream prediction. We demonstrate on two application domains that such practical settings (i.e., labeled graphs with naturally-given node and edge attributes) are very important to consider and that our pre-training can substantially improve model performance."}, "signatures": ["ICLR.cc/2020/Conference/Paper1533/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWWJSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1533/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1533/Authors|ICLR.cc/2020/Conference/Paper1533/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154609, "tmdate": 1576860554365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment"}}}, {"id": "BJgEvjdmsH", "original": null, "number": 4, "cdate": 1573256027632, "ddate": null, "tcdate": 1573256027632, "tmdate": 1573256027632, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "SkxAD0ECtB", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment", "content": {"title": "Re: Official Blind Review #1", "comment": "We thank the reviewer for acknowledging the technical aspects of the paper and for noting that our\u200b \u200bresults\u200b \u200bare\u200b \u200bsolid\u200b \u200band\u200b \u200bour\u200b \u200banalysis\u200b \u200bis\u200b \u200bthorough.\n\nRE: Source code\nThe reviewer makes an important point about the availability of the source code. To address this point, in the link privately shared with the reviewers, we have provided all of our code, datasets together with their train/test splits, as well as our pre-trained models, to help with the reproducibility of our results. We note that we will share PyTorch implementations of all pre-training methods and datasets with the community upon publication. Please feel free to ask any further questions regarding our code and implementation.\n\nRE: Linear time complexity in Appendix F\nWe acknowledge that the time complexity of our pre-training methods was not well explained in Appendix F. In Figure 2 (a) we show that we only sample one node per graph. We then use breadth-first search to extract a K-hop neighborhood of the node, which takes at most linear time with respect to the number of edges in the graph. As a result, pre-training via context prediction has linear time complexity. We will edit Appendix F to include more detailed information and cover this important point. \n\nPlease let us know if you have any further questions or comments!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1533/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWWJSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1533/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1533/Authors|ICLR.cc/2020/Conference/Paper1533/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154609, "tmdate": 1576860554365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment"}}}, {"id": "SJlhViuQsH", "original": null, "number": 3, "cdate": 1573255987904, "ddate": null, "tcdate": 1573255987904, "tmdate": 1573255987904, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "BkxPCGTX9B", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment", "content": {"title": "Re: Official Blind Review #2", "comment": "We thank the reviewer for insightful feedback and for noting that our\u200b experiments \u200bare\u200b \u200bsolid\u200b and our setup and analyses are sound. The reviewer asks great questions, and we provide the answers below. \n\nRE: Total running time\nThe reviewer raises an interesting point about total training time, which includes the time to pre-train a GNN and the time to fine-tune it on a downstream task. To address this point, below, we give the results of the total training time as well as the amortized total time over different downstream tasks. We will include detailed results and a discussion in the final version of the paper. \n\nWe note that although pre-training does take some time, it is a one-time-effort only. That is, we pre-train a GNN model only once and then reuse it many times by fine-tuning the model on any number of downstream prediction tasks. Overall, we find that GNNs, once pre-trained, tend to converge much faster on downstream tasks. Most importantly, we find (details below) that validation set performance converges 5-12 times more quickly when GNNs are pre-trained. We emphasize that this cannot be achieved by mere training of (non-pre-trained) GNNs longer. The following summarizes training time for chemistry and biology datasets. \n\n1) Chemistry dataset (single GPU implementation)\n**Pre-training**\n\u2014 Self-supervised pre-training: 24 hours \n\u2014 Supervised pre-training: 11 hours\n\n**Fine-tuning on MUV dataset** [Time to achieve the best validation set AUC] \n\u2014 From random initialization (i.e., no pre-training): 1 hour; 74.9% AUC\n\u2014 From a pre-trained GNN: 5 minutes; 85.3% AUC\n\n2) Biology dataset\n**Pre-training**\n\u2014 Self-supervised pre-training:  3.8 hours \n\u2014 Supervised pre-training: 2.5 hours\n\n**Fine-tuning** [Time to achieve the best validation set AUC] \n\u2014 From random initialization (i.e., no pre-training): 50 minutes; 84.8% AUC\n\u2014 From a pre-trained GNN: 10 minutes; 88.8% AUC\n\nOn chemistry dataset, we see that fine-tuning a pre-trained GNN on the MUV required only 5 min. This is in sharp contrast with training a GNN from scratch, which required 12x more time, yet it gave a worse performance. We can reach similar conclusions on the biology dataset. We thus recommend using pre-trained models whenever possible as they can give better performance and can be reused for any number of downstream tasks. \n\nWe shall add these results and explanations to the final version of the paper.\n\nRE: Analysis of different pre-training strategies\nThank you for bringing up this valuable point. We agree that it is important to understand why some pre-training strategies work better over others.  Our key insight backed up with extensive empirical evidence is that a combination of graph-level and node-level methods (Figure 1) is important because it allows the model to capture both local and global semantics of graphs. Further, we find that our structure-based node-level methods (Context Prediction and Attribute Masking) are preferred over position-based node-level methods (Edge Prediction, Deep Graph Infomax). As future work, we plan to further investigate what graph-level and node-level methods are most useful in different domains, and understand what domain-specific knowledge has been learned by the pre-trained models."}, "signatures": ["ICLR.cc/2020/Conference/Paper1533/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWWJSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1533/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1533/Authors|ICLR.cc/2020/Conference/Paper1533/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154609, "tmdate": 1576860554365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment"}}}, {"id": "rkesRlWXoH", "original": null, "number": 3, "cdate": 1573224659283, "ddate": null, "tcdate": 1573224659283, "tmdate": 1573224659283, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Public_Comment", "content": {"title": "Related Work", "comment": "Dear authors,\nThis is a very interesting paper! We would like to draw your attention to our recent paper: https://arxiv.org/abs/1905.13192\nOn PTC, our graph neural tangent kernel achieves 67.9, which is the best result to date (we are aware of)."}, "signatures": ["~Simon_Shaolei_Du1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Simon_Shaolei_Du1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWWJSFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504193392, "tmdate": 1576860587529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Public_Comment"}}}, {"id": "BJeDySijKr", "original": null, "number": 1, "cdate": 1571693791463, "ddate": null, "tcdate": 1571693791463, "tmdate": 1572972456068, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes pre-training strategies (PT) for graph neural networks (GNN) from both node and graph levels. Two new large-scale pre-training datasets are created and extensive experiments are conducted to demonstrate the benefits of PT upon different GNN architectures. I am relative positive for this work. Detail review of different aspects and questions are as follows. \n\nNovelty: As far as I know, this work is among the earliest works to think about GNN pre-training. The most similar paper at the same period is [Z Hu, arXiv:1905.13728]. I read both papers and found they have similar idea about PT although they have different designs. This paper leverages graph structure (e.g., context neighbors) and supervised labels/attributes (e.g., node attributes, graph labels) for PT. These strategies are not surprising for me and the novelty is incremental. \n\nExperiment: The experiments are overall good. The authors created two new large scale pre-training graph datasets. Experimental results of different GNN architectures w/o different PT for different tasks are provided. Comparing to non-pretraining GNN, the improvements are significant for most cases. \n\nWriting: The writing is good and easy to follow. \n\nQuestions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728]. Comparing to the other work, what are strengths of this work? In addition, have the authors compared the performances of their work and [Z Hu, arXiv:1905.13728] using the same data? "}, "signatures": ["ICLR.cc/2020/Conference/Paper1533/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1533/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575088903970, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1533/Reviewers"], "noninvitees": [], "tcdate": 1570237735992, "tmdate": 1575088903986, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Official_Review"}}}, {"id": "SkxAD0ECtB", "original": null, "number": 2, "cdate": 1571864166202, "ddate": null, "tcdate": 1571864166202, "tmdate": 1572972456032, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes new pre-training strategies for GNN with both a node-level and a graph-level pretraining. For the node-level pretraining, the goal is to map nodes with similar surrounding structures to nearby context (similarly to word2vec). The main problem is that directly predicting the context is intractable because of combinatorial explosion. The main idea is then to use an additional GNN to encode the context and to learn simultaneously the main GNN and the context GNN via negative sampling. Another method used is attribute masking where some masked node and edge attributes need to be predicted by the GNN. For graph-level pretraining, some general graph properties need to be predicted by the graph.\nExperiments are conducted on datasets in the chemistry domain and the biology domain showing the benefit of the pre-training.\n\nThe paper addresses an important and timely problem. It is a pity that the code is not provided. In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph. In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct."}, "signatures": ["ICLR.cc/2020/Conference/Paper1533/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1533/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575088903970, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1533/Reviewers"], "noninvitees": [], "tcdate": 1570237735992, "tmdate": 1575088903986, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Official_Review"}}}, {"id": "BkxPCGTX9B", "original": null, "number": 3, "cdate": 1572225742654, "ddate": null, "tcdate": 1572225742654, "tmdate": 1572972455989, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors introduce strategies for pre-training graph neural networks. Pre-training is done at the node level as well as at the graph level. They evaluate their approaches on two domains, biology and chemistry on a number of downstream tasks. They find that not all pre-training strategies work well and can in fact lead to negative transfer. However, they find that pre-training in general helps over non pre-training.\n\nOverall, this paper was well written with useful illustrations and clear motivations. The authors evaluate their models over a number of datasets. Experimental construction and analysis also seems sound.\n\nI would have liked to see a bit more analysis as to why some pre-training strategies work over others. However, the authors mention that this is in their planned future work.\n\nAlso, in figure 4, the authors mention that their pre-trained models tend to converge faster. However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1533/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1533/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575088903970, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1533/Reviewers"], "noninvitees": [], "tcdate": 1570237735992, "tmdate": 1575088903986, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Official_Review"}}}, {"id": "HJeOL_2WtS", "original": null, "number": 2, "cdate": 1571043408194, "ddate": null, "tcdate": 1571043408194, "tmdate": 1571043452877, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "BkgPVKOq_S", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Public_Comment", "content": {"comment": "Thank you for your reply. \n\nThese contributions will bring many benefits to the communities (ML, Bio, and Chemical).\nBecause the pre-training GNNs is really essential but was a challenging problem.\n\nI am looking forward to the available URLs for data and models.\n\nNice work!", "title": "Re: Re: Available repository for Reproducibility "}, "signatures": ["~Junhyun_Lee1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Junhyun_Lee1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWWJSFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504193392, "tmdate": 1576860587529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Public_Comment"}}}, {"id": "BkgPVKOq_S", "original": null, "number": 1, "cdate": 1570568494912, "ddate": null, "tcdate": 1570568494912, "tmdate": 1570568494912, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "HkgiRWKV_S", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment", "content": {"comment": "We thank the reader for his comments and suggestions. We are in total agreement with the suggestions.\nWe are working on a comprehensive project website where we will be releasing clean and easy to use data \ntogether with the splits, which will greatly help the community to move beyond small graph classification benchmarks.\n\nWe are also working on releasing the code as well as the final pre-trained models so that the community\ncan benefit from this work and use the models/data for other downstream prediction tasks.", "title": "Re: Available repository for Reproducibility"}, "signatures": ["ICLR.cc/2020/Conference/Paper1533/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWWJSFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1533/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1533/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1533/Authors|ICLR.cc/2020/Conference/Paper1533/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154609, "tmdate": 1576860554365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Official_Comment"}}}, {"id": "HkgiRWKV_S", "original": null, "number": 1, "cdate": 1570177490759, "ddate": null, "tcdate": 1570177490759, "tmdate": 1570262952468, "tddate": null, "forum": "HJlWWJSFDH", "replyto": "HJlWWJSFDH", "invitation": "ICLR.cc/2020/Conference/Paper1533/-/Public_Comment", "content": {"comment": "Because the pre-trained models are commonly used for various down-stream tasks, I think there should be available URL for codes and pre-trained weights to test its scalability (transferability).\n\nSecondly, \"Out-of-distribution prediction (scaffold split)\" is conducted at this work, so it would be better if there are URL providing dataset split.\n\nThird, as I found, there are several settings for download of large scale dataset (ZINC), therefore the providing of large scale dataset you used makes this work more reproducible.\n\n\n* To sum up, could you please provide the URL for codes, large scale datasets, and down-stream datasets (with scaffold split)?", "title": "Available repository for Reproducibility"}, "signatures": ["~Junhyun_Lee1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Junhyun_Lee1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["weihuahu@stanford.edu", "liubowen@stanford.edu", "joegomes@stanford.edu", "marinka@cs.stanford.edu", "pliang@cs.stanford.edu", "pande@stanford.edu", "jure@cs.stanford.edu"], "title": "Strategies for Pre-training Graph Neural Networks", "authors": ["Weihua Hu*", "Bowen Liu*", "Joseph Gomes", "Marinka Zitnik", "Percy Liang", "Vijay Pande", "Jure Leskovec"], "pdf": "/pdf/0572544c8eb8ee9096e54a46c51dc6b6abf3225a.pdf", "TL;DR": "We develop a strategy for pre-training Graph Neural Networks (GNNs) and systematically study its effectiveness on multiple datasets, GNN architectures, and diverse downstream tasks.", "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\u00efve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.", "keywords": ["Pre-training", "Transfer learning", "Graph Neural Networks"], "paperhash": "hu|strategies_for_pretraining_graph_neural_networks", "code": "https://github.com/snap-stanford/pretrain-gnns/", "_bibtex": "@inproceedings{\nHu*2020Strategies,\ntitle={Strategies for Pre-training Graph Neural Networks},\nauthor={Weihua Hu* and Bowen Liu* and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJlWWJSFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/341e619d4bc12981f88a79b9dc402d348eff11e1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJlWWJSFDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504193392, "tmdate": 1576860587529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1533/Authors", "ICLR.cc/2020/Conference/Paper1533/Reviewers", "ICLR.cc/2020/Conference/Paper1533/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1533/-/Public_Comment"}}}], "count": 12}