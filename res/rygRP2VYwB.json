{"notes": [{"id": "rygRP2VYwB", "original": "SkedNKb6BH", "number": 23, "cdate": 1569438821767, "ddate": null, "tcdate": 1569438821767, "tmdate": 1577168233464, "tddate": null, "forum": "rygRP2VYwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4NGYY_v5o", "original": null, "number": 1, "cdate": 1576798685276, "ddate": null, "tcdate": 1576798685276, "tmdate": 1576800949639, "tddate": null, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "invitation": "ICLR.cc/2020/Conference/Paper23/-/Decision", "content": {"decision": "Reject", "comment": "All the reivewers find the similarity between this paper and the references in terms of the algorithm and the proof. The theoretical results may not better than the existing results.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710830, "tmdate": 1576800259909, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper23/-/Decision"}}}, {"id": "S1gA4PuTYB", "original": null, "number": 2, "cdate": 1571813173829, "ddate": null, "tcdate": 1571813173829, "tmdate": 1574782445593, "tddate": null, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "invitation": "ICLR.cc/2020/Conference/Paper23/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This paper proposes a new method for empirical composition problems to which the vanilla SGD is not applicable because it has a finite-sum structure inside non-linear loss functions. A proposed method (named SCCG) is a combination of stochastic compositional gradient descent (SCGD) and stochastically controlled stochastic gradient (SCSG). In a theoretical analysis part, a linear convergence rate and a sub-linear convergence rate are derived under the strong convex and non-convex settings, respectively. In experiments, the superior performance of the method to competitors is verified on both strongly convex and non-convex problems.\n\nClarity:\nThe paper is clear and well written.\n\nQuality:\nThe work is of good quality and is technically sound.\n\nSignificance:\nThe problem treated in this paper is important and contains several applications as mentioned in the paper. Hence, developing an efficient method for this problem is important and interesting. Although derived convergence rates are better than existing primal methods, this paper lacks a comparison with the recently proposed primal-dual method by [A.Devraj & J.Chen (2019)].\n\n[A.Devraj & J.Chen (2019)] Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization. NeurIPS, 2019.\n\nA convergence rate obtained in [A.Devraj & J.Chen (2019)] seems faster than that of SCCG for ill-conditioned strongly convex problems. However, there exists a certain setting (large-scale setting) where SCCG outperforms their method. Thus, the contribution of the paper is not lost, but it is better to compare SCCG with the method in [A.Devraj & J.Chen (2019)], empirically and theoretically.\nIf the authors can show an empirical advantage over their method, it will make the paper stronger.\n\n-----\nUpdate:\nI thank the authors for the response and hard work. I am convinced of the advantage of the proposed method. I would like to keep my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper23/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper23/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574940018786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper23/Reviewers"], "noninvitees": [], "tcdate": 1570237758254, "tmdate": 1574940018799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper23/-/Official_Review"}}}, {"id": "Bkxe6xAfiH", "original": null, "number": 2, "cdate": 1573212343992, "ddate": null, "tcdate": 1573212343992, "tmdate": 1573808532107, "tddate": null, "forum": "rygRP2VYwB", "replyto": "S1gA4PuTYB", "invitation": "ICLR.cc/2020/Conference/Paper23/-/Official_Comment", "content": {"title": "Response to Reviewer 1 ", "comment": "Thanks for your suggestions. We will cite this paper to make more discussion.  [A.Devraj & J.Chen (2019) ] consider the strongly convex composition problem based on the primal-dual method. Our query complexity is better than [A.Devraj & J.Chen (2019) ] when n is large. This is because our estimated gradient depends on the relationship between $n$ and $1/\\epsilon$, does not depend only on $n$. Thus the query complexity is more general than the previous. The target of our experiment is to compare our proposed method to the non-variance reduction based method, in which the complexity does not contain $n$ . What\u2019s more, besides the two applications, we also add a reinforcement learning application to our proposed algorithm, which also demonstrate that our proposed method is better than the compositional gradient methods. The added experimental results are shown in the appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygRP2VYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference/Paper23/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper23/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper23/Reviewers", "ICLR.cc/2020/Conference/Paper23/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper23/Authors|ICLR.cc/2020/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177550, "tmdate": 1576860553853, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference/Paper23/Reviewers", "ICLR.cc/2020/Conference/Paper23/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper23/-/Official_Comment"}}}, {"id": "BylbXbAGir", "original": null, "number": 3, "cdate": 1573212440690, "ddate": null, "tcdate": 1573212440690, "tmdate": 1573804856883, "tddate": null, "forum": "rygRP2VYwB", "replyto": "SygFY6kpKS", "invitation": "ICLR.cc/2020/Conference/Paper23/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thanks for your suggestions. \nWe will give more discussion of the query complexity (QC) to demonstrate that our proposed method is more general.  Comparing with [1] and [2], query complexity of our proposed method is better than [1] and [2]  when the number of n is large. The QC of [1] and [2] are the mini-batch version, while we separately analyzed the non-mini-batch and mini-batch. The updated revised paper clearly shows the comparison of mini-batch versions among different algorithms, and also shows that our proposed method is better than [1] and [2] when n is large.\n\nFor non-convex:\n1)\tWhen $n<\\frac{1}{\\epsilon}$, the QC of our proposed method is non-mini-batch $\\mathcal{O}(n^{4/5} / \\epsilon)$ and mini-batch version $\\mathcal{O}(n^{4/5} /( b^{1/5} \\epsilon))$, where b is the mini-batch size. QC of [1] and [2]  are equal to our mini-batch version $\\mathcal{O}(n^{4/5} /( b^{1/5} \\epsilon))$, when $b=n^{2/3}$. Because $\\mathcal{O}(n^{4/5} /( b^{1/5} \\epsilon))= \\mathcal{O}(n^{2/3} /\\epsilon)$. Furthermore, $b\\le min\\{n,1/\\epsilon\\}^{2/3}$ is based on the requirement of $\\eta <1$, which is clearly updated in the revised paper.\n2)\tWhen $n >\\frac{1}{\\epsilon}$, this is our key motivation for our proposed method. QC of our proposed methods is better than [1] and [2]. That is $b= 1/\\epsilon ^{2/3}$, the QC of our algorithm is $\\mathcal{O}(1/\\epsilon ^{5/3})$, which is better than [1][2]. Furthermore, when n is large enough, it is not proper to compute the gradient at each epoch, or when $1/\\epsilon$ is not large enough, we can quickly get the desired results.\n\nFor strongly convex:\nmini-batch QC of our proposed algorithm is $\\mathcal{O}(min\\{n,1/\\epsilon \\mu^2\\}+ 1/\\mu$ $ min\\{n, 1/\\mu^2\\} log(1/\\epsilon))$ by setting $b=\\frac{1}{\\mu}$(from the stepsize $\\eta<1$), which  is better than [1] and [2] when n is large. This is the motivation of our proposed method when facing number of $n$.\n\nIn order to verify our proposed method, we also add another reinforcement learning application to our proposed method. The experimental results are shown in the appendix. We compare our proposed algorithm with compositional gradient methods, which show that the performance of our proposed is better on both objective value and the norm of the gradient."}, "signatures": ["ICLR.cc/2020/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygRP2VYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference/Paper23/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper23/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper23/Reviewers", "ICLR.cc/2020/Conference/Paper23/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper23/Authors|ICLR.cc/2020/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177550, "tmdate": 1576860553853, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference/Paper23/Reviewers", "ICLR.cc/2020/Conference/Paper23/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper23/-/Official_Comment"}}}, {"id": "H1l-_gRfjr", "original": null, "number": 1, "cdate": 1573212265023, "ddate": null, "tcdate": 1573212265023, "tmdate": 1573803780341, "tddate": null, "forum": "rygRP2VYwB", "replyto": "HklwLDlucr", "invitation": "ICLR.cc/2020/Conference/Paper23/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thanks for your suggestions. \nAlthough both the proposed stochastically controlled compositional gradient (SCCG) and C-SVRG [Xiangru Lian, Mengdi Wang, and Ji Liu, 2017] are variance reduction-based methods, we respectively argue they are significantly different. This is mainly because:\n        (a) The estimated gradient involves the inner function estimation and biased gradient estimation, which is greatly different from C-SVRG. The motivation for the proposed estimated gradient is that we can avoid the direct computation of inner function and the gradient at each epoch when $n$ is large, which is our advantage over C-SVRG.\n        (b) The proposed estimated gradient makes the challenge for the proof as the estimated inner function is also biased. However, we give a new proof for the convergence, which is essentially different from C-SVRG. What\u2019s more, our proposed estimated gradient included two kinds of stochastically controlled gradients, which is more general than C-SVRG.\n        (c) The query complexity of our proposed method is better than C-SVRG when $n$ is large. \n\nOur experiments include two applications: nonlinear-embedding problems for non-convex and risk-averse learning for strongly convex, which are both used to verify our proposed algorithms.  Moreover, we also add the reinforcement learning application to our proposed algorithms, which also demonstrate that our proposed algorithm is better than the non-variance-reduction based method. The corresponding experimental results are in the appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper23/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygRP2VYwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference/Paper23/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper23/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper23/Reviewers", "ICLR.cc/2020/Conference/Paper23/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper23/Authors|ICLR.cc/2020/Conference/Paper23/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177550, "tmdate": 1576860553853, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper23/Authors", "ICLR.cc/2020/Conference/Paper23/Reviewers", "ICLR.cc/2020/Conference/Paper23/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper23/-/Official_Comment"}}}, {"id": "SygFY6kpKS", "original": null, "number": 1, "cdate": 1571777921369, "ddate": null, "tcdate": 1571777921369, "tmdate": 1572972648395, "tddate": null, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "invitation": "ICLR.cc/2020/Conference/Paper23/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the paper, the authors consider composition problems and use the stochastically controlled stochastic gradient method (SCSG) to approximate the gradient G(x) and \\nabla f(x). The authors also provide convergence analysis of the proposed method for strongly convex problems and non-convex problems. Authors then conduct experiments on the\nmean-variance optimization in portfolio management task and the nonlinear embedding problem, results show that the proposed method is faster. \n\nThe following are my concerns:\n1) There are several important related works missing in the paper, e.g., [1][2].  \n2) The convergence results of the proposed method in the paper are not state-of-the-art. For a strongly convex case, the result in the paper is O( n+ k^2 min(n, 1/u^2) log1/e), it is not necessarily better than O(n+k^3 log 1/e) in [1] or O(n+kn^{2/3} log 1/e). For a non-convex case, the result in the paper is O(\\min{1/e^{9/5}, n^{4/5} / e}), it  is not necessarily better than O(n^{2/3}/e) in [1] or [2]. \n3) More compared results should be conducted in the experiments, e.g. [1][2]. \n\n[1]Huo, Zhouyuan, et al. \"Accelerated method for stochastic composition optimization with nonsmooth regularization.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n[2] Zhang, Junyu, and Lin Xiao. \"A Composite Randomized Incremental Gradient Method.\" International Conference on Machine Learning. 2019.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper23/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper23/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574940018786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper23/Reviewers"], "noninvitees": [], "tcdate": 1570237758254, "tmdate": 1574940018799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper23/-/Official_Review"}}}, {"id": "HklwLDlucr", "original": null, "number": 3, "cdate": 1572501326521, "ddate": null, "tcdate": 1572501326521, "tmdate": 1572972648311, "tddate": null, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "invitation": "ICLR.cc/2020/Conference/Paper23/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a variance reduction based algorithm to solve compositional problems. The idea comes from the stochastically controlled stochastic gradient (SCSG) methods. The paper applies the idea from SCSG to estimating the inner function G(x) and the gradient \\nabla f_k to solve compositional problems. The paper provides a theoretical analysis of the query complexity of the algorithm in both convex and non-convex setting. The experiments show the performance of the proposed algorithm is better than other recent methods. The paper seems to be the first attempt to extending stochastically controlled functions to the compositional problems. However, I vote for rejecting this submission for the following concerns. (1) Since SCSG is a member of the SVRG family of algorithms, the difference between this paper and [Xiangru Lian, Mengdi Wang, and Ji Liu, 2017] is not significant enough, especially in the algorithm design and the proof of the theoretical theorem. (2) The formulation of the compositional problems comes from reinforcement learning, risk-averse learning, nonlinear embedding, etc. However, the experiments are only performed on nonlinear-embedding problems. I think performing the experiments on different kinds of problems will be helpful to justify the significance."}, "signatures": ["ICLR.cc/2020/Conference/Paper23/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper23/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastically Controlled Compositional Gradient for the Composition problem", "authors": ["Liu Liu", "Ji Liu", "Cho-Jui Hsieh", "Dacheng Tao"], "authorids": ["liu.liu1@sydney.edu.au", "ji.liu.uwisc@gmail.com", "chohsieh@cs.ucla.edu", "dacheng.tao@sydney.edu.au"], "keywords": ["Non-convex optimisation", "Composition problem", "Stochastically controlled compositional gradient"], "TL;DR": "We devise a stochastically controlled compositional gradient algorithm for the composition problem", "abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.", "pdf": "/pdf/4a3fbc65150507158cb0c34b9dd28f3339bcea77.pdf", "paperhash": "liu|stochastically_controlled_compositional_gradient_for_the_composition_problem", "original_pdf": "/attachment/3557935543304e0532cba01c7860305cf9d169cf.pdf", "_bibtex": "@misc{\nliu2020stochastically,\ntitle={Stochastically Controlled Compositional Gradient for the Composition problem},\nauthor={Liu Liu and Ji Liu and Cho-Jui Hsieh and Dacheng Tao},\nyear={2020},\nurl={https://openreview.net/forum?id=rygRP2VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygRP2VYwB", "replyto": "rygRP2VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper23/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574940018786, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper23/Reviewers"], "noninvitees": [], "tcdate": 1570237758254, "tmdate": 1574940018799, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper23/-/Official_Review"}}}], "count": 8}