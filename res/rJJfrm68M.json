{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573613781, "tcdate": 1521573613781, "number": 297, "cdate": 1521573613433, "id": "r1Lg11y5G", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJJfrm68M", "replyto": "rJJfrm68M", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss surface and expressivity of deep convolutional neural networks", "abstract": "We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights. We show that such CNNs produce linearly independent features (and thus linearly separable) at every ``wide'' layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are equally important in deep learning.  While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide CNN  has a well-behaved loss surface with almost no bad local minima.", "pdf": "/pdf/e0330a116a5de79b171b93cd9db81e77fb5b9827.pdf", "TL;DR": "Deep CNN learn linearly independent features at every wide layer and potentially has almost no bad local minima", "paperhash": "nguyen|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks", "_bibtex": "@misc{\nnguyen2018the,\ntitle={The loss surface and expressivity of deep convolutional neural networks},\nauthor={Quynh Nguyen and Matthias Hein},\nyear={2018},\nurl={https://openreview.net/forum?id=BJjquybCW},\n}", "keywords": ["loss surface", "deep CNN", "local minima", "global minima"], "authors": ["Quynh Nguyen", "Matthias Hein"], "authorids": ["quynh@cs.uni-saarland.de", "hein@cs.uni-saarland.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730175828, "tcdate": 1518314759051, "number": 63, "cdate": 1518314759051, "id": "rJJfrm68M", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJJfrm68M", "original": "BJjquybCW", "signatures": ["~Quynh_Nguyen1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "The loss surface and expressivity of deep convolutional neural networks", "abstract": "We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights. We show that such CNNs produce linearly independent features (and thus linearly separable) at every ``wide'' layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are equally important in deep learning.  While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide CNN  has a well-behaved loss surface with almost no bad local minima.", "pdf": "/pdf/e0330a116a5de79b171b93cd9db81e77fb5b9827.pdf", "TL;DR": "Deep CNN learn linearly independent features at every wide layer and potentially has almost no bad local minima", "paperhash": "nguyen|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks", "_bibtex": "@misc{\nnguyen2018the,\ntitle={The loss surface and expressivity of deep convolutional neural networks},\nauthor={Quynh Nguyen and Matthias Hein},\nyear={2018},\nurl={https://openreview.net/forum?id=BJjquybCW},\n}", "keywords": ["loss surface", "deep CNN", "local minima", "global minima"], "authors": ["Quynh Nguyen", "Matthias Hein"], "authorids": ["quynh@cs.uni-saarland.de", "hein@cs.uni-saarland.de"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730175828, "tcdate": 1509124242758, "number": 493, "cdate": 1518730175818, "id": "BJjquybCW", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BJjquybCW", "original": "rJ5c_1b0Z", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "The loss surface and expressivity of deep convolutional neural networks", "abstract": "We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a \u201cwide\u201d layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.", "pdf": "/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf", "paperhash": "nguyen|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks", "_bibtex": "@misc{\nnguyen2018the,\ntitle={The loss surface and expressivity of deep convolutional neural networks},\nauthor={Quynh Nguyen and Matthias Hein},\nyear={2018},\nurl={https://openreview.net/forum?id=BJjquybCW},\n}", "keywords": ["convolutional neural networks", "loss surface", "expressivity", "critical point", "global minima", "linear separability"], "authors": ["Quynh Nguyen", "Matthias Hein"], "authorids": ["quynh@cs.uni-saarland.de", "hein@cs.uni-saarland.de"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}