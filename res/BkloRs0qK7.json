{"notes": [{"id": "BkloRs0qK7", "original": "BJeNwd_qK7", "number": 925, "cdate": 1538087890843, "ddate": null, "tcdate": 1538087890843, "tmdate": 1548151415845, "tddate": null, "forum": "BkloRs0qK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs", "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning.\nA new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios.\nAs the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF.\nOur results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.", "keywords": ["incremental learning", "deep neural networks", "catatrophic forgetting", "sequential learning"], "authorids": ["benedikt.pfuelb@cs.hs-fulda.de", "alexander.gepperth@cs.hs-fulda.de"], "authors": ["B. Pf\u00fclb", "A. Gepperth"], "TL;DR": "We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.", "pdf": "/pdf/65af1b654096d947a212ab642b657ea444e43fb7.pdf", "paperhash": "pf\u00fclb|a_comprehensive_applicationoriented_study_of_catastrophic_forgetting_in_dnns", "_bibtex": "@inproceedings{\npf\u00fclb2018a,\ntitle={A comprehensive, application-oriented study of catastrophic forgetting in {DNN}s},\nauthor={B. Pf\u00fclb and A. Gepperth},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkloRs0qK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryxTgMHJgV", "original": null, "number": 1, "cdate": 1544667636537, "ddate": null, "tcdate": 1544667636537, "tmdate": 1545354505230, "tddate": null, "forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "invitation": "ICLR.cc/2019/Conference/-/Paper925/Meta_Review", "content": {"metareview": "This paper has two main contributions. The first is that it proposes a specific framework for measuring catastrophic forgetting in deep neural networks that incorporates three application-oriented constraints: (1) a low memory footprint, which implies that data from prior tasks cannot be retained; (2) causality, meaning that data from future tasks cannot be used in any way, including hyperparameter optimization and model selection; and (3) update complexity for new tasks that is moderate and also independent of the number of previously learned tasks, which precludes replay strategies. The second contribution is an extensive study of catastrophic forgetting, using different sequential learning tasks derived from 9 different datasets and examining 7 different models. The key conclusions from the study are that (1) permutation-based tasks are comparatively easy and should not be relied on to measure catastrophic forgetting; (2) with the application-oriented contraints in effect, all of the examined models suffer from catastrophic forgetting (a result that is contrary to a number of other recent papers); (3) elastic weight consolidation provides some protection against catastrophic forgetting for simple sequential learning tasks, but fails for more complex tasks; and (4) IMM is effective, but only if causality is violated in the selection of the IMM balancing parameter. The reviewer scores place this paper close to the decision boundary. The most negative reviewer (R2) had concerns about the novelty of the framework and its application-oriented constraints. The authors contend that recent papers on catastrophic forgetting fail to apply these quite natural constraints, leading to the deceptive conclusion that catastrophic forgetting may not be as big of a problem as it once was. The AC read a number of the papers mentioned by the authors and agrees with them: these constraints have been, at least at times, ignored in the literature, and they shouldn't be ignored. The other two reviewers appreciated the scope and rigor of the empirical study. On the balance, the AC thinks this is an important contribution and that it should appear at ICLR.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "Catastrophic forgetting: not dead yet"}, "signatures": ["ICLR.cc/2019/Conference/Paper925/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper925/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs", "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning.\nA new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios.\nAs the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF.\nOur results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.", "keywords": ["incremental learning", "deep neural networks", "catatrophic forgetting", "sequential learning"], "authorids": ["benedikt.pfuelb@cs.hs-fulda.de", "alexander.gepperth@cs.hs-fulda.de"], "authors": ["B. Pf\u00fclb", "A. Gepperth"], "TL;DR": "We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.", "pdf": "/pdf/65af1b654096d947a212ab642b657ea444e43fb7.pdf", "paperhash": "pf\u00fclb|a_comprehensive_applicationoriented_study_of_catastrophic_forgetting_in_dnns", "_bibtex": "@inproceedings{\npf\u00fclb2018a,\ntitle={A comprehensive, application-oriented study of catastrophic forgetting in {DNN}s},\nauthor={B. Pf\u00fclb and A. Gepperth},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkloRs0qK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper925/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353032384, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper925/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper925/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper925/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353032384}}}, {"id": "B1ljgG7inm", "original": null, "number": 2, "cdate": 1541251571165, "ddate": null, "tcdate": 1541251571165, "tmdate": 1543674327116, "tddate": null, "forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "invitation": "ICLR.cc/2019/Conference/-/Paper925/Official_Review", "content": {"title": "An empirical study of CF, but more recent methods could have been also added for the study", "review": "Thanks for the updates and rebuttals from the authors. \n\nI now think including the results for HAT may not be essential for the current version of the paper. I now understand better about the main point of the paper - providing a different setting for evaluating algorithms for combatting CF, and it seems the widespread framework may not accurately reflect all aspects of the CF problems. \n\nI think showing the results for only 2 tasks are fine for other settings except for DP10-10 setting, since most of them already show CF in the given framework for 2 tasks. Maybe only for DP10-10, the authors can run multiple tasks setting, to confirm their claims about the permuted datasets. (but, I believe the vanilla FC model should show CF for multiple permuted tasks.)\n\nI have increased my rating to \"6: Marginally above acceptance threshold\" - it could have been much better to at least give some hints to overcome the CF for the proposed setting, but I guess giving extensive experimental comparisons could be valuable for a publication. \n\n=====================\nSummary:\n\nThe paper evaluates several recent methods regarding catastrophic forgetting with some stricter application scenarios taken into account. They argue that most methods, including EWC and IMM, are prone to CF, which is against the argument of the original paper. \n\nPro:\n- Extensive study on several datasets, scenarios give some intuition and feeling about the CF phenomenon. \n\nCon:\n- There are some more recent baselines., e.g., Joan Serr\u00e0, D\u00eddac Sur\u00eds, Marius Miron, Alexandros Karatzoglou, \"Overcoming catastrophic forgetting with hard attention to the task\" ICML2018, and it would be interesting to see the performance of those as well. \n- The authors say that the permutation based data set may not be useful. But, their experiments are only on two tasks, while many work in literature involves much larger number of tasks, sometimes up to 50. So, I am not sure whether the paper's conclusion that the permutation-based SLT should not be used since it's only based on small number of tasks. \n- While the empirical findings seem useful, it would have been nicer to propose some new method that can get around the issues presented in the paper. ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper925/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs", "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning.\nA new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios.\nAs the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF.\nOur results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.", "keywords": ["incremental learning", "deep neural networks", "catatrophic forgetting", "sequential learning"], "authorids": ["benedikt.pfuelb@cs.hs-fulda.de", "alexander.gepperth@cs.hs-fulda.de"], "authors": ["B. Pf\u00fclb", "A. Gepperth"], "TL;DR": "We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.", "pdf": "/pdf/65af1b654096d947a212ab642b657ea444e43fb7.pdf", "paperhash": "pf\u00fclb|a_comprehensive_applicationoriented_study_of_catastrophic_forgetting_in_dnns", "_bibtex": "@inproceedings{\npf\u00fclb2018a,\ntitle={A comprehensive, application-oriented study of catastrophic forgetting in {DNN}s},\nauthor={B. Pf\u00fclb and A. Gepperth},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkloRs0qK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper925/Official_Review", "cdate": 1542234345486, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper925/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335834473, "tmdate": 1552335834473, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper925/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1lXXQf5n7", "original": null, "number": 1, "cdate": 1541182234730, "ddate": null, "tcdate": 1541182234730, "tmdate": 1543169009358, "tddate": null, "forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "invitation": "ICLR.cc/2019/Conference/-/Paper925/Official_Review", "content": {"title": "A large and interesting analysis of CF in DNNs", "review": "# [Updated after author response]\nThank you for your response. I am happy to see the updated paper. In particular, the added item in section 1.3 highlights where the novelty of the paper lies, and as a consequence, I think the significance of the paper is increased. Furthermore, the clarity of the paper has increased. \n\nIn its current form, I think the paper would be a valuable input to the deep learning community, highlighting an important issue (CF) for neural networks. I have therefore increased my score.\n\n------------------------------------------\n\n# Summary\nThe authors present an empirical study of catastrophic forgetting (CF) in deep neural networks. Eight models are tested against nine datasets with 10 classes each but a varying number of samples. The authors construct a number of sequential learning tasks to test the model performances in different scenarios. The main conclusion is that CF is still a problem in all models, despite claims in other papers.\n\n# Quality\nThe paper shows healthy criticism of the methods used to evaluate CF in previous works. I very much like this.\n\nWhile I like the different experimental set-ups and the attention to realistic scenarios outlined in section 1.2, I find the analysis of the experiments somewhat superficial. The accuracies of each model for each task and dataset are reported, but there is little insight into what causes CF. For instance, do some choices of hyperparameters consistently cause a higher/lower degree of CF across models? I also think the metrics proposed by Kemker et al. (2018) are more informative than just reporting the last and best accuracy, and that including these metrics would improve the quality of the paper.\n\n# Clarity\nThe paper is generally clearly written and distinct paragraphs are often highlighted, which makes reading and getting an overview much easier. In particular, I like the summary given in sections 1.3 and 1.4.\n\nSection 2.4 describing the experimental setup could be clearer. It takes a bit of time to decipher Table 2, and it would have been good with a few short comments on what the different types of tasks (D5-5, D9-1, DP10-10) will tell us about the model performances. E.g. what do you expect to see from the experiments of D5-5 that is not covered by D9-1 and vice versa? And why are the number of tasks in each category so different (8 vs 3 vs 1)?\n\nI am not a huge fan of 3D plots, and I don't think they do anything good in section 4. The perspective can make it tricky to compare models, and the different graphs overshadow each other. I would prefer 2D plots in the supplementary, with a few representative ones shown in the main paper. I would also experiment with turning Table 3 into a heat map.\n\n# Originality\nTo my knowledge, the paper presents the largest evaluation of CF in terms of evaluated datasets. Kemker et al. (2018) conduct a somewhat similar experiment using fewer datasets, but a larger number of classes, which makes the CF even clearer. I think it would be good to cite this paper and briefly discuss it in connection with the current work.\n\n# Significance\nThe paper is mostly a report of the outcome of a substantial experiment on CF, showing that all tested models suffer from CF to some extent. While this is interesting and useful to know, there is not much to learn in terms of what can cause or prevent CF in DNNs. The paper's significance lies in showing that CF is still a problem, but there is room for improvement in the analysis of the outcome of the experiments.\n\n# Other notes\nThe first sentence of the second paragraph in section 5 seems to be missing something.\n\n# References\nKemker, R., McClure, M., Abitino, A., Hayes, T., & Kanan, C. (2018). In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper925/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs", "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning.\nA new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios.\nAs the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF.\nOur results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.", "keywords": ["incremental learning", "deep neural networks", "catatrophic forgetting", "sequential learning"], "authorids": ["benedikt.pfuelb@cs.hs-fulda.de", "alexander.gepperth@cs.hs-fulda.de"], "authors": ["B. Pf\u00fclb", "A. Gepperth"], "TL;DR": "We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.", "pdf": "/pdf/65af1b654096d947a212ab642b657ea444e43fb7.pdf", "paperhash": "pf\u00fclb|a_comprehensive_applicationoriented_study_of_catastrophic_forgetting_in_dnns", "_bibtex": "@inproceedings{\npf\u00fclb2018a,\ntitle={A comprehensive, application-oriented study of catastrophic forgetting in {DNN}s},\nauthor={B. Pf\u00fclb and A. Gepperth},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkloRs0qK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper925/Official_Review", "cdate": 1542234345486, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper925/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335834473, "tmdate": 1552335834473, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper925/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeHw6IOAQ", "original": null, "number": 9, "cdate": 1543167324661, "ddate": null, "tcdate": 1543167324661, "tmdate": 1543167324661, "tddate": null, "forum": "BkloRs0qK7", "replyto": "HJgBfdUNpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper925/Official_Comment", "content": {"title": "Update after reading author response", "comment": "Thank you for your comprehensive response and for updating your paper. The story is now much clearer to me, and as a result, the conclusion is more significant. I will increase my score.\n\nI do, however, have suggestions for two changes, which I hope you will incorporate in the final version of your paper. \n\nFirstly, regarding your point 3) above, I am happy that you added some text to your paper, but I actually like your explanation here more. Specifically, knowing *what* you expect (e.g., D9-1 adds less new information compared to D5-5, so one might expect CF to be less pronounced here) is valuable to prepare the reader for what you may find later in the paper. Right now, you only say that you expect this to \"have an impact on CF\". Nothing major, but I think you will help the reader by detailing your expectation.\n\nSecondly, regarding your point 6), I agree that the colouring helps to underline your conclusion, but I find the chosen colour scheme quite hard on the eyes. Also, colour blind readers will find it difficult to distinguish between the red and green. I suggest you change the colour scheme, e.g. to a sequential, colour blind safe scheme from http://colorbrewer2.org/. This would also make the table more readable if printed in greyscale. :)\n\nThank you for your hard work and good luck with your paper!"}, "signatures": ["ICLR.cc/2019/Conference/Paper925/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper925/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper925/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs", "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning.\nA new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios.\nAs the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF.\nOur results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.", "keywords": ["incremental learning", "deep neural networks", "catatrophic forgetting", "sequential learning"], "authorids": ["benedikt.pfuelb@cs.hs-fulda.de", "alexander.gepperth@cs.hs-fulda.de"], "authors": ["B. Pf\u00fclb", "A. Gepperth"], "TL;DR": "We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.", "pdf": "/pdf/65af1b654096d947a212ab642b657ea444e43fb7.pdf", "paperhash": "pf\u00fclb|a_comprehensive_applicationoriented_study_of_catastrophic_forgetting_in_dnns", "_bibtex": "@inproceedings{\npf\u00fclb2018a,\ntitle={A comprehensive, application-oriented study of catastrophic forgetting in {DNN}s},\nauthor={B. Pf\u00fclb and A. Gepperth},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkloRs0qK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper925/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616526, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkloRs0qK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper925/Authors", "ICLR.cc/2019/Conference/Paper925/Reviewers", "ICLR.cc/2019/Conference/Paper925/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper925/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper925/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper925/Authors|ICLR.cc/2019/Conference/Paper925/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper925/Reviewers", "ICLR.cc/2019/Conference/Paper925/Authors", "ICLR.cc/2019/Conference/Paper925/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616526}}}, {"id": "ryereK-TnQ", "original": null, "number": 3, "cdate": 1541376236718, "ddate": null, "tcdate": 1541376236718, "tmdate": 1541533573800, "tddate": null, "forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "invitation": "ICLR.cc/2019/Conference/-/Paper925/Official_Review", "content": {"title": "Interesting topic of limited novelty", "review": "The paper presents a study of the application of some well known methods on 9 datasets focusing on the issue of catastrophic forgetting when considering a sequential learning task in them.  In general the presentation of concepts and results is a bit problematic and unclear. Comments, such that the paper presents ' a novel training and model selection paradigm for incremental learning in DNNs ' is not justified.  A better description of the results, e.g., in Table 3 should be presented, as well a better linking with the findings; a better structure of the latter would also be required to improve consistency of them. Improving these could make the paper candidate for a poster presentation.  ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper925/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A comprehensive, application-oriented study of catastrophic forgetting in DNNs", "abstract": "We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning.\nA new experimental protocol is proposed that takes into account typical constraints encountered in application scenarios.\nAs the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF.\nOur results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.", "keywords": ["incremental learning", "deep neural networks", "catatrophic forgetting", "sequential learning"], "authorids": ["benedikt.pfuelb@cs.hs-fulda.de", "alexander.gepperth@cs.hs-fulda.de"], "authors": ["B. Pf\u00fclb", "A. Gepperth"], "TL;DR": "We check DNN models for catastrophic forgetting using a new evaluation scheme that reflects typical application conditions, with surprising results.", "pdf": "/pdf/65af1b654096d947a212ab642b657ea444e43fb7.pdf", "paperhash": "pf\u00fclb|a_comprehensive_applicationoriented_study_of_catastrophic_forgetting_in_dnns", "_bibtex": "@inproceedings{\npf\u00fclb2018a,\ntitle={A comprehensive, application-oriented study of catastrophic forgetting in {DNN}s},\nauthor={B. Pf\u00fclb and A. Gepperth},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkloRs0qK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper925/Official_Review", "cdate": 1542234345486, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkloRs0qK7", "replyto": "BkloRs0qK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper925/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335834473, "tmdate": 1552335834473, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper925/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}