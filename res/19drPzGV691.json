{"notes": [{"id": "19drPzGV691", "original": "7lGGVJG2Ap", "number": 3109, "cdate": 1601308344874, "ddate": null, "tcdate": 1601308344874, "tmdate": 1614985645403, "tddate": null, "forum": "19drPzGV691", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6mtznOnrt1o", "original": null, "number": 1, "cdate": 1612394784559, "ddate": null, "tcdate": 1612394784559, "tmdate": 1612394784559, "tddate": null, "forum": "19drPzGV691", "replyto": "19drPzGV691", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Comment", "content": {"title": "Correction", "comment": "To anyone interested, we note that Proposition 1 in the paper is wrongly stated -- the distributional operator results in \"underestimate\" instead of \"overestimate\" of the dynamic CVaR. The proof is wrong. We will present the corrected results in a future version of the work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"forum": "19drPzGV691", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3109/Authors|ICLR.cc/2021/Conference/Paper3109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649469232, "tmdate": 1610649469232, "id": "ICLR.cc/2021/Conference/Paper3109/-/Comment"}}}, {"id": "6CyjKH5iZYb", "original": null, "number": 1, "cdate": 1610040516730, "ddate": null, "tcdate": 1610040516730, "tmdate": 1610474124987, "tddate": null, "forum": "19drPzGV691", "replyto": "19drPzGV691", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers found found the paper well motivated and well written, they found both the theoretical contributions limited in novelty and the experiments too rudimentary to be insightful."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"forum": "19drPzGV691", "replyto": "19drPzGV691", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040516717, "tmdate": 1610474124971, "id": "ICLR.cc/2021/Conference/Paper3109/-/Decision"}}}, {"id": "nBGlP5KOlU0", "original": null, "number": 7, "cdate": 1605670666697, "ddate": null, "tcdate": 1605670666697, "tmdate": 1605670666697, "tddate": null, "forum": "19drPzGV691", "replyto": "VJ1fsYrqONl", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment", "content": {"title": "Re: Distributional RL for optimal static CVaR policy versus markovian/dynamic CVaR", "comment": "Thank you for your thoughtful comments.\n\n1. We did start this work by trying to solve the augmented MDP directly but the results were pretty bad -- nowhere close to what we obtained here. We decided not to show these. Our code can be modified to run this as well so we welcome any interested reader to reproduce or improve on our results.\n\n2. This is a very good question. Our view is that this is a tradeoff between computational complexity and sample complexity. It has been previously shown [Lyle et al 2019] that under tabular or linear function approximation, there is no advantage to using the entire distribution compared to just the Q-value when the objective is to optimize for the expected return. The difference, however, can be significant under non-linear function approximation and the conjecture is that by keeping track of the entire distribution (more computational work done), additional robustness is built into the solution. We believe that the phenomenon is even more prominent here in the case of CVaR objectives."}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "19drPzGV691", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3109/Authors|ICLR.cc/2021/Conference/Paper3109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841021, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment"}}}, {"id": "KYMCWi9_ME", "original": null, "number": 6, "cdate": 1605670441637, "ddate": null, "tcdate": 1605670441637, "tmdate": 1605670441637, "tddate": null, "forum": "19drPzGV691", "replyto": "yXbo1LMuSmm", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment", "content": {"title": "Re: Recommendation to Marginally Reject", "comment": "\nThank you for your thoughtful comments.\n\nOne of the main \"selling points\" in the proposed algorithm is that it only requires mild modification to the existing distributional RL algorithms. However, we do not think these modifications are trivial, and as shown by our analysis as well as the empirical results, the effect can be significant.\n\nAs pointed out in Section 4.2, we use actual daily closing prices from 2005 to 2015 for training and 2016-2019 for testing. For testing on real data, these are the exact prices used. For training, instead of sampling from only 10 years of daily prices (only 2K+ data points), we fit a model to them and sample from the model instead. Further details can be obtained from the provided code, where all our results can be reproduced. Regarding sample complexity, performance improves with the training size as expected but we do not have any sampling complexity guarantee.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "19drPzGV691", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3109/Authors|ICLR.cc/2021/Conference/Paper3109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841021, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment"}}}, {"id": "VY_EsTSbV-", "original": null, "number": 5, "cdate": 1605670250175, "ddate": null, "tcdate": 1605670250175, "tmdate": 1605670250175, "tddate": null, "forum": "19drPzGV691", "replyto": "fpJ9pXJFiKQ", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment", "content": {"title": "Re: Review of the paper", "comment": "Thank you for your thoughtful comments.\n\nOn the real dataset for option trading, we note that both our proposed algorithm and Dabney et al's algorithm perform very similarly for large alpha -- note the overlapping errorbars. In fact, we can tweak the results in our favor by choosing different trading periods but we choose not to do so since for large alpha we expect very similar results anyway. Our main focus is on small alphas, where the performance gap between the two is statistically significant.\n\nWe agree that experimental results on larger domains would be illuminating. We indeed are working on producing more results in future work but we believe that our present results in this paper are already useful contributions to the research community.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "19drPzGV691", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3109/Authors|ICLR.cc/2021/Conference/Paper3109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841021, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment"}}}, {"id": "oDeYivcozru", "original": null, "number": 4, "cdate": 1605670107628, "ddate": null, "tcdate": 1605670107628, "tmdate": 1605670107628, "tddate": null, "forum": "19drPzGV691", "replyto": "q1IBUUxuS0", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment", "content": {"title": "Re: Convergance and Novelty", "comment": "Thank you for your thoughtful comments.\n\n1. Convergence: For the case of policy evaluation (fixed pi), convergence is guaranteed for distributional RL. This result has been proven in [Bellemere et al 2017]. In the case of optimal control (either the expected return or the CVaR), convergence in general is not guaranteed. The intuition is this: there can be more than one optimal policy (with the same expectation or CVaR) with different value distributions. Since we work in the space of value distributions, convergence would imply that we consistently favor one policy, which cannot be achieved in general but may be achievable with a consistent tie-breaking strategy -- see [Bellemere et al 2017] for more on this. This is an important issue that we intend to pursue in a future work.\n\n2. Our main contribution is the proposed algorithm itself, which, to the best of our knowledge, is novel. The existing distributional RL algorithms allow a straightforward way to select actions by the CVaR of the value distribution, but we have shown that this leads to a solution closer to the dynamic CVaR. One naturally asks whether we can steer the solution towards optimizing the static CVaR -- we showed how, and the modification required is non-trivial.\n\n3. We agree that experimental results on larger domains would be illuminating. We indeed are working on producing more results in future work but we believe that our present results in this paper are already useful contributions to the research community.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "19drPzGV691", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3109/Authors|ICLR.cc/2021/Conference/Paper3109/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841021, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Comment"}}}, {"id": "yXbo1LMuSmm", "original": null, "number": 2, "cdate": 1603867658180, "ddate": null, "tcdate": 1603867658180, "tmdate": 1605024066429, "tddate": null, "forum": "19drPzGV691", "replyto": "19drPzGV691", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review", "content": {"title": "Recommendation to Marginally Reject", "review": "This paper consider the problem of learning a risk-averse policy base on CVaR measure using distributional reinforcement learning. The main contributions of this paper are twofold. First, they show that the standard distributional RL algorithm overestimate the dynamic, Markovian CVaR, which might be too conservative. Secondly, they propose a modified algorithm that can learn a proper CVaR-optimized policy based on static, non-Markovian CVaR. \n\nOverall this paper is well-written and easy to follow. The problem is well-motivated, and the proofs of the main propositions are clean and easy to check. \n\nHowever, I have two main concerns on this paper. First, the theory part of this paper (proposition 1 & 2) are quite straight forward, and the modifications to the existing algorithm in [1] are mild, thus the novelty of this work is somewhat limited. Second, the option trading experiment train on a mixture of real stock prices and simulated stock prices (the authors use simulated data to allow training on unlimited data), and we don\u2019t know the exact size of training set, this seems a bit wired to me. Is there any sampling complexity guarantee for the proposed algorithm? How does the performance of the algorithm scale with the training sample size\uff1f\n\n[1]  Will Dabney, Mark Rowland, Marc G. Bellemare, and R\u00b4emi Munos. Distributional reinforcement learning with quantile regression.  In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "19drPzGV691", "replyto": "19drPzGV691", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082109, "tmdate": 1606915804809, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review"}}}, {"id": "VJ1fsYrqONl", "original": null, "number": 1, "cdate": 1603731406806, "ddate": null, "tcdate": 1603731406806, "tmdate": 1605024066367, "tddate": null, "forum": "19drPzGV691", "replyto": "19drPzGV691", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review", "content": {"title": "Distributional RL for optimal static CVaR policy versus markovian/dynamic CVaR", "review": "This paper addresses the problem of learning the optimal policy for static CVaR with distributional RL.\nThe authors underline the difference between static CVaR which is the CVaR on the whole cumulative discounted rewards and the dynamic CVaR which is a Markovian alternative proposed by (Ruszczynski 2010).\nAs pointed out by the authors, the optimal policy for static CVaR must take into account the accumulated rewards to take decisions: the optimal policy is hence not stationary.  A nice property of dynamic CVaR is that its optimal policy is stationary.\nA key result of the paper is to show that optimal solution for the dynamic CVaR is suboptimal for static CVaR (Proposition 1).\nLeveraging on this remark, the authors propose an algorithm based on distributional (quantile regression) to solve the static CVaR problem. The obtained policy is stationary on an augmented MDP where the states are decorated with he reward collected so-far.\nThe experiments on synthetic and real data underline the relevance of the proposed approach.\n\nI really enjoyed reading this theoretical paper. It is very well written and easy to follow. The ideas presented are interesting.\nMy only criticisms 'or questions' are about the state augmentation:\n1. As mentioned on page 4 it would be sample inefficient but one could solve the augmented MDP directly. Why not provide this approach in your experiments ?\n2. As mentioned on page 5 you are using the distribution estimates to \"store the information needed\", instead of using state augmentation. But estimating a distribution is a costly task. Do you have any argument to justify that this approach is indeed more \"sample efficient\" than a naive state augmentation version without distributional estimates ? Any convergence bound ?\n\nMinor remark:\nI would be nice to have at least a small conclusion or perspective\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "19drPzGV691", "replyto": "19drPzGV691", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082109, "tmdate": 1606915804809, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review"}}}, {"id": "fpJ9pXJFiKQ", "original": null, "number": 3, "cdate": 1603934452382, "ddate": null, "tcdate": 1603934452382, "tmdate": 1605024066302, "tddate": null, "forum": "19drPzGV691", "replyto": "19drPzGV691", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review", "content": {"title": "Review of the paper", "review": "This paper is about risk-sensitive RL based on the CVaR risk measure. This paper is mainly based on the work presented in Dabney et al. in 2018 which is about distributional RL for a family of risk measures which includes CVaR as well. The main motivation for this work was the point that the method presented in Dabney et al. 2018 overestimates the dynamics and could be excessively conservative in certain scenarios. Authors have proposed to use static CVaR instead and have developed algorithms to do that.\n\nThis paper has solid theoretical results (propositions 1 and 2). Authors have identified a problem in Dabney et al. and proposed an algorithm to resolve it.\n\nThe major issue though, is about the evaluation and experimental results. authors have provided results on a synthetic dataset and a real dataset related to options trading. Even on the real data results, for larger values of alpha, Dabney et al. 2018 has outperformed the proposed approach. In order to make this paper ready for a venue such as ICLR, authors should provide a more comprehensive evaluation of their methods. At least, it is expected to show the performance of their approach and its comparison vs Dabney et al. on several Atari games. Otherwise, the contribution of this paper would be limited.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "19drPzGV691", "replyto": "19drPzGV691", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082109, "tmdate": 1606915804809, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review"}}}, {"id": "q1IBUUxuS0", "original": null, "number": 4, "cdate": 1604173683607, "ddate": null, "tcdate": 1604173683607, "tmdate": 1605024066238, "tddate": null, "forum": "19drPzGV691", "replyto": "19drPzGV691", "invitation": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review", "content": {"title": "Convergance and Novelty", "review": "First I wanted to thank authors for putting the manuscript together, I enjoyed reading it. \n\nSummary : Authors proposed using DRL to find a good (optimal) CVaR policy, by pointing that the optimal CVaR policy is non-stationary, and DRL can be leveraged to learn and execute this kind of policies. In addition they showed just taking a max over CVaR rather than E will not result in an optimal policy. \n\nStrength : \n1. Paper is well written, it's easy to follow and it provides necessary background for the reader to follow. \n2. Important Issue : I believe finding a scalable way to optimize for CVaR is an important problem to tackle, as most of the previous work are not scalable (e.g. Chow et al)\n\nWeakness/ Concerns:\n1. Convergence: I have a concern about the convergence of the algorithm, by applying the proposed algorithm, is there any guarantee for convergence? Even in the case of policy evaluation and not control (not taking max, picking action on \\pi) is there a convergence guarantee? Or if not can authors provide intuition/ reason why is that the case?\n\n2. Novelty: Reading the paper, I challenge the novelty of current algorithm/ manuscript. Or maybe I had a hard time pinpointing it, to the best of my knowledge most of the claims have been already known, can authors please explain what they think their main contribution is? (I'm happy to change my score given the explanation)\n\n3. Experiments: The premise of using DRL for CVaR is mainly \"scalability\" so that we can solve larger problems (state space mainly). However, this is not reflected in the experiments, and experiments are in small domains. I think the paper can benefit from an experiment in larger state space. \n\nScore: \nAt this point I think the manuscript is not ready for a publication, however, I did enjoy reading it, and I think it's a great work so far with potentials. I am happy to change my score given authors response to my concerns. \n\nThanks.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3109/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3109/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributional Reinforcement Learning for Risk-Sensitive Policies", "authorids": ["~Shiau_Hong_Lim1", "malikilyas1996@gmail.com"], "authors": ["Shiau Hong Lim", "Ilyas Malik"], "keywords": [], "abstract": "We address the problem of learning a risk-sensitive policy based on the CVaR risk measure using distributional reinforcement learning. In particular, we show that applying the distributional Bellman optimality operator with respect to a risk-based action-selection strategy overestimates the dynamic, Markovian CVaR. The resulting policies can however still be overly conservative and one often prefers to learn an optimal policy based on the static, non-Markovian CVaR. To this end, we propose a modification to the existing algorithm and show that it can indeed learn a proper CVaR-optimized policy. Our proposed approach is a simple extension of standard distributional RL algorithms and can therefore take advantage of many of the recent advances in deep RL. On both synthetic and real data, we empirically show that our proposed algorithm is able to produce a family of risk-averse policies that achieves a better tradeoff between risk and the expected return.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lim|distributional_reinforcement_learning_for_risksensitive_policies", "supplementary_material": "/attachment/3bb9ab33ce7d20453dfa26ea46257ed7507f2016.zip", "pdf": "/pdf/6160d5dd871fe6da5410d896f24307add831ffb9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1ebNdYZLT7", "_bibtex": "@misc{\nlim2021distributional,\ntitle={Distributional Reinforcement Learning for Risk-Sensitive Policies},\nauthor={Shiau Hong Lim and Ilyas Malik},\nyear={2021},\nurl={https://openreview.net/forum?id=19drPzGV691}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "19drPzGV691", "replyto": "19drPzGV691", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3109/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082109, "tmdate": 1606915804809, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3109/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3109/-/Official_Review"}}}], "count": 11}