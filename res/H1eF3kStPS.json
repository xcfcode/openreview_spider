{"notes": [{"id": "H1eF3kStPS", "original": "SkgA4z1twH", "number": 1960, "cdate": 1569439665149, "ddate": null, "tcdate": 1569439665149, "tmdate": 1577168247694, "tddate": null, "forum": "H1eF3kStPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SpKGoqpcUJ", "original": null, "number": 1, "cdate": 1576798736948, "ddate": null, "tcdate": 1576798736948, "tmdate": 1576800899407, "tddate": null, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a new graph Hierarchy representation (HAG) which eliminates the redundancy during the aggregation stage and improves computation efficiency. It achieves good speedup and also provide theoretical analysis. There has been several concerns from the reviewers; authors' response addressed them partially. Despite this, due to the large number of strong papers, we cannot accept the paper at this time. We encourage the authors to further improve the work for a future version.  \n\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710764, "tmdate": 1576800259835, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Decision"}}}, {"id": "ryxmxkCRtB", "original": null, "number": 2, "cdate": 1571901162861, "ddate": null, "tcdate": 1571901162861, "tmdate": 1574387601155, "tddate": null, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper aims to propose a speeding-up strategy to reduce the training time for existing GNN models by reducing the redundant neighbor pairs. The idea is simple and clear. The paper is well-written. However, major concerns are:\n\n1. This strategy is only for equal contribution models (e.g., GCN, GraphSAGE), not for methods which consider distinct contribution weights for individual node neighbor (e.g. GAT). However, in my opinion, for one target node, different neighbors should be assigned with different contributions instead of equal contributions.\n\n2. What kind of graphs can the proposed model be applied to? This paper seems to only consider unweighted undirected graphs. How about directed and weighted graphs? Even for an unweighted undirected graph, the symmetric information may also be redundant for further elimination. Then can HAG reduce this symmetric redundancy?\n\n3. There is no effectiveness evaluation comparing the original GNN models with the versions with HAG. The authors claim that, with the HAG process, the efficiency could be improved without losing accuracy. But there are no experimental results verifying that the effectiveness of the HAG-versions which could obtain comparable performance with the original GNN models for some downstream applications (e.g., node classification).\n\n--------------------------------------------------Update------------------------------------------------\nThanks very much for the authors' feedback. The revised version has clarified some of my concerns. However, the equal-contribution (in Comment 1) is still a big one that the authors should pay attention. I increase my score to 3.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699645462, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Reviewers"], "noninvitees": [], "tcdate": 1570237729793, "tmdate": 1575699645476, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Review"}}}, {"id": "rkl9lZZhoB", "original": null, "number": 7, "cdate": 1573814513595, "ddate": null, "tcdate": 1573814513595, "tmdate": 1573814513595, "tddate": null, "forum": "H1eF3kStPS", "replyto": "SygZInyhsS", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment", "content": {"title": "Thank you for the quick response", "comment": "It seems there is a slight misunderstanding in Theorem 1 and 2. \n\nTheorem 1 shows that HAG always maintains the exact same model accuracy as GNN-graph for both categories of GNN models (i.e., set and sequential AGGREGATE). In fact, training a GNN model on HAG produces the exact same activations/gradients/weights as traditional training on GNN-graph in each epoch.\n\nTheorem 2 and 3 describe the runtime performance (i.e., the execution time to train an epoch) of the HAGs discovered by our search algorithm, since there exist numerous HAGs functionally equivalent to the original GNN-graph. In particular, Theorem 2 proves that, for GNN models with sequential AGGREGATE, the search algorithm finds a HAG with globally optimal runtime performance (i.e., minimal execution time to train an epoch). Theorem 3 proves a similar bound for GNN models with set AGGREGATE.\n\nFigure 6 compares the training time of HAG and GNN-graph for GCN, which uses set AGGREGATE. We are happy to also include a time-to-accuracy comparison for a GNN model with sequential AGGREGATE in the final paper. Note that Theorem 1 proves that HAG maintains the same model accuracy as the original GNN-graph by design."}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eF3kStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1960/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1960/Authors|ICLR.cc/2020/Conference/Paper1960/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148364, "tmdate": 1576860548044, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment"}}}, {"id": "SygZInyhsS", "original": null, "number": 6, "cdate": 1573809225273, "ddate": null, "tcdate": 1573809225273, "tmdate": 1573809225273, "tddate": null, "forum": "H1eF3kStPS", "replyto": "ryexiZJ2jB", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment", "content": {"title": "Thank you for the response", "comment": "\nThank you for the clarification for GraphSAGE. I understand.\n\n>The HAG graph representation achieves the same training and test results as the original GNN-graph representation for each GNN model, even though GNN models with different aggregation methods (i.e., set v.s. sequential) may obtain different accuracy--- this issue is orthogonal to the HAG optimizations.\n>Theorem 1 in the paper proves the equivalence of HAG and GNN-graph representations for both training and inference. This means that HAG performs exactly the same computations as the traditional model training/inference but in a non-redundant way. This means, that HAG obtains exactly the same model as traditional training (but HAG is much faster).\n>To address reviewer\u2019s comment we have added an experiment to evaluate the training effectiveness of HAG (Figure 6 on page 13), which compares the time-to-accuracy performance between original GNN-graph representation and HAG, and show that HAG can reduce the training time by 1.8x while obtaining the same model accuracy.\n\nThanks for the comment.\nI am completely fine for the exact case (i.e., Theorem 1). This case the performance should be the same.\n\nSo, Figure 6 is based on Theorem1 or Theorem 2? This part is still not clear.\nIf this is exact one (based on Theorem 1),  I still believe the accuracy comparison of sequential one is necessary, since we can make the method extremely fast with very poor accuracy. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eF3kStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1960/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1960/Authors|ICLR.cc/2020/Conference/Paper1960/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148364, "tmdate": 1576860548044, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment"}}}, {"id": "ryexiZJ2jB", "original": null, "number": 5, "cdate": 1573806487552, "ddate": null, "tcdate": 1573806487552, "tmdate": 1573807261092, "tddate": null, "forum": "H1eF3kStPS", "replyto": "rkxBrYqaYH", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for a thorough review and valuable questions. The reviewer has concerns about (1) how HAG affects model accuracy and (2) how sampling rate affects the runtime performance. We believe there are a few important misunderstandings, which we address in detail in our response below. We have also updated the paper to further clarify and emphasize these points.\n\n### Need to report the model accuracy\nThe HAG graph representation achieves the same training and test results as the original GNN-graph representation for each GNN model, even though GNN models with different aggregation methods (i.e., set v.s. sequential) may obtain different accuracy--- this issue is orthogonal to the HAG optimizations.\n\nTheorem 1 in the paper proves the equivalence of HAG and GNN-graph representations for both training and inference. This means that HAG performs exactly the same computations as the traditional model training/inference but in a non-redundant way. This means, that HAG obtains exactly the same model as traditional training (but HAG is much faster).\n\nTo address reviewer\u2019s comment we have added an experiment to evaluate the training effectiveness of HAG (Figure 6 on page 13), which compares the time-to-accuracy performance between original GNN-graph representation and HAG, and show that HAG can reduce the training time by 1.8x while obtaining the same model accuracy.\n\n### What is the trade-off between the sampling rate and the speedup\nWe think there is a misunderstanding here. The reviewer seems to assume that HAG is designed for mini-batch training, probably because we use GraphSAGE as an example to demonstrate different aggregation functions in Table 1. In fact, HAG is designed for full-batch training, which is also the training method used in most existing GNN models, including GCN (Kipf & Welling, 2016), GIN (Xu et al., 2019), and SGC (Wu et al., 2019). We will fix this confusion by emphasizing the full-batch training setting in the introduction and use other GNN models (with full-batch training) as examples in Table 1.\n\n### Equations are used without explaining the meaning (e.g., AGGREGATE in Equation (1)) \nWe apologize for the missing explanation in the equations. The AGGREGATE in Equation (1) can be arbitrary associative and commutative operations performed on a set (i.e., invariant to the order in which the aggregations are performed). We have updated the paper to clarify this.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eF3kStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1960/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1960/Authors|ICLR.cc/2020/Conference/Paper1960/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148364, "tmdate": 1576860548044, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment"}}}, {"id": "r1gKPbyhor", "original": null, "number": 4, "cdate": 1573806432718, "ddate": null, "tcdate": 1573806432718, "tmdate": 1573806904682, "tddate": null, "forum": "H1eF3kStPS", "replyto": "ryxmxkCRtB", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for providing a thorough review and asking valuable questions. The reviewer raises several concerns about the broad applicability of HAGs. As we argue below, HAG is broadly applicable to the majority of GNN models, it can support directed and weighted graphs, and it provides significant speedups with no loss in model performance (both at training as well as prediction time). We have revised the paper to further clarify and emphasize all these points.\n\n###  HAG does not support GAT\nWe thank the reviewer for making this point. This is correct and HAG optimization does not apply to graph attention networks (GAT). The HAG graph representation is designed for GNNs with a neighborhood aggregation scheme (formally defined in Algorithm 1), and is applicable to most existing GNN models, including GraphSAGE, PinSAGE, GCN, GIN, SGC, DCNN, DGCNN, and many others. Thus, it is reasonable to conclude that HAG applies to a significant majority of GNN models. Because such GNN-graphs are processed individually in these GNN models, they contain significant redundant computation (up to 84%), and HAG can reduce the overall computation by 6.3x, while provably preserving the original model accuracy. \nWe do appreciate the comment, and we have added a paragraph at the end of page 8 to discuss this limitation of HAG in the revised paper.\n\n### Can HAG support directed and weighted graph?\nHAG can support directed and/or weighted graphs as long as the GNN models can be abstracted as in Algorithm 1. In particular, HAG can support directed graphs by changing N(v) in Algorithm 1 to be the set of incoming-neighbors of node v (instead of the set of all neighbors). For weighted graphs, HAG can incorporate edge weights in neighborhood aggregation by updating the AGGREGATE function in Algorithm 1 to consider edge weights. For weighted graphs, rather than identifying common subsets of neighbors, HAG identifies common neighbors with shared edge weights as redundant computation. The fact that existing GNN models in the literature do not consider edge weights and are designed for undirected graphs makes it hard to find a realistic benchmark to evaluate the performance of HAG on directed and weighted graphs.\n\nTo address the reviewer\u2019s point we have added a discussion on potential extensions of our HAG algorithm to directed and weighted graphs in the revised paper.\n\n### The training effectiveness of HAG is questionable\nThere is a slight misunderstanding here. HAG eliminates redundancy in GNN training while exactly maintaining the original computation (proved in Theorem 1), therefore it is guaranteed to preserve the original model accuracy by design. However, to address reviewer\u2019s comment we have added an experiment to evaluate the training effectiveness of HAG (Figure 6 on page 13), which compares the time-to-accuracy performance between original GNN-graph representation and HAG, and shows that HAG can reduce the end-to-end training time by 1.8x while obtaining the same model accuracy. Thus, HAG leads to significant faster training time with no loss in model performance.\n\nWe have further clarified this in the main paper, because we see it as one of the important benefits of HAG that it maintains original model performance (by performing exactly the same computations), while leading to significant speed-ups.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eF3kStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1960/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1960/Authors|ICLR.cc/2020/Conference/Paper1960/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148364, "tmdate": 1576860548044, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment"}}}, {"id": "S1x2xZ1njH", "original": null, "number": 3, "cdate": 1573806323892, "ddate": null, "tcdate": 1573806323892, "tmdate": 1573806559784, "tddate": null, "forum": "H1eF3kStPS", "replyto": "rJxFqXA1ir", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment", "content": {"title": "Response to Review#4", "comment": "We thank the reviewer for providing a thorough review and asking valuable questions. The reviewer raises a concern about the broad applicability of HAGs. HAG is broadly applicable to the majority of GNN models, including GraphSage, PinSage, GCN, GIN, SGC, DCNN, DGCNN, and many others. HAG provides significant speedups while provably preserving model accuracy (both for training and inference).\n\n### HAG does not support GAT\nWe thank the reviewer for making this point. This is correct and HAG optimization does not apply to graph attention networks (GAT). The HAG graph representation is designed for GNNs with a neighborhood aggregation scheme (formally defined in Algorithm 1), and is applicable to most existing GNN models, including GraphSAGE, PinSAGE, GCN, GIN, SGC, DCNN, DGCNN, and many others. Thus, it is reasonable to conclude that HAG applies to a significant majority of GNN models. Because such GNN-graphs are processed individually in these GNN models, they contain significant redundant computation (up to 84%), and HAG can reduce the overall computation by 6.3x, while provably preserving the original model accuracy. \nWe do appreciate the comment, and we have added a paragraph at the end of page 8 to discuss this limitation of HAG in the updated paper.\n\n### More GNN results in different models would make the paper more convincing\nWe thank the reviewer for the constructive feedback. In the updated paper, we have evaluated HAG on more GNN models and observed similar or even better performance improvement. In particular, we have further evaluated HAG on GIN (Xu et al., 2019) and SGC (Wu et al., 2019). The results are shown in Figure 5 on page 12. Compared to the GCN model, our HAG optimizations achieve similar speedups on GIN and better speedups on SGC."}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eF3kStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1960/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1960/Authors|ICLR.cc/2020/Conference/Paper1960/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148364, "tmdate": 1576860548044, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment"}}}, {"id": "rJxFqXA1ir", "original": null, "number": 3, "cdate": 1573016465272, "ddate": null, "tcdate": 1573016465272, "tmdate": 1573017724040, "tddate": null, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposes a new graph Hierarchy representation named HAG. The HAG aiming at eliminating the redundancy during the aggregation stage In Graph Convolution networks. This strategy can speed up the training and inference time while keeping the GNN output unchanged, which means it can get the same predict result as before. The idea is clear and easy to follow. For the theory part, I do not thoroughly check the theoretical proof but the theorem statement sounds reasonable for me. The experiment shows the HAG performs faster in both training and inference.\n\nGenerally speaking, I think this paper has good theory analysis, the speed-up effect is also good from the experimental result.  However, I still have some concerns and comments.\n\n1. The algorithm seems hard to apply on the attention-based Graph Neural network, which achieves good performance in several benchmarks these years. In other words, the redundancy of the node aggregate only exists in the Graph Convolution model with the fix node weight, which is replaced by a dynamic weight in many latest models with higher performance. That weakens the empirical use of this algorithm.\n\n2. The authors state that the HAG can optimize various kinds of GNN models, but the experiment only shows the results on a small GCN model. More GNN results in different models and settings would make the algorithm more convincing.\n\nIn conclusion, I think this is a good paper. Regards the comments above, I prefer a grade around the borderline. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699645462, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Reviewers"], "noninvitees": [], "tcdate": 1570237729793, "tmdate": 1575699645476, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Review"}}}, {"id": "rkxBrYqaYH", "original": null, "number": 1, "cdate": 1571821885202, "ddate": null, "tcdate": 1571821885202, "tmdate": 1572972401354, "tddate": null, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #32", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, authors propose a way to speed up the computation of GNN. More specifically, the hierarchically aggregate computation graphs are proposed to aggregate the intermediate node and utilize this to speed up a GNN computation. Authors proof that the computation based on HAGs are equivalent to the vanilla computation of GNN (Theorem1). Moreover, for sequential aggregation, it can find a HAG that is at least (1-1/e)-approximation of the globally optimal HAGs (Theorem 3). These theoretical results are nice. Through experiments, the authors demonstrate that the proposed method can get faster computation than vanilla algorithms.\n\nThe paper is clearly written and easy to follow. However, there are some missing piece needed to be addressed.\nI put 6 (weak accept), since we cannot put 5. However, current my intention about the score is slightly above 5.\n\nDetailed comments:\n1. Experiments are only done for computational time comparison. In particular, for the sequential one, prediction accuracy can be changed due to the aggregation algorithm. Thus, it needs to report the prediction accuracy.\n\n2. In GraphSAGE, what is the sampling rate? It would be nice to have the trade-off between the sampling rate and the speedup. I guess if we sample small number of points in GraphSAGE, the performance can be degraded. In contrast, the proposed algorithm can get similar performance with larger sampling rate? Related to the question 1, the performance comparison is needed. \n\n3. Equations are used without not explaining the meaning. For instance AGGREGATE function (1), there is no definition how to aggregate. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575699645462, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Reviewers"], "noninvitees": [], "tcdate": 1570237729793, "tmdate": 1575699645476, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Review"}}}, {"id": "Syly8HwpKH", "original": null, "number": 1, "cdate": 1571808582853, "ddate": null, "tcdate": 1571808582853, "tmdate": 1571808582853, "tddate": null, "forum": "H1eF3kStPS", "replyto": "BJlw11mpFr", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment", "content": {"comment": "Thanks for your interests in the paper.\n\nThe negligible memory overhead is because the intermediate aggregations nodes do not need to be memorized for back propagation, and our HAG implementation uses the same memory across all layers. Caching the intermediate results for 100K nodes (with 16 activations per node) requires approximately 6MB GPU memory, which is negligible compared to the overall memory usage (~6GB) for training COLLAB.\n\nFor the second question, our HAG approach can actually reduce the memory usage for edges, since a HAG contains 1.3x-5.6x fewer edges than the original graph representation. The two bottom charts in Figure 3 show the edge comparison between HAG and the original graph representation.\n\nWe will include more analysis on the memory overhead in the revised paper.", "title": "The HAG memory overhead is negligible (~0.1%), and HAG can reduce the number of edges by 1.3-5.6x."}, "signatures": ["ICLR.cc/2020/Conference/Paper1960/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eF3kStPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1960/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1960/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1960/Authors|ICLR.cc/2020/Conference/Paper1960/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148364, "tmdate": 1576860548044, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Official_Comment"}}}, {"id": "BJlw11mpFr", "original": null, "number": 1, "cdate": 1571790559411, "ddate": null, "tcdate": 1571790559411, "tmdate": 1571790559411, "tddate": null, "forum": "H1eF3kStPS", "replyto": "H1eF3kStPS", "invitation": "ICLR.cc/2020/Conference/Paper1960/-/Public_Comment", "content": {"comment": "In section 5.5, you mentioned \"by gradually increasing the capacity,the search algorithm eventually \ufb01nds a HAG with 100K aggregation nodes, which consume 6MB of memory (0.1% memory overhead) while improving the training performance by 2.8\u00d7.\"  It seems that a lot of extra nodes are added but only 0.1% memory overhead is introduced, could you explain how this number was calculated?\nAnd how much memory overhead will be introduced for extra edges?", "title": "clarification on memory overhead"}, "signatures": ["~Xiaojian_Wu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xiaojian_Wu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhihao@cs.stanford.edu", "silin@microsoft.com", "rexying@stanford.edu", "jiaxuan@stanford.edu", "jure@cs.stanford.edu", "aiken@cs.stanford.edu"], "title": "Redundancy-Free Computation Graphs for Graph Neural Networks", "authors": ["Zhihao Jia", "Sina Lin", "Rex Ying", "Jiaxuan You", "Jure Leskovec", "Alex Aiken."], "pdf": "/pdf/a1ce3d20aa426d8f78214ce4b8e53940f245ff5a.pdf", "TL;DR": "We present Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundant computations in GNN training and inference.", "abstract": "Graph Neural Networks (GNNs) are based on repeated aggregations of information across nodes\u2019 neighbors in a graph.  However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation  results  hierarchically,  and  eliminating  repeated  computations  and unnecessary data transfers in GNN training and inference. We introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs.  Experiments show that the HAG representation significantly outperforms the standard GNN graph representation by increasing the end-to-end training throughput by up to 2.8x and reducing the aggregations and data transfers in GNN training by up to 6.3x and 5.6x. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.", "code": "https://www.dropbox.com/sh/pce8j67byr6bq83/AACh0UKkua0toPM_ZjtTTVava?dl=0", "keywords": ["Graph Neural Networks", "Runtime Performance"], "paperhash": "jia|redundancyfree_computation_graphs_for_graph_neural_networks", "original_pdf": "/attachment/fcf9fb10e52df3a0a071176f69e60c4440f4abcb.pdf", "_bibtex": "@misc{\njia2020redundancyfree,\ntitle={Redundancy-Free Computation Graphs for Graph Neural Networks},\nauthor={Zhihao Jia and Sina Lin and Rex Ying and Jiaxuan You and Jure Leskovec and Alex Aiken.},\nyear={2020},\nurl={https://openreview.net/forum?id=H1eF3kStPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1eF3kStPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187113, "tmdate": 1576860581369, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1960/Authors", "ICLR.cc/2020/Conference/Paper1960/Reviewers", "ICLR.cc/2020/Conference/Paper1960/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1960/-/Public_Comment"}}}], "count": 12}