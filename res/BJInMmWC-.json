{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730156696, "tcdate": 1509139117596, "number": 1149, "cdate": 1518730156681, "id": "BJInMmWC-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "BJInMmWC-", "original": "r1enGXbR-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260085731, "tcdate": 1517249849401, "number": 555, "cdate": 1517249849377, "id": "BJ-HHy6rM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "BJInMmWC-", "replyto": "BJInMmWC-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper presents a novel model for generating images and natural language descriptions simultaneously. The aim is to distangle representations learned for image generation by connecting them to the paired text. The reviews praise the problem setup and the mathematical formulation. However they point out significant issues with the clarity of the presentation in particular the diagrams, citations, and optimization procedure in general. They also point out issues with the experimental setup in terms of datasets used and lack of natural images for the tasks in question.  Reviews are impressively thorough and should be of use for a future submission. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642390264, "tcdate": 1511741325874, "number": 1, "cdate": 1511741325874, "id": "ryU5DROgM", "invitation": "ICLR.cc/2018/Conference/-/Paper1149/Official_Review", "forum": "BJInMmWC-", "replyto": "BJInMmWC-", "signatures": ["ICLR.cc/2018/Conference/Paper1149/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "The paper needs to be polished, and more experimental evaluations are required. ", "rating": "4: Ok but not good enough - rejection", "review": "This paper presented a Generative entity networks (GEN). It is a multi-view extension of variational autoencoder (VAE) for disentangled representation. It uses the image and its attributes.  The paper is very well motivated and tackles an important problem. However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished. \n\nPros:\n1. This paper tackles an important research question. \nLearning a meaningful representation is needed in general. For the application of images, using text description to refine the representation is a natural and important research question. \n\n2. The proposed idea is very well motivated, and the proposed model seems correct. \n\nCons and questions:\n1. The presentation of the model is not clear. \nFigure 2 which is the graphic representation of the model is hard to read. There is no meaningful caption for this important figure.  Which notation in the figure corresponds to which variable is not clear at all.  This also leads to unclarity of the text presentation of the model, for example, section 3.2. Which latent variable is used to decode which part?\n\n2. Missing important related works.\nThere are a couple of highly related work with multi-view VAE tracking similar problem have been proposed in the past. The paper did not discuss these related work and did not compare the performances. Examples of these related work include [1] and [2] (at the end of the review).\nAdditionally, the idea of factorized representation idea (describable component and indescribable component) has a long history.  It can be traced back to [3], used in PGM setting in [4] and used in VAE setting in [1]. This group of related work should also be discussed. \n\n3. Experiment evaluation is not sufficient. \nFirstly, only one toy dataset is used for experimental evaluations. More evaluations are needed to verify the method, especially with natural images. \nSecondly, there are no other state-of-the-art baselines are used. The baselines are various simiplied versions of the proposed model. More state-of-the-art baselines are needed, e.g. [1] and [2].\n\n4. Maybe overclaiming.\nIn the paper, only attributes of objects are used which is not semi-natural languages.\n\n5. The paper, in general, needs to be polished. \nThere are missing links and references in the paper and un-explained notations, and non-informative captions.\n\n6. Possibility to apply to natural images. \nThis method does not model spatial information. How can the method make sure that  simple adding generated images with each component will lead to a meaningful image in the end? Especially with natural images,  the spacial location and the scale should be critical. \n\n[1] Wang, Weiran, Honglak Lee, and Karen Livescu. \"Deep variational canonical correlation analysis.\" arXiv preprint arXiv:1610.03454 (2016).\n[2] Suzuki, Masahiro, Kotaro Nakayama, and Yutaka Matsuo. \"Joint Multimodal Learning with Deep Generative Models.\" arXiv preprint arXiv:1611.01891 (2016).\n[3] Tucker, Ledyard R. \"An inter-battery method of factor analysis.\" Psychometrika 23.2 (1958): 111-136.\n[4] Zhang, Cheng, Hedvig Kjellstr\u00f6m, and Carl Henrik Ek. \"Inter-battery topic representation learning.\" European Conference on Computer Vision. Springer International Publishing, 2016.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642390158, "id": "ICLR.cc/2018/Conference/-/Paper1149/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1149/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1149/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1149/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1149/AnonReviewer1"], "reply": {"forum": "BJInMmWC-", "replyto": "BJInMmWC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642390158}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642390218, "tcdate": 1511791818340, "number": 2, "cdate": 1511791818340, "id": "r1MR39Kgz", "invitation": "ICLR.cc/2018/Conference/-/Paper1149/Official_Review", "forum": "BJInMmWC-", "replyto": "BJInMmWC-", "signatures": ["ICLR.cc/2018/Conference/Paper1149/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Extension of Attend Infer Repeat to include \"language\", evaluated on a question answering task. Approach is evaluated on a image caption agreement task.", "rating": "5: Marginally below acceptance threshold", "review": "**Summary**\nThe paper proposes an extension of the attend, infer, repeat generative model of Eslami, 2016 and extends it to handle ``visual attribute descriptions. This straightforward extension is claimed to improve image quality and shown to improve performance on a previously introduced image caption ranking task. In general, the paper shows improvements on an image caption agreement task introduced in Kuhnle and Copestake, 2017.  The paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for ICLR. See more detailed points below in Weaknesses.\n\n**Strengths**\nI like the high-level motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images. I buy the premise and think the work addresses an important issue. \n\n**Weakness**\n\nApproach:\n* A major limitation of the model seems to be that one needs access to both images and attribute vectors at inference time to compute representations which is a highly restrictive assumption (since inference networks are discriminative). The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference. The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper).\n\nClarity:\n* Eqn. 5, LHS can be written more clearly as \\hat{a}_k. \n\n* It would also be good to cite the following related work, which closely ties into the model of Eslami 2016, and is prior work: \n\nEfficient inference in occlusion-aware generative models of images,\nJonathan Huang, Kevin Murphy.\nICLR Workshops, 2016\n\n* It would be good to clarify that the paper is focusing on the image caption agreement task from Kuhnle and Copestake, as opposed to generic visual question answering.\n\n* The claim that the paper works with natural language should be toned down and clarified. This is not natural language, firstly because the language in the dataset is synthetically generated and not \u201cnatural\u201d. Secondly, the approach parses this \u201csynthetic\u201d language into structured tuples which makes it even less natural. Also, Page. 3. What does \u201cpartial descriptions\u201d mean?\n\n* Section 3: It would be good to explicitly draw out the graphical model for the proposed approach and clarify how it differs from prior work (Eslami, 2016).\n\n* Sec. 3. 4 mentions that the \u201conly image\u201d encoder is used to obtain the representation for the image, but the \u201conly image\u201d encoder is expected to capture the \u201cindescribable component\u201d from the image, then how is the attribute information from the image captured in this framework? One cannot hope to do image caption association prediction without capturing the image attributes...\n\n*, In general, the writing and presentation of the model seem highly fragmented, and it is not clear what the specifics of the overall model are. For instance, in the decoder, the paper mentions for the first time that there are variables \u201cz\u201d, but does not mention in the encoder how the variables \u201cz\u201d were obtained in the first place (Sec. 3.1). For instance, it is also not clear if the paper is modeling variable length sequences in a similar manner to Eslami, 2016 or not, and if this work also has a latent variable [z, z_pres] at every timestep which is used in a similar manner to Eqn. 2 in Eslami, 2016. Sec. 3.4 \u201cGEN Image Encoder\u201d has some typo, it is not clear what the conditioning is within q(z) term.\n\n* Comparison to baselines: \n  1. How well does this model do against a baseline discriminative image caption ranking approach, similar to [D]? This seems like an important baseline to report for the image caption ranking task.\n  2. Another crucial baseline is to train the Attend, Infer, Repeat model on the ShapeWorld images, and then take the latent state inferred at every step by that model, and use those features instead of the features described in Sec. 3.4 \u201cGen Image Encoder\u201d and repeat the rest of the proposed pipeline. Does the proposed approach still show gains over Attend Infer Repeat?\n  3. The results shown in Fig. 7 are surprising -- in general, it does not seem like a regular VAE would do so poorly. Are the number of parameters in the proposed approach and the baseline VAE similar? Are the choices of decoder etc. similar? Did the model used for drawing Fig. 7 converge? Would be good to provide its training curve. Also, it would be good to evaluate the AIR model from Eslami, 2016 on the same simple shapes dataset and show unconditional samples. If the claim from the work is true, that model should be just as bad as a regular VAE and would clearly establish that using language is helping get better image samples.\n\n* Page 2: In general the notion of separating the latent space into content and style, where we have labels for the \u201ccontent\u201d is an old idea that has appeared in the literature and should be cited accordingly. See [B] for an earlier treatment, and an extension by [A]. See also the Bivcca-private model of [C] which has \u201cprivate\u201d latent variables for vision similar to this work (this is relevant to Sec. 3.2.)\n\nReferences:\n[A]: Upchurch, Paul, Noah Snavely, and Kavita Bala. 2016. \u201cFrom A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators.\u201d arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.02003.\n\n[B]: Kingma, Diederik P., Danilo J. Rezende, Shakir Mohamed, and Max Welling. 2014. \u201cSemi-Supervised Learning with Deep Generative Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1406.5298.\n\n[C]: Wang, Weiran, Xinchen Yan, Honglak Lee, and Karen Livescu. 2016. \u201cDeep Variational Canonical Correlation Analysis.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1610.03454.\n\n[D]: Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. \u201cUnifying Visual-Semantic Embeddings with Multimodal Neural Language Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1411.2539.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642390158, "id": "ICLR.cc/2018/Conference/-/Paper1149/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1149/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1149/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1149/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1149/AnonReviewer1"], "reply": {"forum": "BJInMmWC-", "replyto": "BJInMmWC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642390158}}}, {"ddate": null, "original": null, "tddate": 1511798858766, "tmdate": 1515642390174, "tcdate": 1511792104991, "number": 3, "cdate": 1511792104991, "id": "BJbx0qYlz", "invitation": "ICLR.cc/2018/Conference/-/Paper1149/Official_Review", "forum": "BJInMmWC-", "replyto": "BJInMmWC-", "signatures": ["ICLR.cc/2018/Conference/Paper1149/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "GENERATIVE ENTITY NETWORKS: DISENTANGLING ENTITIES AND ATTRIBUTES IN VISUAL SCENES USING PARTIAL NATURAL LANGUAGE DESCRIPTIONS", "rating": "5: Marginally below acceptance threshold", "review": "Summary: The authors observe that the current image generation models generate realistic images however as the dimensions of the latent vector is fully entangled, small changes to a single neuron can effect every output pixel in arbitrary ways. In this work, they explore the effect of using partial natural language scene descriptions for the task of disentangling the latent entities visible in the image.  The proposed Generative Entity Networks jointly generates the natural language descriptions and images from scratch. The core model is Variational Autoencoders (VAE) with an integrated visual attention mechanism that also generates the associated text. The experiments are conducted on the Shapeworld dataset.\n\nStrengths:\nSimultaneous text and image generation is an interesting research topic that is relevant for the community.\nThe paper is well written, the model is formulated with no errors (although it could use some more detail) and supported by illustrations (although there are some issues with the illustrations detailed below). \nThe model is evaluated on tasks that it was not trained on which indicate that this model learns generalizable latent representations. \n\nWeaknesses:\nThe paper gives the impression to be rushed, i.e. there are citations missing (page 3 and 6), the encoder model illustration is not as clear as it could be. Especially the white boxes have no labels, the experiments are conducted only on one small-scale proof of concept dataset, several relevant references are missing, e.g. GAN, DCGAN, GAWWN, StackGAN. Visual Question answering is mentioned several times in the paper, however no evaluations are done in this task.\n\nFigure 2 is complex and confusing due to the lack of proper explanation in the text. The reader has to find out the connections between the textual description of the model and the figure themselves due to no reference to particular aspects of the figure at all. In addition the notation of the modules in the figure is almost completely disjoint so that it is initially unclear which terms are used interchangeably.\n\nDetails of the \u201cwhite components\u201d in Figure 2 are not mentioned at all. E.g., what is the purpose of the fully connected layers, why do the CNNs split and what is the difference in the two blocks (i.e. what is the reason for the addition small CNN block in one of the two)\n\nThe optimization procedure is unclear. What is the exact loss for each step in the recurrence of the outputs (according to Figure 5)? Or is only the final image and description optimized. If so, how is the partial language description as a target handled since the description for a different entity in an image might be valid, but not the current target. (This is based on my understanding that each data point consists of one image with multiple entities and one description that only refers to one of the entities).\n\nAn analysis or explanation of the following would be desirable: How is the network trained on single descriptions able to generate multiple descriptions during evaluation. How does thresholding mentioned in Figure 5 work?\n\nIn the text, k suggests to be identical to the number of entities in the image. In Figure 5, k seems to be larger than the number of entities. How is k chosen? Is it fixed or dynamic?\n\nEven though the title claims that the model disentangles the latent space on an entity-level, it is not mentioned in the paper. Intuitively from Figure 5, the network generates black images (i.e. all values close to zero) whenever the attention is on no entity and, hence, when attention is on an entity the latent space represents only this entity and the image is generated only showing that particular entity. However, confirmation of this intuition is needed since this is a central claim of the paper.\n\nAs the main idea and the proposed model is simple and intuitive, the evaluation is quite important for this paper to be convincing. Shapeworlds dataset seems to be an interesting proof-of-concept dataset however it suffers from the following weaknesses that prevent the experiments from being convincing especially as they are not supported with more realistic setups. First, the visual data is composed of primitive shapes and colors in a black background. Second, the sentences are simple and non-realistic. Third, it is not used widely in the literature, therefore no benchmarks exist on this data. \n\nIt is not easy to read the figures in the experimental section, no walkthrough of the results are provided. For instance in Figure 4a, the task is described as \u201cshowing the changes in the attribute latent variables\u201d which gives the impression that, e.g. for the first row the interpolation would be between a purple triangle to a purple rectangle however in the middle the intermediate shapes also are painted with a different color. It is not clear why the color in the middle changes.\n\nThe evaluation criteria reported on Table 1 is not clear. How is the accuracy measured, e.g. with respect to the number of objects mentioned in the sentence, the accuracy of the attribute values, the deviation from the ground truth sentence (if so, what is the evaluation metric)? No example sentences are provided for a qualitative comparisons. In fact, it is not clear if the model generates full sentences or attribute phrases.\n\nAs a summary, this paper would benefit significantly with a more extensive overview of the existing relevant models, clarification on the model details mentioned above and a more through experimental evaluation with more datasets and clear explanation of the findings.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642390158, "id": "ICLR.cc/2018/Conference/-/Paper1149/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1149/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1149/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1149/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1149/AnonReviewer1"], "reply": {"forum": "BJInMmWC-", "replyto": "BJInMmWC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642390158}}}, {"tddate": null, "ddate": null, "tmdate": 1515189392801, "tcdate": 1515189392801, "number": 3, "cdate": 1515189392801, "id": "HyK9VuT7M", "invitation": "ICLR.cc/2018/Conference/-/Paper1149/Official_Comment", "forum": "BJInMmWC-", "replyto": "H1zZE_6mf", "signatures": ["ICLR.cc/2018/Conference/Paper1149/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1149/Authors"], "content": {"title": "RE: Reviewer 2", "comment": "(a)\t\"A major limitation of the model seems to be that one needs access to both images and attribute vectors at inference time to compute representations which is a highly restrictive assumption (since inference networks are discriminative). The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference. The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper).\"\n\nThere is a misunderstanding here and we should have clarified this in the paper. The model only needs access to an image as input in order to perform inference. In fact this is how we obtain the representations used in the auxilliary ShapeWorld tasks. We encode images without paired language by ignoring the language inputs in equation (5), instead using \\hat{a} = a^{I}.\n\n\n(b)\t\"* Sec. 3. 4 mentions that the only image encoder is used to obtain the representation for the image, but the only image encoder is expected to capture the indescribable component from the image, then how is the attribute information from the image captured in this framework? One cannot hope to do image caption association prediction without capturing the image attributes...\"\n\nAs we discussed in section 3.1, the image encoder generates both the attribute information as well as the \"indescribable component\".  When the encoder is also provided the language, then the multi-modal aggregator is used to coherently combine the attribute predictions generated from the language with the predictions from the image encoder. \n\n(c)\t\"in the decoder, the paper mentions for the first time that there are variables z, but does not mention in the encoder how the variables z were obtained in the first place (Sec. 3.1).\u201d\n\nWe appreciate that this is not sufficiently explained in the paper and will clarify this in future work.  We did mention in Sec 3.1 the process for obtaining the visual latent variables z^V_k: \"Finally each visual object representation v_k is passed through an MLP to obtain the parameters of the approximate posterior distribution over that object\u2019s visual latent variables.\" However we did not state the equivalent process for obtaining the attribute latent variables z^A_k (which is achieved by applying a MLP to a^hat_k). \n\n(d)\t\u201cSec. 3.4 GEN Image Encoder has some typo, it is not clear what the conditioning is within q(z) term.\u201d\nYes this is a typo, the notation should read q*(z | I), where q* is the encoder applied to only to input images with the modification described earlier.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723734, "id": "ICLR.cc/2018/Conference/-/Paper1149/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJInMmWC-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1149/Authors|ICLR.cc/2018/Conference/Paper1149/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1149/Authors|ICLR.cc/2018/Conference/Paper1149/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1149/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1149/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1149/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1149/Reviewers", "ICLR.cc/2018/Conference/Paper1149/Authors", "ICLR.cc/2018/Conference/Paper1149/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723734}}}, {"tddate": null, "ddate": null, "tmdate": 1515189320726, "tcdate": 1515189320726, "number": 2, "cdate": 1515189320726, "id": "H1WUNO6Xz", "invitation": "ICLR.cc/2018/Conference/-/Paper1149/Official_Comment", "forum": "BJInMmWC-", "replyto": "H1zZE_6mf", "signatures": ["ICLR.cc/2018/Conference/Paper1149/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1149/Authors"], "content": {"title": "RE: Reviewer 1", "comment": "(a)\t\"Visual Question answering is mentioned several times in the paper, however no evaluations are done in this task\"\n\nMost of the mentions of visual question answering in the paper are meant to refer to the general task of answering questions about an image, and in this sense, the Shapeworld caption classification we evaluate on is a visual question answering task.\n\n(b)\t\"The optimization procedure is unclear. What is the exact loss for each step in the recurrence of the outputs (according to Figure 5)? Or is only the final image and description optimized. If so, how is the partial language description as a target handled since the description for a different entity in an image might be valid, but not the current target. (This is based on my understanding that each data point consists of one image with multiple entities and one description that only refers to one of the entities).\"\n\nWe discuss this in Section 3.3 where we say: \"However as the model outputs language predictions for multiple objects, and yet only one object is described in language per scene, we maximize over assignments of predicted language to the true caption.\"\n\n(c)\t\"An analysis or explanation of the following would be desirable: How is the network trained on single descriptions able to generate multiple descriptions during evaluation.\"\n\nThe network always generates multiple descriptions (one for each recurrent step), but as we just highlighted, a loss signal is only generated between the provided description, and the generated description which most closely matches it.\n\n(d)\t\"How does thresholding mentioned in Figure 5 work?\"\nFor every encoded entity, the model decoder outputs word probabilities. We report all words assigned probability greater than 0.5  by the model.\n\n(e)\t\"In the text, k suggests to be identical to the number of entities in the image. In Figure 5, k seems to be larger than the number of entities. How is k chosen? Is it fixed or dynamic?\"\nk is chosen statically as an upper bound on the number of entities in the image.  The model can then avoid using entities, by not drawing anything to the image for a given entity, and by generating zeros for all natural language attributes.\n\n(f)\t\"Even though the title claims that the model disentangles the latent space on an entity-level, it is not mentioned in the paper. Intuitively from Figure 5, the network generates black images (i.e. all values close to zero) whenever the attention is on no entity and, hence, when attention is on an entity the latent space represents only this entity and the image is generated only showing that particular entity. However, confirmation of this intuition is needed since this is a central claim of the paper.\"\n\nFigure 4(b)(d) indicates that manipulation of latent variables associated with a particular entity results in visual changes in only one object (e.g. the location / rotation of the green rectangle in 4d). This indicates that the latent representation is disentangled on an entity-level.  Did you have a specific experiment in mind that you thought would more clearly show that the representation was disentangled?\n\n(g)\t\"no benchmarks exist on this data\"\nThere do exist a carefully chosen set of benchmarks for the VQA dataset which were adapted for this dataset, and these are the benchmarks that we compare to. But we agree that benchmarks for generative modeling don't exist for this dataset.\n\n(h)\t\"It is not easy to read the figures in the experimental section, no walkthrough of the results are provided. For instance in Figure 4a, the task is described as showing the changes in the attribute latent variables which gives the impression that, e.g. for the first row the interpolation would be between a purple triangle to a purple rectangle however in the middle the intermediate shapes also are painted with a different color. It is not clear why the color in the middle changes.\"\n\nWe attempted to address this issue in the figure caption where we said:  \"Note that we should not expect the division between the color and shape semantic attributes to align to the two latent dimensions since the GEN model leaves the encoding of the attribute dimensions completely entangled for a given entity.\"\n\n\n(i)\t\"The evaluation criteria reported on Table 1 is not clear. How is the accuracy measured, e.g. with respect to the number of objects mentioned in the sentence, the accuracy of the attribute values, the deviation from the ground truth sentence (if so, what is the evaluation metric)? No example sentences are provided for a qualitative comparisons. In fact, it is not clear if the model generates full sentences or attribute phrases.\"\n\nWe should have made this more clear in the paper.  Each natural language description in the dataset is labeled as either True or False, and the task is to predict this label.  So the accuracy numbers simply indicate whether or not a given description is correctly predicted to be True or False.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723734, "id": "ICLR.cc/2018/Conference/-/Paper1149/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJInMmWC-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1149/Authors|ICLR.cc/2018/Conference/Paper1149/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1149/Authors|ICLR.cc/2018/Conference/Paper1149/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1149/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1149/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1149/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1149/Reviewers", "ICLR.cc/2018/Conference/Paper1149/Authors", "ICLR.cc/2018/Conference/Paper1149/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723734}}}, {"tddate": null, "ddate": null, "tmdate": 1515189241795, "tcdate": 1515189241795, "number": 1, "cdate": 1515189241795, "id": "H1zZE_6mf", "invitation": "ICLR.cc/2018/Conference/-/Paper1149/Official_Comment", "forum": "BJInMmWC-", "replyto": "BJInMmWC-", "signatures": ["ICLR.cc/2018/Conference/Paper1149/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1149/Authors"], "content": {"title": "Thanks!", "comment": "We would like to thank the reviewers for reading the paper so carefully and for their detailed reviews. In addition to polishing the writing, and filling out the related work section, the main weakness of the paper seems to be both our evaluation on only one dataset, as well as comparing to only the dataset baselines rather than more recent stronger baselines.  We plan to work on this, and resubmit to a later conference.\n\nHowever, we did want to clarify a few confusions which were brought up in the reviews.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions", "abstract": "Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.", "pdf": "/pdf/3cf45610469af5c3ecdef0638ed8c83937f59c27.pdf", "paperhash": "nash|generative_entity_networks_disentangling_entitites_and_attributes_in_visual_scenes_using_partial_natural_language_descriptions", "_bibtex": "@misc{\nnash2018generative,\ntitle={Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions},\nauthor={Charlie Nash and Sebastian Nowozin and Nate Kushman},\nyear={2018},\nurl={https://openreview.net/forum?id=BJInMmWC-},\n}", "keywords": ["VAE", "Generative Model", "Vision", "Natural Language"], "authors": ["Charlie Nash", "Sebastian Nowozin", "Nate Kushman"], "authorids": ["charlie.nash@ed.ac.uk", "sebastian.nowozin@microsoft.com", "nate@kushman.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723734, "id": "ICLR.cc/2018/Conference/-/Paper1149/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJInMmWC-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1149/Authors|ICLR.cc/2018/Conference/Paper1149/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1149/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1149/Authors|ICLR.cc/2018/Conference/Paper1149/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1149/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1149/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1149/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1149/Reviewers", "ICLR.cc/2018/Conference/Paper1149/Authors", "ICLR.cc/2018/Conference/Paper1149/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723734}}}], "count": 8}