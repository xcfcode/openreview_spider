{"notes": [{"id": "S1lBVgHYvr", "original": "Ske5UIeFvS", "number": 2246, "cdate": 1569439788758, "ddate": null, "tcdate": 1569439788758, "tmdate": 1577168234349, "tddate": null, "forum": "S1lBVgHYvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sjzhao@stanford.edu", "yangsong@cs.stanford.edu", "ermon@cs.stanford.edu"], "title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "authors": ["Shengjia Zhao", "Yang Song", "Stefano Ermon"], "pdf": "/pdf/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "abstract": "Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.", "keywords": ["Adversarial Defense", "Certified Defense", "Adversarial Examples"], "paperhash": "zhao|towards_certified_defense_for_unrestricted_adversarial_attacks", "original_pdf": "/attachment/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "_bibtex": "@misc{\nzhao2020towards,\ntitle={Towards Certified Defense for Unrestricted Adversarial Attacks},\nauthor={Shengjia Zhao and Yang Song and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lBVgHYvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KoJ5dScvHj", "original": null, "number": 1, "cdate": 1576798744233, "ddate": null, "tcdate": 1576798744233, "tmdate": 1576800891912, "tddate": null, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2246/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a certified defense under the more general threat model beyond additive perturbation. The proposed defense method is based on adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters, which is similar to differential privacy mechanism. The authors proved the query complexity for any attacker to generate a successful adversarial attack. The main objection of this work is (1) the assumption of the attacker and the definition of the query complexity (to recover the optimal classifier rather than generating an adversarial example successfully) is uncommon, (2) the claim is misleading, and (3) the experimental evaluation is not sufficient (only two attacks are evaluated). The authors only provided a brief response to address the reviewers\u2019 comments/questions without submitting a revision. Unfortunately none of the reviewer is in support of this paper even after author response.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sjzhao@stanford.edu", "yangsong@cs.stanford.edu", "ermon@cs.stanford.edu"], "title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "authors": ["Shengjia Zhao", "Yang Song", "Stefano Ermon"], "pdf": "/pdf/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "abstract": "Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.", "keywords": ["Adversarial Defense", "Certified Defense", "Adversarial Examples"], "paperhash": "zhao|towards_certified_defense_for_unrestricted_adversarial_attacks", "original_pdf": "/attachment/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "_bibtex": "@misc{\nzhao2020towards,\ntitle={Towards Certified Defense for Unrestricted Adversarial Attacks},\nauthor={Shengjia Zhao and Yang Song and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lBVgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725172, "tmdate": 1576800276955, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2246/-/Decision"}}}, {"id": "r1guL36CFS", "original": null, "number": 2, "cdate": 1571900495996, "ddate": null, "tcdate": 1571900495996, "tmdate": 1574270144375, "tddate": null, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2246/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "Although this paper's title contains \"certified defense\" and \"unrestricted adversarial attack\",  what I believe this paper is doing is analyzing the query complexity of query-based black-box attacks under simple linear models such as logistic regressions (or kernelized versions). The authors considered a binary classifier with the additional capability of giving \"no response\" when the confidence is low.  In addition, the output of the classifier has to be perturbed by a random Gaussian vector. The authors then define several metrics including defensibility and query privacy to develop the query complexity on the considered model. The authors tested the query performance on two attacks: (1) the sign attack proposed by the authors and (2) the simba attack proposed by Guo et al. \n\nI have several concerns regarding this paper:\n\n1. In my perspective, the title is very misleading and does not properly justify the claims made in this paper. \"Certified defense\" usually refers to consistent top-1 prediction of a perturbed data sample under a defined threat model. The paper reads like the authors are actually certifying the defined defensibility metric but without a threat model to certify. In addition, the attack setting is limited to black-box attacks (i.e. zero-order adversary), whereas in certified defense the attack assumption is white-box. \n\n2. It is also very unclear how unrestricted attack plays a role in the studied problem.  In the introduction, the authors' definition of adversarial examples is \"any input is considered a valid adversarial example as long as it induces the classifier to predict a different label than an oracle classifier.\" But what is the oracle classifier? How do we justify the credibility of the \"adversarial examples\" in the experiments?\n\n3. Only two black-box attacks were compared in this paper, one is the sign attack proposed by the authors, the other is the simba attack proposed by Guo et al. To my knowledge, simba attack paper has not been published at any peer-reviewed venue. In other words, both attacks are not widely recognized attacks or methods from published papers. Therefore, the performance evaluation is not fully justified. Since there are many black-box attacks from published papers, why not do performance analysis on those attacks?\n\n4. Similar to 3, the classifier setting is also uncommon. Although I am happy to see classifiers have the ability to give no-response,  admittedly this type of classifier is rarely used in practice, not to mention the analysis is tied with Gaussian perturbation on the output. The technical contributions can be limited if the main contribution of this paper is characterizing the query complexity (or defensibility) of an uncommon classifier with Gaussian perturbation on the output. I believe providing more insights on how the analysis can be useful to mainstream classifiers are critical and necessary.\n\n***Post-rebuttal comments\nI thank the authors for the response. I hope the comments areuseful for preparing a future version of this work.\n***", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2246/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2246/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sjzhao@stanford.edu", "yangsong@cs.stanford.edu", "ermon@cs.stanford.edu"], "title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "authors": ["Shengjia Zhao", "Yang Song", "Stefano Ermon"], "pdf": "/pdf/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "abstract": "Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.", "keywords": ["Adversarial Defense", "Certified Defense", "Adversarial Examples"], "paperhash": "zhao|towards_certified_defense_for_unrestricted_adversarial_attacks", "original_pdf": "/attachment/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "_bibtex": "@misc{\nzhao2020towards,\ntitle={Towards Certified Defense for Unrestricted Adversarial Attacks},\nauthor={Shengjia Zhao and Yang Song and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lBVgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575717402939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2246/Reviewers"], "noninvitees": [], "tcdate": 1570237725596, "tmdate": 1575717402953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2246/-/Official_Review"}}}, {"id": "BJxBz-t7cH", "original": null, "number": 3, "cdate": 1572208908632, "ddate": null, "tcdate": 1572208908632, "tmdate": 1574229783981, "tddate": null, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2246/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper proposes adding noise to the output of scoring function to defend from black-box attacks. This topic is actually very interesting so I enjoyed reading the paper, although it is currently only working for logistic regression and Naive Bayes and there are several unclear parts. I have several concerns about this paper, especially the claim of robust towards arbitrary perturbation. \n\n- My main concern is about the assumption of the attacker. Based on the discussions in Section 3, it seems the authors assume that the query complexity of attacker relies on how many queries the attacker needs to recover a w that is close enough to w*. I don't think this is the correct assumption for the current attacks --- given an example, black box attacks are trying to find some x' for each x without trying to recover  or even estimate w. Therefore I wonder why the query complexity can be linked to the complexity of estimating w and is there any further assumption you need to make? \n\nIf the goal is to protect w, then this has been studied in several privacy/security papers and it's a different topic from adversarial attack. So the connection here is important but somehow unclear in the current draft. \n\n- For the experiments, to justify it is robust to attack I think it's important to try on various black-box attacks, including ZOO (Chen et al., 2017), Natural evolution strategy (Ilyas et al., 2018), Nattack (Li et al, 2019). For decision-based black box settings Boundary attack (Brendel et al., 2018) and OPT-attack (Cheng et al., 2019). \n(Not saying you should try all of them, but I feel more than 1 attack is needed to justify the claim). \n\n- Some unclear points that need further clarification: \n\nI feel assuming there's an optimal w* that correctly classifies data is unrealistic. Is is possible to relax this? \n\nCondition 1: I fail to understand how is this related to q (attacker)? This seems only guaranteeing there's a majority mass of w centered at w*. \n\nCondition 2: What is I ? (I didn't see the definition). \n\n- Some related work: \nIn DNN defense there are some related work on adding random noise. In [1], I think they only require adding a random layer which can be in the final layer of network, corresponding to adding random to the scoring function. In [2], they assume adding randomness to each layer so only adding random to final layer is a special case of that. I know the guarantees here are very different from those papers, but it will be nice to have some discussions. \n\n[1] \"Certified Robustness to Adversarial Examples with Differential Privacy\" Lecuyer et al., (S&P'19)\n[2] \"Towards Robust Neural Networks via Random Self-ensemble\" Liu et al., (ECCV '18)\n\n======\n\nThank you for the response and the additional experiments. I feel the paper has some interesting ideas and could be improved by a more careful writing and slightly adjusting the claim. I will rate the current draft borderline but slightly leaning to reject. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2246/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2246/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sjzhao@stanford.edu", "yangsong@cs.stanford.edu", "ermon@cs.stanford.edu"], "title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "authors": ["Shengjia Zhao", "Yang Song", "Stefano Ermon"], "pdf": "/pdf/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "abstract": "Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.", "keywords": ["Adversarial Defense", "Certified Defense", "Adversarial Examples"], "paperhash": "zhao|towards_certified_defense_for_unrestricted_adversarial_attacks", "original_pdf": "/attachment/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "_bibtex": "@misc{\nzhao2020towards,\ntitle={Towards Certified Defense for Unrestricted Adversarial Attacks},\nauthor={Shengjia Zhao and Yang Song and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lBVgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575717402939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2246/Reviewers"], "noninvitees": [], "tcdate": 1570237725596, "tmdate": 1575717402953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2246/-/Official_Review"}}}, {"id": "BJeYzwt2iB", "original": null, "number": 1, "cdate": 1573848849420, "ddate": null, "tcdate": 1573848849420, "tmdate": 1573848849420, "tddate": null, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2246/-/Official_Comment", "content": {"title": "Thank You for Your Suggestions and Comments ", "comment": "We highly appreciate the reviewers for taking the time to provide helpful reviews and suggestions. We agree that the current writing can be organized better, and empirical results can be strengthened with additional experiments. Unfortunately, the rebuttal period is too short to address all these issues, so we would like to further improve the paper and submit to a future venue. \n\nThat being said, we firmly believe in the value of the framework proposed in our paper. In this response we would like to clarify several concerns about the framework.\n\nQ: The threat model is non-standard\n\nResponse: Our approach is the standard black box attack setup. Attacker need to produce an input x that is mis-classified by the classifier. The difference is that we allow the classifier to answer \u201cI don\u2019t know\u201d. This is necessary to defend against unrestricted attack. For example, the attacker could choose random noise as input; if a classifier outputs a prediction on invalid input\u2014and the prediction is used for high risk decisions\u2014this can be a security threat. \n\nWe show how to preserve privacy with respect to classifier weights w, but this is not the end goal.  Our main contribution is to associate knowledge about w with the attack\u2019s ability to generate an adversarial example x (Theorem 1). Intuitively, if the attacker knows w, he or she can certainly generate an adversarial example; preventing any attacker from knowing w accurately is a way to prevent adversarial attack, and Theorem 1 precisely quantifies this. \n\nQ: Meaning of \u201coracle classifier\u201d\n\nIn our theoretical analysis, an oracle classifier is the optimal classifier that (globally) minimizes classification loss given infinite data. \n\nQ: Defensibility (condition 1) does not mention attacker or threat model\n\nWe believe condition 1 should be unrelated to the attacker. Defensibility (condition 1) is solely a property of the classifier. This is necessary to provide guarantees on *any* attacker instead of a fixed attack. \n\nQ: More experiments are needed. \n\nResponse: We performed additional experiments on NES (Ilyas et al, 2018) and Sign-OPT (Cheng et al, 2019). We observed similar results as Simba (Guo et al, 2019). We will include these results in the future submission. There is certainly a gap between theory (linear models) and deep neural networks; we will invest considerable effort in bridging this gap. We will include empirical analysis of the best theoretical guarantee, and comparison with other defense methods in the next revision. \n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2246/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2246/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sjzhao@stanford.edu", "yangsong@cs.stanford.edu", "ermon@cs.stanford.edu"], "title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "authors": ["Shengjia Zhao", "Yang Song", "Stefano Ermon"], "pdf": "/pdf/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "abstract": "Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.", "keywords": ["Adversarial Defense", "Certified Defense", "Adversarial Examples"], "paperhash": "zhao|towards_certified_defense_for_unrestricted_adversarial_attacks", "original_pdf": "/attachment/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "_bibtex": "@misc{\nzhao2020towards,\ntitle={Towards Certified Defense for Unrestricted Adversarial Attacks},\nauthor={Shengjia Zhao and Yang Song and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lBVgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lBVgHYvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2246/Authors", "ICLR.cc/2020/Conference/Paper2246/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2246/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2246/Reviewers", "ICLR.cc/2020/Conference/Paper2246/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2246/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2246/Authors|ICLR.cc/2020/Conference/Paper2246/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144192, "tmdate": 1576860553506, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2246/Authors", "ICLR.cc/2020/Conference/Paper2246/Reviewers", "ICLR.cc/2020/Conference/Paper2246/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2246/-/Official_Comment"}}}, {"id": "HJemqGSatB", "original": null, "number": 1, "cdate": 1571799691397, "ddate": null, "tcdate": 1571799691397, "tmdate": 1572972363893, "tddate": null, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2246/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new certified defense strategy that considers unrestricted black box attacks.  The paper provides bounds for the minimum number of queries needed for the attacker to attack the classifier successfully and then the authors prove that they can devise a defender to be robust against that attack. Here are a few points to consider:\n1.\tThe paper is a bit difficult to understand (also not well-structured). The specific contributions are not quite clear with respect to the existing literature (which is reviewed in a sparse manner in the paper). Especially, the novelty of the theory and analysis presented here is a bit difficult to assess. \n2.\t The results are not really validating the points they make in the analysis (for example in part 4.2 they talk about upper and lower bounds for number of queries as a function of \u03c4, \u03b1, etc. but they never provide some plots or tables regarding that in the results section).\n4.\tAlso, in Fig 2, it is hard to grasp the performance of the defended vs undefended classifiers with respect to the lower and upper bound that they have computed theoretically in the previous section.\n5.\tThey emphasize on the \u201cdefensibility\u201d and \u201cquery privacy\u201d in the analysis but they do not provide anything in the results section considering them.\n6.\tAs this is primarily a theoretical paper, focusing on simple classifiers is probably okay, but some sort of empirical comparison with other certified defense strategies is necessary. Just claiming that none of the existing methods would work for unrestricted attacks will not work is not sufficient. Some empirical results to show the specific advantages (e.g., at what budget the existing methods start to fail and the proposed method continues to perform well). "}, "signatures": ["ICLR.cc/2020/Conference/Paper2246/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2246/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sjzhao@stanford.edu", "yangsong@cs.stanford.edu", "ermon@cs.stanford.edu"], "title": "Towards Certified Defense for Unrestricted Adversarial Attacks", "authors": ["Shengjia Zhao", "Yang Song", "Stefano Ermon"], "pdf": "/pdf/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "abstract": "Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.", "keywords": ["Adversarial Defense", "Certified Defense", "Adversarial Examples"], "paperhash": "zhao|towards_certified_defense_for_unrestricted_adversarial_attacks", "original_pdf": "/attachment/fdd11f45bd1e780ba8741516809efa2109bf9cc1.pdf", "_bibtex": "@misc{\nzhao2020towards,\ntitle={Towards Certified Defense for Unrestricted Adversarial Attacks},\nauthor={Shengjia Zhao and Yang Song and Stefano Ermon},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lBVgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lBVgHYvr", "replyto": "S1lBVgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2246/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575717402939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2246/Reviewers"], "noninvitees": [], "tcdate": 1570237725596, "tmdate": 1575717402953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2246/-/Official_Review"}}}], "count": 6}