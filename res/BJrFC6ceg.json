{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486491359554, "tcdate": 1478315645334, "number": 548, "id": "BJrFC6ceg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJrFC6ceg", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "content": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396670442, "tcdate": 1486396670442, "number": 1, "id": "Sy8W6MLOl", "invitation": "ICLR.cc/2017/conference/-/paper548/acceptance", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": " The authors acknowledge that the ideas in the paper are incremental, but assert these are not-trivial improvements upon prior work on pixel CNNs. The reviewers tended to agree with this characterization. The paper presents SOTA pixel likelihood results on CIFAR-10. This work is also coupled with a high quality source code contribution, which also appears to have already been well received by the github community. Reviewer 1 made the point that in terms of raw novelty this work is probably a little below the bar for an oral presentation. A public reviewer rated this paper as a strong accept. Given the statistics, quality and originality of the other papers in my AC batch I recommend poster.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396671001, "id": "ICLR.cc/2017/conference/-/paper548/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396671001}}}, {"tddate": null, "tmdate": 1484824726495, "tcdate": 1484824726495, "number": 2, "id": "ByR9xQAIl", "invitation": "ICLR.cc/2017/conference/-/paper548/official/comment", "forum": "BJrFC6ceg", "replyto": "B1edzW6Bg", "signatures": ["ICLR.cc/2017/conference/paper548/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper548/AnonReviewer1"], "content": {"title": "Thanks for your comments", "comment": "Thank you for the response, and for the revised submission detailing comparisons to continuous likelihoods.\n\nAfter re-evaluating the submission, and all the responses, I have increased the score and reduced the confidence for my review.\n\nJust to clarify, I do not want to dispute the value of the contribution itself, which is indeed evidenced by all the things you mention (github stars, citations), and high quality open source code is a great contribution, etc. My review was more about whether this should be one of the papers presented at the conference, especially given the unusually good crop of papers I have personally reviewed this year (it might have been a very lucky sample) which, in many cases, presented more out-of-the-box and original ideas. I do agree I probably have been too harsh in my initial judgment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287526964, "id": "ICLR.cc/2017/conference/-/paper548/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper548/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper548/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287526964}}}, {"tddate": null, "tmdate": 1484824069628, "tcdate": 1482406527516, "number": 3, "id": "B1DF5VFEg", "invitation": "ICLR.cc/2017/conference/-/paper548/official/review", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["ICLR.cc/2017/conference/paper548/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper548/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "Apologies for the late submission of this review, and thank you for the author\u2019s responses to earlier questions.\n\nThis submission proposes an improved implementation of the PixelCNN generative model. Most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections, while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis. The submission demonstrates state-of-the-art likelihood results on CIFAR-10.\n\nMy summary of the main contribution:\nAutoregressive-type models - of which PixelCNN is an example - are a nice class of models as their likelihood can be evaluated in closed form. A main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled:\n\n- In one line of work such as (Theis et al, 2012 MCGSM, Theis et al 2015 Spatial LSTM) the conditional distribution is modelled as a continuous density over real numbers. This approach has limitations: We know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods. Furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities, which may hurt the likelihood.\n- In more recent work by van den Oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the 256 possible values for pixel intensities. This does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.\n\nThe authors propose something in the middle by keeping the discretized nature of the conditional likelihood, but restricting the discrete distribution to ones whose CDF that can be modeled as a linear combination of sigmoids. This approach makes sense to me, and is new in a way, but it doesn\u2019t appear to be very revolutionary or significant to me.\n\nThe second somewhat significant modification is the use of downsampling and multiscale modelling (as opposed to dilated convolutions). The main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model. The authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling. Again, I feel that this modification not particularly revolutionary. Multiscale image analysis with autoregressive generative models has been done for example in (Theis et al, 2012) and several other papers.\n\nOverall I felt that this submission falls short on presenting substantially new ideas, and reads more like documentation for a particular implementation of an existing idea.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512546048, "id": "ICLR.cc/2017/conference/-/paper548/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper548/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper548/AnonReviewer2", "ICLR.cc/2017/conference/paper548/AnonReviewer3", "ICLR.cc/2017/conference/paper548/AnonReviewer1"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512546048}}}, {"tddate": null, "tmdate": 1484774808011, "tcdate": 1484774808011, "number": 10, "id": "B1ei6LaUx", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "new revision incorporating reviewer recommendations", "comment": "We uploaded a new revision incorporating some of the suggestions made by the reviewers. Thank you for your input!\n\nSpecifically, the revision incorporates additional references and an experimental comparison with continuous mixture models as suggested by reviewer 2, and a more detailed description of the likelihood model, including equations, as requested by reviewer 3."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1484703304813, "tcdate": 1484703304813, "number": 9, "id": "ByZ8IBn8e", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "S1H7heprl", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "with continuous mixture likelihood the model gets to 3.11 bits per dim", "comment": "I ran the model without discretization of the mixture distribution on the pixels. Using the same model settings as for the 2.92 bits per sub-pixel reported in the paper, it got to 3.11 bits per dim. (This is a variational lower bound, adding small uniform noise to the discrete pixels). The code can be found here: https://www.dropbox.com/sh/tcx5bg76xngmm8a/AADyvRZ0H98rcGU-7wULjvV0a?dl=1, and I will also include this result in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1483702888083, "tcdate": 1483702888083, "number": 8, "id": "B1edzW6Bg", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "B1DF5VFEg", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "are useful technical contributions not enough for acceptance?", "comment": "Thanks for you review! The ideas in our paper are incremental, but non-trivial, improvements upon existing work. We have deliberately been modest in the presentation of these ideas, rather than overclaiming paradigm shifting insights as already happens all too often in our field. The contributions we make are useful to many people, as evidenced by the SOTA results, the number of github stars, the papers building on our work (e.g. https://arxiv.org/pdf/1611.02731.pdf, https://arxiv.org/pdf/1612.08185v1.pdf), and the large amount of positive personal communication I have received. Taking this into account, I feel the paper contributes more to our field than most ICLR submissions. Could you please reconsider your rating in this light?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1483701561148, "tcdate": 1483701561148, "number": 7, "id": "BJ-SalpBx", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "B1wgPCMVx", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "will try to improve clarity of likelihood explanation", "comment": "Thanks for the review! I will add a bit more detail to the explanation of the mixture likelihood."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1483701277200, "tcdate": 1483701277200, "number": 6, "id": "S1H7heprl", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "Bkc_sOZ4l", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "will add comparison + references for continuous mixture models", "comment": "Thanks for the citation suggestions. I will add these, as well as a comparison of the proposed likelihood model to a continuous mixture likelihood. (I have done experiments using continuous mixtures before, but not yet with the final architecture presented in the paper)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1483667993063, "tcdate": 1483667706587, "number": 1, "id": "HJQbY_3Be", "invitation": "ICLR.cc/2017/conference/-/paper548/public/review", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["~Galin_Georgiev1"], "readers": ["everyone"], "writers": ["~Galin_Georgiev1"], "content": {"title": "Good experimental work, reinstating classic heuristics back into AR CNN.", "rating": "9: Top 15% of accepted papers, strong accept", "review": "It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN. Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets. So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford. In this sense, OpenAI is fulfilling its mission and we are all very grateful! Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway. \n\nOn a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems. Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]? Forgive the pun, but aren't we beating a dead horse here? Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously! Despite the claims put forward by some on the DeepMind team, PixelCNN is not a \"fully-generative\" neural net (as rigorously defined in section 3 of reference [1]), but merely a perturbative net, in the vain of the Boltzmann machine. After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics?\n\n[1] https://arxiv.org/pdf/1508.06585v5.pdf\n[2] https://arxiv.org/pdf/1511.02841v3.pdf", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1483667707124, "id": "ICLR.cc/2017/conference/-/paper548/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com", "yaroslav@openai.com", "ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs", "~Galin_Georgiev1"], "cdate": 1483667707124}}}, {"tddate": null, "tmdate": 1481987822947, "tcdate": 1481987822947, "number": 2, "id": "B1wgPCMVx", "invitation": "ICLR.cc/2017/conference/-/paper548/official/review", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["ICLR.cc/2017/conference/paper548/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper548/AnonReviewer3"], "content": {"title": "Great empirical work, insightful ablation experiments, code is available which is a nice contribution to the community", "rating": "7: Good paper, accept", "review": "# Review\nThis paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10.\nImproving generative models, especially for images, is an active research area and this paper definitely contributes to it.\n\n\n# Pros\nThe authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.\n\nThe authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).\n\nThe authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.\n\n\n# Cons\nIt is not clear to me what the predictive distribution for the green channel (and the blue) looks like. More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel? I would have liked to see the equations for them.\n\n\n# Minor Comments\nIn Fig.2 it is written \"Sequence of 6 layers\" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers. What is the remaining layer?\nIn Fig.2 what does the first \"green square -> blue square\" which isn't in the white rectangle represents?\nIs there any reason why the mixture indicator is shared across all three channels?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512546048, "id": "ICLR.cc/2017/conference/-/paper548/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper548/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper548/AnonReviewer2", "ICLR.cc/2017/conference/paper548/AnonReviewer3", "ICLR.cc/2017/conference/paper548/AnonReviewer1"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512546048}}}, {"tddate": null, "tmdate": 1481898865608, "tcdate": 1481898865608, "number": 1, "id": "Bkc_sOZ4l", "invitation": "ICLR.cc/2017/conference/-/paper548/official/review", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["ICLR.cc/2017/conference/paper548/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper548/AnonReviewer2"], "content": {"title": "Related work", "rating": "7: Good paper, accept", "review": "Summary:\nThis paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.\n\nReview:\nThis is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.\n\nMy main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).\n\nThe main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has \u201cproved to be a problem for earlier models based on continuous distributions\u201d. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).\n\n60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content \u2013 and this can serve as a good example to demonstrate the usefulness of dropout \u2013 why not use \u201c80 million tiny images\u201d (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model\u2019s likelihood is tractable), so this data could even be used in the class-conditional case.\n\nIt would be interesting to know how fast the different models are at test time (i.e., when generating images).", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512546048, "id": "ICLR.cc/2017/conference/-/paper548/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper548/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper548/AnonReviewer2", "ICLR.cc/2017/conference/paper548/AnonReviewer3", "ICLR.cc/2017/conference/paper548/AnonReviewer1"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512546048}}}, {"tddate": null, "tmdate": 1480836629990, "tcdate": 1480793341522, "number": 4, "id": "B1HbaceXg", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "B1IZCB0Ge", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "No, it's the conditional log-likelihood", "comment": "The reported number in section 3.2 of 2.94 bits per dimension is indeed class-conditional, conditioned on the true class, not marginalized. (Note that this is not the number we compare against the original pixelCNN). The text in this section makes a comparison with our unconditional model, with the reasoning being that conditioning on class labels adds at most log(10)/(32*32*3*log(2))=0.001 bits per dimension to the log-likelihood, which is smaller than the reported precision. This means that 2.94 + 0.001 = 2.94 is an upper bound on the bits per pixel (negative log likelihood when marginalizing out the class label) and is thus comparable to the unconditional model.\nHowever, on second thought, this bound might be pessimistic: If there are examples in the test set that the model, mistakenly, assigns a much larger probability to when conditioning on the wrong label than on the right label, the marginal log-likelihood could be significantly better than what is reported. I will check this when I get back from NIPS."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1480794180227, "tcdate": 1480792313255, "number": 2, "id": "B1Z-K9gXg", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "BJyUo7yml", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "double-checked this by computing gradients in randomly initialized model", "comment": "The precise structure of the downsampling/upsampling steps can be found at https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py: We use strided convolution and strided deconvolution, so there is no information leakage. I double-checked this by computing the gradient of the output parameters at each spatial location with respect to all the pixels in the input image: The gradient of the output at location (i,j) is strictly zero for all pixels at locations (i,>=j) and (>i,:)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1480793715442, "tcdate": 1480793715437, "number": 5, "id": "rJsd0cgmg", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "rJjq2-RMx", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "did not try this", "comment": "We used the same ordering as the original PixelCNN and did not explore alternatives. My guess is that it would not matter much, but it's worth trying."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1480793586740, "tcdate": 1480793052031, "number": 3, "id": "By4JncgXg", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "H1dU8-1Ql", "signatures": ["~Tim_Salimans1"], "readers": ["everyone"], "writers": ["~Tim_Salimans1"], "content": {"title": "Discretization at the borders is important for optimal performance", "comment": "1. Early on I experimented with continuous distributions and no discretization, but this didn't work very well. The main benefit of using discretization is to capture the relatively large probability mass of the data distribution at pixel values of 0 and 255: Continuous distributions cannot model this without effectively reserving a mixture component for all possible combinations of edge values.\n2. Yes, ease of evaluation was the main motivation, since the logistic CDF is simply the sigmoid function. The number of mixture components seems to be the dominant factor in determining the distribution expressiveness. Since we saw virtually no effect from adding more than 5 components, we did not further explore methods for making the distribution more flexible. For other types of data, Gaussian scale mixtures might prove very useful though."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1480698695168, "tcdate": 1480698695163, "number": 3, "id": "BJyUo7yml", "invitation": "ICLR.cc/2017/conference/-/paper548/pre-review/question", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["ICLR.cc/2017/conference/paper548/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper548/AnonReviewer2"], "content": {"title": "Model validity after down- and upsampling", "question": "Perhaps this is easy to see, but can you provide a bit more detail to ensure us that after down- and upsampling the logistic distribution can still not access the pixel it is trying to predict?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959220006, "id": "ICLR.cc/2017/conference/-/paper548/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper548/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper548/AnonReviewer3", "ICLR.cc/2017/conference/paper548/AnonReviewer1", "ICLR.cc/2017/conference/paper548/AnonReviewer2"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959220006}}}, {"tddate": null, "tmdate": 1480689232266, "tcdate": 1480689232260, "number": 2, "id": "H1dU8-1Ql", "invitation": "ICLR.cc/2017/conference/-/paper548/pre-review/question", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["ICLR.cc/2017/conference/paper548/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper548/AnonReviewer1"], "content": {"title": "Comparison to non-discretized distributions", "question": "1. Do you have any experiments where the discretization is dropped?\n2. Why the particular choice of logistic distributions? How do the properties compare to the more flexible family of Gaussian scale mixtures which have been used successfully in very similar work e.g. (Theis et al. 2012) http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0039857 Is it mainly/only about ease of evaluation?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959220006, "id": "ICLR.cc/2017/conference/-/paper548/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper548/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper548/AnonReviewer3", "ICLR.cc/2017/conference/paper548/AnonReviewer1", "ICLR.cc/2017/conference/paper548/AnonReviewer2"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959220006}}}, {"tddate": null, "tmdate": 1480642045927, "tcdate": 1480642045922, "number": 1, "id": "B1IZCB0Ge", "invitation": "ICLR.cc/2017/conference/-/paper548/public/comment", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["~Ryan_Dahl1"], "readers": ["everyone"], "writers": ["~Ryan_Dahl1"], "content": {"title": "Conditional likelihood score", "comment": "Was the class conditional log-likelihood score marginalized over the 10 classes? (Section 3.2)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287527092, "id": "ICLR.cc/2017/conference/-/paper548/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJrFC6ceg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper548/reviewers", "ICLR.cc/2017/conference/paper548/areachairs"], "cdate": 1485287527092}}}, {"tddate": null, "tmdate": 1480625299387, "tcdate": 1480625299383, "number": 1, "id": "rJjq2-RMx", "invitation": "ICLR.cc/2017/conference/-/paper548/pre-review/question", "forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "signatures": ["ICLR.cc/2017/conference/paper548/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper548/AnonReviewer3"], "content": {"title": "Traversal Ordering", "question": "Did you try different ordering (ex. starting from the bottom right corner instead and going upward and leftward)? If so was the results similar?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications", "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.", "pdf": "/pdf/a881773bdcae79cd4c483f8f48710fdc20a5c9d2.pdf", "TL;DR": "Adding discretized logistic mixture Likelihood and other modifications to PixelCNN improves performance.", "paperhash": "salimans|pixelcnn_improving_the_pixelcnn_with_discretized_logistic_mixture_likelihood_and_other_modifications", "conflicts": ["openai.com"], "keywords": [], "authors": ["Tim Salimans", "Andrej Karpathy", "Xi Chen", "Diederik P. Kingma"], "authorids": ["tim@openai.com", "karpathy@openai.com", "peter@openai.com", "dpkingma@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959220006, "id": "ICLR.cc/2017/conference/-/paper548/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper548/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper548/AnonReviewer3", "ICLR.cc/2017/conference/paper548/AnonReviewer1", "ICLR.cc/2017/conference/paper548/AnonReviewer2"], "reply": {"forum": "BJrFC6ceg", "replyto": "BJrFC6ceg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper548/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959220006}}}], "count": 20}