{"notes": [{"id": "K8NTvvn5aYv", "original": null, "number": 4, "cdate": 1588630405270, "ddate": null, "tcdate": 1588630405270, "tmdate": 1588630405270, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "lFLO6HkLiph", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment", "content": {"title": "Reply", "comment": "Thanks for getting back to me. I have checked with the authors and it turns out the 1x expts are run for the same epochs as dense baselines and it is the tensorflow google TPU implementation that they used. 1.5x has 50% more epochs than the dense baselines. \n\nI have used GMP and it does get good accuracies in the same number of epochs as normal dense training. 128000 iterations ~ 90-100 epochs for 1024 batch size which is the typical training time for ResNet50 on ImageNet."}, "signatures": ["~Aditya_Kusupati1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Aditya_Kusupati1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192865, "tmdate": 1576860582336, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment"}}}, {"id": "lFLO6HkLiph", "original": null, "number": 11, "cdate": 1588323861865, "ddate": null, "tcdate": 1588323861865, "tmdate": 1588323861865, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "jbk602lLWk2", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Reply", "comment": "I have carefully read the survey paper, especially section 5 for several times. The author only mentions that \"Each model was trained for 128000 iterations with a batch size of 1024 images\". Referring to table 2, there is a sparsity range from 50% to 98%. \n\nFrom my understanding, this means that all the sparse models are trained and pruned with the same configuration. I do not find any clear statement about how the dense baseline is trained and whether the dense baseline is trained with the same training steps. This point is somehow vague. Maybe you can contact the authors of the survey paper for more detailed information. \n\nMaybe we need to first figure out whether the GMP could get good accuracies even for sparse networks in the same number of epochs as normal dense training."}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "jbk602lLWk2", "original": null, "number": 3, "cdate": 1588322045910, "ddate": null, "tcdate": 1588322045910, "tmdate": 1588322045910, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "hWadPLd9bR", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment", "content": {"title": "Reply", "comment": "Thanks for the clarification. I agree with 2nd point. \n\nI am not sure about the first point though. What do you mean by \"authors do not clearly indicate that GMP can get good accuracies even for sparse networks in the same number of epochs as normal dense training.\"? I don't understand that statement. The survey paper on GMP (the survey paper does a much better hyperparam sweep for GMP improving results significantly. The original paper's experiments are limited when compared to the survey paper) runs the dense networks and GMP for the same number of epochs. Please check Section 5 of https://arxiv.org/pdf/1902.09574.pdf. They run both dense and sparse training for the same number of epochs. \n\nIMP and GMP are conceptually similar, but their inference costs change vastly due to the allocation of sparsity. \n\nhttps://github.com/google-research/google-research/tree/master/state_of_sparsity has the models and numbers of all the sparse networks from GMP and they are very comparable to dense training and drops are similar to that of your results. \n\nAny clarification on this would be great. Sorry I missed your session in ICLR. "}, "signatures": ["~Aditya_Kusupati1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Aditya_Kusupati1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192865, "tmdate": 1576860582336, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment"}}}, {"id": "hWadPLd9bR", "original": null, "number": 10, "cdate": 1588143011782, "ddate": null, "tcdate": 1588143011782, "tmdate": 1588143083906, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "9PLgo4CwAxV", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Reply", "comment": "1) I think IMP and GMP are basically the same pruning methods. Meanwhile, referring to the original GMP paper (https://openreview.net/pdf?id=Sy1iIDkPM) and the survey paper, it seems that the authors do not clearly indicate that GMP can get good accuracies even for sparse networks in the same number of epochs as normal dense training. Could you please indicate more clearly about this part. \n\n2)  Considering the additional overhead for a layer with parameter matrix $W$, Sparse Momentum needs to store and compute the momentum for the parameter of each layer at each training step. Our method only adds a vector threshold for each layer.  Meanwhile, I believe that an implicit mask will be used to prevent updating the pruned weights in practical implementation. So the overall computation overhead of our method is very likely to be less than Spare Momentum.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "9PLgo4CwAxV", "original": null, "number": 2, "cdate": 1587423900427, "ddate": null, "tcdate": 1587423900427, "tmdate": 1587423900427, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "HiBkPJsoJcT", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment", "content": {"title": "Minor follow-up", "comment": "I didn't get a notification about the reply, I just saw it now. Thanks for the clarification. A couple of follow-up questions:\n\n1) I agree we need fine-tuning for three state pruning techniques like IMP (Iterative Magnitude Pruning), however, GMP (Gradual Magnitude Pruning) - https://arxiv.org/abs/1902.09574 (a good survey paper) shows that GMP can get good accuracies even for sparse networks in the same number of epochs as normal dense training. Can you please let me know if there is something else here?\n\n2) Yes, Sparse Momentum uses dense gradients (more recent works built on SM also do the same), but it is occasional and periodic which leads to a minimal overhead during training. These methods are not completely sparse-sparse in spirit but are very close to that. I would also like to point you to Discovering Neural Wirings - https://arxiv.org/abs/1906.00586 which uses STE for pruning as well and runs for the same number of epochs as the dense training.\n\nTo be clear none of them are training the masked layers but rather still rely on sorting. However, their underlying pruning ideology is still the same.\n\nLet me know,\nAditya"}, "signatures": ["~Aditya_Kusupati1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Aditya_Kusupati1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192865, "tmdate": 1576860582336, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment"}}}, {"id": "HiBkPJsoJcT", "original": null, "number": 9, "cdate": 1587144833667, "ddate": null, "tcdate": 1587144833667, "tmdate": 1587145031950, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "-EqbxMoa73", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Reply about the comparison problems", "comment": "Hi, Aditya.\n\nThank you for your attention to our work. \n\nThe traditional three-stage pruning algorithms you mentioned usually require many additional fine-tuning epochs compared with normal dense training. One advantage of DST is avoiding the expensive pruning and fine-tuning iterations. DST only requires the same number of training epochs as normal dense training to obtain a sparse model.\n\nTo compare fairly with those traditional pruning algorithms, we may need to train the same number of epochs with DST. However, this kind of comparison is not fair either cause increasing the number of training epochs usually won't lead to better performance. The model accuracy tends to saturate after a certain number of training epochs. Those additional fine-tuning epochs are only required for the three-stage pruning algorithms to regain the model performance loss due to pruning operation.\n\nMeanwhile, Sparse Momentum also utilizes the dense gradients to revive the pruned weights. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "-EqbxMoa73", "original": null, "number": 1, "cdate": 1587113968296, "ddate": null, "tcdate": 1587113968296, "tmdate": 1587124165091, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment", "content": {"title": "Comparison to pruning methods", "comment": "Hi,\n\nGreat work on trainable masked layers. \n\nMy question is about the comparison to DSR and Sparse Momentum methods. Both these techniques are end-to-end sparse training mechanisms, but DST starts off dense and gets dense gradients (always) due to STE and gets to become sparse over time. In this case, won't it be fair to compare against pruning techniques that start out dense and become sparse like global thresholding, iterative magnitude pruning or gradual magnitude pruning?\n\nLet me know if I am missing something."}, "signatures": ["~Aditya_Kusupati1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Aditya_Kusupati1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192865, "tmdate": 1576860582336, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Public_Comment"}}}, {"id": "SJlbGJrtDB", "original": "HyxQ8gn_PH", "number": 1570, "cdate": 1569439497393, "ddate": null, "tcdate": 1569439497393, "tmdate": 1583912026943, "tddate": null, "forum": "SJlbGJrtDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "QKf-ZlqnW", "original": null, "number": 1, "cdate": 1576798726763, "ddate": null, "tcdate": 1576798726763, "tmdate": 1576800909695, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper lies on the borderline. An accept is suggested based on majority reviews and authors' response.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715592, "tmdate": 1576800265541, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Decision"}}}, {"id": "Bkl9Cpy0Yr", "original": null, "number": 1, "cdate": 1571843537975, "ddate": null, "tcdate": 1571843537975, "tmdate": 1574439681137, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "## Update after the rebuttal\nI appreciate the author's clarification in the rebuttal and the additional result on ImageNet, which addressed some of my concerns.\n\n# Summary\nThis paper proposes a trainable mask layer in neural networks for compressing neural networks end-to-end. The main idea is to apply a differentiable mask to individual weights such that the mask itself is also trained through backpropagation. They also propose to add a regularization term that encourages weights are masked out as much as possible. The result on MNIST and CIFAR show that their method can achieve the highest (weight) compression rate and the lowest accuracy reduction compared to baselines. \n\n# Originality\n- The idea of applying trainable mask to weights and regularizing toward masking out is quite interesting and new to my knowledge. \n\n# Quality\n- The performance seems to be good, though it would be more convincing if the paper showed results on larger datasets like ImageNet. \n\n- The analysis is interesting, but I am not fully convinced by the \"strong evidence to the efficiency and effectiveness of our algorithm\". For example, the final layer's remaining ratio is constantly 1 in Figure 3, while it starts from nearly 0 in Figure 4. The paper also argues that the final layer was not that important in Figure 4 because the lower layers have not learned useful features. This seems not only contradictory to the result of Figure 3 but also inconsistent of the accuracy being quickly increasing up to near 90% while the remaining ratio is nearly 0 in Figure 4. \n\n- If the motivation of the sparse training is to reduce memory consumption AND computation, showing some results on the reduction of the computation cost after sparse training would important to complete the story. \n\n# Clarity\n- The description of the main idea is not clear. \n\n- What are \"structure gradient\" and \"performance gradient\"? They are not mathematically defined in the paper.\n\n- I do not understand how the proposed method can \"recover\" from pruned connection, although it seems to be indeed happening in the experiment. The paper claims that the use of long-tailed higher-order estimator H(x) makes it possible to recover. However, H(x) still seems to have flat lines where the derivative is 0. Is H(x) in Equation 3 and Figure 2d are showing \"derivative\" or step function itself? In any cases, I do not see how the gradient flows once a weight is masked out. \n\n# Significance\n- This paper proposes an interesting idea (trainable mask), though I did not fully get how the mask is defined/trained and has a potential to recover after pruning. The analysis of the compression rate throughout training is interesting but does not seem to be fully convincing. It would be stronger if the paper 1) included more results on bigger datasets like ImageNet, 2) described the main idea more clearly, and 3) provided more convincing evidence why the proposed method is effective. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575636282299, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Reviewers"], "noninvitees": [], "tcdate": 1570237735457, "tmdate": 1575636282314, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Review"}}}, {"id": "ryeb52WxiB", "original": null, "number": 2, "cdate": 1573031049510, "ddate": null, "tcdate": 1573031049510, "tmdate": 1573795777465, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "Bkl9Cpy0Yr", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 - The concerns about Quality", "comment": "Thank you for the constructive comments about the quality of our paper. \nOur method achieve state of the art performance compared with other sparse training algorithms due to the fine-grained neuron-wise or filter-wise trainable thresholds together with the proper update of both network parameters and threshold  via back-propagation during the whole Dynamic Sparse Training process. We have published the source code for reproducibility. \n\nTo make the illustration more clear, the models that replace the conventional layers with corresponding trainable masked layers and are trained with Dynamic Sparse Training (DST) are referred to as sparse models. For example, LeNet-300-100 with a trainable masked layer and trained with DST are referred to as sparse LeNet-300-100. The dense models train with normal SGD are referred to as dense models. Meanwhile, sparse accuracy indicates the test accuracy after a certain epoch for the sparse models and dense accuracy indicates that for the dense models.\n\nThe following are the explanation for your question about the Quality. \n1) The result on ImageNet \nThe result are updated in the revision\n\n2) The \"contradiction\" in Figure 3 and Figure 4.\nThank you for pointing out. it is our negligence to improperly present the experiment results.\nAs illustrated in section 4.2, the last layer itself can be regarded as a single layer classi\ufb01er that takes the features extracted by preceding layers as input. The layer remaining ratio of the last layer can be regarded as an indicator of the e\ufb00ectiveness of the features extracted. If bad features are extracted, the \ufb01nal test accuracy will not be high and a relatively large portion of weights in the last layer can be masked. This means that bad features make a large portion of weights in the last layer unimportant. In turn, if a large portion of features extracted is helpful for the classification task, the layer remaining ratio of the last layer as well as the test accuracy should be high. \nReferring to Figure 3(b), the test accuracy of sparse LeNet-300-100 on MNIST reaches over 93% just after the \ufb01rst epoch and reaches over 97% after the fourth epoch. Since MNIST is a simple dataset, the model converges very fast. This means that just after the first training epoch, all the features extracted by the preceding layers are somehow helpful for the classification.\nDue to our additional experiment, If a more \ufb01ne-grained change of layer remaining ratio is present, a similar trend of the remaining ratio of the last layer will be discovered for sparse LeNet-300-100. For example, If the layer remaining ratio after each training step during the first epoch is present, the remaining ratio of the last layer will decrease to a lower value first and then increases up to 1, which is consistent with the trend in Figure 4(a). This phenomenon can be observed with the source code we publish. \n\n3) More illustration about Figure 4.\nWe indeed claim that the parameter importance of the last layer for the sparse VGG-16 model during the first 80 epochs is quite low. However, we also argue that the parameter importance will change dramatically during the training process as the hyperparameters like learning rate change. \nThe fact that the accuracy increases up to near 90% before the learning rate decay does not conflict with the fact that most of the features extracted by preceding layers are useless. As present in Figure 4(b), although the remaining ratio of the last layer keeps around 0.05 before the learning rate decay, the test accuracy of the sparse model after each training epoch is better than the test accuracy of the dense model that use all the features extracted. \nWe have conducted an additional experiment with the code we publish by training the sparse model without learning rate decay. The accuracy just keeps fluctuating around 85% and the remaining ratio of the last layer also keeps around 0.05. Training more epochs will not increase both the sparse and dense accuracy. If there is no decay of the learning rate, only around 5% of the features extracted are indeed helpful for classification.\nHowever, if the learning rate is decayed at 80 epoch, the sparse accuracy and the remaining ratio of the last layer increases immediately at the same time as present in Figure 4(b). More useful features are extracted, higher test accuracy will get and the higher remaining ratio will be for the last layer with the decay of learning rate. \nWe want to demonstrate the parameter importance may change dramatically and our method can handle this kind of situation properly in Figure 4.\n\n\n4) Reduction of the computation\nThank you so much for pointing this out. As you suggest, the quantified result about the reduction of computation and memory should be included. We will add this in the updated version \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "B1xLNtN-sB", "original": null, "number": 3, "cdate": 1573108013528, "ddate": null, "tcdate": 1573108013528, "tmdate": 1573795743311, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "BJgGbhJ-iB", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you so much for the constructive comments. We find these comments really helpful. Following are the explanations about the limitations:\n\n1) The Performance of sparse models.\nOur method indeed gets sparse models with increased performance than dense models. \nAs present in section 4.4, our method can get sparse models with better performance when the sparsity of models is less than 90%. It is only when we want to get modes with high sparsity (>90%) that there will be a noticeable performance loss. \nWe think this is a common case for network pruning that there will be a loss of performance when over 90% of parameters are removed.\n\n2) The lack of result on ImageNet\nThe results are updated in the revision.\n\n3) Summary of Novelty \nIt is our negligence that does not present the novelty clearly.\nThe followings present the novelty of our method:\n\n1. Directly get sparse models in the training process\n\nThe typical pruning process is a three-stage pipeline, i.e., training, pruning and fine-tuning. In our method, no further pruning and fine-tuning are needed. We can directly get sparse models in the training process.\nThere exist some similar works but we get better performance compared with the existing methods as present in Section 4.1. \n\n\n2. Trainable fine-grained pruning thresholds\n\nMost of the previous pruning algorithm adopt a single pruning threshold for each layer or the whole architecture. In our method, since a threshold vector $t\\in R^m$ is used for each layer with parameter $W\\in R^{m\\times n}$, we have neuron-wise pruning thresholds for fully connected and recurrent layer and filter-wise pruning thresholds for convolutional layer.\nMeanwhile, all these fine-grained pruning thresholds can be updated automatically via back-propagation as present in Section 3. We are not aware of any other method that achieves this.\n\n\n3. The ability to properly recover the previously pruned weights. \n\nMost of the current pruning and all the sparse training methods conduct hard pruning with following properties:\n- Pruned weights will be directly set to 0\n- No further update via back-propagation for pruned weights \n\nDirectly setting pruned weight to 0 causes the loss of historical parameter importance, which make it hard to determine:\n- When and which pruned weights should be recovered. \n- What value should we assigned to the recovered weights.\nTherefore,  current sparse training methods that allow the recovery of pruned weights randomly choose a predefined portion of pruned weights to recover and these recover weights are randomly initialized.\n\nOur method has following properties that properly solve these problems:\n- Pruned weights will not be directly set to 0. Instead, the mask $W$ will store the information about which weights are pruned.\n- Pruned weights can still be updated via back-propagation. \n- The corresponding pruning thresholds will also be updated via back-propagation\nTherefore, it is the dynamic change of both pruned weights and the corresponding pruning thresholds that determine when and which pruned weights should be recovered. Meanwhile, the recovered weight has the value that it learns via back-propagation instead of a randomly assigned value. \n\n\n4. Continuous fine-grained pruning and recovering over the whole training process\n\nA typical pruning process is conducted after a certain training epoch as following: \n- Determine current importance of weight\n- Prune certain percentage of weight. \n\nUsually, a training epoch will have tens of thousands of training steps, which is the feed-forward and back-propagation pass for a single mini-batch. \nIn our method, since both the network weights and the pruning thresholds will be updated vis back-propagation at each training step. Our method can have continuous fine-grained pruning and recovering at each training step. We are not aware of any other method that can achieves this. \n\n\n5. Automatic and dynamic layer-wise pruning rates adjustment over the network\n\nThere are two critical problems in network pruning:\n- How many weights should be pruned in each pruning step\n- What is the proper pruning rates for each layer over the network\n\nUsually, the pruning is conduct by some predefined pruning schedule like pruning 5% at each step with totally 10 pruning steps. Meanwhile, it is quite hard to properly determine the pruning rates for each layer. Current methods either use a single global pruning threshold for the whole model or layer-by-layer greedy pruning. We illustrate their limitation on Page 1. \n\nIn our method, with the property present above, the portion of weights to be pruned at each step and the proper pruning rates for each layer are automatically determined by the dynamic update of parameter $W$ and threshold $t$. Meanwhile, as present in section 4.2 and 4.3, the swift adjustment of pruning rates and consistent sparse patterns prove the effectiveness of our method in proper adjustment of layer-wise pruning rates.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "rkxycJcqsS", "original": null, "number": 6, "cdate": 1573719943262, "ddate": null, "tcdate": 1573719943262, "tmdate": 1573719943262, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Summary of Revision", "comment": "We appreciate all the detailed reviews and suggestions . Following reviewers' suggestions, we have updated the manuscript and uploaded a revision.  Here we give a summary of the major changes. \n\n In response to Reviewer 1: \n\n- We present our motivations and contributions more clearly\n- We add more experimental results on CIFAR-10 and add the missing result on ImageNet in Section 4.\n- We add more analysis about the experimental results on Section 5.1 and 5.2\n\n\nIn response to Reviewer 2:\n\n- Our motivations and contributions are listed more clearly in the introduction part.\n- More experimental results on CIFAR-10 and the result on ImageNet are presented in Section 4.1\n- We revise the Section 3 to present the main idea more clearly. More details about the feed-forward and back-propagation process are included in Appendix A.2 and A.3. to address the concerns about the  \"structure gradient\" and \"performance gradient\".\n- We present more details to in Section 5.1 and 5.2 the address the ambiguity of previous edition and  provide more evidences to the effectiveness of our method\n\n  \nIn response to Reviewer 4:\n\n- In Section 4.1 and 4.2, we present that  our method is able to get sparse models with increased performance than dense models. \n- The results on ImageNet-2012 is present in Section 4.1\n- Our motivations and the novelty of our method are summarized and highlighted in the introduction part (Section 1)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "ByxiKSw-or", "original": null, "number": 4, "cdate": 1573119362894, "ddate": null, "tcdate": 1573119362894, "tmdate": 1573460810853, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "Bkl9Cpy0Yr", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 - Summary of contributions", "comment": "Thank you so much for your time and constructive comments.\nIt is our negligence that does not clearly present our motivations and contributions of our work in the early manuscript. Here we present the motivations and contributions of our paper. \n\nThere are several problems about network pruning that previous methods cannot properly settle:\n\n1. Expensive pruning and fine-tuning iterations and non-trivial hyperparameters setting\nThe pruning and fine-tuning iterations are expensive and need many additional hyperparameters like:\n- How many pruning steps should be adopted\n- How many epochs for the fine-tuning stage after each pruning step \n- Use the same pruning rate or dynamic pruning rate at each step\n\nOur Contribution:\nWe propose an end-to-end sparse training process that can get sparse models directly during training without the expensive and non-trivial pruning and fine-tuning iterations. \nThere are some similar works but we get better performance than these methods as present in Section 4.1\n\n\n2. Coarse-grained pruning thresholds\n\nAlmost all the previous pruning algorithms adopt a single pruning threshold for each layer or the whole architecture. \n\nOur Contribution:\nIn our method, since a threshold vector $t\\in R^m$ is used for each layer with parameter $W\\in R^{m\\times n}$, we have neuron-wise pruning thresholds for fully connected and recurrent layer and filter-wise pruning thresholds for convolutional layer. Usually, there are hundreds of neurons or dozens of filters in a single layer. Thesefore, our method has much more fine-grained pruning thresholds than existing methods.\n\n\n3. Cannot recover the pruned weights or improper recovery of pruned weights\n\nMost of the current pruning and all the sparse training methods conduct hard pruning with following properties:\n- Pruned weights will be directly set to 0\n- No further update via back-propagation for pruned weights \n\nThe importance of weight is not fixed and will change dynamically during the pruning and training process. Previously unimportant weights may tend to be important. So the ability to recover pruned weight is of high significance. \n\nThose sparse learning methods present in Section 4.1 support the recover of pruned weights. However, they all conduct hard  pruning. Directly setting pruned weight to 0 causes the loss of historical parameter importance, which make it hard to determine:\n- When and which pruned weights should be recovered. \n- What value should be assigned to the recovered weights.\nTherefore,  these methods that allow the recovery of pruned weights randomly choose a predefined portion of pruned weights to recover and these recover weights are randomly initialized.\n\nOur contribution:\n- Pruned (masked) weights will not be directly set to 0. Instead, the mask $M$ will store the information about which weights are pruned.\n- Pruned (masked) weights can still be updated via back-propagation. \n- The corresponding pruning thresholds will also be updated via back-propagation\nTherefore, it is the dynamic change of both pruned weights and the corresponding pruning thresholds that determine when and which pruned weights should be recovered. Meanwhile, the recovered weight has the value that it learns via back-propagation instead of a randomly assigned value. \n\n\n4. How to properly determine the layer-wise pruning rates for complex modern architectures at each pruning step\n\nMost of the existing pruning methods use predefined pruning schedule like like pruning 5% at each step with totally 10 pruning steps for all the network architecture.\nConsidering the complexity of modern network architectures, there are dozens of layers that may have various degree of importance and redundancy.  The fixed predefined pruning schedule may not be the optimal choice.\n\nOur contribution:\nThe layer-wise pruning rates will be automatically determined by the dynamic change of network parameter $W$ and threshold vector $t$. Meanwhile, the swift adjustment of pruning rates regarding the change of parameter importance present in Section 4.2 and the consistent sparse patterns present in Section 4.3 prove the effectiveness of our method in proper adjustment of layer-wise pruning rates\n\n\n5. Coarse-grained pruning schedule\nA typical pruning step is conducted after a certain training epoch. Usually, a training epoch will have tens of thousands of training steps, which is the feed-forward and back-propagation pass of a single mini-batch. \n\nOur contribution:\nIn our method, since both the network weights and the pruning thresholds will be updated vis back-propagation at each training step. Our method can have continuous fine-grained pruning and recovering at each training step. \nThat is our method support step-wise pruning instead of epoch-wise pruning.\n\n\nThank you again for the detailed review and look forward to your feedback"}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "SJg4p8keoS", "original": null, "number": 1, "cdate": 1573021371599, "ddate": null, "tcdate": 1573021371599, "tmdate": 1573258491746, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "Bkl9Cpy0Yr", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Response to AnonReviewer2  - The concerns about Clarity", "comment": "Thank you so much for the detailed reviews and valuable remarks.  I am sorry for the unclarity caused by the lack of information and  improper usage of terms. Here I will use the trainable masked fully connected layer as an example to explain your concerns about Clarity. \nConsider a trainable masked fully connected layer with parameter $W\\in R^{m\\times n}$ and trainable threshold vector $t\\in R^m$. This means that this layer get $n$ input neurons and $m$ output neurons. A neuron-wise threshold $t_i$ is defined for the $i$th output neuron. \n\n1) How the mask $M\\in R^{m\\times n} $ is generated and used in the feed forward process\n\n$M_{ij} = S(|W_{ij}|-t_i)$ for $1\\leq i \\leq m$, $1\\leq j \\leq n$, where $S(x)$ is the unit step function.\n\nFor each connection connects to output neuron $i$, the magnitude of the corresponding weight $W_{ij}$  will be compared with the neuron-wise threshold $t_i$. Instead of directly setting $W_{ij}$ to 0 like traditional pruning algorithms, the value of $W_{ij}$ is preserved in our method. The information about whether to prune this connection is stored in $M_{ij}$, where 0 means pruned (masked) and 1 means unpruned (unmasked). We denote $P = W\\odot M$.  Instead of the original parameter $W$,  $P$ will be used in the matrix-vector multiplication.\n\nMeanwhile $Q\\in R^{m\\times n}$ is just a intermediate variable, where $Q_{ij} = |W_{ij}|-t_i$ for $1\\leq i \\leq m$, $1\\leq j \\leq n$\n\n2) What are \"structure gradient\" and \"performance gradient\" mathematically\n\nRefer to Figure 1, in the back-propagation process,  $P$ will receive a gradient and we denote it as $dP$. \nLet's consider the gradients that flow from right to left.\n\nThe performance gradient is $dP \\odot M$\n\nThe gradient received by $M$ is $dP\\odot W$\n\nThe gradient received by $Q$ is $dP\\odot W\\odot H(Q)$, where $H(x)$ is the long-tail derivative estimation for $S(x)$ and $H(Q)$ is the result of $H(x)$ applied to $Q$ elementwisely. \n\nThe structure gradient is $dP\\odot W\\odot H(Q)\\odot sgn(W)$, where $sgn(W)$ is the result of sign function applied to $W$ elementwisely. \n\nThe gradient received by the vector threshold $t$ is $dt\\in R^m$. We denote $dT = -dP\\odot W\\odot H(Q)$, then $dT\\in R^{m\\times n}$. And we will have $dt_i = \\sum_{j=1}^nT_{ij}$ for $1\\leq i \\leq m$.\n\n3) How the gradient flow to pruned (masked) weights\n\nThe gradient received by the parameter $W$ is $dW = dP\\odot M + dP\\odot W\\odot H(Q)\\odot sgn(W)$ \n\nSince we add $\\ell_2$ regularization in the training process, all the elements in $W$ are distributed within $[-1, 1]$. Meanwhile, almost all the elements in the vector threshold are distributed within $[0,  1]$. The exceptions are the situation as shown in Figure 3(a) and Figure 4(a) where the last layer get no weight pruned (masked). Regarding the process of getting $Q$, all the elements in $Q$ are within $[-1, 1]$. Therefore $H(Q)$ is a dense matrix. Then $W$, $H(Q)$ and $sgn(W)$ are all dense matrices and the pruned (masked) weights can receive the structure gradient $dP\\odot W\\odot H(Q)\\odot sgn(W)$ \n\n4) How the pruned (masked) connection get recovered\n\nThe masked weights, the unmasked weights and the vector threshold can all receive gradients and be updated constantly during the training process. A connection with corresponding weight $W_{ij}$ and threshold $t_i$ may be pruned (masked) if $|W_{ij}| < t_i$ at certain time point in the training process. Meanwhile, it can be easily recovered (unmasked) if $|W_{ij}| > t_i$ during the later training process. \n\n5) Question regarding Equation 3 and Figure 2(d)\n\nThe Equation 3 and Figure 2(d) are both present the long-tail derivative estimation. The Figure 2(a) present the unit step function. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "rylmPotfjB", "original": null, "number": 5, "cdate": 1573194587274, "ddate": null, "tcdate": 1573194587274, "tmdate": 1573202355625, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "BkxVwrQM5S", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you so much for your time and constructive comments.\n\nIt is our negligence that does not present our motivation and contributions of our work clearly in the early manuscript. We are revising it to clearly present our motivations, methods and contributions. \n\nWe really appreciate your positive assessment of our experimental results. We are running experiments on the more complex dataset (ImageNet 2012) to make our results more convinced. \n\nThank you so much for pointing out the language problems in our manuscript. We are polishing the writing continuously and will update the revised manuscript soon. \n\nFollowing are the responses to the concerns:\n\n1. The sparsity of activations.\nUsually, only the sparsity of weights is considered for the evaluation of network pruning methods. Your suggestion that we should evaluate the sparsity of activations provides a new point of view. We are conducting related experiments and will add this in the revised manuscript. \n\n2. The choice of sparse regularizer.\nThe sparse regularizer is used to penalize low threshold values to increase the degree of sparsity. So the basic requirement is that the value of the regularizer function $f(x)$ should decrease as $x$ increases.\n\nWe actually tested several options like $\\exp(-x)$,  $\\frac{1}{x}$, and $\\log({\\frac{1}{x}})$. It seems that other choices except $\\exp(-x)$ penalize too much. Therefore the training loss is dominated by the sparse regularizer term $L_s$, which tends to mask out all the weights easily. \nDue to our experiments, $\\exp(-x)$ is the best choice among all these options that support wider range choice of $\\alpha$ and get higher degree of sparsity. So $\\exp(-x)$ is adpoted for the sparse regularizer.  We are still searching for the better sparse regularizer.\n\n\n3. Sparse regularizer dominated by the smallest thresholds\nYes, as you point out, the sparse regularizer will be dominated by some small thresholds. Although we discuss the layer-wise sparsity in the paper, the thresholds are actually neuron-wise or filter-wise. Considering a masked fully connected layer with parameter $W\\in R^{m\\times n}$ and threshold vector $t\\in R^m$, this layer will have $m$ output neurons. For each output neuron $i$, our method assigns a neuron-wise threshold $t_i$. \n\nThe elements in $t$ are all initialized to $0$, which means that we assume the neurons and weights in this layer have the same importance before the training. And the same penalties for small value are added for all these thresholds. \n\nAt the end of the training process, if some thresholds still have small values, it only indicates that these neurons are more important than other neurons so that the weights corresponding to these neurons should have a small degree of sparsity. \nUsually, there are hundreds of neurons in each layer. So the layer-wise sparsity will still be high even if there are few small neuron-wise thresholds. \n\n4. The analysis in 4.3\nThank you for your positive feedback about the analysis in section 4.3. \nAs we present in section 4.3, our method can generate consistent sparse pattern that indicates the degree of redundancy for each layer.  Besides, our method can distinguish neuron-wise or filter-wise importance with fine-grained neuron-wise and layer-wise thresholds as we present above. Currently, we are not aware of any other method that can have similar effects. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlbGJrtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1570/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1570/Authors|ICLR.cc/2020/Conference/Paper1570/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154060, "tmdate": 1576860549046, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Authors", "ICLR.cc/2020/Conference/Paper1570/Reviewers", "ICLR.cc/2020/Conference/Paper1570/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Comment"}}}, {"id": "BJgGbhJ-iB", "original": null, "number": 3, "cdate": 1573088250153, "ddate": null, "tcdate": 1573088250153, "tmdate": 1573088250153, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "This paper presents a novel network pruning algorithm  -- Dynamic Sparse Training. It aims at jointly finding the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds.  The experiments on MNIST, and cifar-10 show that proposed model can find sparse neural network models, but unfortunately with little performance loss.\nThe key limitation of the proposed model come from the experiments.\n(1) Nowadays, the nature and important question is that, one can not tolerate the degraded performance, even with sparse neural network. Thus, it is important to show that the proposed model can find sparse neural network models, and with increased performance. \n(2) Another weakness is that proposed model has to be tested on large scale dataset, e.g. ImageNet-2012.current two datasets are too small to support the conclusive results of this proposed model. \n(3) As for the model itself, I donot find very significant novelty. For example, Sec. 3.3 (TRAINABLE MASKED LAYERS) in general is quite following previous works\u2019 designing principle. Thus, the novelty should be summarized, and highlighted in the paper.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575636282299, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Reviewers"], "noninvitees": [], "tcdate": 1570237735457, "tmdate": 1575636282314, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Review"}}}, {"id": "BkxVwrQM5S", "original": null, "number": 2, "cdate": 1572119899928, "ddate": null, "tcdate": 1572119899928, "tmdate": 1572972451239, "tddate": null, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "invitation": "ICLR.cc/2020/Conference/Paper1570/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an algorithm for training networks with sparse parameter tensors. This involves achieving sparsity by application of a binary mask, where the mask is determined by current parameter values and a learned threshold. It also involves the addition of a specific regularizer which encourages the thresholds used for the mask to be large. Gradients with respect to both masked-out parameters, and with respect to mask thresholds, are computed using a \"long tailed\" variant of the straight-through-estimator.\n\nThe algorithm proposed in this paper seems sensible, but rather ad hoc. It is not motivated by theory or careful experiments. As such, the value of the paper will be determined largely by the strength of the experimental results. I believe the experimental results to be strong, though I am not familiar enough with this subfield to be confident there are not missing baselines.\n\nThere are many minor English language problems (e.g. with articles, prepositions, plural vs. singular forms, and verb tense), though these don't significantly interfere with understanding.\n\nRounding up to weak accept, though my confidence is low because I am basing this positive assessment on experimental results for tasks on which I am not well calibrated.\n\nmore detailed comments:\n\n\"using the same training epochs\" -> \"using the same number of training epochs\"\n\"achieves prior art performance\" -> \"achieves state of the art performance\"\n\"the inference of deep neural network\" -> \"inference in deep neural networks\"\n\nThis paper considered only sparsity of weights -- it might have been nice to also discuss/run experiments exploring sparsity of activations.\n\neq. 4 -- Can you say more about why this particular form is the right one for the regularizer? It seems rather arbitrary. (it will tend to be dominated by the smallest thresholds, and so would seem to encourage a minimum degree of sparsity in every layer)\n\nI appreciate the analysis in section 4.3."}, "signatures": ["ICLR.cc/2020/Conference/Paper1570/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1570/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "authors": ["Junjie LIU", "Zhe XU", "Runbin SHI", "Ray C. C. Cheung", "Hayden K.H. So"], "authorids": ["jjliu@eee.hku.hk", "zhexu22-c@my.cityu.edu.hk", "rbshi@eee.hku.hk", "r.cheung@cityu.edu.hk", "hso@eee.hku.hk"], "keywords": ["neural network pruning", "sparse learning", "network compression", "architecture search"], "TL;DR": "We present a novel network pruning method that can find the optimal sparse structure during the training process with trainable pruning threshold", "abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly \ufb01nd the optimal network parameters and sparse network structure in a uni\ufb01ed optimization process with trainable pruning thresholds. These thresholds can have \ufb01ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef\ufb01ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "pdf": "/pdf/4d00f8b99a24facb19dc0b3931f82dd5eeab579b.pdf", "code": "https://github.com/junjieliu2910/DynamicSaprseTraining", "paperhash": "liu|dynamic_sparse_training_find_efficient_sparse_network_from_scratch_with_trainable_masked_layers", "_bibtex": "@inproceedings{\nLIU2020Dynamic,\ntitle={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},\nauthor={Junjie LIU and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlbGJrtDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f879ed89c9fece77a45698112125b95b783dc253.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlbGJrtDB", "replyto": "SJlbGJrtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1570/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575636282299, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1570/Reviewers"], "noninvitees": [], "tcdate": 1570237735457, "tmdate": 1575636282314, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1570/-/Official_Review"}}}], "count": 18}