{"notes": [{"id": "tC6iW2UUbJf", "original": "C_1T-czxjK", "number": 1443, "cdate": 1601308160736, "ddate": null, "tcdate": 1601308160736, "tmdate": 1612074524249, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "qmzg4EBEIVn", "original": null, "number": 1, "cdate": 1610040466780, "ddate": null, "tcdate": 1610040466780, "tmdate": 1610474070393, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper aims at understanding why self-supervised/contrastive learning methods  transfer well when used as pretraining for fine-tuning downstream tasks  (compared to e.g., supervised pretraining based on the cross-entropy loss). Three reviewers recommend acceptance, whereas one reviewer recommends borderline rejection, arguing the take home message of the paper is not very clear. While this is a legitimate concern, the AC agrees with the majority that the paper does shed light on the differences between supervised and self-supervised pretraining (based on interesting empirical findings) and recommends acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040466765, "tmdate": 1610474070377, "id": "ICLR.cc/2021/Conference/Paper1443/-/Decision"}}}, {"id": "GwONnCL97dW", "original": null, "number": 6, "cdate": 1606208989749, "ddate": null, "tcdate": 1606208989749, "tmdate": 1606208989749, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "JIhkbpmMIc", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment", "content": {"title": "Thanks!", "comment": "Dear Authors,\n\nThanks for your response to my concerns.\n\nI think all of them have been addressed and I will keep my original score.\n\nThanks!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tC6iW2UUbJf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1443/Authors|ICLR.cc/2021/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment"}}}, {"id": "JIhkbpmMIc", "original": null, "number": 4, "cdate": 1605862091673, "ddate": null, "tcdate": 1605862091673, "tmdate": 1605862752224, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "SWtxG_ZVTkP", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment", "content": {"title": "Responses to R3", "comment": "We thank the reviewer for the valuable feedback. \n\n \nQ1: It would have been interesting to see whether this presented result comparing MoCo with supervised pretraining hold with other self-supervised methods such as SimCLR. Do authors have any intuition on that? \n\nIn Figure 2, we examine three typical contrastive models (InstDisc, MoCo, SimCLR) on their ability to reconstruct the input image. The conclusion of preserving holistic information holds with these three models.  \n\nFor the rest of the transfer studies, though we are optimistic about generality, we are sorry that we cannot provide reliable experiments at this stage. For SimCLR, the original paper does not report the transfer protocol as well as the results for detection and segmentation. To the best of our efforts, we failed to obtain a competitive number using SimCLR compared with supervised pretrained models. Nevertheless, we agree with the reviewer on this promising direction to examine other self-supervised methods.    \n\n\nQ2: Some Table references might be wrong. Section 2 refers to Table 7 and 8 which are in the supplemental (probably referring to Table 1 and Table 2). \n\nThanks for pointing this out! These errors are fixed in the updated paper. \n\n \nQ3: The face landmark task is a bit outside the main story of the paper. \n\nAside from detection transfer, face landmark prediction is another task where task misalignments occur between the pretraining and the downstream task. Traditional cross-entropy supervised pretraining may lose spatial information when pretrained to recognize faces, causing sub-optimal results for face landmark transfer. We introduce this in order to show that our proposed exemplar-based pretraining may mitigate the negative effect of supervised pretraining.   "}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs", "ICLR.cc/2021/Conference/Paper1443/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tC6iW2UUbJf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1443/Authors|ICLR.cc/2021/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment"}}}, {"id": "LilYpscbrgK", "original": null, "number": 5, "cdate": 1605862131134, "ddate": null, "tcdate": 1605862131134, "tmdate": 1605862733996, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "bgrgBaXPoLB", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment", "content": {"title": "Responese to R4", "comment": "We thank the reviewer for the valuable feedback. \n\n\nQ1: The \"take home\" message of the paper is not very clear. This paper would be much stronger if it was reorganized to focus on the key take-away in this paper. \n\nThe single focus of the paper is to answer the question \u2013 why contrastive learning transfers so well for downstream tasks. The empirical evidence from data augmentations, dataset semantics as well as task misalignments all shed light on this central problem. With insights and understandings from these studies, a straightforward implication for supervised pretraining is to use negatives without positives. The consistent improvements for the new supervised pretraining approach validate our prior findings about task misalignments for transfer learning. While the paper presents a spectrum of content, we believe all messages conveyed in the paper are tightly targeted to a central problem. \n\n \nQ2: What is the definition of low, mid, and high-level features? \n\nIn this paper, we mainly distinguish high-level features from the others. Features are high-level if they pertain to the semantics of a particular dataset. In Table 2, since we manipulate the semantics of the datasets (e.g., faces, objects, scenes), we may control the high-level features that could be learned.  Features learned from solely faces cannot represent general high-level objects. In the paper, we do not further distinguish between low and mid-level features. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1443/Reviewers", "ICLR.cc/2021/Conference/Paper1443/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tC6iW2UUbJf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1443/Authors|ICLR.cc/2021/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment"}}}, {"id": "3Gb-elrp35y", "original": null, "number": 2, "cdate": 1605861396907, "ddate": null, "tcdate": 1605861396907, "tmdate": 1605862705054, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "WpyoPr4hJsa", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment", "content": {"title": "Responses to R1", "comment": "We thank the reviewer for the valuable feedback. \n\nQ1: Do the authors have any insight or conclusion from their experiments that can be drawn w.r.t the semantic inclusion relationship of the datasets? \n\nSupervised transfers rely heavily on the semantic inclusion/alignment of datasets, as they can suffer significantly from semantic misalignment of datasets. In contrast, unsupervised transfers are mostly agnostic to the semantics of the pretraining data. For example, on VOC object detection, supervised transfer drops sharply from 46.2 (ImageNet pretrain) to 39.1 (places pretrain), while unsupervised pretrain drops just marginally from 48.5 (ImageNet pretrain) to 46.7 (places pretrain). Similar observations can be found in COCO detection and Cityscapes segmentation. \n&nbsp;\n \nQ2: It would have been good if the presentation of the results is better organized and summarized. The current presentation is busy. \n\nIn the revision, we revised the presentation of Table 1 and Table 2 to improve readability. We would be glad to incorporate any additional changes suggested by the reviewer. \n&nbsp;\n \nQ3: It seems unnecessary to make a link with exemplar SVM. \n\nWe made this link because we were truly inspired by exemplar SVM paper to come up with the new pretraining approach. The idea to become free from explicit constraints on intra-class examples was pioneered by exemplar SVM. However, following the reviewer\u2019s suggestion, we have removed the reference to exemplar SVM in the revision of the introduction section. \n&nbsp;\n \nQ4: In sec 2.2, it seems that Table 8 is a typo, which is in the supplementary material. \n\nThanks for catching this! The paper has been updated to fix this problem.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1443/Reviewers", "ICLR.cc/2021/Conference/Paper1443/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tC6iW2UUbJf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1443/Authors|ICLR.cc/2021/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment"}}}, {"id": "j4cWKt3eQvP", "original": null, "number": 3, "cdate": 1605862037908, "ddate": null, "tcdate": 1605862037908, "tmdate": 1605862037908, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "09VovtiUIOA", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment", "content": {"title": "Responses to R2", "comment": "We thank AnonReviewer2 for the positive assessment and detailed comments on our paper. We have fixed the typo of Eq (2) in the revised manuscript.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1443/Reviewers", "ICLR.cc/2021/Conference/Paper1443/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tC6iW2UUbJf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1443/Authors|ICLR.cc/2021/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923859621, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Comment"}}}, {"id": "SWtxG_ZVTkP", "original": null, "number": 1, "cdate": 1603822571949, "ddate": null, "tcdate": 1603822571949, "tmdate": 1605024442541, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review", "content": {"title": "I think the paper addresses a relevant problem with strong experimental results. In my opinion, a very good paper. ", "review": "Summary: The paper addresses the important topic of understanding why self-supervised learning methods show very good performance when used as pretraining for fine-tuning tasks. Authors analyse in detail the difference in performance between self-supervised and supervised pretraining and propose a new method to train model which improves over standard supervised models when used as pretraining.\n\nStrengths:\n\n- I think the paper addresses a relevant problem. Understanding difference between self-supervised and supervised pretraining is relevant to advance in this field. I particularly like the amount of evidence the paper provides to back all the claims, and the originality of some of the experiments such as Figure 2.\n\n- The paper is well written and motivates the issue very well. I think it's particularly interesting to question traditional training techniques such as cross-entropy training, when the models are planed to be used for a different goal.\n\n- The insights of the transferability experiments are useful for the community as point the strengths and weaknesses of each methods. I think it's good that authors analyse many different tasks such as classification, detection and segmentation with variate datasets. \n\n- The proposed learning loss utilising the labels to produce the negatives is simple and seems to produce promising results according to Table 4 and Table 5. \n\n- The extensive supplementary materials are also useful for additional information.\n\nWeaknesses:\n\n- It would have been interesting to see whether this presented results comparing MoCo with supervised pretraining hold with other self-supervised methods such as SimCLR. Do authors have any intuition on that? Are some of those effects from the particularities of the MoCo training or can we generalise to all self-supervised methods?\n\n- Some Table references might be wrong. Section 2 refers to Table 7 and 8 which are in the supplemental (probably referring to Table 1 and Table 2).\n\n- The face landmark task is a bit outside the main story of the paper. It is introduced very late in the paper and it is not clear where the proposed loss helps. I believe authors should clarify this points.\n\n\nConclusion: I believe the paper is strong enough for publication. I think it would be good for the reader if authors clarify a bit more the face landmark section and discuss a bit how would this compare to other self-supervison methods, but overall the paper is very good. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118483, "tmdate": 1606915791161, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1443/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review"}}}, {"id": "bgrgBaXPoLB", "original": null, "number": 2, "cdate": 1603850482937, "ddate": null, "tcdate": 1603850482937, "tmdate": 1605024442459, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review", "content": {"title": "Official Review #4", "review": "This paper presents a detailed analysis of the task transfer abilities of an self-supervised representation, instance discrimination. The paper studies, in extensive detail, how these induced representations work for different tasks. The paper also proposes a new representation learning framework.\n\nOverall, there are a lot of details in this paper, and it must have been a tremendous of work to put together. However, the \"take home\" message of the paper is not very clear. There are a lot of empirical findings throughout the paper, but it is left to the reader to decide what to do with the empirical findings. Which ones are important? What lessons should the field take away from these findings? This paper would be much stronger if it was re-organized to focus on the key take-away in this paper.\n\nA central discussion point in the paper is about low, mid, or high-level features. These features were never fully defined. What is the difference between a mid or high-level feature? ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118483, "tmdate": 1606915791161, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1443/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review"}}}, {"id": "09VovtiUIOA", "original": null, "number": 3, "cdate": 1603857166380, "ddate": null, "tcdate": 1603857166380, "tmdate": 1605024442383, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review", "content": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "review": "\n**Summary:**\n\nThis work aims to explore why unsupervised contrastive pretraining works as well (if not better) than the tried-and-true Supervised ImageNet classification pretraining.  They explore a number of different transfer tasks to give some intuition:\n1.  Interesting findings:\n - Augmentation doesn\u2019t make much difference for supervised transfer (from imagenet) and is essential for unsupervised transfer, with the effect monotonically increasing as more augmentations are added.\n - Dataset semantics are less important for unsupervised pretraining:  They transfer from a variety of tasks, faces, objects, etc using supervised and unsupervised pretraining.\n - Imagenet pretraining leads to greater localization errors (using analysis of Hoiem 2012), and  more generally a loss of spatial information (tested via image reconstruction)\n2. Propose a new supervised pretraining method to combine the idea of unsupervised contrastive training with supervised exemplar training. \n - Extend MoCo to use supervised labels so the loss doesn\u2019t contrast examples from the same class.\n - This improves supervised transfer performance from ImageNet to the other tasks.  \n3. Finally, they look at the impact of this new pretraining on two other tasks\n - Few shot learning: The Supervised Exemplar model outperforms the unsupervised methods, and the original cross-entropy Imagenet supervised model.\n - Facial landmark prediction: Both the unsupervised and exemplar-supervised pretraining perform similarly and outperform either training from scratch or imagenet-supervised pretraining.  This again supports the observation that imagenet classification pretraining dilutes the spatial acuity of the model.\n\n**Positives:**\n- They introduce a supervised pretraining method that can transfer better than the unsupervised method and the original supervised imagenet method.\n- Overall, this work is clearly written.\n- Ultimately, I do believe I have a better understanding of the differences between the supervised and unsupervised pretrained models.\n\n**Negatives:**\n- The major insight into the differences is limited:  mainly that we pay a price when the low-level information is lost by the supervised pretrained model. \n\n**Recommendation:**\nThese analysis papers are always tricky to rate -- often quite a bit of work goes into what seems like a small insight (maybe even obvious in retrospect).  However, I do think that this work is worthwhile for the community because 1) it shines light on a somewhat mysterious exciting new technique and 2) already shows how the findings are useful by using it to improve supervised pretraining, and a new vocabulary for evaluating pretraining techniques.\n\n**Minor comments:**\nEq (2), should $v_i$ be $q_i$?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118483, "tmdate": 1606915791161, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1443/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review"}}}, {"id": "WpyoPr4hJsa", "original": null, "number": 4, "cdate": 1603897470419, "ddate": null, "tcdate": 1603897470419, "tmdate": 1605024442301, "tddate": null, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "invitation": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review", "content": {"title": "An insightful paper", "review": "Summary:\n\nThe paper draws an interesting research question: why instance discrimination (ID) pretraining good for transfer learning? \nThe authors dissect ID for transfer learning by extensively comparing with supervised pretraining in several task combinations, so that they attempted to empirically answer what knowledge is learned by ID and transferred, what is differences with supervised pretraining, and when it is effective according to task relationships and dataset sizes.\nBased on their findings, they propose a new supervised pretraining method, which has a good trade-off for transfer learning applications, and validates with other contexts such as few-shot classification and landmark localization.\n\nThe message has been deemed by researchers, but this paper shows empirical evidence with extensive experiments. Thus, the message contained in this work is worthwhile to report to our community.\n\n\nReasons for score: \n\nOverall, I vote for accepting. I like the messages the authors want to convey through this work. If it presents and analyzes the effects according to task semantic relationship more specifically, it would have been a stronger paper. Also, it seems that there is room to improve the paper presentation further. \n\n\nPros:\n- The paper opens and specifies which direction the transfer learning research should go.\n- Also, the findings could be extended to few-shot, semi-supervised, and fully-supervised learning regimes.\n- Extensive analysis and clear summarization of their findings\n\nCons:\n- It would have been good if the presentation of the results is better organized and summarized. The current presentation is busy.\n\n\nOther comments:\n- This reviewer thinks that VOC and COCO datasets could be considered as a subset of the ImageNet dataset at a semantic level. Do the authors have any insight or conclusion from their experiments that can be drawn w.r.t the semantic inclusion relationship of the datasets?\n- For the proposed supervised pretraining method, the authors intentionally draw a link from exemplar SVM. However, it turns out the final proposed method is a simple modification from [Wu et al. 2018]. It seems unnecessary to make a link with exemplar SVM.\n- In sec 2.2, it seems that Table 8 is a typo, which is in the supplementary material. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1443/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1443/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Makes Instance Discrimination Good for Transfer Learning?", "authorids": ["~Nanxuan_Zhao1", "~Zhirong_Wu1", "~Rynson_W._H._Lau1", "~Stephen_Lin1"], "authors": ["Nanxuan Zhao", "Zhirong Wu", "Rynson W. H. Lau", "Stephen Lin"], "keywords": ["Transfer Learning", "Unsupervised Learning", "Self-supervised Learning"], "abstract": "Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.", "one-sentence_summary": "Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|what_makes_instance_discrimination_good_for_transfer_learning", "supplementary_material": "/attachment/97fffe6d3b855459854743d5db5407a510fc5ab2.zip", "pdf": "/pdf/a051d411b95ea8650fc38090dc6150f3d948d35c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021what,\ntitle={What Makes Instance Discrimination Good for Transfer Learning?},\nauthor={Nanxuan Zhao and Zhirong Wu and Rynson W. H. Lau and Stephen Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=tC6iW2UUbJf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tC6iW2UUbJf", "replyto": "tC6iW2UUbJf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538118483, "tmdate": 1606915791161, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1443/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1443/-/Official_Review"}}}], "count": 11}