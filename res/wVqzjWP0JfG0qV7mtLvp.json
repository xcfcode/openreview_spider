{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458743628811, "tcdate": 1458743628811, "id": "WLA6B5kBQc5zMX2Kf2LQ", "invitation": "ICLR.cc/2016/workshop/-/paper/195/comment", "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "wVqzjWP0JfG0qV7mtLvp", "signatures": ["~Christos_Louizos1"], "readers": ["everyone"], "writers": ["~Christos_Louizos1"], "content": {"title": "Updated version of the workshop paper", "comment": "Please find an updated version of the workshop paper here: https://drive.google.com/file/d/0Bx3kAuASMMrnTmIzV255S3laM1k/view?usp=sharing"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455926755616, "ddate": null, "super": null, "final": null, "tcdate": 1455926755616, "id": "ICLR.cc/2016/workshop/-/paper/195/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "wVqzjWP0JfG0qV7mtLvp", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/195/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458743460095, "tcdate": 1458743460095, "id": "L7m9Bj6POhRNGwArs4Kv", "invitation": "ICLR.cc/2016/workshop/-/paper/195/comment", "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "wVqzjWP0JfG0qV7mtLvp", "signatures": ["~Christos_Louizos1"], "readers": ["everyone"], "writers": ["~Christos_Louizos1"], "content": {"title": "Answer to reviewer 10 (part 2)", "comment": "* However, it'd be nice if the authors could expand on their role. Is their role purely computational, i.e. in terms of low-rank approximation to the covariance? Furthermore, in the deep GP [Damianou and Lawrence] the pseudo-data are variational parameters and only affect the approximation and not the model, therefore allowing the approach to remain non-parametric. It is not clear to me that this is the case in this paper: the pseudo-data do seem to change the model, as there is no underlying process (like the Gaussian process).\n- The role of the pseudo-data is to both provide efficient sampling for the pre-activation latent variables and also maintain the GP-like properties of the original model. To further elaborate: a simple assumption for efficient sampling is to assume that the outputs of each layer are independent; in this way the row covariance matrix is diagonal and therefore we can cheaply get the square root. However this approach discards the (finite) GP properties of the Matrix Gaussian parametrization. \nNow if we assume that the real inputs to each layer are conditionally independent given the pseudo data then the input to each layer is of the form (\\tilde{A}, a), where \\tilde{A} are the pseudo-inputs and \u2018a\u2019 is a real vector input. Therefore, an exact sample of the joint distribution p(\\tilde{B}, b | \\tilde{A}, a) is (\\tilde{B}, b) where b ~ p(b | \\tilde{B}, \\tilde{A}, a) (note that we have to make the assumption that the amount of pseudo-data is less than the input dimensionality; in this way we ensure that the pseudo-data combined with a real input provide a positive definite covariance for the row covariance of p(\\tilde{B}, b | \\tilde{A}, a)). In this way we can maintain the properties of the original parametrization but we add the computational complexity cost of the inversion of Sigma_11 which is cubic on the amount of pseudo-data. However, in practice this does not incur a significantly extra cost as the pseudo-data pairs are usually a few. \n\n* Also, can you please expand on the number of the pseudo data typically used and how their inclusion affects optimization/complexity in practice? How does that compare to the mean-field approach?\n- The asymptotic computational complexity of a mean-field approach sampled \u2018locally\u2019 at the hidden unit level is O(K^2) for the mean and variance of each layer where K is the input and output dimensionality. The model proposed here also adds the inversion of Sigma_11 thus resulting into O(K^2 + M^3) complexity where M is the amount of pseudo-data for that layer. Since usually K>>M this does not incur a significantly extra cost.\n\n* One other place where clarification would be good is the dropout posteriors (below eq. 7), which are only mentioned but not explained.\n- Indeed we did not further explained them due to space constraints. The form of the dropout posterior used here is q(\\tilde{B}) = \\prod_{i=1}_{M}\\prod_{j=1}^{K} N(\\tilde{b}_{ij} | \\mu_ij, \\sigma_m^2_{i}\\sigma_k^2_{j}\\mu^2_ij) and similarly for q(\\tilde{A}) (where \\sigma_m is shared with q(\\tilde{B})).\n\n* I also recommend to the authors to have a look at a recent paper by Bui et al., \"Deep Gaussian Processes for Regression using Approximate Expectation Propagation\".\n- We were not aware of this work during the development of our method. We are investigating it now."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455926755616, "ddate": null, "super": null, "final": null, "tcdate": 1455926755616, "id": "ICLR.cc/2016/workshop/-/paper/195/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "wVqzjWP0JfG0qV7mtLvp", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/195/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458743430231, "tcdate": 1458743430231, "id": "mOWZXDpoLtj1gPZ3Ul32", "invitation": "ICLR.cc/2016/workshop/-/paper/195/comment", "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "wVqzjWP0JfG0qV7mtLvp", "signatures": ["~Christos_Louizos1"], "readers": ["everyone"], "writers": ["~Christos_Louizos1"], "content": {"title": "Answer to reviewer 10 (part 1)", "comment": "We would like to primarily thank the reviewer for the in-depth review and constructive criticism that he/she provided. We now continue in addressing each one of the concerns:\n\n* The ideas behind the proposed framework are close to those of Gal & Ghahramani 2015. For this reason I expected a more thorough discussion on the qualitative difference of the approaches. \n- Indeed we did not discussed the differences due to space constraints.  Formally, Gal & Gharamani 2015 consider independent Gaussians for each column of the weight matrix (which in our case correspond to p(W)=\\mathcal{MN}(M, \\sigma^2 * I , I)) and do not model the covariance of the hidden units. Furthermore the approximating variational distribution is quite limited as it corresponds to simple Bernoulli noise and delta approximating distributions for the weight matrix: it is a mixture of two delta functions for each column of the weight matrix, one at zero and the other at the mean of the Gaussian. This is in contrast to our parametrization where we can explicitly learn the (possibly non-diagonal) covariance for both the input and output dimensions of each layer through the matrix variate Gaussian posterior. Finally, sampling in the Gal & Gharamani 2015 is done in the weight space and not the function space (as it happens in our model), thus preventing the use of pseudo-data. \n\n* However, I am not convinced that there is enough support to claim that the proposed model is non-parametric. It is true that the marginalization property of the Gaussian (and matrix variate) gives rise to a GP-like equation, but the proposed model does not inherently seem to be a \"process\", from the functional support point of view (referring to the mapping between the layers).\n- Strictly speaking the proposed model has finite support for datapoints in each layer due to the finite rank nature of the row-covariance: it is required that the amount of datapoints in each layer is less or equal to the dimensionality of the input so as to have a positive definite row covariance. However do note that the general form of the row covariance kernel K(x, y) = sigma(x) U sigma(y) can be seen as an approximation to a non-parametric kernel if we employ Mercer\u2019s theorem. This approximation becomes tighter by increasing the size of each hidden layer."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455926755616, "ddate": null, "super": null, "final": null, "tcdate": 1455926755616, "id": "ICLR.cc/2016/workshop/-/paper/195/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "wVqzjWP0JfG0qV7mtLvp", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/195/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457645909345, "tcdate": 1457645909345, "id": "Jy94B6xR2Uqp6ARvt5v5", "invitation": "ICLR.cc/2016/workshop/-/paper/195/review/10", "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "wVqzjWP0JfG0qV7mtLvp", "signatures": ["ICLR.cc/2016/workshop/paper/195/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/195/reviewer/10"], "content": {"title": "Very interesting paper containing significant amount of work. A few concerns regarding clarity and claimed properties.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is an interesting paper, which includes two key themes: firstly, how to improve the definition and inference in Bayesian Neural Networks. Secondly, how the employed modeling/inference choices make the proposed approach a specific case of a deep Gaussian process. The key idea employed to achieve the above, is to use the matrix variate Gaussian as a distribution over the whole weight matrix together with the variational reparametrization trick. \n\nSUMMARY OF ASSESSMENT:\n\nOverall I enjoyed reading this paper. I have some concerns regarding the clarity and some claimed properties of the approach (see below), but overall it is conveying interesting ideas and the contained amount of novel work -including experiments- is larger than most workshop submissions. Therefore, the paper is well suited for the workshop. \n\n\nDETAILED ASSESSMENT: \n\nThe research area of Bayesian neural networks and deep Gaussian processes are currently attracting a lot of interest, which adds to the significance of this paper. The ideas behind the proposed framework are close to those of Gal & Ghahramani 2015. For this reason I expected a more thorough discussion on the qualitative difference of the approaches. \n\nThe use of the matrix variate Gaussian in the context of Bayesian NNs is a novel element in this paper, and results in the interesting property of correlations in the hidden units. The arguments in favour of its use are convincing, and the relation to deep GPs follows. However, I am not convinced that there is enough support to claim that the proposed model is non-parametric. It is true that the marginalization property of the Gaussian (and matrix variate) gives rise to a GP-like equation, but the proposed model does not inherently seem to be a \"process\", from the functional support point of view (referring to the mapping between the layers).\n\nThe inclusion of pseudo-data is novel and interesting in the context of NNs. However, it'd be nice if the authors could expand on their role. Is their role purely computational, i.e. in terms of low-rank approximation to the covariance? Furthermore, in the deep GP [Damianou and Lawrence] the pseudo-data are variational parameters and only affect the approximation and not the model, therefore allowing the approach to remain non-parametric. It is not clear to me that this is the case in this paper: the pseudo-data do seem to change the model, as there is no underlying process (like the Gaussian process).\n\nAlso, can you please expand on the number of the pseudo data typically used and how their inclusion affects optimization/complexity in practice? How does that compare to the mean-field approach?\n\nOne other place where clarification would be good is the dropout posteriors (below eq. 7), which are only mentioned but not explained.\n\nThe experiments are performed in comparison with related methods in the literature and are convincing. I also recommend to the authors to have a look at a recent paper by Bui et al., \"Deep Gaussian Processes for Regression using Approximate Expectation Propagation\".\n\n\nTypos:\n- \"presented in Gal & ...\" -> \"presented in (Gal & ...)\". Same typo in other citations too.\n- Some typos with apostrophes: lets', GPs', its', ...\n- Some refs need capitalization: bayesian -> Bayesian, gaussian -> Gaussian ...\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580180944, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580180944, "id": "ICLR.cc/2016/workshop/-/paper/195/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "wVqzjWP0JfG0qV7mtLvp", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/195/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457619290247, "tcdate": 1457619290247, "id": "mO91jBZBLfj1gPZ3Ul92", "invitation": "ICLR.cc/2016/workshop/-/paper/195/comment", "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "yovEYZmqYHr682gwszQv", "signatures": ["~Christos_Louizos1"], "readers": ["everyone"], "writers": ["~Christos_Louizos1"], "content": {"title": "Answer to reviewer 12 (part 2)", "comment": "* The model is not non-parametric. First, a linear covariance function is of finite-rank. Second, even without a finite-rank covariance function, the sparse input approximation makes this into a parametric approximation. You might want to look into the work of [Titsias] in this regard. Lastly, you lose the marginalisation property with multiple layers.\n- Yes, to be more precise our model has finite support for N_h datapoints for each layer (where N_h is the dimensionality of the input to the layer). However, do note that we can also view the \\sigma(x)U\\sigma(x)^T kernel as an approximation to a \"non-parametric\" kernel by employing Mercer's theorem. In our experiments the amount of pseudo-data, M, is always less than N_h therefore with the independence assumptions that we make (i.e. the inputs to each layer are conditionally independent given the pseudo data) the row covariance (the linear kernel) of the joint Gaussian output distribution p(\\tilde{B}, b_i) is composed from M + 1 <= N_h datapoints therefore ensuring that the M+1 inputs have indeed a positive definite covariance. As for the second point; indeed we are treating the pseudo data as parameters for optimisation and thus transform the model in a fully parametric one. However this transformation tries to maintain the Gaussian Process properties of each layer. As for the marginalization property; we did not mention that the model maintains *globally* the marginalization property, only locally within each layer. Furthermore, apart from the fact that we are using a linear kernel among layers (that has support for a maximum of N_h datapoints per layer) this model maintains the exact same properties as a deep Gaussian Process. For example, if we assume that the weight posterior has zero mean then the resulting distribution of the output of each layer, conditioned on the pseudo data, is the exact same equation as that of a conditional Gaussian Process. \n\n* You actually use Multiplicative Gaussian Noise rather than \"dropout posteriors\"\n- Indeed it can be also called multiplicative Gaussian noise. We used the term \u201cdropout posterior\u201d to be consistent with the nomenclature of [Kingma et al.].\n\n* What number of inducing points was used in the experiments?\n- The number of inducing points was relatively small: 5 for the wine dataset, 20 for the bigger year and protein datasets and 10 for the rest. For the input layer we set an upper bound to the amount of the inducing points, that of the input dimensionality. \n\n* You might want to cite [Gal and Turner, see below] which also approximate the Gaussian process by placing a Gaussian posterior distribution over the weight matrices.\n- Indeed this paper is also somewhat relevant to this work. It was cited in [Gal and Gharamani] therefore we did not include it here.\n\n* The words \"let's\", \"GP's\", \"its'\" are misspelt multiple times. The sentence \"This corresponds to samples from the marginal...\" is very long and difficult to parse.\n- Thank you very much for pointing these out. We fixed them in the manuscript.\n\n* H is not defined in eq. preceding eq. 5\n- Indeed this is a typo; instead of H we should have B.\n\n* The assessment compares apples and oranges (the deep GP approximation is quite unrelated to the VI and dropout models compared).\n- We think that the comparison between VI and dropout is valid as both treat the  same problem as us: that of inference in Bayesian neural networks (and dropout [Gal and Ghahramani] in particular also treats it as a Gaussian process)\n\n* Time complexity of the model is O(K**3 + M**3) with K hidden units and M inducing points.\n- According to our analysis the time complexity is not O(K**3 + M**3). A typical variational Bayesian neural network with a fully factorized Gaussian posterior sampled ``locally'' [Kingma et al.] has asymptotic per-datapoint time complexity O(K^2) for the mean and variance in each layer. Our model adds the extra cost of inverting Sigma_{11}^{-1} that has cubic complexity with respect to the amount of pseudo-data M. Therefore the asymptotic time complexity is O(K^2 + M^3)$ and since M is usually small this does not incur a significantly extra computational cost. \n\n* the structure is simplified with diagonalisation assumptions\n- There is nothing preventing us of using full covariance matrices in our model; only computational reasons when we have a lot of data. In fact in the experiments we used diagonal matrices with rank-1 corrections (e.g. V^{1/2} = diag(v) + uu^T), which correspond to non diagonal covariance matrices.\n\nReferences:\nKingma et al., \u201cVariational  Dropout and the Local Reparametrization Trick\u201d\nhttp://arxiv.org/abs/1506.02557"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455926755616, "ddate": null, "super": null, "final": null, "tcdate": 1455926755616, "id": "ICLR.cc/2016/workshop/-/paper/195/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "wVqzjWP0JfG0qV7mtLvp", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/195/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457619193107, "tcdate": 1457619193107, "id": "p8j4mNNM9unQVOGWfpGj", "invitation": "ICLR.cc/2016/workshop/-/paper/195/comment", "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "yovEYZmqYHr682gwszQv", "signatures": ["~Christos_Louizos1"], "readers": ["everyone"], "writers": ["~Christos_Louizos1"], "content": {"title": "Answer to reviewer 12 (part 1)", "comment": "We would like to particularly thank the reviewer for the in-depth review that he/she provided. Before addressing each one of the comments we would like to comment on the following remark from the reviewer: \"seems to be mostly a composition of several existing works\". We argue that this is not the case for multiple reasons. First of all this is the first (according to our so far knowledge) application of the concept of pseudo data in the context of (Bayesian) neural networks, which, empirically, significantly increase performance. Secondly, this is also one of the first works that goes beyond the fully factorized assumption for the variational parameter posteriors of a neural network. Thirdly with the matrix Gaussian distributions we can straightforwardly introduce correlations among the hidden units and furthermore with approximations to the covariance we can greatly reduce the amount of parameters of the network. Finally the fact that we are working with the primal space, i.e. weight space, allow us to easily scale this model to large datasets via the use of mini-batches. This is in contrast to the original deep GPs  [Damianou and Lawrence] where the amount of parameters grows linearly with the size of the data and even the \u201cvariational auto-encoded deep Gaussian Process\u201d which requires distributed computation for the evaluation of some terms in the likelihood in order to be scalable. \n\nWe now continue in addressing each one of the concerns. \n\n*  with no output dependence (V=I)\n- From our experiments it seems that correlations between the input/output dimensions do play a role (results with a rank one approximation to V and U are better than diagonal V and U). We did not experiment with an identity matrix for V though. We will do experiments  with an identity matrix for V and U in the future.\n\n* with no non-linearity between the layers (sigma=I and eg RBF covariance function instead of a linear one - the usual method with deep GPs)\n- With our model we are not able to do that as we are \u201cconstrained\u201d to the linear kernel function since we are working in the primal (weight) space. In order to use an RBF covariance we will have to experiment with the original deep GP [Damianou and Lawrence] framework.  We plan to directly compare against it in the future. \n\n* by optimising over \\tilde{A} and \\tilde{B} instead of putting a variational distribution over these (the usual approach in sparse GPs is to optimise the locations of the inducing points \\tilde{A} and optimise / solve analytically for \\tilde{B})\n- We found out that generally putting a distribution over \\tilde{A} and \\tilde{B} improves the generalisation properties of the model. Without them the model tends to be sometimes overconfident (although the degree of this effect seems to be mostly dataset dependent).\n\n* Extrinsically, I would suggest to compare to the results of [Bui et al., see below]. Bui et al. collected results from many sources and evaluated various methods for inference in Bayesian neural networks and GPs.\n- We were not aware of this work and indeed it makes sense to compare our results. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455926755616, "ddate": null, "super": null, "final": null, "tcdate": 1455926755616, "id": "ICLR.cc/2016/workshop/-/paper/195/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "wVqzjWP0JfG0qV7mtLvp", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/195/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1456835781228, "tcdate": 1456835781228, "id": "yovEYZmqYHr682gwszQv", "invitation": "ICLR.cc/2016/workshop/-/paper/195/review/12", "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "wVqzjWP0JfG0qV7mtLvp", "signatures": ["ICLR.cc/2016/workshop/paper/195/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/195/reviewer/12"], "content": {"title": "Review for Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "rating": "7: Good paper, accept", "review": "The authors propose a deep Gaussian process (GP) model with a *linear* covariance function (eq. 7) and non-linear transformations between the GPs. The use of a linear covariance function connects the model to Bayesian neural networks (single layer with no non-linearities). The use of a non-linear transformation between the GPs corresponds to the non-linearity between a Bayesian neural network's layers. The GP output dimensions are correlated (through the use of matrix Gaussian distributions), and a sparse inducing point approximation is used to make the approximation efficient (following [Damianou and Lawrence]). The work offers an approach to connect Bayesian neural networks to deep GPs (with a certain imposed structure), following the line of work of [Gal and Ghahramani].\n\nThe paper is clear (to someone coming from the GP community at least), but seems to be mostly a composition of several existing works. The ideas discussed are interesting, and the assessment is sufficient for a workshop submission, but I would suggest to extend it for a conference submission (see below). \n\nSome comments for the authors:\n* For a conference submission, I would suggest to compare the model intrinsically with several changes, to see where the improvement comes from:\n- with no output dependence (V=I)\n- with no non-linearity between the layers (sigma=I and eg RBF covariance function instead of a linear one - the usual method with deep GPs)\n- by optimising over \\tilde{A} and \\tilde{B} instead of putting a variational distribution over these (the usual approach in sparse GPs is to optimise the locations of the inducing points \\tilde{A} and optimise / solve analytically for \\tilde{B})\n* Extrinsically, I would suggest to compare to the results of [Bui et al., see below]. Bui et al. collected results from many sources and evaluated various methods for inference in Bayesian neural networks and GPs.\n* The model is not non-parametric. First, a linear covariance function is of finite-rank. Second, even without a finite-rank covariance function, the sparse input approximation makes this into a parametric approximation. You might want to look into the work of [Titsias] in this regard. Lastly, you lose the marginalisation property with multiple layers.\n\nMinor comments:\n* You actually use Multiplicative Gaussian Noise rather than \"dropout posteriors\"\n* What number of inducing points was used in the experiments?\n* You might want to cite [Gal and Turner, see below] which also approximate the Gaussian process by placing a Gaussian posterior distribution over the weight matrices.\n* The words \"let's\", \"GP's\", \"its'\" are misspelt multiple times\n* H is not defined in eq. preceding eq. 5\n* The sentence \"This corresponds to samples from the marginal...\" is very long and difficult to parse.\n\nPros:\n* The experiments are of good quality for a workshop paper\n* The idea of using inducing points with neural networks is intriguing (I've been working on it myself)\n* The paper is interesting\nCons:\n* The assessment compares apples and oranges (the deep GP approximation is quite unrelated to the VI and dropout models compared).\n* Time complexity of the model is O(K**3 + M**3) with K hidden units and M inducing points.\n* the structure is simplified with diagonalisation assumptions\n* The model is not non-parametric\n\nReferences:\nBui et al., \"Deep Gaussian Processes for Regression using Approximate Expectation Propagation\"\nhttp://arxiv.org/abs/1602.04133\n\nGal and Turner, \"Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs\"\nhttp://jmlr.org/proceedings/papers/v37/galb15.html", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580180474, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580180474, "id": "ICLR.cc/2016/workshop/-/paper/195/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "wVqzjWP0JfG0qV7mtLvp", "replyto": "wVqzjWP0JfG0qV7mtLvp", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/195/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455725981329, "tcdate": 1455725981329, "id": "wVqzjWP0JfG0qV7mtLvp", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "wVqzjWP0JfG0qV7mtLvp", "signatures": ["~Christos_Louizos1"], "readers": ["everyone"], "writers": ["~Christos_Louizos1"], "content": {"CMT_id": "", "title": "Deep Bayesian Neural Nets as Deep Matrix Gaussian Processes", "abstract": "We show that by employing a distribution over random matrices, the matrix variate Gaussian~\\cite{gupta1999matrix}, for the neural network parameters we can obtain a non-parametric interpretation for the hidden units after the application of the ``local reprarametrization trick\"~\\citep{kingma2015variational}. This provides a nice duality between Bayesian neural networks and deep Gaussian Processes~\\cite{damianou2012deep}, a property that was also shown by~\\cite{gal2015dropout}. We show that we can borrow ideas from the Gaussian Process literature so as to exploit the non-parametric properties of such a model. We empirically verified this model on a regression task. ", "pdf": "/pdf/wVqzjWP0JfG0qV7mtLvp.pdf", "paperhash": "louizos|deep_bayesian_neural_nets_as_deep_matrix_gaussian_processes", "conflicts": ["uva.nl"], "authorids": ["c.louizos@uva.nl", "m.welling@uva.nl"], "authors": ["Christos Louizos", "Max Welling"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 8}