{"notes": [{"id": "SJeYe0NtvH", "original": "BkgGGFXODS", "number": 936, "cdate": 1569439216858, "ddate": null, "tcdate": 1569439216858, "tmdate": 1583912038092, "tddate": null, "forum": "SJeYe0NtvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "a4gzivpZp", "original": null, "number": 1, "cdate": 1576798710222, "ddate": null, "tcdate": 1576798710222, "tmdate": 1576800926104, "tddate": null, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "invitation": "ICLR.cc/2020/Conference/Paper936/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper introduces a new objective for text generation with neural nets.  The main insight is that the standard likelihood objective assigns excessive probability to sequences containing repeated and frequent words.  The paper proposes an objective that penalizes these patterns.  This technique yields better text generation than alternative methods according to human evaluations.\n\nThe reviewers found the paper to be written clearly. They found the problem to be relevant and found the proposed solution method to be both novel and simple.  The experiments were carefully designed and the results were convincing.  The reviewers raised several concerns on particular details of the method.  These concerns were largely addressed by the authors in their response.  Overall, the reviewers did not find the weaknesses of the paper to be serious flaws.\n\nThis paper should be published. The paper provides a clearly presented solution for a relevant problem, along with careful experiments. \n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728054, "tmdate": 1576800280387, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper936/-/Decision"}}}, {"id": "H1lxTev3Yr", "original": null, "number": 1, "cdate": 1571741880149, "ddate": null, "tcdate": 1571741880149, "tmdate": 1574431892439, "tddate": null, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "invitation": "ICLR.cc/2020/Conference/Paper936/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "\nThis paper proposes training losses, unlikelihood objective, for mitigating the repetition problem of the text generated by recent neural language models. The problem is well-motivated by evidence from the existing literature. Specifically, the paper argues that the main cause of the degenerated output is the maximum likelihood objective commonly used to train language models. Their main contribution is to introduce additional objectives to penalize \u201cunlikely\u201d word probabilities. The proposed penalty is derived into 2 objectives: token level (previous words in context) and sentence level (future decoded words). The prior objective is used along with the MLE, while the later and more expensive is used for fine-tuning. They perform experiments on Wikitext-103 and evaluate models on the perplexity of the models, and n-gram statistics such as repetition, and uniqueness of the decoded texts. The proposed training scheme (UL-token+seq) is shown to have the closest statistics to the original corpus while the perplexity slightly suffers. The additional manual analysis shows that human annotators prefer the outputs (sentence completion) of the proposed method over the other baselines.\n\nOverall, this paper tackles a relevant problem and could propose a novel method.\n\nFor the unlikelihood objectives, there are a few clarifications on the design decision. There are some \u201ccorrect\u201d repetitions in the ground-truth text as well. However, the proposed loss minimizes all repetitions regardless. In addition, it is unclear how the proposed method mitigates the token distribution mismatch. Finally, there is a similar work where they attempt to \u201cmatch\u201d repetition (or n-gram distribution) of a reference corpus (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16961). A discussion of how this paper distinguishes from previous work would be helpful.\n\nFor the experiments, there are some missing detail and concerns:\n\n1. The fine-tuning using UL-seq (eq 7) procedure is not well explained. For example, how many sequences are used per update? How many times that you decode in the 1,500 updates?\n\n2. the stochastic decoding results are related as the paper motivation is built on top of Holtzman et al., 2019, the results also support the claim. However, how do you explain the discrepancy between the experts and the crowd workers?\n\n3. GPT-2 results show that UL-seq does not significantly improve several evaluation metrics from MLE. This is a conflict with the result in Table 2. This could be a sign of the generalization problem of the proposed method. Additional results on different model architectures would be helpful.\n\nMinor question:\n1. It is uncertain how these repetition and uniqueness statistics translate to a downstream task (e.g. summarization or NMT). Do you have results regarding this?\n\n2. It appears that UL-token does not do much. What is the increase in training time? Do you recommend using the token loss? \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper936/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper936/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575567605357, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper936/Reviewers"], "noninvitees": [], "tcdate": 1570237744805, "tmdate": 1575567605373, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper936/-/Official_Review"}}}, {"id": "HkxH_epujS", "original": null, "number": 3, "cdate": 1573601389337, "ddate": null, "tcdate": 1573601389337, "tmdate": 1573601389337, "tddate": null, "forum": "SJeYe0NtvH", "replyto": "H1lxTev3Yr", "invitation": "ICLR.cc/2020/Conference/Paper936/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the review; the primary comments were clarifications and the degree of improvement with GPT-2, which we address below.\n\n> \u201cThere are some \u201ccorrect\u201d repetitions in the ground-truth text as well. However, the proposed loss minimizes all repetitions regardless.\u201d\n\nThe token-level loss does not penalize ground-truth repetitions, as the ground-truth next-token is not a candidate (due to the \\ {x_t} in equation 5).\n\nFor the sequence-level loss, the sequence is model-generated, so there is no notion of ground-truth. We also showed that penalizing randomly selected tokens gave improvements (Table 8), so the penalties do not need to be tied to the notion of repetition.\n\nWe track the wrep metric which only counts _incorrect_ repetitions (as opposed to rep which counts all repetitions). You can see in Table 2 that wrep improves after unlikelihood training.\n\n> \u201cIt is unclear how the proposed method mitigates the token distribution mismatch.\u201d\n\nIf frequent words are appearing too often in the decoded sequences then they are the words being penalized with the unlikelihood loss, which naturally shifts mass to less frequent words. Reducing repetitions towards the human rate also results in a closer distribution match. Together, it seems these effects do indeed mitigate token distribution mismatch, as shown in Figure 1.\n\n> \u201cthere is a similar work where they attempt to \u201cmatch\u201d repetition (or n-gram distribution) of a reference corpus (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16961).\u201d\n\nWe will add this citation to the related work. The method there maintains a pool of generated text to estimate model marginals, which is a different approach than unlikelihood. Moreover, they only consider local repetition within a 3-word window, which does not address structural repetition or phrase-level repetition for a sufficiently long phrase (e.g. see Table 1).\n\n> \u201cHow do you explain the discrepancy between the experts and the crowd workers?\u201d\n\nCrowdworkers and experts all agree that our methods outperform existing approaches, while experts are more confident. This makes sense because crowdworkers are less experienced in this task, and more likely to tend closer to unsure (closer to 50%).\n\n> \u201cGPT-2 results show that UL-seq does not significantly improve several evaluation metrics from MLE.\u201d\n\nThe token-level ppl and acc metrics improve over zero-shot GPT-2, while substantially decreasing sequence repetition (over both zero-shot GPT-2 and fine-tuned GPT-2_MLE). \n\nOur primary experiments are conducted with the (Baevski and Auli) model since it allowed us to train from scratch. The GPT-2 results show that unlikelihood is also applicable in a setting where the model is trained on a corpus different from the one it is fine-tuned on; further investigating these variants is interesting future work.\n\n> \u201cIt is uncertain how these repetition and uniqueness statistics translate to a downstream task\u201d\n\nIndeed, this is interesting as future work to apply to further downstream tasks. We focus on proposing the general framework here and thoroughly investigating the completion task, which had been previously shown in Holtzman et al 2019 to be an open problem that should be tackled. \n\n> \u201cIt appears that UL-token does not do much. What is the increase in training time? Do you recommend using the token loss?\u201d\n\nYes, we recommend using the token loss. Considering that UL-token uses the same amount of supervision as normal MLE training (i.e. previous-context candidates are already present in the training sequence), the ~16-20% improvement in repetition (Table 2) is substantial. We also see that token+seq is better than only seq (Table 2). There is only a slight increase in training time and it is not a significant limiting factor since the candidates can be computed efficiently.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper936/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeYe0NtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference/Paper936/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper936/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper936/Reviewers", "ICLR.cc/2020/Conference/Paper936/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper936/Authors|ICLR.cc/2020/Conference/Paper936/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163826, "tmdate": 1576860552427, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference/Paper936/Reviewers", "ICLR.cc/2020/Conference/Paper936/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper936/-/Official_Comment"}}}, {"id": "Sklyik6ujB", "original": null, "number": 2, "cdate": 1573601174600, "ddate": null, "tcdate": 1573601174600, "tmdate": 1573601174600, "tddate": null, "forum": "SJeYe0NtvH", "replyto": "HJeHygz6YH", "invitation": "ICLR.cc/2020/Conference/Paper936/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the thoughtful review and the points about clarity, novelty, and strong experimental quality. We address your concerns below, and clarify points raised in the review:\n\n> \u201cUsing previous generated tokens as the unlikely tokens for the current generation step in the token-level unlikelihood training is intuitive, but also seems too simple.\u201d\n\nThe simplicity can be viewed as a strength. It is desirable to have candidates that do not require additional supervision (previous context tokens are already present in the training sequence), are fast to compute, and are interpretable. \n\n> \u201cHow is the training looking like? Do we need Gumbel-softmax-like trick to backpropagate through the generated tokens in the sequence-level training? Or, this is not needed? Can the authors clarity the training process?\u201d\n\nIt is not complicated. The token-level training is similar to standard MLE training, just with the additional candidate and loss computation (which can be done efficiently with matrix operations, for full details the code is provided in the link above, and you can look in the candidate_penalty_ce_loss.py file).\n\nThe sequence-level training consists of decoding a sequence from the model, then computing the unlikelihood loss using the decoded sequence instead of ground truth; again, it is simple, and does not require gumbel-softmax nor policy gradient, which is a benefit of our approach. For full details the implementation is provided in the link above in the sequence_penalty_loss.py file.\n\n> \u201cThe proposed model can be directly applied to dialog response generation task, which also requires diversity in generated responses.\u201d\n\nIndeed, unlikelihood training is a general framework that can be applied to other sequence generation tasks, and this is an exciting area for future work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper936/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeYe0NtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference/Paper936/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper936/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper936/Reviewers", "ICLR.cc/2020/Conference/Paper936/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper936/Authors|ICLR.cc/2020/Conference/Paper936/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163826, "tmdate": 1576860552427, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference/Paper936/Reviewers", "ICLR.cc/2020/Conference/Paper936/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper936/-/Official_Comment"}}}, {"id": "S1etXJadjB", "original": null, "number": 1, "cdate": 1573601057164, "ddate": null, "tcdate": 1573601057164, "tmdate": 1573601057164, "tddate": null, "forum": "SJeYe0NtvH", "replyto": "ryeEqCDG5B", "invitation": "ICLR.cc/2020/Conference/Paper936/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Reviewer #1 raised minor concerns and had a few misunderstandings which we clarify below; we emphasize that our proposal, unlikelihood training, and its strong empirical results are significant and we encourage the reviewer to re-evaluate the rating after considering these clarifications.\n\n> \u201cThis raises several practical issues: how to choose a reasonable \\alpha.\u201d\n\nWe found that \\alpha = 1.0 worked, and we did not have to perform a hyper-parameter search for \\alpha, so it was not a practical issue. For further insight, the gradient analysis (eqn. 6) shows how \\alpha can affect the gradient, and we provide an interpretation with \\alpha = 1.0.\n\n> \u201cThis is a reasonable choice, but the author does not state why the other choices are not working,e.g. Sharpening the distribution using temperature.\u201d\n\nUnlikelihood is indeed a general framework, so there are different possibilities for choosing candidates. We show that previous context candidates result in substantial improvements, and provide an interpretation for why they work (see below eqn. 5). Can you clarify what you mean by \u201csharpening the distribution using temperature\u201d, with respect to choosing candidates?\n\n> \u201cA potential counter case is that there are similar words exists in the sequences, but the unlikely loss trends to distinguish these synonyms.\u201d\n\nWe have not observed evidence for this counter case as seen in the human evaluation of Table 3 or examples in Table 1; if the model was resorting to a simplistic strategy, it is unlikely that the human evaluation quality would show such substantial gains over the MLE baseline.\n\n> \u201cThe other unlikelyhood training choice is called sequence-level set. However, it seems not sequence-level but just n-gram center.\u201d\n\nThis is a misunderstanding we should clarify. Sequence-level refers to the fact that the loss is computed over a model-decoded sequence. A token is penalized if it is in any part of a repeating n-gram, not just the center of the n-gram (this is the intention of equation 8). \n\nWe will clarify equation (8) by changing it to \u201cx_t is the (single) negative candidate for step t if it is part of an n-gram that previously occurred in x_{<t}\u201d. These are minor clarifications, and our method and results do not change.\n\n> \u201cAlso, why a prefix is really needed is questionable.\u201d\n\nThe task is sequence completion, which uses a prefix (defined in section 3).\n\n> \u201cTable 2 should have shown the original sequences on the repetition metrics to show it indeed make sense.\u201d\n\nThe completions are not necessarily supposed to match the human completion, just resemble a human completion. A full evaluation of this question is done in the human evaluation (Table 3).\n\n> \u201cPpl would be enough, acc seems redundant.\u201d\n\nNext-token prediction accuracy (acc) more closely measures the model\u2019s ability to highly rank the top token instead of modeling the full distribution. We report both to provide as much information as possible in interpreting our results.\n\n> Re: \u201cIt seems that unlikely training may be harmful to ppl, which is the common metric to evaluate generation quality. A better discussion should be made on this to explain why it performance or if ppl has some problem.\u201d\n\nIn Table 2, the perplexity did not degrade after sequence-level training, actually it\u2019s slightly better than the MLE baseline (L_{UL-seq} 25.42 vs. L_{MLE} 25.64), while having substantially better generations (e.g. seq-rep-4, 71% human eval win rate). \n\nWe comment on this phenomenon in the paper: \u201cthe likelihood objective is not constrained enough, in the sense that two models with the same perplexity can exhibit wildly difference generation performance\u201d, and perform human evaluations to quantify the phenomenon. \n\n>\u201cTable 3 comparison may not be reasonable. As Nucleus sampling and beam blocking is not in training phase. This comparison is not really fair.\u201d\n\nNucleus sampling and beam blocking are not training-time methods (https://arxiv.org/abs/1904.09751, https://arxiv.org/abs/1705.04304). They are used only at inference time given a trained model. This is in fact a motivation of this paper: investigating a training-time method to address text degeneration.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper936/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeYe0NtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference/Paper936/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper936/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper936/Reviewers", "ICLR.cc/2020/Conference/Paper936/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper936/Authors|ICLR.cc/2020/Conference/Paper936/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163826, "tmdate": 1576860552427, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper936/Authors", "ICLR.cc/2020/Conference/Paper936/Reviewers", "ICLR.cc/2020/Conference/Paper936/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper936/-/Official_Comment"}}}, {"id": "HJeHygz6YH", "original": null, "number": 2, "cdate": 1571786716937, "ddate": null, "tcdate": 1571786716937, "tmdate": 1572972533496, "tddate": null, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "invitation": "ICLR.cc/2020/Conference/Paper936/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nContributions:\n\nThe main contribution of this paper lies in the proposed unlikelihood training objective for open-ended text generation. The key idea is to enforce the unlikely generations to be assigned lower probability by the model. Both token and sequence-level unlikelihood training objectives are provided. Impressively, the authors show that models trained with the proposed method can generate high-quality text via only beam search, without using top-k, nucleus sampling, or beam blocking methods. \n\nStrengths:\n\n(1) Writing & Clarity: The proposed model is very well motivated, the paper is well written, and clearly presented. I enjoyed reading the paper. \n\n(2) Novelty: Though the proposed model is simple, I think it has novelty inside. The proposed model makes connection to negative sampling, and is very intuitive to reduce repetition during the training stage, instead of decoding stage. I find the gradient analysis in Section 5.1 is especially interesting. \n\n(3) Experiments: The authors did a careful job in experiments design, and conducting the experiments. Human evaluation is also provided. A lot of additional results are provided in Appendix. I feel the experiments are solid and convincing.  \n\nWeaknesses:\n\n(1) Clarity: I have three questions regarding this paper. \n\na) Using previous generated tokens as the unlikely tokens for the current generation step in the token-level unlikelihood training is intuitive, but also seems too simple. Can the authors provide some comments on this? Or, are there any better designs? \n\nb) How is the training looking like? Do we need Gumbel-softmax-like trick to backpropagate through the generated tokens in the sequence-level training? Or, this is not needed? Can the authors clarity the training process? \n\nc) The proposed model can be directly applied to dialog response generation task, which also requires diversity in generated responses. Any reason why this conditional generation task is not performed? Or, do the authors plan to also apply the proposed method to this application?\n \n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper936/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper936/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575567605357, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper936/Reviewers"], "noninvitees": [], "tcdate": 1570237744805, "tmdate": 1575567605373, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper936/-/Official_Review"}}}, {"id": "ryeEqCDG5B", "original": null, "number": 3, "cdate": 1572138636222, "ddate": null, "tcdate": 1572138636222, "tmdate": 1572972533449, "tddate": null, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "invitation": "ICLR.cc/2020/Conference/Paper936/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper targets on solving the dull and repetitive outputs in MLE training for neural text generation. The authors propose a new unlikelyhood training to avoid assigning much probability to frequent and repetitive words. The authors combine the proposed algorithms and beam search and state that the results improved over beam blocking and neculus decoding.  \n\nThe unlikelyhood training is to provide a set of negative candidates and minimize the probability of these tokens. This raises several practical issues: how to choose a reasonable $\\alpha$. \nThis set can be chosen as the previous tokens in the sequence. This is a reasonable choice, but the author does not state why the other choices are not working,e.g. Sharpening the distribution using temperature. A potential counter case is that there are similar words exists in the sequences, but the unlikely loss trends to distinguish these synonyms.  The other unlikelyhood training choice is called sequence-level set. However, it seems not sequence-level but just n-gram center.  A question would be why not chose the whole n-gram instead of just choosing the center of n-gram. Also, why a prefix is really needed is questionable. \n\n\nEq 8 seems wrong, why$i \\leq n \\leq j$\n\nTable 2 should have shown the original sequences on the repetition metrics to show it indeed make sense.ppl should be enough, acc seems redundant. It seems that unlikely training may be harmful to ppl, which is the common metric to evaluate generation quality. A better discussion should be made on this to explain why it performance or if ppl has some problem.\n\nTable 3 comparison may not be reasonable. As Nucleus sampling and beam blocking is not in training phase. This comparison is not really fair.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper936/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper936/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Text Generation With Unlikelihood Training", "authors": ["Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston"], "authorids": ["wellecks@nyu.edu", "kulikov@cs.nyu.edu", "roller@fb.com", "edinan@fb.com", "kyunghyun.cho@nyu.edu", "jase@fb.com"], "keywords": ["language modeling", "machine learning"], "abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.", "pdf": "/pdf/b1605077dc551ef983c69ef382a249fb5048060d.pdf", "code": "https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj", "paperhash": "welleck|neural_text_generation_with_unlikelihood_training", "_bibtex": "@inproceedings{\nWelleck2020Neural,\ntitle={Neural Text Generation With Unlikelihood Training},\nauthor={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeYe0NtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a725d3064ae37da1c53efbf7029d26323852cf41.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeYe0NtvH", "replyto": "SJeYe0NtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper936/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575567605357, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper936/Reviewers"], "noninvitees": [], "tcdate": 1570237744805, "tmdate": 1575567605373, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper936/-/Official_Review"}}}], "count": 8}