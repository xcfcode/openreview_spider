{"notes": [{"id": "Hkxx3o0qFX", "original": "rJl9ZUr9K7", "number": 678, "cdate": 1538087847756, "ddate": null, "tcdate": 1538087847756, "tmdate": 1545355399118, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJxdcNSleN", "original": null, "number": 1, "cdate": 1544733840111, "ddate": null, "tcdate": 1544733840111, "tmdate": 1545354513142, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Meta_Review", "content": {"metareview": "All reviewers gave a 5 rating.\nThe author rebuttal was not able to alter the consensus view of reviewers.\nSee below for details.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "All reviewers assess the paper as being marginally below acceptance threshold "}, "signatures": ["ICLR.cc/2019/Conference/Paper678/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper678/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353129665, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper678/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper678/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper678/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353129665}}}, {"id": "H1lHPv8iAm", "original": null, "number": 4, "cdate": 1543362397188, "ddate": null, "tcdate": 1543362397188, "tmdate": 1543362397188, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "Skex8Gg067", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "content": {"title": "The rebuttal is not convincing", "comment": "The authors claimed that the results in the paper are some \"typical cases\", neither completely random, nor cherry-picked bad results for CTX. However, \"typical cases\" are still very vague. The author still did some kind of selection. Since there are user studies, why not showing some top selected results and least selected results by the users when comparing with CTX. It is weird to say that one method is much better than the other, while the images look very similar. \n\nThe response is also self-contradictory. The author stated that \"it is not fair to compare the performance of two GAN approaches by looking at only a few samples\", but only a few examples are selected to demonstrate the benefit of FAM in Fig. 8. Since FAM is one of the claimed novel parts of the paper, why not including the results without FAM in the user study. "}, "signatures": ["ICLR.cc/2019/Conference/Paper678/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper678/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624233, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkxx3o0qFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper678/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper678/Authors|ICLR.cc/2019/Conference/Paper678/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624233}}}, {"id": "Skex8Gg067", "original": null, "number": 3, "cdate": 1542484551547, "ddate": null, "tcdate": 1542484551547, "tmdate": 1542484551547, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "Syl6qnpoj7", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "content": {"title": "Response to Reviewer Three", "comment": "\u201cThe results did not significantly outperform previous methods such as CTX in terms of visual quality\u2026In figure 6, the results compared to CTX look similar. And the figure is too small to see the details.\u201d\n\nWhen we chose the sample images (Figure 6), we did not intentionally choose bad samples of CTX and good samples of our approach. Instead, we want to demonstrate some typical cases when each of these approaches failed or succeeded. Since it is not fair to compare the performance of two GAN approaches by looking at only a few samples, we used the results of a user study, which is known as the \u201cgolden standard\u201d to evaluate GANs, to show the overall performance of different approaches, which we think should be more convincing. \n\n\u201cThe visualization of the attention features look like normal feature in a neural network\u201d\n\nThe filters (Figure 1) showed clear and regular patterns as we expected. For instance, while the resolution increased from 8x8 to 1024x1024, the model attended on higher frequency information. Regions with rich details (e.g. eyes) got more attention, especially at high resolutions. It is unlikely that they are simply some normal features in a neural network.\n\n\u201cin Figure 8, the quality of results with and without FAM look very similar. These 4 images were selected from 3000 test images, but the difference is too small to show the benefit of FAM.\u201d\n\nThe FAM is designed to enhance details. If we look closely at the third and fourth row of Figure 8, the results without FAM are blurrier, especially at regions with rich details (e.g. eye regions). Also, results with FAM usually have less distortions. \n\nAgain, we demonstrated the typical performances of models with and without FAM, instead of intentionally choosing images that showed the worst cases of images without FAM.  \n\n\u201cIn figure 8, it is unclear how the performance changes with each loss. Probably the results without L_bdy, L_rec, L_feat should be analyzed separately.\u201d\n\nThis is a good suggestion, it would be better to do a more thorough ablation study.  However, the effects of many losses (e.g. L_rec, L_feat) have been well studied in previous literatures (e.g. Li et al., 2017) and thus they are not the focus of our work.\n\n\u201cHow many images were used in the user study? Did each subjects evaluate the entire test set 3009 images?\u201d\n\nFor session 1 where the experiment directly compares our method with context encoder, each subject evaluates 100 chosen randomly images. For session 2, 3 and 4 where each method compares with ground truth, each subject evaluates another random 100 images. The total coverage rate over the entire test set is about 86%.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper678/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624233, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkxx3o0qFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper678/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper678/Authors|ICLR.cc/2019/Conference/Paper678/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624233}}}, {"id": "BkxC-fgA6X", "original": null, "number": 2, "cdate": 1542484486318, "ddate": null, "tcdate": 1542484486318, "tmdate": 1542484486318, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "ryl5fByTjQ", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "content": {"title": "Response to Reviewer Two", "comment": "Thanks for the professional reviews. We would like to make some clarification to better demonstrate our work.\n\n\u201cAlso the experimental results did not demonstrate better performance of the proposed approach. Why is that?\u201d\n\nCould you please explain which part of the results you are referring to? \nBoth the results of the quantitative evaluation and user study showed our model performed better. In Table 1, for L1 and L2, the smaller value is better. For PSNR, the larger value is better. In Figure 6, the larger value is better. \n\n\u201cWhat are the major novelty compared with these existing works? (Progressive GAN and Wang et al.)\u201d\n\nThe Progressive GAN is an image GENERATION network and the work of Wang et al. is an image TRANSLATION network. They are both different from the image COMPLETION networks (e.g. Pathak et al., 2016, Li et al., 2017, Iizuka et al., 2017, Yu et al., 2018, Liu et al., 2018, etc.) in terms of goals, network structures, training methods and loss functions, and are not directly comparable with our model. Neither of the Progressive GAN nor the work of Wang et al. can be applied to the image completion task directly, though some of their can be adopted to design completion models (e.g. the progressive training methodology in Progressive GAN). \n\nThe input of an image generation model (e.g. Progressive GAN) is noise, and the output is a random realistic image. The image completion task is more challenging because it not only requires generating plausible content, but also expects the generated content to match the contextual information perfectly. \n\nThe input of an image translation model is a complete image from one domain (e.g. segmentation labels), and the output is a transformed image in another domain, such as a realistic photo or a painting of another style (e.g. Zhu et al., 2017). The key difference is that some information is missing in the input of an image completion network, and the completion model needs to infer plausible content conditioned on contextual information.\n\nTherefore, it is more reasonable to compare our work with other completion models, rather than a generation or translation model. As we discussed in the response to R1, we have adopted many ideas from networks outside the image completion area and successfully integrate them to obtain an effective completion model. We have also designed novel structures, pipelines and loss functions so that our model can work appropriately as a whole. To our knowledge, our method is quite unique comparing to other image completion networks.  \n\n\u201cwhy the frequency attention module will yield better results?\u201d\n\nTraditionally, researchers use the attention mechanism in spatial domain. Instead of learning to generate/complete the whole image at once, the model is encouraged to focus on a small region in one step. For instance, the DRAW model (Gregor et al., 2015) learns to read and write a small region of image in each timestep, and the whole image can be produced after many iterations. CTX (Yu et al., 2018) uses a contextual attention layer to help the model borrow contextual information from distant locations while filling in missing \u201choles\u201d. \n\nLike these spatial-attention-based methods, we design an attention mechanism in frequency domain. Instead of generating image features at different level of details in a single step, our model is encouraged to learn the structures in a coarse-to-fine manner. The detailed design of FAM is described in Section 3.2.1. The results (Figure 1) shows that our model performed as we expected: it focused on coarse structures when the resolution was low and switched its attention to finer details (e.g. hair or eye regions) as the resolution increased. This attention mechanism works because the complex problem of completing high-resolution images is divided into many sub-problems.  \n\n\u201cImprove the experiment, compared with stronger baselines: consider at least one or two of these state-of-the-art approaches\u201d\n\nCTX (Yu et al., cvpr18) is considered state-of-the-art. When we ran the user study and it was the only approach that worked for 256x256 images. We also included the comparison with another state-of-the-art approach GL (Iizuka et al., siggraph17) in the quantitative comparison (Table 1). "}, "signatures": ["ICLR.cc/2019/Conference/Paper678/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624233, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkxx3o0qFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper678/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper678/Authors|ICLR.cc/2019/Conference/Paper678/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624233}}}, {"id": "ByeFPbgATX", "original": null, "number": 1, "cdate": 1542484320739, "ddate": null, "tcdate": 1542484320739, "tmdate": 1542484320739, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "rylzbX3ThX", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "content": {"title": "Response to Reviewer One", "comment": "Thanks for the professional reviews. We would like to make some clarification to better demonstrate our work.\n\n\u201cit is not clear to me what aspect of their GAN is particularly new\u201d\n\nWe agree that some building blocks of our model, such as the Context Encoder structure (Pathak et al., 2016), Progressive Training Methodology (Progressive GAN, Karras et al., 2017), Conditional GAN (Mirza et al., 2014) etc., are based on existing approaches. But it is a challenging task to integrate these methods to obtain an effective completion model. On the top of these existing approaches, we have also designed new structures (e.g. our novel Frequency-Oriented Attentive Module), novel pipeline (Figure 2) and loss functions (e.g. boundary loss) to significantly improve the performance.\n\nPlease note that most of these building blocks are not originally designed for image completion and are seldomly used in completion models. For instance, the Progressive GAN is an image GENERATION model whose input is noise and the output are random realistic images. However, image COMPLETION is a more challenging task. Conditioned on corrupted images (i.e. the input), we not only need to generate plausible content, but also need to make sure that the content matches the contextual information perfectly. In sum, our network structure is significantly different from any of the existing approaches we built on. Additionally, to our knowledge, our method is also unique in the image completion area. \n\nBecause of the novel architecture/method, our model achieves significantly better performance than state-of-the-art approaches. First, our model is the first one that can complete face images at 1024x1024 while state-of-the-art (CTX, Yu et al., 2018) can only handle 256x256 images. By running a user study, which is currently the \u201cgolden standard\u201d to evaluate GANs, we found our model outperformed CTX in terms of visual quality at 256x256 resolution. Second, our model can control multiple attributes of synthesized content (including subtle facial expressions) while other completion models can only produce random content images. Third, our model does not need post-processing and can generate completed images directly while other approaches often have to post-process images (e.g. Lizuka et al., 2017) or paste synthesized content to original context (e.g. Yu et al., 2018).\n\n\u201cDetailed experimental comparisons with more state-of-the-arts (e.g., RLA, Zhao et al., TIP 2018, 3D-PIM, Zhao et al., IJCAI 2018) are needed to justify the superiority of the proposed method\u201d\n\nThanks. We will include these literatures in our reference and compare with them in the future experiments.\n\n\u201cMore in-the-wild qualitative and quantitative experiments on recent benchmarks with large occlusion variations are needed to verify the efficacy of the proposed method.\u201d\n\nAgreed, this is a good suggestion. But our current experiments followed the standard of experiments in state-of-the-art works (Pathak et al., 2016, e.g. Iizuka et al., 2017, Yu et al., 2018, etc.) and tested the performance of our model for various challenging mask types including center squared, random rectangular and arbitrary hand-drawn masks.\n\n\u201cHow did authors update each component and ensure stable yet fast convergence while optimizing the whole GAN-based framework?\u201d\n\nWe started with empirical parameters of existing approaches and updated them with trial and error . \n\n\u201cCan the proposed method solve other challenging in-the-wild facial variations except occlusion? e.g., pose, expression, lighting, noise, etc.\u201d\n\nThis is an interesting idea. We focused on solving the face completion (or the \u201cinpainting\u201d) problem in this paper. But it would be great if we could apply our model to other tasks. This is left for our future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper678/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624233, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkxx3o0qFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper678/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper678/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper678/Authors|ICLR.cc/2019/Conference/Paper678/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper678/Reviewers", "ICLR.cc/2019/Conference/Paper678/Authors", "ICLR.cc/2019/Conference/Paper678/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624233}}}, {"id": "rylzbX3ThX", "original": null, "number": 3, "cdate": 1541419769676, "ddate": null, "tcdate": 1541419769676, "tmdate": 1541533782610, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Official_Review", "content": {"title": "This work uses GANs to recover clean faces from occluded counterparts. The effectiveness of the proposed method is verified qualitatively and quantitatively on CelebA-HQ. The proposed framework can be generalized to several face-related tasks, such as unconstrained face recognition. Although the novelty of the method is not really impressive, the proposed method seems to be useful for face-related applications and the experimental results are convincing to me.", "review": "This work uses GANs to recover clean faces from occluded counterparts. The effectiveness of the proposed method is verified qualitatively and quantitatively on CelebA-HQ. The proposed framework can be generalized to several face-related tasks, such as unconstrained face recognition. Although the novelty of the method is not really impressive, the proposed method seems to be useful for face-related applications and the experimental results are convincing to me.\n\nPros:\n- This method is simple, apparently effective and is a nice use of GANs for a practical task. The paper is written clearly and the English is fine.\n\nCons:\n- My main concern with this paper is regarding the novelty. The authors seem to claim a novel GAN architecture by using an adversarial auto-encoder-based architecture. However, it is not clear to me what aspect of their GAN is particularly new.\n\n- Missing experimental comparisons with state-of-the-arts. Detailed experimental comparisons with more state-of-the-arts (e.g., RLA, Zhao et al., TIP 2018, 3D-PIM, Zhao et al., IJCAI 2018) are needed to justify the superiority of the proposed method.\n\n- Missing more in-the-wild comparisons in the Experiment section. This paper mainly performed experiments on CelebA-HQ. More in-the-wild qualitative and quantitative experiments on recent benchmarks with large occlusion variations are needed to verify the efficacy of the proposed method.\n\nAdditional comments:\n- How did authors update each component and ensure stable yet fast convergence while optimising the whole GAN-based framework? \n\n- Can the proposed method solve other challenging in-the-wild facial variations except occlusion? e.g., pose, expression, lighting, noise, etc.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper678/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Official_Review", "cdate": 1542234404892, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper678/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335778605, "tmdate": 1552335778605, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper678/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryl5fByTjQ", "original": null, "number": 2, "cdate": 1540318482010, "ddate": null, "tcdate": 1540318482010, "tmdate": 1541533782410, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Official_Review", "content": {"title": "Review", "review": "The paper proposes a complex generative framework for image completion (particularly human face completion). It aims at solving the following challenges: 1) complete the human face at both low and high resolution; 2) control the attribute of the synthetic content; 3) without the need of complex post-processing. To achieve so, this paper proposes a progressively attentive GAN to complete face image at high resolution with multiple controllable attributes in a single forward pass without post-processing. Particularly it introduces a frequency-oriented attentive module (FAM) to attend on finer details.  \n\nThe method seems interesting, however it seems to make slight change based on ProGAN (ICLR' 18   https://arxiv.org/abs/1710.10196). Also similar idea could be found in many other papers, e.g., Wang et al. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs, CVPR' 18.  \n\nThe authors should \n1) clarify why this paper makes non-incremental contribution? What are the major novelty compared with these existing works? \n2) why the frequency attention module will yield better results?\n3) Improve the experiment, compared with stronger baselines: consider at least one or two of these state-of-the-art approaches. Also in my opinion model size and training time needs to be compared as well.\n\nAlso the experimental results did not demonstrate better performance of the proposed approach. Why is that?\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper678/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Official_Review", "cdate": 1542234404892, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper678/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335778605, "tmdate": 1552335778605, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper678/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syl6qnpoj7", "original": null, "number": 1, "cdate": 1540246677155, "ddate": null, "tcdate": 1540246677155, "tmdate": 1541533782210, "tddate": null, "forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "invitation": "ICLR.cc/2019/Conference/-/Paper678/Official_Review", "content": {"title": "High quality results but limited novelty. Need more evidence of improvements over previous methods", "review": "This paper proposed a new method for face completion using progressive GANs. The novelty seems very limited compared with previous methods. The results did not significantly outperform previous methods such as CTX in terms of visual quality. In addition, some of the features for the proposed method were not evaluated properly. \n\n1. The frequency attention module is not convincing. The visualization of the attention features look like normal feature in a neural network. Also, in Figure 8, the quality of results with and without FAM look very similar. These 4 images were selected from 3000 test images, but the difference is too small to show the benefit of FAM. \n\n2. In figure 8, it is unclear how the performance changes with each loss. Probably the results without L_bdy, L_rec, L_feat should be analyzed separately. \n\n3.  In figure 6, the results compared to CTX look similar. And the figure is too small to see the details. For example, from row 1, the result by CTX seems even better. \n\n4. How many images were used in the user study? Did each subjects evaluate the entire test set 3009 images? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper678/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "High Resolution and Fast Face Completion via Progressively Attentive GANs", "abstract": "Face completion is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of \"holes\" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes. We design a novel coarse-to-fine attentive module network architecture. Our model is encouraged to attend on finer details while the network is growing to a higher resolution, thus being capable of showing progressive attention to different frequency components in a coarse-to-fine way. We term the module Frequency-oriented Attentive Module (FAM). Our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.54 seconds for images at 1024x1024 resolution. A pilot human study  shows our approach outperforms state-of-the-art face completion methods. The code will be released upon publication. ", "keywords": ["Face Completion", "progressive GANs", "Attribute Control", "Frequency-oriented Attention"], "authorids": ["zchen23@ncsu.edu", "snie@ncsu.edu", "tianfu_wu@ncsu.edu", "healey@ncsu.edu"], "authors": ["Zeyuan Chen", "Shaoliang Nie", "Tianfu Wu", "Christopher G. Healey"], "pdf": "/pdf/1b988167bd0e3e1fd0cc277c281cbc870ca8e46a.pdf", "paperhash": "chen|high_resolution_and_fast_face_completion_via_progressively_attentive_gans", "_bibtex": "@misc{\nchen2019high,\ntitle={High Resolution and Fast Face Completion via Progressively Attentive {GAN}s},\nauthor={Zeyuan Chen and Shaoliang Nie and Tianfu Wu and Christopher G. Healey},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkxx3o0qFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper678/Official_Review", "cdate": 1542234404892, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkxx3o0qFX", "replyto": "Hkxx3o0qFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper678/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335778605, "tmdate": 1552335778605, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper678/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}