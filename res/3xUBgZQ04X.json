{"notes": [{"id": "3xUBgZQ04X", "original": "BVdLB0QTxQ", "number": 1914, "cdate": 1601308210942, "ddate": null, "tcdate": 1601308210942, "tmdate": 1614985739945, "tddate": null, "forum": "3xUBgZQ04X", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "VpoUZGidlO", "original": null, "number": 1, "cdate": 1610040398334, "ddate": null, "tcdate": 1610040398334, "tmdate": 1610473993849, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors propose the Bures metric (a distance between covariance matrices of the last feature layer of a discriminator) as an extra loss to mitigate mode collapse. The metrics bears some similarity to the covariance term in FID, and builds upon a number of GAN papers that augment GAN losses with differences in covariances between real and generated data. As the reviewers noted, the authors did an admirable job of performing an apples-to-apples comparison with other GAN alternatives, and use a number of metrics to demonstrate their results. Unfortunately, the most extensive comparisons usedthe DCGAN architecture, which is now 2-3 years too old for a potential reader to ascertain how well the proposed method would work on her problem. Moreover, the reviewers identified discrepancies in the baselines of those the experiments, as the numbers reported in this paper seemed to indicate poorer performance and the numbers reported in the original papers.\n\nDuring the rebuttal phase, the authors demonstrated that these methods also perform well with using ResNet architectures on CIFAR-10 and STL-10, and the method is competitive with more recent models. As noted by the reviewers, however, these new comparisons are not as extensive and controlled as those that used DCGAN. Furthermore, results on more difficult datasets, such as ImageNet, are missing.\n\nHad the extensive experiments used ResNets instead of DCGAN, or if the authors demonstrated promising results on ImageNet, I would recommend acceptance. Unfortunately, I think the audience for this paper in 2020-2021 would be relatively limited, so I have to recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040398320, "tmdate": 1610473993833, "id": "ICLR.cc/2021/Conference/Paper1914/-/Decision"}}}, {"id": "nArCN7vUuag", "original": null, "number": 1, "cdate": 1603571552079, "ddate": null, "tcdate": 1603571552079, "tmdate": 1607108190571, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review", "content": {"title": "Review on THE BURES METRIC FOR TAMING MODE COLLAPSE IN GENERATIVE ADVERSARIAL NETWORKS (revised)", "review": "\n\nSummary: This paper proposes to use the squared Bures distance in discriminator feature space to match the generated and real distributions. This proposed method does not require any modification to the network architectures, and is easy to implement. The proposed method produces good empirical results with simple generator architectures in synthetic and real datasets. \n\nReason for score: While I find strong interest in the proposed method, the way in which it is presented in this paper does not permit me to fairly judge the merits of this method. The experiments are not thorough and the quality of writing is subpar. Thus, I vote for reject on this paper.\n\nPros. \nThis method introduced in this paper is theoretically sound. The idea of imposing additional distribution matching in the feature space of the discriminator is an interesting direction that should warrant more studies. Extending beyond the current scope, the use of the adversarial network as both a discriminator and a feature extractor has great potential as a research direction.\n\nCons\n1. The choice of synthetic experiments may be too easy to discern difference between methods. Most GANs get similar results on the 2D grid experiments as the bottleneck is mostly in the modelling capacity of the generator at expressing non-linearities. Previous works have also achieved perfect mode coverage on stacked-mnist with similar network architecture (https://arxiv.org/pdf/1712.04086.pdf). \n2.The Bures metric is one among many metrics for comparing covariance matrices. Ablation on the choice of distance metric should be performed. \n3. Higher performing methods with similar network capacities have been left out in the performance tables. This is counter-productive to the purpose of establishing context. Very few recent methods evaluate DCGAN on cifar10 or STL as the architecture is too limiting for these complex datasets. Only inception scores are reported on resnet experiments. Thorough experimentation and evaluation with a modern architecture on RGB datasets would greatly help the case for this paper.\n\n4. The writing in this paper requires significant rework to reach publication quality. Grammatical mistakes and convoluted sentences are too frequent and significantly detracts from the idea being presented.\n\nMinor comments\n\u201c...images, although, the training...\u201d run-on sentence\n\u201c; and a discriminator\u201d what follows a semicolon must be a full sentence.\n\u201cissue \u2013 the \u2018mode collapse\u2019 \u2013 appears\u201d -> remove \u201cthe\u201d\n\u201ccomplemented by a additional term\u201d -> \u201can additional term\u201d; I stopped tracking grammar mistakes after this point.\n\u201cMDGAN (Li et al., 2017),\u201d -> MMD-GAN, MDGAN is (Che et al., 2017).\nIn table 1, does \u201ctime\u201d column correspond to total training time of 25k iterations? Why is BuresGAN so much slower than MDGAN here but faster in iteration time according to appendix D table 16?\n\n[Post rebuttal]\nI've done a complete re-read of the updated manuscript. I am not convinced with the efficacy of the proposed method in solving mode drops beyond what has already been achieved in the literature. In particular, there is no guarantee that matching covariances within the feature space of a NN prevents mode dropping, as the NN (discriminator) itself must abstract away visual information to perform discrimination. The quality of writing has not improved significantly either. I am now more confident with my original assessment. \n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108038, "tmdate": 1606915772110, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1914/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review"}}}, {"id": "DuO35RmK-ye", "original": null, "number": 4, "cdate": 1603882656895, "ddate": null, "tcdate": 1603882656895, "tmdate": 1606735656962, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review", "content": {"title": "The experiments should be designed and evaluated thoroughly", "review": "*Update after reading the authors' rebuttal:\nThe revision of the manuscript was much improved. However, the lack of controlled experiments does not convince me. This paper proposes a new penalty to deal with mode collapse, and the authors claimed that it could be easily added to any existing GAN variants. The authors may need to provide a controlled experiment to show their claim, e.g., what happens if adding their penalty to some GAN variants, fixing the same setting. \nOne good point from the new revision is that BuresGAN can be competitive with the state-of-the-art baselines, with some slight changes in the network architectures. I suggest the authors to test their penalty with other GAN variants to stronger support their claim.\n\n--------------------------------------------\nThis paper concerns the problem of mode collapse in Generative Adversarial Networks (GANs). A new measure (Bures distance) is investigated to overcome mode collpase in GANs. Bures distance can help us to measure the similarity of two possitive semi-definite matrices and was proposed before. This paper proposes a penalty to the generator loss to encourage the diversity of fake data to match the diversity of real data. To do this, the last layer (providing the representation for each input) of the discriminator is used to define the diversity in input. Bures distance is then used to define the similarity between the diversity of real data with that of fake data, leading to the novel method called BuresGAN. Four datasets are used for their evaluation and comparison with 7 baselines. The experimental results seem to be promising.\n\nPros:\n- Bures distance is an interesting metric and promising to deal with mode collapse.\n- The experimental results are promising.\n\nCoins:\n- Unclear context: mode collapse is a challenging problem. There exist various approaches to deal with this problem, such as using more generators, more discriminators, using different losses, or penalty. However, this paper does not provide an extensive overview of the existing literature on mode collapse. As a result, it is unclear about the context of this paper and the significance of their contributions. The authors should provide an extensive summary and then place their work in a clear context.\n- Unclear significance: The authors use different network architectures for different methods in their experiments, e.g., MDGAN often uses architectures which are different from other methods. Such different architectures make us hard to see which components (e.g. architecture, loss, penalty) really contribute to the success/failure of a method. Also, it is unclear whether the good performance of BuresGAN comes from the new penalty or not. The authors should design a controlled experiment to see the practical effect of their new penalty compared with other penalties, such as by fixing the shared architectures for generator and discriminator and their losses.\n- Baselines: some other types of baselines for dealing mode collapse should be included, e.g. multi-generator or multi-discriminator based methods. Such a comprison will provide more evidents to see the significance of the proposed penalty. Some examples are MAD-GAN [Ghosh et al., 2018] and D2GAN [Nguyen et al., 2017].\n\nMinor comment:\n- The results of some baselines, e.g., MDGAN, VEEGAN, UnrolledGAN, are sometimes not very good as reported in Table 3. Why this happened? Was it because of the use of default settings or unconvergence when training? \n\nReference:\n- Nguyen, T., Le, T., Vu, H., & Phung, D. (2017). Dual discriminator generative adversarial nets. In Advances in Neural Information Processing Systems (pp. 2670-2680).\n- Ghosh, A., Kulharia, V., Namboodiri, V. P., Torr, P. H., & Dokania, P. K. (2018). Multi-agent diverse generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8513-8521).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108038, "tmdate": 1606915772110, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1914/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review"}}}, {"id": "Ef-s870nQL", "original": null, "number": 3, "cdate": 1603843132084, "ddate": null, "tcdate": 1603843132084, "tmdate": 1606242246610, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review", "content": {"title": "Lack of experimental comparisons to the state-of-the-art methods on mode collapse", "review": "Update:\n\nThank the authors for providing the additional results and updating the paper. Since the comparison to one state-of-the-art method has been added (though some other state-of-the-arts are still missing) and the benefit is shown across different settings, I increase the score from 5 to 6.\n\n(One suggestion: I would recommend you to highlight the changes in the revision with a different color, so that readers can identify the changes easily.)\n\n----\n----\nThe paper addresses the mode collapse issue in GANs. More specifically, the paper proposes to add a regularization which matches the Bures distance between the covariance matrices of the features of real and generated data. The paper demonstrates the performance across different datasets and architectures.\n\nOverall, the paper has several merits:\n* Experiments across a wide range of datasets and architectures.\n* A detailed comparison of training time.\n\nHowever, I cannot recommend this work for acceptance at this point, mainly because the paper did not compare with the state-of-the-art (and some widely-used) methods for fighting mode collapse, and the improvements on the benchmark datasets are rather weak. The details are below.\n* The paper does compare with several GANs on the standard benchmark datasets for evaluating mode collapse (e.g. 2D-grid, 2D-ring, stacked MNIST). However, the baselines are rather weak and old. Even in that case, the scores of (Alt-)BuresGAN in Table 1 and Table 2 are just similar to (or sometimes even worse than) those baselines. In fact, there are many other newer and/or better methods that have shown to be outperforming the baselines the paper considered by a large margin, and some of them are already been out for years (e.g. [1,2,3,4]). But the paper did not compare with those methods. Therefore, it is unclear at all whether (Alt-)BuresGAN is useful or not.\n* More specifically, on the stacked MNIST dataset, for example, many methods can dramatically improve the DCGAN baseline from ~100 modes (a relatively poor score) to 1000 modes (the best possible score on this dataset) [1,3,4]. The experiments in your paper seem to build on a better DCGAN architecture (993.3 modes already). But even in that case, (Alt-)BuresGAN only achieves 995.0 modes, and in fact, this score is within std to the DCGAN baseline. Also, on 2D-grid and 2D-ring datasets, some methods can achieve much better results and improvements than yours both quantitatively and qualitatively [2]. I understand that possibly the hyper-parameters and architectures in (Alt-)BuresGAN and those papers are different, so we cannot directly conclude that (Alt-)BuresGAN is worse than [1,2,3,4]. But these results do raise critical concerns about the performance of (Alt-)BuresGAN in fighting mode collapse, compared with the state-of-the-art methods.\n\nBesides this point, I also have some other questions/suggestions:\n* You use the features from the last layer of the discriminator. Why do you choose the last layer? How the performance would be if you are using other layers?\n* \"Algorithmic details\" paragraph on page 4: the regularization term is already mentioned before, so you might consider removing it here.\n* In the same paragraph, you might want to move \"In the tables hereafter, we indicate the largest scores in bold if they differ from lower scores by at least one std\" to the experimental section, because it is for result presentation, not for your algorithm.\n* In fact, it is unclear to me if you are using this rule to mark the bold numbers. For example, in table 1, 84(6) and 22.9(4) shouldn't be marked as bold according to this rule. The same problem exists in all other tables in the paper. To me, how you mark the bold numbers seems random.\n\nIn conclusion, the paper does have some merits, but also have some critical problems, especially lacking the comparisons to the state-of-the-art methods, which makes it hard to judge the contribution of the method. **However, I am happy to adjust the scope if the authors can provide evidence regarding comparisons to the state-of-the-art methods during the rebuttal.**\n\n[1] Lin, Zinan, et al. \"Pacgan: The power of two samples in generative adversarial networks.\" Advances in neural information processing systems. 2018.\n\n[2] Xiao, Chang, Peilin Zhong, and Changxi Zheng. \"Bourgan: Generative networks with metric embeddings.\" Advances in Neural Information Processing Systems. 2018.\n\n[3] Belghazi, Mohamed Ishmael, et al. \"Mine: mutual information neural estimation.\" arXiv preprint arXiv:1801.04062 (2018).\n\n[4] Eghbal-zadeh, Hamid, Werner Zellinger, and Gerhard Widmer. \"Mixture density generative adversarial networks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108038, "tmdate": 1606915772110, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1914/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review"}}}, {"id": "BTMoHeQebZW", "original": null, "number": 10, "cdate": 1606242154672, "ddate": null, "tcdate": 1606242154672, "tmdate": 1606242154672, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "I6HoBUuxWmi", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Thanks for the revision.", "comment": "Thank the authors for providing the additional results and updating the paper. Since the comparison to one state-of-the-art method has been added (though some other state-of-the-arts are still missing) and the benefit is shown across different settings, I increase the score from 5 to 6.\n\n(One suggestion: I would recommend you to highlight the changes in the revision with a different color, so that readers can identify the changes easily.)"}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "kULh6dg-XY", "original": null, "number": 9, "cdate": 1606238624139, "ddate": null, "tcdate": 1606238624139, "tmdate": 1606239494436, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Overview of changes in the new version of the paper", "comment": "The paper has been updated according to the feedback received by the reviewers.\nAn overview of the changes can be found below.\n\nMajor changes:\n\n- High quality generation (CIFAR-10 and STL-10): we added the FID scores to our results for the resnet experiments. For the STL-10 dataset, we noticed that the scores reported by the other methods had been obtained for a rescaled size of 48x48x3. Therefore, we ran our simulations again for this setting and modified the corresponding entries in the table. To the best of our knowledge, the FID scores obtained by BuresGAN improve over the results available in the literature for a Resnet architecture on STL-10.\n\n- In order to compare with a more recent method advised by most of the reviewers, we repeated our experiments for evaluating mode collapse with PacGAN. The upshot is that PacGAN indeed achieves good results although BuresGAN is often competitive or yields better results. The tables were modified accordingly.\n\n- There was no significant winner in the original stacked MNIST experiment. Therefore a similar experiment, following the PacGAN and VEEGAN paper, with a more challenging architecture which includes 4 convolutional layers for both the generator and discriminator is included. BuresGAN outperforms PACGAN and other direct competitors, only  WGAN-GP is capable of getting a better result. However note that our empirical results indicate that WGAN-GP is sensitive to the choice of architecture and hyperparameters while its training time is significantly longer. This experiment further confirms that BuresGAN is very consistent in performance over different architectures.\n\nMinor changes:\n\n- The grammar was rechecked and several sentences were rephrased to improve the readability.\n\n- To avoid any confusion, in all tables, bold values now indicate the best scores.\n\n\nThanks again to all the reviewers for their feedback."}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "I6HoBUuxWmi", "original": null, "number": 8, "cdate": 1605810096677, "ddate": null, "tcdate": 1605810096677, "tmdate": 1605810096677, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "GDsDqzzzKtA", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Thank you for the clarifications and corrections!", "comment": "Thank you for the detailed clarifications and corrections! (Just one more minor correction: I think VEEGAN is using uniform[-1,1], if you look at the history of the public DCGAN repo at the time the paper was published).\n\nI guess the conclusion of this discussion is that on stacked MNIST dataset, you are using different architectures and/or hyperparameters compared with prior work, which makes it easier for the GAN baseline. This is fine, again, as long as you can provide experimental comparisons to the state-of-the-arts (which you said you will add). Looking forward to the results :) "}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "GDsDqzzzKtA", "original": null, "number": 7, "cdate": 1605797491410, "ddate": null, "tcdate": 1605797491410, "tmdate": 1605799131615, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "cnyDnZnAj3g", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Clarification of parameters", "comment": "Thank you pointing out a confusion aspect of the paper and for comparing the original implementations.\n\nFor the stacked MNIST results, our implementation is based on the architecture given in the GDPP GAN paper. Only the learning rate of the optimizer (and $\\beta_2$) and the latent distribution differ. For GDPP and BuresGAN, both the discriminator and generator have 3 convolutional layers. In the main part of our paper, we report these results in Table 2 for a discriminator with 3 conv layers.\n\nTable 18 in our appendix also gives results for a discriminator with only 2 convolutional layers cfr. the architecture described in Table 8. This might be the cause of the confusion. Our code allows to use any number of conv layers for the discriminator. We will add a Table in the appendix to also describe the architecture with 3 conv layers, in order to remove the ambiguity.\n\nThe discriminator and generator of VEEGAN and PacGAN both have 4 convolutional layers and are therefore more complicated.\n\nCorrection: it is then true that we do not have exactly the same architecture as VEEGAN and PacGAN as you correctly point out. Indeed, these differences in architecture and hyperparameters cause the difference in the number of recovered modes.\n\nTo avoid confusion, the hyperparameters and architectures of the stacked MNIST experiment performed by the different methods are given below in a table, which represents our best efforts at providing a comparison between the different parameters used.\n\n| Parameters           | BuresGAN               | PacGAN                 | VEEGAN                 |GDPP                              |\n|----------------------|:------------------------:|:------------------------:|:------------------------:|:-----------------------------------:|\n| learning rates       | 1 x 10^(-3)            | 2 x 10^(-4)            | 2 x 10^(-4)            | 1 x 10^(-4)  **      |\n| learning rate decay  | no                     | no                     | no                     | no **                |\n| Adam beta_1          | 0.5                    | 0.5                    | 0.5                    | 0.5                               |\n| Adam beta_2          | 0.999                  | 0.999                  | 0.999                  | 0.9                               |\n| iterations           | 25000                  | 20000                  | ?                      | 15000*                            |\n| disc. conv. layers   | 2 / 3                  | 4                      | 4                      | 3                                 |\n| gen. conv. layers    | 2 / 3                  | 4                      | 4                      | 3                                 |\n| z dim.               | 100                    | 100                    | 100                    | 128                               |\n| batch size           | 64 / 128 / 256         | 64                     | 64                     | 64                                |\n| evaluation samples   | 10000                  | 26000                  | 26000                  | 26000                             |\n| z dist.              | normal                 | uniform[-1,1]          | normal                 | uniform[-1,1]                     |\n\n \nHyperparameters for the Stacked MNIST experiments as reported in the corresponding papers and code repositories. The GDPP paper used 30000 iterations for training DCGAN and unrolled GAN (indicated by *). **  means as found on Github.\n\nWe agree that the differences in Table 2 for stacked MNIST are not always significant. Many methods can retrieve almost all modes on stacked MNIST."}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "cnyDnZnAj3g", "original": null, "number": 6, "cdate": 1605726282099, "ddate": null, "tcdate": 1605726282099, "tmdate": 1605728264915, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "EPJH7Y9KtVx", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Doubts", "comment": "Thank you for the response!\n\n* **You said you \"we re-use the same architecture as in VEEGAN, GDPP\". I highly doubt if that's true.**\n    * According to the VEEGAN paper (footnote 3), they are using the public implementation of DCGAN (https://github.com/carpedm20/DCGAN-tensorflow). But according to Table 8 in your paper, you are using a different architecture.\n    * In VEEGAN paper, the number of recovered modes for the GAN baseline is 99. In your paper, your GAN baseline recovers over 990 modes. These are clearly coming from different architectures/hyperparameters.\n    * I don't think you are using the same architecture as GDPP either. I checked both the code and architecture description in GDPP and your paper, and they are different (e.g. the number of convolution layers in the discriminator).\n    * In GDPP paper, the number of recovered modes for the GAN baseline is 427. In your paper, your GAN baseline recovers over 990 modes. These are clearly coming from different architectures/hyperparameters.\n\n* **You said PacGAN uses a simpler DCGAN architecture and makes the stacked MNIST experiment artificially harder. HOWEVER,**\n    * According to PacGAN's released code, PacGAN uses exactly the same architecture as VEEGAN, based on the public implementation of DCGAN (https://github.com/carpedm20/DCGAN-tensorflow).\n    * According to the description in the papers, MINE and Mixture Density GANs also use the same architecture as PacGAN & VEEGAN.\n    * Even in this hard setting as you said, PacGAN, MINE, and Mixture Density GANs can recover all 1000 modes, which demonstrates their performance.\n\n* From the above, it seems to me that you are making the stacked MNIST experiment artificially simpler, compared with what have been used in **many** prior papers.\n\n* **As I already mentioned in my initial review**, I totally agree that we cannot directly compare the results across papers and draw conclusions from them. However, since you didn't compare with many state-of-the-arts **(as both other reviewers and I pointed out)**, I have to use other ways to get a sense/guess of how Bures GAN performs. The point I was trying to say is that the improvement of Bures GAN on Gaussian mixture datasets and stacked MNIST datasets is relatively very small, even if the setting is simple as you agree.\n\n* Looking forward to seeing the results!\n\nPlease free to comment/discuss/correct me if I miss something!"}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "EPJH7Y9KtVx", "original": null, "number": 5, "cdate": 1605701271237, "ddate": null, "tcdate": 1605701271237, "tmdate": 1605701271237, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "Ef-s870nQL", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Initial comments and clarification", "comment": " Thank you for your comments. \n\nIndeed, mixture density GANs are interesting. By nature, they are particularly suited to the mixture of Gaussians used in ring/grid examples. However, BuresGAN could also be considered in combination with mixture models, and therefore we view these two contributions as complementary. Additionally, we did not compare with many methods using a multiple generators and/or discriminator architecture since we simply propose a different objective function. \nFor a fixed architecture, BuresGAN improves over the other cited methods, while it is also robust as we vary the networks' architectures.\n\nIn order to have another more recent benchmark, we will add PacGAN to our comparisons, since a majority of the reviewers advised it. We will add some initial results for the PacGAN experiments before the end of the rebuttal.\nIn comparison with our manuscript, in the PacGAN paper, a simpler DCGAN  architecture  gives worse results for the vanilla GAN. In our paper, we do not want to make the stacked MNIST experiment artificially harder by choosing a very simple architecture. **On the contrary, we re-use the same architecture as in VEEGAN, GDPP and other works that we compared with.**\n\nIt is hard to compare with the setting of PacGAN paper, since the associated code repository does not provide the CNN classifier for counting the modes. Furthermore, in PacGAN paper, 26000 test points are generated to assess model collapse, while we use 10000 test samples in our manuscript. Therefore, it is not fair to compare the results reported in our paper with PacGAN paper. Indeed, for assessing the number of modes in stacked MNIST experiment, we claim that the relative performance comparison is meaningful given the same experimental settings, while cross-paper comparisons should be avoided. Finally, the packing approach can be also applied to any existing GAN, so that one could also consider a packed BuresGAN.\n\nWe use the final layer because we assume that this final layer will give the most relevant features because that is the one used for the final classification. \nConcerning the bold numbers, thank you for spotting this mistake. We will make it more consistent\n\nAdditionally, if there are any other remarks we would be happy to discuss them in this forum."}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "qQXohQ8t_f", "original": null, "number": 4, "cdate": 1605640975545, "ddate": null, "tcdate": 1605640975545, "tmdate": 1605640975545, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "nArCN7vUuag", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Initial comments and clarification", "comment": "Thanks for your remarks.\n\nSince several reviewers suggested it, PacGAN will be added to our comparisons. Indeed, the PacGAN paper reports a perfect mode coverage in stacked MNIST experiment. The number of modes is computed thanks to a pretrained classifier which is not provided in PacGAN's repository, and therefore differs from the classifier that we use. The number of samples to evaluate as well as the architecture differ which makes a direct comparison impossible.\nWe will add initial results for the PacGAN experiments before the end of the rebuttal.\n\nIt is indeed instructive to add the FID scores for the resnet experiments since FID can also provide some information about generation quality. Our revised manuscript will contain FID scores for the resnet experiments.\n\nConcerning the training times, BuresGAN scales significantly better with respect to the feature map dimensions and data dimensions, although the training times are a bit longer for the lower dimensional synthetic examples. This is the reason behind the different ordering of the timings of the methods in Appendix D Table 16.\n\nThe grammar of our manuscript will be rechecked. Any other suggestions are welcome.\n\nAdditionally, if there are any other remarks we would be happy to discuss them in this forum."}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "0YRVIbSCT8r", "original": null, "number": 3, "cdate": 1605640729044, "ddate": null, "tcdate": 1605640729044, "tmdate": 1605640729044, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "DuO35RmK-ye", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Initial comments and clarification", "comment": "Thank you for your remarks.\n\nConcerning the presentation of the related work, we will try to improve the description of the context in the revised version of our manuscript.\n\nGANs with multiple generators and/or discriminators are certainly of interest. We did not compare with these GANs since their approach is complementary to ours.\nTo introduce another recent benchmark, we will include PacGAN since it has been advised by several reviewers.\nWe will add some initial PacGAN experiment results before the end of the rebuttal.\n\nIt can be noticed from PacGAN paper, that VEEGAN and DCGAN also have very bad performance in some cases. This might be due a sensitivity to the architecture choice. MDGAN and VEEGAN often have problems with non-convergence for the given architecture, and many other architectures that we have tested. It should be mentioned that there were runs where these methods achieved very good results but they were inconsistent, as indicated by the standard deviation.\nThe results in our manuscript were obtained for the parameters proposed in the original papers, while the architecture is the same for all the methods.\n\nAdditionally, if there are any other remarks we would be happy to discuss them in this forum."}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "wZ-cLWZ7rO", "original": null, "number": 2, "cdate": 1605638738651, "ddate": null, "tcdate": 1605638738651, "tmdate": 1605638738651, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "JEFcAOT1OIG", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment", "content": {"title": "Initial comments and clarification", "comment": "Thank you for your comments. \n\nIndeed, we did not compare with mixture density GANs. This is certainly of interest although we consider this modification as being 'orthogonal' to our approach since BuresGAN's objective can be also used in combination with mixture density GANs. Indeed,  Gaussian Mixture Models affect the generation, while our paper discusses the loss function.\nThe aim of our comparison was to illustrate the improvement given by BuresGAN for a `fixed architecture in order to provide a fair comparison. The only changes between the methods given in our paper is possibly the choice of hyperparameters that we set to the values advised in the original papers.\n\nThough IS already indicates the generation quality, we agree that FID scores can be also instructive for assessing the high quality generations of CIFAR 10 and STL 10 data sets (Section 4) since it also relies on the data distribution. We are going to provide FID scores in the revised version of our manuscript.\n\nConcerning the cost of BuresGAN, its training is indeed slower for the synthetic examples compared to some other GANs used in the comparisons. However, BuresGAN scales better with the dimension of the input space and of the feature map. In the case of CIFAR-10 and STL-10, for example, BuresGAN is significantly faster compared to WGAN-GP.\n\nWe are afraid that implementing and running Progressive GAN and NCSN on STL datasets ourselves is not feasible given the time constraints of the ICLR timeline.\n\nThe IvO scores depend on the number of correction pairs and restarts, and the parameters of the optimizer. We emphasize that IvO scores cannot be compared between different papers if the same parameters are not used, although the comparison of the different scores within this manuscript is meaningful.\nIf the reviewer is interested, the full code of the experiments, including the IvO evaluation code, is available in the supplementary material.\n\nAdditionally, if there are any other remarks we would be happy to discuss them in this forum."}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "3xUBgZQ04X", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1914/Authors|ICLR.cc/2021/Conference/Paper1914/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854370, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Comment"}}}, {"id": "JEFcAOT1OIG", "original": null, "number": 2, "cdate": 1603754792063, "ddate": null, "tcdate": 1603754792063, "tmdate": 1605024329918, "tddate": null, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "invitation": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review", "content": {"title": "Interesting ideas but somewhat lacking in experimental evidence ", "review": "The main strengths of the work are,\n* The proposed idea is relatively novel, although similar ideas were explored in (Mroueh et al., 2017; Elfeki et al., 2019).\n* The paper uses a computationally efficient expression (4) for the Bures distance. However, this expression has been proposed in prior work (Oh et al., 2020).\n* The paper discusses connections with Wasserstein GAN and integral probability metrics. In particular, shows that the proposed distance os proportional to the 2-Wasserstein distance.\n* Detailed information on the architectures and datasets is given in the Appendix, aiding reproducibility.\n* The paper is very well written and easy to follow.\n\nMy main concerns are with the evaluation,\n* Experiments on synthetic data: The following two papers outperform the proposed approach [1,2], especially in terms of number of high quality samples on both the Ring and Grid sets.\n* Experiments on CIFAR-10 with DCGAN architecture - Both [1,2] again outperform the proposed approach in terms of the the FID metric. The order of the IvO score also does not seem to match prior works e.g. VEEGAN (Table 2), [1,2]. Please clarify the exact procedure used to compute the IvO scores.\n* Experiments on CIFAR-10/STL-10 using ResNet architecture: The FID scores should be also reported for fair comparison with the state of the art. More importantly: the IS scores of best performing methods on CIFAR-10: ProgressiveGAN (Karras et al., 2017) and NCSN (Song & Ermon, 2019) are to reported for STL-10. It is unclear whether the proposed approach really achieves a new state of the art inception score on STL-10. Comparison with ProgressiveGAN (Karras et al., 2017) and NCSN (Song & Ermon, 2019) on STL-10 must be performed.\n* The cost of the computing the Bures distance in terms of training time in comparison to simpler losses like Hinge loss [3] or gradient penalty loss of WGAN-GP should be clarified. It is unclear whether the additional resources required (if any) justly the limited performance gain of the proposed method.\n\n\n[1] Mixture Density Generative Adversarial Networks, CVPR 2019.\n\n[2] \"Best-of-Many-Samples\" Distribution Matching, NeurIPS Workshop, 2019.\n\n[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2019.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1914/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1914/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks", "authorids": ["~Hannes_De_Meulemeester1", "joachim.schreurs@esat.kuleuven.be", "michael.fanuel@kuleuven.be", "bart.demoor@esat.kuleuven.be", "~Johan_Suykens1"], "authors": ["Hannes De Meulemeester", "Joachim Schreurs", "Micha\u00ebl Fanuel", "Bart De Moor", "Johan Suykens"], "keywords": ["Generative Adversarial Networks", "Deep Learning", "Neural Networks"], "abstract": "Generative Adversarial Networks (GANs) are performant generative methods yielding high-quality samples. However, under certain circumstances, the training of GANs can lead to mode collapse or mode dropping, i.e. the generative models not being able to sample from the entire probability distribution. To address this problem, we use the last layer of the discriminator as a feature map to study the distribution of the real and the fake data. During training, we propose to match the real batch diversity to the fake batch diversity by using the Bures distance between covariance matrices in feature space. The computation of the Bures distance can be conveniently done in either feature space or kernel space in terms of the covariance and kernel matrix respectively. We observe that diversity matching reduces mode collapse substantially and has a positive effect on the sample quality. On the practical side, a very simple training procedure, that does not require additional hyperparameter tuning, is proposed and assessed on several datasets.  ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "meulemeester|the_bures_metric_for_taming_mode_collapse_in_generative_adversarial_networks", "supplementary_material": "/attachment/9791d963cd9ef45a7292afaf1b4e9c6203278972.zip", "pdf": "/pdf/cb90e54faca93cd35dda39dee85584b282b2acff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=eD-yCTFv-l", "_bibtex": "@misc{\nmeulemeester2021the,\ntitle={The Bures Metric for Taming Mode Collapse in Generative Adversarial Networks},\nauthor={Hannes De Meulemeester and Joachim Schreurs and Micha{\\\"e}l Fanuel and Bart De Moor and Johan Suykens},\nyear={2021},\nurl={https://openreview.net/forum?id=3xUBgZQ04X}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "3xUBgZQ04X", "replyto": "3xUBgZQ04X", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1914/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108038, "tmdate": 1606915772110, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1914/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1914/-/Official_Review"}}}], "count": 15}