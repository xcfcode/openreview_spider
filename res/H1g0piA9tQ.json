{"notes": [{"id": "H1g0piA9tQ", "original": "HJgyvUoqYm", "number": 849, "cdate": 1538087877629, "ddate": null, "tcdate": 1538087877629, "tmdate": 1545355436488, "tddate": null, "forum": "H1g0piA9tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Evaluation Methodology for Attacks Against Confidence Thresholding Models", "abstract": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).", "keywords": ["adversarial examples"], "authorids": ["goodfellow@google.com", "yaoqin@google.com", "dberth@google.com"], "authors": ["Ian Goodfellow", "Yao Qin", "David Berthelot"], "TL;DR": "We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding", "pdf": "/pdf/58ee81bbb9ab33424599a4a7ac6ff9642b54721a.pdf", "paperhash": "goodfellow|evaluation_methodology_for_attacks_against_confidence_thresholding_models", "_bibtex": "@misc{\ngoodfellow2019evaluation,\ntitle={Evaluation Methodology for Attacks Against Confidence Thresholding Models},\nauthor={Ian Goodfellow and Yao Qin and David Berthelot},\nyear={2019},\nurl={https://openreview.net/forum?id=H1g0piA9tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJg3eiqSgE", "original": null, "number": 1, "cdate": 1545083636034, "ddate": null, "tcdate": 1545083636034, "tmdate": 1545354480527, "tddate": null, "forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper849/Meta_Review", "content": {"metareview": "The reviewers agree the paper is not ready for publication. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper849/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper849/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation Methodology for Attacks Against Confidence Thresholding Models", "abstract": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).", "keywords": ["adversarial examples"], "authorids": ["goodfellow@google.com", "yaoqin@google.com", "dberth@google.com"], "authors": ["Ian Goodfellow", "Yao Qin", "David Berthelot"], "TL;DR": "We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding", "pdf": "/pdf/58ee81bbb9ab33424599a4a7ac6ff9642b54721a.pdf", "paperhash": "goodfellow|evaluation_methodology_for_attacks_against_confidence_thresholding_models", "_bibtex": "@misc{\ngoodfellow2019evaluation,\ntitle={Evaluation Methodology for Attacks Against Confidence Thresholding Models},\nauthor={Ian Goodfellow and Yao Qin and David Berthelot},\nyear={2019},\nurl={https://openreview.net/forum?id=H1g0piA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper849/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353062938, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper849/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper849/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper849/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353062938}}}, {"id": "Hye-MhpZT7", "original": null, "number": 3, "cdate": 1541688328663, "ddate": null, "tcdate": 1541688328663, "tmdate": 1541688328663, "tddate": null, "forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper849/Official_Review", "content": {"title": "A good topic to explore, but suffers from methodological problems", "review": "This paper introduces a family of attack on confidence thresholding algortihms. Such algorithms are allowed to refuse to make predictions when their confidence is below a certain threshold. \n\nThere are certainly interesting links between such models and KWIK [1] algorithms (which are also supposed to be able to respond 'null' to queries), however they are not mentioned in this paper, which focuses mainly on evaluation methodologies.\n\nThe definition of the metric is certainly natural: you would expect some trade-off between performance in the normal versus the adversarial regime. I am not certain why the authors don't simply measure the success rate on both natural and adversarial conditions, so as to have the performance metric uniform. Unfortunately the paper's notationleaves something to be desired, as it fails to concretely define the metric.\nLet me do so instead, and consider the classification accuracy of a classification rule $P_t$ using a threshold $t$ under a (possibly adaptive) distribution $Q$ to be $U(P,Q)$. Then, we can consider $Q_N, Q_A$ as the normal and adversarial distribution and measure the corresponding accuracies. \n\nEven if we do this, however, the authors do not clarify how they propose to select the classification rule. Should they employ something like a convex combination:\n\\[\nV(P_t) := \\alpha U(P_t, Q_N) + (1 - \\alpha) U(P_t, Q_A) \n\\]\nor maybe take a nimimax approach\n\\[\nV(P_t) := \\min \\{U(P_t, Q) | Q = Q_A, Q_N\\}\n\\]\n\nIn addition, the authors simply plot curves for various choices of $t$, however it is necessary to take into account the fact that measuring performance in this way and selecting $t$ aftewards amounts to a hyperparameter selection [2]. Thus, the thresholding should be chosen on an independent validation set in order to optimise the chosen performance measure, and then the choice should evaluated on a new test set with respect to the same measure $V$\n\nThe MaxConfidence attack is not very well described, in my opinion. However, it seems it simply wishes to find to find a single point $x \\in \\mathbb{S}$ that maximises the probability of misclassification. It is not clear to me why performance against an attack of this type is interesting to measure.\n\nThe main contribution of the paper seems to be the generalisation of the attack by Goodfellow et al to softmax regression. The proof of this statement is in a rather obscure place in the paper. \n\nI am not sure I follow the idea for the proof, or what they are trying to prove. The authors should follow a standard Theorem/Proof organisation, clearing stating assumptions and what the theorem is showing us. It seems that they want to prove that if a solution to (1) exists, then MaxConfidence() finds it. But the only definition of MaxConfidence is (1). Hence I think that their theorem is vacuous. There are quite a few details that are also unclear such as what the authors mean by 'clean example' etc. \n\nHowever the authors do not explain their attack very well, their definition of the performance metric is not sufficiently formal, and their evaluation methodology is weak. Since evaluation methodology is the central point of the paper, this is a serious weaknes. Finally, there doesn't seem to be a lot of connection with the conference's topic.\n\n[1] Li, Lihong, Michael L. Littman, and Thomas J. Walsh. \"Knows what it knows: a framework for self-aware learning.\" Proceedings of the 25th international conference on Machine learning. ACM, 2008.\n\n[2] Bengio, Samy, Johnny Mari\u00e9thoz, and Mikaela Keller. \"The expected performance curve.\" International Conference on Machine Learning, ICML, Workshop on ROC Analysis in Machine Learning. No. EPFL-CONF-83266. 2005.\n", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper849/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation Methodology for Attacks Against Confidence Thresholding Models", "abstract": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).", "keywords": ["adversarial examples"], "authorids": ["goodfellow@google.com", "yaoqin@google.com", "dberth@google.com"], "authors": ["Ian Goodfellow", "Yao Qin", "David Berthelot"], "TL;DR": "We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding", "pdf": "/pdf/58ee81bbb9ab33424599a4a7ac6ff9642b54721a.pdf", "paperhash": "goodfellow|evaluation_methodology_for_attacks_against_confidence_thresholding_models", "_bibtex": "@misc{\ngoodfellow2019evaluation,\ntitle={Evaluation Methodology for Attacks Against Confidence Thresholding Models},\nauthor={Ian Goodfellow and Yao Qin and David Berthelot},\nyear={2019},\nurl={https://openreview.net/forum?id=H1g0piA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper849/Official_Review", "cdate": 1542234363069, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper849/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335816253, "tmdate": 1552335816253, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper849/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1x0HKfqh7", "original": null, "number": 2, "cdate": 1541183813757, "ddate": null, "tcdate": 1541183813757, "tmdate": 1541533639349, "tddate": null, "forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper849/Official_Review", "content": {"title": "Hard to understand", "review": "This paper proposes an evaluation method for confidence thresholding defense models, as well as a new approach for generating of adversarial examples by choosing the wrong class with the most confidence when employing targeted attacks.\n\nAlthough the idea behind this paper is fairly simple, the paper is very difficult to understand.  I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method. Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2. However, in Figure 2, it is used for evaluating defense schemes. Again, this confuses me on what is the main topic of this paper. Indeed, why the commonly used attack success ratio or other similar measures cannot be used in the case? Intuitively, it should provide similar results to the success-failure curve.\n\nThe paper also lacks experimental results, and the main conclusion from these results seems to be \"MNIST is not suitable for benchmarking of adversarial attacks\". If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.\n\nMeanwhile, the computational cost on large dataset such as ImageNet could be huge, the authors should further develop the method to make sure it works in all situations.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper849/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation Methodology for Attacks Against Confidence Thresholding Models", "abstract": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).", "keywords": ["adversarial examples"], "authorids": ["goodfellow@google.com", "yaoqin@google.com", "dberth@google.com"], "authors": ["Ian Goodfellow", "Yao Qin", "David Berthelot"], "TL;DR": "We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding", "pdf": "/pdf/58ee81bbb9ab33424599a4a7ac6ff9642b54721a.pdf", "paperhash": "goodfellow|evaluation_methodology_for_attacks_against_confidence_thresholding_models", "_bibtex": "@misc{\ngoodfellow2019evaluation,\ntitle={Evaluation Methodology for Attacks Against Confidence Thresholding Models},\nauthor={Ian Goodfellow and Yao Qin and David Berthelot},\nyear={2019},\nurl={https://openreview.net/forum?id=H1g0piA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper849/Official_Review", "cdate": 1542234363069, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper849/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335816253, "tmdate": 1552335816253, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper849/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygQJELD3X", "original": null, "number": 1, "cdate": 1541002202530, "ddate": null, "tcdate": 1541002202530, "tmdate": 1541533639143, "tddate": null, "forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper849/Official_Review", "content": {"title": "Interesting attack but an unclear paper with limited experimental support.", "review": "The paper presents an evaluation methodology for evaluating attacks on confidence thresholding methods and proposes a new kind of attack. In general I find the writing poor, as it is not exactly clear what the focus of the paper is - the evaluation or the new attack? The experiments lacking and the proposed evaluation methodology & theoretical guarantees trivial.\n\t\nMajor remarks:\n- Linking the code and asking the reviewers not to look seems like bad practice and close to violating double blind, especially when considering that the cleavhans library is well known. Should have just removed the link and cleavhans name and state it will be released after review. \n\n- It is unclear what the focus of the paper is, is it the evaluation methodology or the new attack? While the evaluation methodology is presented as the main topic in title, abstract and introduction most of the paper is dedicated to the attack.\n\n- The evaluation methodology is a good idea but is quiet trivial. Also, curves are nice visually but hard to compare between close competitors. A numeric value like area-under-the-curve should be better.\n\n- The theoretical guarantees is also quiet trivial, more or less saying that if a confident adversarial attack exists then finding the most confident attack will be successful. Besides that the third part of the proof can be simplified significantly.\n\n- The experiments are very lacking. The authors do not compare to any other attack so there is no way to evaluate the significance of their proposed method\n\n- That being said, the max-confidence attack by itself sounds interesting, and might be useful even outside confidence thresholding.\n\n- One interesting base-line experiment could be trying this attack on re-calibrated networks e.g. \u201cOn Calibration of Modern Neural Networks\u201d \n\n- Another baseline for comparison could be doing just a targeted attack with highest probability wrong class.\n\n- I found part 4.2 unclear \n\n- In the conclusion, the first and last claims are not supported by the text in my mind. \n\n\n\nMinor remarks:\n\n- The abstract isn\u2019t clear jumping from one topic to the next in the middle without any connection.\n\n- Having Fig.1 and 2 right on the start is a bit annoying, would be better to put in the relevant spot and after the terms have been introduced.\n\n-In 6.2 the periodically in third line from the end seems out of place.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper849/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation Methodology for Attacks Against Confidence Thresholding Models", "abstract": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).", "keywords": ["adversarial examples"], "authorids": ["goodfellow@google.com", "yaoqin@google.com", "dberth@google.com"], "authors": ["Ian Goodfellow", "Yao Qin", "David Berthelot"], "TL;DR": "We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding", "pdf": "/pdf/58ee81bbb9ab33424599a4a7ac6ff9642b54721a.pdf", "paperhash": "goodfellow|evaluation_methodology_for_attacks_against_confidence_thresholding_models", "_bibtex": "@misc{\ngoodfellow2019evaluation,\ntitle={Evaluation Methodology for Attacks Against Confidence Thresholding Models},\nauthor={Ian Goodfellow and Yao Qin and David Berthelot},\nyear={2019},\nurl={https://openreview.net/forum?id=H1g0piA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper849/Official_Review", "cdate": 1542234363069, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1g0piA9tQ", "replyto": "H1g0piA9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper849/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335816253, "tmdate": 1552335816253, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper849/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1xAN6UJT7", "original": null, "number": 2, "cdate": 1541528886234, "ddate": null, "tcdate": 1541528886234, "tmdate": 1541528886234, "tddate": null, "forum": "H1g0piA9tQ", "replyto": "S1x0HKfqh7", "invitation": "ICLR.cc/2019/Conference/-/Paper849/Official_Comment", "content": {"title": "Reply", "comment": "The main topic of the paper is how to evaluate models that use confidence thresholding. The primary purpose is to compare *defenses*. However, to justify the attack strategy that we propose to use, we also compare *attacks*. Specifically, we provide an experiment demonstrating that our attack actually is stronger than the baseline. However, it is not really necessary to provide multiple experiments demonstrating that MaxConfidence is more powerful because the superiority of MaxConfidence is theoretically guaranteed."}, "signatures": ["ICLR.cc/2019/Conference/Paper849/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper849/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper849/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluation Methodology for Attacks Against Confidence Thresholding Models", "abstract": "Current machine learning algorithms can be easily fooled by adversarial examples. One possible solution path is to make models that use confidence thresholding to avoid making mistakes. Such models refuse to make a prediction when they are not confident of their answer. We propose to evaluate such models in terms of tradeoff curves with the goal of high success rate on clean examples and low failure rate on adversarial examples. Existing untargeted attacks developed for models that do not use confidence thresholding tend to underestimate such models' vulnerability. We propose the MaxConfidence family of attacks, which are optimal in a variety of theoretical settings, including one realistic setting: attacks against linear models. Experiments show the attack attains good results in practice. We show that simple defenses are able to perform well on MNIST but not on CIFAR, contributing further to previous calls that MNIST should be retired as a benchmarking dataset for adversarial robustness research.  We release code for these evaluations as part of the cleverhans (Papernot et al 2018) library  (ICLR reviewers should be careful not to look at who contributed these features to cleverhans to avoid de-anonymizing this submission).", "keywords": ["adversarial examples"], "authorids": ["goodfellow@google.com", "yaoqin@google.com", "dberth@google.com"], "authors": ["Ian Goodfellow", "Yao Qin", "David Berthelot"], "TL;DR": "We present metrics and an optimal attack for evaluating models that defend against adversarial examples using confidence thresholding", "pdf": "/pdf/58ee81bbb9ab33424599a4a7ac6ff9642b54721a.pdf", "paperhash": "goodfellow|evaluation_methodology_for_attacks_against_confidence_thresholding_models", "_bibtex": "@misc{\ngoodfellow2019evaluation,\ntitle={Evaluation Methodology for Attacks Against Confidence Thresholding Models},\nauthor={Ian Goodfellow and Yao Qin and David Berthelot},\nyear={2019},\nurl={https://openreview.net/forum?id=H1g0piA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper849/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624786, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1g0piA9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper849/Authors", "ICLR.cc/2019/Conference/Paper849/Reviewers", "ICLR.cc/2019/Conference/Paper849/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper849/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper849/Authors|ICLR.cc/2019/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper849/Reviewers", "ICLR.cc/2019/Conference/Paper849/Authors", "ICLR.cc/2019/Conference/Paper849/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624786}}}], "count": 6}