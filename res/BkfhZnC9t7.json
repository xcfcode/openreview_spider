{"notes": [{"id": "BkfhZnC9t7", "original": "BkxXl7hcYm", "number": 1210, "cdate": 1538087940078, "ddate": null, "tcdate": 1538087940078, "tmdate": 1545355381719, "tddate": null, "forum": "BkfhZnC9t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BygSwQO-g4", "original": null, "number": 1, "cdate": 1544811356956, "ddate": null, "tcdate": 1544811356956, "tmdate": 1545354527873, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Meta_Review", "content": {"metareview": "This paper studies the really hard problem of zero-shot learning in acoustic modeling for languages with limited resources, using data from English. Using a novel universal phonetic model, the authors show improvements compared to using an English model for 20 other languages in phone recognition quality.\n\nStrengths\n- Reviewers agree that the problem is an important one, and the presented ideas are novel.\n- Universal phonetic model to represent phones in any language is interesting.\n\nWeaknesses\n- The results are really weak, to the point that it is unclear how effective or general the techniques are. The work is an interesting first step, but is not developed enough to be accepted at this point.\n- The universal phonetic model being trained only in English might affect generalizability to languages that do not share phonetic characteristics. The authors agree partly, and argue that the method already addresses some issues since the model can already represent unseen phones. But, coupled with the high phone error rates, it is still unclear how appropriate the technique will be in addressing this issue.\n- Novelty: Although the idea of mapping phones to attributes, and using those for ASR is not novel (e.g., using articulatory features), application for zero-shot learning is. The work assumes availability of a small text corpus to learn phone-sequence distribution, so is similar to other zero-resource approaches that assume some data (audio, as opposed to text) is available in the new language.\n\nThis paper presents interesting first steps, but lacks sufficient experimental validation at this point. Therefore, AE recommendation is to reject the paper. I encourage the authors to improve and resubmit in the future.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Good first step, but error rates are too high"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1210/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352923842, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352923842}}}, {"id": "H1e4b3x9hQ", "original": null, "number": 3, "cdate": 1541176315843, "ddate": null, "tcdate": 1541176315843, "tmdate": 1544712915761, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Review", "content": {"title": "Review", "review": "This paper presents an approach to address the task on zero-shot learning for speech recognition, which consist of learning an acoustic model without any resources for a given language. The universal phonetic model is proposed, which learns phone attributes (instead of phone label), which allows to do prediction on any phone set, i.e. on any language. The model is evaluated on 20 languages and is shown to improve over a baseline trained only on English.\n\nThe proposed UPM approach is novel and significant: being able to learn a more abstract representation for phones which is language-independent is a very promising lead to handle the problem of ASR on languages with low or no resources available. \n\nHowever, the results are the weak point of the paper. While the results demonstrate the viability of the approach, the gain between the baseline performance and the UPM model is quite small, and it's still far from being usable in practice. \n\nTo improve the paper, the authors should discuss the future work, i.e. what are the next steps to improve the model.\n\nOverall, the paper is significant and can pave the way for a new category of approaches to tackle zero-shot learning for speech recognition. Even if the results are not great, as a first step they are completely acceptable, so I recommend to accept the paper.\n\nRevision:\nThe approach of using robust features is interesting and promising, as well as the idea of training on multiple languages. Overall, the authors response addressed most of the issues, therefore I am not changing my rating.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Review", "cdate": 1542234280370, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335896893, "tmdate": 1552335896893, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgHrFCT0X", "original": null, "number": 12, "cdate": 1543526716555, "ddate": null, "tcdate": 1543526716555, "tmdate": 1543526716555, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "HJlUHCjo0m", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for updating your rate. After your rating update on 26th Nov we edited the paper and also added in some more discussions regarding the points from your comments that we may have missed before."}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "HJlUHCjo0m", "original": null, "number": 10, "cdate": 1543384638264, "ddate": null, "tcdate": 1543384638264, "tmdate": 1543384638264, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "SkxEjxzm0X", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Update", "comment": "Based on the rebuttal I've changed my rating from 4 to 5."}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "Bkx9UfE5Am", "original": null, "number": 8, "cdate": 1543287378132, "ddate": null, "tcdate": 1543287378132, "tmdate": 1543287823621, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "ByeYvTNOhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Paper revision comment for reviewer 2", "comment": "Thanks again for the detailed comments and references! We have tried to answer almost all the detailed comments, suggestions and questions that you mentioned in our revised paper. \n\nIn detail the changes that we made and some comments - \n1. Abstract now mentions what the baseline model is.\n2. Corrected the strong statements mentioned in the introduction with proper justification and added in the missing references. We added a discussion about zero resource speech processing tasks in related work and introduction referencing 1,2,3 and some more work.  \n3. Thanks for pointing out to reference 4 and 5! We have talked about them in detail in related work section.\n4. Language model can be integrated using WFST decoding by assuming that our language model is well resourced and considering the words seen during language model training as the words being predicted during decoding.   \n5. Thanks a lot for pointing out the grammatical errors we have now fixed them.\n6. Yes we agree multilingual bottleneck features would be helpful, but the problem of having \u201cunseen\u201d phones would still exist no matter how many languages we add. This paper focuses on solving this issue where the model has seen only English phones during training. Infact this idea would lead to a better model as it would have better coverage of the phonemes and it would be a good extension towards this paper, hence mentioned as part of future work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "BylKaWEqAm", "original": null, "number": 7, "cdate": 1543287232983, "ddate": null, "tcdate": 1543287232983, "tmdate": 1543287232983, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "HkgGv8a7A7", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for your question. We are running some new experiments with the domain robust acoustic feature. Our initial experiment on a reduced dataset suggests that these features have potential to improve performance by about 5 percent, but due to our computational limitations we could not complete our experiments on the whole dataset and have mentioned it as part of the future work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "BkgCYZV5Cm", "original": null, "number": 6, "cdate": 1543287174347, "ddate": null, "tcdate": 1543287174347, "tmdate": 1543287174347, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "H1e4b3x9hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Paper revision comment for reviewer 1", "comment": "Thanks again for your comments, as suggested we revise the paper and mention a few potential works that can be extended on top of the proposed framework. For instance, label smoothing might be a useful technique to regularize attribute distribution or phone distribution [1], we can also increase the coverage of our phonemes by training the model on more diversity of languages or by training them with better features. \n\n[1] Pereyra, Gabriel, et al. \"Regularizing neural networks by penalizing confident output distributions.\" arXiv preprint arXiv:1701.06548 (2017).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "ByeYvTNOhQ", "original": null, "number": 1, "cdate": 1541061984805, "ddate": null, "tcdate": 1541061984805, "tmdate": 1543213934367, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Review", "content": {"title": "Claims of being first not completely justified", "review": "Overview:\n\nThis paper proposed an approach for zero-shot phoneme recognition, where it is possible to recognise phonemes in a target language which has never been seen before. Rather than just training a phoneme recogniser directly on background data and then applying it to unseen data, phonetic features are first predicted, allowing phonemes not in the source language set to be predicted.\n\nMain strengths:\n\nThe paper's main strength lies in that this is a very unexplored area that could assist in the development of speech technology where it is currently not possible. The proposed model (Section 2) has also not been considered in prior work.\n\nMain weaknesses:\n\nThe paper's main weakness is in some of its claims and that it misses some very relevant literature. Detailed comments together with a minimal list of references are given below (but I would encourage the authors to also read a bit more broadly). But in short I do not think it is that easy to claim that this is the first paper to do zero-shot learning on speech; many of the zero-resource studies where unlabelled audio is used could be seen as doing some for of zero-shot matching. Specifically [5] is able to predict unseen phoneme targets.  Multilingual bottleneck features can be applied to languages that have never been seen before [2], and the output of phoneme recognisers trained on one language have long been applied to get output on another unseen language. The first one-shot learning speech paper [4] (to my knowledge) is also not mentioned at all. The approach in the paper also still relies on some text data from the target language; if this then can be described as \"zero-shot\" learning, then I think many of these previous studies c also make this claim.\n\nOverall feedback:\n\nThere is definitely value in this work, but it should be much better situated within the broader literature. Below I give some editorial suggestions and also outline some suggestions for further experiments.\n\nDetailed comments, suggestions and questions:\n\n- Abstract: It would be useful to have some details of the \"baseline model\" here already, especially since it is such a new task.\n- Introduction: \"... but they can hardly predict phones or words directly due to their unsupervised nature.\" This is a strong statement that maybe requires more justification. On the one hand, the statement is true, and the high word error rates in e.g. [3] can be cited. On the other hand, it has been shown that at the phone-distinction level, these models perform quite well and sometimes outperform supervised models [1]. Since this paper also considers phone error rate as a metric, I think care should be taken with such statements.\n- Introduction: \"While zero-shot learning has attracted a lot of attention in *the* computer vision community, this setup has hardly been studied in speech recognition research especially in acoustic modeling.\" Definitely look at some of the studies mentioned below, and also [4] specifically.\n- \"However, we note that our model can be combined with a well-resourced language model to recognize words.\" How would this be done, since I think this is actually quite a challenging task.\n- Section 2: \"... useful the original ESZSL architecture ...\" -> \"... useful in the original ESZSL architecture ...\"\n- Section 2.2: I assume the small text corpus is at the phone level (and not characters directly)? This should be clarified, and it could raise the question of whether this approach is truly \"zero-shot\".\n- Section 3.2: \"We used EESEN framework ...\" -> \"We used the EESEN framework ...\"\n- Section 4: You could look at the recent work in [2], which uses multilingual bottleneck features trained on 10 languages and applied to multiple unseen languages. It would be interesting to also train your approach on multiple languages instead of only English.\n\nMissing references:\n\n1. M. Heck, S. Sakti, and S. Nakamura, \"Feature Optimized DPGMM Clustering for Unsupervised Subword Modeling: A Contribution to Zerospeech 2017,\" in Proc. ASRU, 2017.\n2. E. Hermann and S. J. Goldwater, \"Multilingual bottleneck features for subword modeling in zero-resource languages,\" in Proc. Interspeech, 2018.\n3. H. Kamper, K. Livescu, and S. Goldwater, An embedded segmental k-means model for unsupervised segmentation and clustering of speech,\" in Proc. ASRU, 2017.\n4. B. M. Lake, C.-Y. Lee, J. R. Glass, and J. B. Tenenbaum, \"One-shot learning of generative speech concepts,\" in Proc. CogSci, 2014.\n5. O. Scharenborg, F. Ciannella, S. Palaskar, A. Black, F. Metze, L. Ondel, and M. Hasegawa-Johnson, \"Building an ASR system for a low-resource language through the adaptation of a high-resource language asr system: Preliminary results,\"in Proc. ICNLSSP, 2017.\n\nEdit: Based on the rebuttal I've changed my rating from 4 to 5.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Review", "cdate": 1542234280370, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335896893, "tmdate": 1552335896893, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgGv8a7A7", "original": null, "number": 5, "cdate": 1542866521652, "ddate": null, "tcdate": 1542866521652, "tmdate": 1542866521652, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "rkxpSgGXAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Clarification", "comment": "Will these additional experiments be included in the revised manuscript?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "SkxEjxzm0X", "original": null, "number": 3, "cdate": 1542819996089, "ddate": null, "tcdate": 1542819996089, "tmdate": 1542819996089, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "ByeYvTNOhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Reply to reviewer 2", "comment": "Thank you for detailed comments and references ! We will use them to enhance our paper by providing more discussion of related works. We discuss several points that distinguish our paper from the suggested references here - \n\n1.2.3: Those papers are works in zero resource speech recognition. As you suggested, we will discuss more about the connection between our work and those papers. Those zero resource works assume that no transcribed labels are available but a lot of audio data is provided for the target language. In contrast, our work assumes that both transcribed labels and audio are not available for the test language, but we use a limited amount of text sentences instead.\n\n4. We were unaware of this work and will update our related work. The work is applying one-shot learning to speech recognition by proposing a generative model. As its name suggests, the work was trying to classify words with only one training sample available per word. Our work is different from this one because we are using no training speech data for the target corpus.\n\n5. Thanks for pointing us to this work. We investigated further on this paper and also talked to some of the authors of the paper. We agree that the motivation behind this work is similar but it is limited to some extent. This work proposes an extrapolation approach to predict phones for low resource languages, however, the extrapolation mapping is done manually. Additionally the evaluation is carried out on Dutch/English pair which is similar in terms of their phonetics and language family. It does not show whether the approach will work for language pairs from unknown/unrelated linguistic groups . In contrast, our work proposes a generic algorithm to recognize any unknown phones by decomposing them into its phone attributes. We have shown that our approach is effective over 20 languages from different language families.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "rke7KlGmCQ", "original": null, "number": 2, "cdate": 1542819963276, "ddate": null, "tcdate": 1542819963276, "tmdate": 1542819963276, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "r1eK6OJKnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Reply to reviewer 3", "comment": "Thank you for your valuable comments ! \n\nWe think that the term \u201cUniversal Phonetic Model\u201d might have confused the reviewer. We are sorry about that. The problem that we want to address is the task of zero-shot learning for speech recognition, which consist of learning an acoustic model without any resources for a given target language. We call our model \u201cUniversal Phonetic Model\u201d, because it has the ability to predict any phoneme, even the ones that are not present during training (therefore it covers a \u201cuniversal\u201d set). We achieve this by decomposing the phone label into its phone attributes. \n\nOne of the weakness that has been pointed out is that the idea and model is not novel. However, we did not find any works that attempt the same problem with a similar model. It is possible we are unaware of related work, it would be helpful if the reviewer can give some references so that we can investigate further. Most of the work on zero-shot in speech community that we found only identified \u201csimilar\u201d speech concepts or sounds, but could not ground them to phone labels making it hard to do speech recognition. Similarly, the idea of decomposing sounds into articulatory features is old, but our work presents the first approach that actually decomposes sounds into \u201cuniversal\u201d articulatory features and recognizes speech in unseen languages using such representations.\n\nAs we mentioned to AnonReviewer1, we agree that our baseline model has too high a phone error rate to be usable in practice. Unfortunately this is what the current CTC acoustic models provide for the task of zero-shot speech recognition. Both the baseline and UPM models had practical and competitive phoneme error rate in the test data of the 3 english datasets that were used during training. However, we do believe that some of the performance reduction when using this model cross-lingually and cross-domain could because our input features are not robust against acoustic domain mismatch. We are currently re-running the experiments with a new set of input features proposed in [1], and first results indicate that we can get even better improvements in the same settings, and on top of a much improved baseline. We believe this is due to the stronger (noise robust and domain invariant) overall baseline allowing for a better sharing of the linguistically informative information across languages, and we are working towards applying this idea to all the experiments including baseline, so that an updated version of the paper will again be consistent. \n\n\nWe do agree that a better universal phoneme recognizer can be built by training on even more languages. But we believe that our experiments show that problem we want to address here, specifically the ability to predict unseen phonemes, in a zero-shot speech recognition scenario, can be tackled with the proposed method. Using more training languages would reduce appearance of unknown phonemes, but there will still almost always be at least a few unseen phones, which we show our model is effective in reducing..\n\n[1] S.Dalmia, X. Li, F. Metze and AW Black, \u201cDomain Robust Feature Extraction for Rapid Low Resource ASR Development\u201d, in Proc SLT 2018, https://arxiv.org/pdf/1807.10984.pdf\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "rkxpSgGXAQ", "original": null, "number": 1, "cdate": 1542819909337, "ddate": null, "tcdate": 1542819909337, "tmdate": 1542819909337, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "H1e4b3x9hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "content": {"title": "Reply to reviewer 1", "comment": "We appreciate your time reviewing our paper ! Thank you for your encouraging comments and remarks. We agree that our baseline model has too high a phone error rate to be usable in practice. We believe that this is because the input features currently being used are not robust against acoustic domain mismatch. We are currently re-running the experiments with a new set of input features proposed in [1], and first results indicate that we can get even better improvements in the same settings, and on top of a much improved baseline. We believe this is due to the stronger (noise robust and domain invariant) overall baseline allowing for a better sharing of the linguistically informative information across languages, and we are working towards applying this idea to all the experiments including baseline, so that an updated version of the paper will again be consistent. \n\n[1] S.Dalmia, X. Li, F. Metze and AW Black, \u201cDomain Robust Feature Extraction for Rapid Low Resource ASR Development\u201d, in Proc SLT 2018, https://arxiv.org/pdf/1807.10984.pdf\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607240, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkfhZnC9t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1210/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1210/Authors|ICLR.cc/2019/Conference/Paper1210/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers", "ICLR.cc/2019/Conference/Paper1210/Authors", "ICLR.cc/2019/Conference/Paper1210/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607240}}}, {"id": "r1eK6OJKnQ", "original": null, "number": 2, "cdate": 1541105856876, "ddate": null, "tcdate": 1541105856876, "tmdate": 1541533329718, "tddate": null, "forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1210/Official_Review", "content": {"title": "Interesting but not good enough!", "review": "This paper proposes to train a Universal Phonetic Model for building speech recognition for new languages without any training data. It suggests to use X-SAMPA to map phones from all the languages into a single phonetic space. The prediction models are designed to first predict the phonetic features and then the phones depending on the target language.\nOverall , the paper is quite clear written. \n- Strengthens:\n+ It observed overall improvements for all the target languages.\n\n- Weaknesses:\n+ The idea and the proposed model are not novel. \n+ All the baseline systems have relative high phone error rates.\n+ The authors claimed to have a universal phonetic model but actually the model was trained only with English data. Therefore, experimental setup could be improved. In my opinion, it makes more sense to define a bunch of resource-rich languages as source and then train a real universal phonetic model. \n+ Overall, this paper lacks an analysis what are exactly improved and why the improvements for some target languages are larger than for the others.\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1210/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Zero-shot Learning for Speech Recognition with Universal Phonetic Model", "abstract": "There are more than 7,000 languages in the world, but due to the lack of training sets, only a small number of them have speech recognition systems. Multilingual speech recognition provides a solution if at least some audio training data is available. Often, however, phoneme inventories differ between the training languages and the target language, making this approach infeasible. In this work, we address the problem of building an acoustic model for languages with zero audio resources. Our model is able to recognize unseen phonemes in the target language, if only a small text corpus is available. We adopt the idea of zero-shot learning, and decompose phonemes into corresponding phonetic attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over phonetic attributes, and then compute phoneme distributions with a customized acoustic model. We extensively evaluate our English-trained model on 20 unseen languages, and find that on average, it achieves 9.9% better phone error rate over a traditional CTC based acoustic model trained on English.", "keywords": ["zero-shot learning", "speech recognition", "acoustic modeling"], "authorids": ["xinjianl@andrew.cmu.edu", "sdalmia@cs.cmu.edu", "dmortens@cs.cmu.edu", "fmetze@cs.cmu.edu", "awb@cs.cmu.edu"], "authors": ["Xinjian Li", "Siddharth Dalmia", "David R. Mortensen", "Florian Metze", "Alan W Black"], "TL;DR": "We apply zero-shot learning for speech recognition to recognize unseen phonemes", "pdf": "/pdf/5e9a77e615c3c7b55ce9960657cdc9d83c78ebec.pdf", "paperhash": "li|zeroshot_learning_for_speech_recognition_with_universal_phonetic_model", "_bibtex": "@misc{\nli2019zeroshot,\ntitle={Zero-shot Learning for Speech Recognition with Universal Phonetic Model},\nauthor={Xinjian Li and Siddharth Dalmia and David R. Mortensen and Florian Metze and Alan W Black},\nyear={2019},\nurl={https://openreview.net/forum?id=BkfhZnC9t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1210/Official_Review", "cdate": 1542234280370, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkfhZnC9t7", "replyto": "BkfhZnC9t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1210/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335896893, "tmdate": 1552335896893, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1210/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 14}