{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396679578, "tcdate": 1486396679578, "number": 1, "id": "rkgz6MLOg", "invitation": "ICLR.cc/2017/conference/-/paper564/acceptance", "forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers agree that there are issues in the paper (in particular, the weakness of the experimental part), and that it is not ready for publication."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396680073, "id": "ICLR.cc/2017/conference/-/paper564/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396680073}}}, {"tddate": null, "tmdate": 1482323757928, "tcdate": 1482323757928, "number": 3, "id": "BJL4vxu4e", "invitation": "ICLR.cc/2017/conference/-/paper564/official/review", "forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "signatures": ["ICLR.cc/2017/conference/paper564/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper564/AnonReviewer1"], "content": {"title": "Interesting idea, need further work", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a multiview learning approach to finding dependent subspaces optimized for maximizing cross-view similarity between neighborhoods of data samples. The motivation comes from information retrieval tasks. Authors position their work as an alternative to CCA-based multiview learning; note, however, that CCA based techniques have very different purpose and are rather broadly applicable than the setting considered here. Main points: \n\n- I am not sure what authors mean by time complexity. It would appear that they simply report the computational cost of evaluating the objective in equation (7). Is there a sense of how many iterations of the L-BFGS method? Since that is going to be difficult given the nature of the optimization problem, one would appreciate some sense of how hard or easy it is in practice to optimize the objective in (7) and how that varies with various problem dimensions. Authors argue that scalability is not their first concern, which is understandable, but if they are going to make some remarks about the computational cost, it better be clarified that the reported cost is for some small part of their overall approach rather than \u201ctime complexity\u201d.\n\n- Since authors position their approach as an alternative to CCA, they should remark about how CCA, even though a nonconvex optimization problem, can be solved exactly with computational cost that is linear in the data size and only quadratic with dimensionality even with a naive implementation. The method proposed in the paper does not seem to be tractable, at least not immediately. \n\n- The empirical results with synthetic data are a it confusing. First of all the data generation procedure is quite convoluted, I am not sure why we need to process each coordinate separately in different groups, and then permute and combine etc. A simple benchmark where we take different linear transformations of a shared representation and add independent noise would suffice to confirm that the proposed method does something reasonable. I am also baffled why CCA does not recover the true subspace - arguably it is the level of additive noise that would impact the recoverability - however the proposed method is nearly exact so the noise level is perhaps not so severe. It is also not clear if authors are using regularization with CCA - without regularization CCA can be have in a funny manner. This needs to be clarified. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512540322, "id": "ICLR.cc/2017/conference/-/paper564/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper564/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper564/AnonReviewer2", "ICLR.cc/2017/conference/paper564/AnonReviewer3", "ICLR.cc/2017/conference/paper564/AnonReviewer1"], "reply": {"forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512540322}}}, {"tddate": null, "tmdate": 1481911313930, "tcdate": 1481911313930, "number": 2, "id": "Hy9zhiZNg", "invitation": "ICLR.cc/2017/conference/-/paper564/official/review", "forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "signatures": ["ICLR.cc/2017/conference/paper564/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper564/AnonReviewer3"], "content": {"title": "this paper has good intuition and reasonable idea but empirical results is not strong enough", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents an multi-view learning algorithm which projects the inputs of different views (linearly) such that the neighborhood relationship (transition probabilities) agree across views.\n\nThis paper has good motivation--to study multi-view learning from a more information retrieval perspective. Some concerns:\n-- The time complexity of the algorithm in its current form is high (see last paragraph of page 4). This might be the reason why the authors have conducted experiments on small datasets, and using linear projections.\n-- The proposed method does have some nice properties, e.g., it does not require the projections to have the same dimension across views (I like this). While it more directly models neighborhood relationship than CCA based approaches, it is still not directly optimizing typical retrieval (e.g., ranking-based) criteria. On the other hand, the contrastive loss in \nHermann and Blunsom. Multilingual Distributed Representations without Word Alignment. ICLR 2014. \nis certainly a relevant \"information retrieval\" approach, and shall be discussed and compared with.\n\nMy major concern about this paper is the experiments. As I mentioned in my previous comments, there are limited cases where linear mapping is more desirable than nonlinear mappings for dimension reduction. While the authors have argued that linear projection may provide better interpretability, I have not found empirical justification in this paper. Moreover, one could achieve interpretability by visualizing the projections and see what variations of the input is reflected along certain dimensions; this is commonly done for nonlinear dimension reduction methods. \n\nI agree that the general approach here generalizes to nonlinear projections easily, but the fact that the authors have not conducted experiments with nonlinear projections and comparisons with nonlinear variants of CCA and other multi-view learning algorithms limits the significance of the current paper. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512540322, "id": "ICLR.cc/2017/conference/-/paper564/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper564/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper564/AnonReviewer2", "ICLR.cc/2017/conference/paper564/AnonReviewer3", "ICLR.cc/2017/conference/paper564/AnonReviewer1"], "reply": {"forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512540322}}}, {"tddate": null, "tmdate": 1481790523302, "tcdate": 1481790523296, "number": 1, "id": "Sy7SN0kVg", "invitation": "ICLR.cc/2017/conference/-/paper564/official/review", "forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "signatures": ["ICLR.cc/2017/conference/paper564/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper564/AnonReviewer2"], "content": {"title": "Nice idea, limited validation", "rating": "4: Ok but not good enough - rejection", "review": "The authors develop a way learn subspaces of multiple views such that data point neighborhoods are similar in all of the views.  This similarity is measured between distributions of neighbors in pairs of views. The motivation is that this is a natural criterion for information retrieval.\n\nI like the idea of preserving neighborhood relationships across views for retrieval tasks. And it is nice that the learned spaces can have different dimensionalities for different views.  However, the empirical validation seems preliminary.\n\nThe paper has been revised from the authors' ICLR 2016 submission, and the revisions are welcome, but I think the paper still needs more work in order to be publishable.  In its current form it could be a good match for the workshop track.\n\nThe experiments are all on very small data sets (e.g. 2000 examples in each of train/test on the MNIST task) and not on real tasks.  The authors point out that they are not focusing on efficiency, and presumably computation requirements keep them from considering larger data sets.  However, it is not clear that there is any conclusion that can be drawn that would apply to more realistic data sets.  Considering the wealth of work that's been done on multi-view subspace learning, with application to real tasks, it is very hard to see this as a contribution without showing that it is applicable in such realistic settings.\n\nOn a more minor point, the authors claim that no other information retrieval based approaches exist, and I think this is a bit overstated.  For example, the contrastive loss of Hermann & Blunsom \"Multilingual models for compositional distributed semantics\" ACL 2014 is related to information retrieval and would be a natural one to compare against.\n\nThe presentation is a bit sloppy, with a number of vague points and confusing wordings.  Examples:\n- the term \"dependency\" gets used in the paper a lot in a rather colloquial way.  This gets confusing at times since it is used in a technical context but not using its technical definition.\n- \"an information retrieval task of the analyst\": vague and not quite grammatical\n- \"the probability that an analyst who inspected item i will next pick j for inspection\" is not well-defined\n- In the discussion of KL divergence, I do not quite follow the reasoning about its relationship to the \"cost of misses\" etc.  It would help to make this more precise (or perhaps drop it?  KL divergence is pretty well motivated here anyway).\n- Does C_{Penalty} (7) get added to C (6), or is it used instead?  I was a bit confused here.\n- It is stated that CCA \"iteratively finds component pairs\".  Note that while CCA can be defined as an iterative operation, it need not (and typically is not) solved that way, but rather all projections are found at once.\n- How is PCA done \"between X_i^1 and X_i^2\"?\n- \"We apply nonlinear dimensionality algorithm\": what is this algorithm?\n- I do not quite follow what the task is in the case of the image patches and stock prices.\n\nOther minor comments, typos, etc.:\n- The figure fonts are too small.\n- \"difference measures\" --> \"different measures\"\n- \"...since, hence any two...\": not grammatical\n- \"between feature-based views and views external neighborhoods\": ?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512540322, "id": "ICLR.cc/2017/conference/-/paper564/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper564/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper564/AnonReviewer2", "ICLR.cc/2017/conference/paper564/AnonReviewer3", "ICLR.cc/2017/conference/paper564/AnonReviewer1"], "reply": {"forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512540322}}}, {"tddate": null, "tmdate": 1481293221757, "tcdate": 1481293221750, "number": 2, "id": "Sy0jp4ume", "invitation": "ICLR.cc/2017/conference/-/paper564/public/comment", "forum": "Hk-mgcsgx", "replyto": "HkCeHJO7g", "signatures": ["~Ziyuan_Lin1"], "readers": ["everyone"], "writers": ["~Ziyuan_Lin1"], "content": {"title": "Reply to AnonReviewer2", "comment": "We have done a large number of revisions after the previous ICLR submission, including adding two more experiments. Below is a list of the main changes, in rough order of appearance in the manuscript:\n\n1. We made several general clarifications to the writing, making it more clear and concise. \n\n2. We clarified the meaning of dependent subspaces, and clarified that finding such subspaces is useful both for prediction and for analysis. We also clarified that although CCA is a standard tool for finding correlated linear projections, maximizing correlation alone does not suffice in many situations where data relationships are more complicated.\n\n3. We added speech vs tongue movements in signal processing as a clarifying example of  data with multiple views that have dependent subspaces.\n\n4. Added a clarifying example of scientist's research neighbors vs hobby neighbors as an example where specifically neighborhoods are of interest.\n\n5. Improved the organization of the paper. For example, related methods and our differences compared to them are now presented in a more readable fashion after the presentation of the proposed new method.\n\n6. The contributions of our method (easily interpretable linear transformation coupled with a powerful general function for detection dependencies; detecting nonlinear and local dependencies; strong invariance properties; elegant motivation as performance in an information retrieval task of the analyst; and good experimental performance) are now more clearly stated right in the introduction.\n\n7. Explained the connection in visual data analysis to Shneiderman's well-established taxonomy of tasks and how our method directly optimizes a task of relating data items, thus arguably coming closer to needs of data analysts than maximizing some variant of coordinate correlation.\n\n8. Clearly stated that the advantage of finding linear subspaces is in their simplicity and easy interpretability with respect to original data features, but that we couple this to a flexible dependency criterion which can detect nonlinear dependencies across views, unlike simple methods restricted to correlation.\n\n9. Clarified the overall organization of the paper already in the introduction.\n\n10. Clarified the concept of our probabilistic neighborhoods, and explained that they are more realistic than simple hard neighborhoods.\n\n11. Clarified the notation of the method.\n\n12. Included a more data-driven way to set the falloff rate in the neighborhood definition.\n\n13. Clarified the information retrieval interpretation of the symmetrized divergence.\n\n14. Clarified the advantage of the Sim(p,q) in preferring more sparse, more informative matches of neighborhoods.\n\n15. Added a description of how we use two different remedies to avoid bad local optima: 1) bound KL divergences by neighbor distribution smoothing, in the spirit of Laplace smoothing; and 2) shrinking the KL penalty.\n\n16. Added a clear description of the time complexity and scalability of our method.\n\n17. Clarified the description of the invariances in the method.\n\n18. Greatly improved the description of related work, such as clarifying the relationship to the CODE method.\n\n19. Clarified that our comparison is to linear methods which are interpretable, and that to our knowledge, no other information retrieval based approaches for finding linear subspaces is known.\n\n20. Added a new real-data set on image patches.\n\n21. Added an entire new experiment on real cell-cycle data with multiple (more than two) views, showing our method can find dependent subspaces between more than two views. \n\n22. Added an entire new experiment on Lissajous curve data, showing that our method can find subspaces with different dimensions per view, whereas most existing methods require equal output dimensionalities in each view (such as CCA projection directions which must always come in pairs, one direction for each view)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287519332, "id": "ICLR.cc/2017/conference/-/paper564/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk-mgcsgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper564/reviewers", "ICLR.cc/2017/conference/paper564/areachairs"], "cdate": 1485287519332}}}, {"tddate": null, "tmdate": 1481270517819, "tcdate": 1481270517815, "number": 2, "id": "HkCeHJO7g", "invitation": "ICLR.cc/2017/conference/-/paper564/pre-review/question", "forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "signatures": ["ICLR.cc/2017/conference/paper564/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper564/AnonReviewer2"], "content": {"title": "revision notes", "question": "This appears to be a revised version of an ICLR 2016 submission, whose reviews are publicly available.  If the authors could explicitly describe what has been revised and in what ways they have addressed the reviewer comments, it would be very helpful in reviewing."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481270518306, "id": "ICLR.cc/2017/conference/-/paper564/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper564/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper564/AnonReviewer3", "ICLR.cc/2017/conference/paper564/AnonReviewer2"], "reply": {"forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481270518306}}}, {"tddate": null, "tmdate": 1481030823324, "tcdate": 1481030823318, "number": 1, "id": "rJyh2NNQe", "invitation": "ICLR.cc/2017/conference/-/paper564/public/comment", "forum": "Hk-mgcsgx", "replyto": "BJtPKPkQg", "signatures": ["~Ziyuan_Lin1"], "readers": ["everyone"], "writers": ["~Ziyuan_Lin1"], "content": {"title": "Answers to AnonReviewer3", "comment": "1. Why do the authors focus on linear dimension reduction, while nowadays nonlinear mappings are so powerful and relatively easy to train? And it seems that in the experiments, the authors only compare with linear methods?\n\nSimple mappings, here linear projections, have advantages of simplicity and easy interpretability with respect to original data features. This is crucial when the solution needs to be human-understandable, instead of merely being preprocessing for automated methods. When human experts need to be in the loop, mappings more complicated than linear can often become hard-to-understand \"black boxes\". \n\nIn much of previous work, linear mappings have needed to be coupled to simple cost functions (such as variance maximization in PCA or correlation maximization in CCA). Our work, and the LPCCA comparison method, are part of a research trend where the cost function is allowed to be powerful and detailed even when the mapping itself is kept simple: the dependency criterion we optimize is flexible and detects nonlinear dependencies across views.\n\nSince we focus on linear projections (easily understandable mappings), we compare in experiments with other linear methods only.\n\n(Our dependency criterion itself is not restricted to linear projections, and in future work it could be applied with nonlinear mappings as well. In this first paper we wanted to focus on the easy-to-understand linear mappings in order not to muddle the message.)\n\n\n\n\n2. Learning the neighborhood is already happening in nonlinear variants of CCA. For example, in KCCA, we are effectively learning a new kernel matrix from individual kernel matrices originally defined in each view, and the new kernel matrix absorbs the neighborhood information in multiple views. Can the authors explore the relationship and justify why compared to nonlinear methods, the proposed method shall work better?\n\nSee our comment about the usefulness of interpretable linear mappings above. There is also a mathematical difference discussed below.\n\nThe key difference is that our method directly focuses on neighborhoods, whereas in many other methods they are only by-products. \n\nMathematically, neighborhoods arise through three steps: 1. the metric of the original space yields a metric in each subspace; 2. the metric in turn yields similarities through a similarity (kernel) function; 3. normalizing the similarities can then define probabilistic neighborhoods in the subspace. Cost functions that focus on distances take into account only step 1 above, cost functions based on kernels take into account steps 1-2, and we take into account all steps 1-3. \n\nIn particular, KCCA optimizes alpha'*Kx*Ky*beta/(sqrt(alpha'*Kx*Kx*alpha)*sqrt(beta'*Ky*Ky*beta))\nwith respect to weight vectors alpha and beta. Each of the quadratic functions is a weighted sum, where each term is a product of kernel matrix elements, with no normalization. Hence KCCA is dominated by the largest elements of those kernel products, whereas our probabilistic neighborhoods and our cost function give equal overall importance to the neighborhood of each data point.\n\nWe directly optimize projections to find the subspaces where the probabilistic neighborhoods are most similar. When the aim is to optimize neighborhoods, and resources (projection dimensionalities) are limited, optimizing neighborhoods directly yields better results than optimizing a cost that is only indirectly related to neighborhoods. This is already well known in neighbor embedding (see e.g. the success of methods like t-SNE), hence our approach is promising for finding dependent subspaces as well.\n\nAs noted above, our focus in this paper is on linear mappings but the argument above shows why our approach is useful more generally too."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287519332, "id": "ICLR.cc/2017/conference/-/paper564/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk-mgcsgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper564/reviewers", "ICLR.cc/2017/conference/paper564/areachairs"], "cdate": 1485287519332}}}, {"tddate": null, "tmdate": 1480714593349, "tcdate": 1480714593345, "number": 1, "id": "BJtPKPkQg", "invitation": "ICLR.cc/2017/conference/-/paper564/pre-review/question", "forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "signatures": ["ICLR.cc/2017/conference/paper564/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper564/AnonReviewer3"], "content": {"title": "preliminary comments", "question": "Learning the neighborhood relationship in multi-view learning certainly makes sense. However,\n\n1. Why do the authors focus on linear dimension reduction, while nowadays nonlinear mappings are so powerful and relatively easy to train? And it seems that in the experiments, the authors only compare with linear methods?\n\n2. Learning the neighborhood is already happening in nonlinear variants of CCA. For example, in KCCA, we are effectively learning a new kernel matrix from  individual kernel matrices originally defined in each view, and the new kernel matrix absorbs the neighborhood information in multiple views. Can the authors explore the relationship and justify why compared to nonlinear methods, the proposed method shall work better?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481270518306, "id": "ICLR.cc/2017/conference/-/paper564/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper564/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper564/AnonReviewer3", "ICLR.cc/2017/conference/paper564/AnonReviewer2"], "reply": {"forum": "Hk-mgcsgx", "replyto": "Hk-mgcsgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper564/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481270518306}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478371376766, "tcdate": 1478365208761, "number": 564, "id": "Hk-mgcsgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hk-mgcsgx", "signatures": ["~Ziyuan_Lin1"], "readers": ["everyone"], "content": {"title": "An Information Retrieval Approach for Finding Dependent Subspaces of Multiple Views", "abstract": "Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.", "pdf": "/pdf/43e43296156b8c1d9c95c0bae66f686e25ef568e.pdf", "TL;DR": "A novel method for seeking dependent subspaces across multiple views, preserving neighborhood relationships of data", "paperhash": "lin|an_information_retrieval_approach_for_finding_dependent_subspaces_of_multiple_views", "keywords": ["Unsupervised Learning"], "conflicts": ["aalto.fi", "uta.fi"], "authors": ["Ziyuan Lin", "Jaakko Peltonen"], "authorids": ["ziyuan.lin@aalto.fi", "jaakko.peltonen@uta.fi"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}