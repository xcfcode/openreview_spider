{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392751920000, "tcdate": 1392751920000, "number": 1, "id": "wgvNzOb-kRw-a", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "bb7SwHahSUpiq", "replyto": "KKryduAqwvKqj", "signatures": ["Taichi Kiwaki"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Anonymous f20c:\r\n\r\n=====summary of the review=====\r\nThe reviewer first pointed out that the word usage like 'overcomplete', 'useful', 'uniform' are vague. Second, he suggested that the discussion on the phenomenon is unnatural and alternative explanations would be possible. Specifically, he pointed out three points: first, shrinkage of low-pass filters can be a more reasonable explanation. Second, the behavior of FED in the introduction indicates overfitting. Finally in section 3, our hypothesis of GRBM filter number adjustment sounds unnatural. Third, he suggested that the computation of mutual information is not clear. Fourth, impact of our work is limited in terms of dataset and performance. Finally, he made a suggestion on alternative measures for early stopping. \r\n===============================\r\n\r\nFirst of all, we are going to correct our word usage by replacing vague expressions with more specific ones (e.g., 'useful filters'->'edge or gradient filters', 'uniform filters'->'near-zero'). Particularly for the definition of overcompleteness, we are introducing an overcompleteness measure in the next revision. This measure is based on counting the number of possible hidden configurations that reproduce data samples. We have shown that the measure can be computed under an approximation where the hidden configurations are relaxed into a continuous space. \r\n\r\nOn the alternative explanations, let us clarify several points. First, shrinkage of low-pass is somewhat different from the phenomenon that we observed. It is true that the filters that we call 'attenuated' or 'zero' after prolonged training are actually low-pass filters with small amplitudes. However, such filters didn't evolve from large amplitude low-pass filters through rescaling. They rather evolved from localized Gabor-like filters. It is more precise to describe that such filters evolved from mixtures of a large amplitude Gabor-like component and small amplitude low-pass component through attenuation of the Gabor-like component. Therefore, the number of filters that attain Gabor-like features decreases as GRBM training proceeds. Observation on rescaled GRBM filter images also revealed this. \r\n\r\nSecond, in the introduction, we intended to argue that overfitting is less likely responsible for the phenomenon, not to deny overfitting. Actually, the increase in FED indicates an overfitting effect as you pointed out (Test data log-likelihood will be useful for monitoring overfitting but AIS estimations were too unreliable to report). We now realizes that the small values of FED can not be a direct evidence for our claim and the mention on it is misleading. However, we consider that our claim still holds because overfitting do not explain the drop of classification performance on training data, which is the key feature of the phenomenon. \r\n\r\nThird, we have found that the discussion on our hypothesis is not sufficient. Let us supplement several points. To begin with, the highest data-likelihood is not necessarily achieved by GRBM representations where every filter represents single training case, despite the intuition. The key is co-activation of filters in similar directions. Suppose that a representation has two filters $vec{w}_1$ and $vec{w}_2$ exactly point at two different data points. When these filters are in similar directions ($vec{w}_1 approx vec{w}_2$), these two filter will generate Gaussian components not only at $vec{w}_1$ and $vec{w}_2$, but also at $vec{w}_1+vec{w}_2$ where no data points lie (biases are omitted for clarity). We can understand this by considering a MCMC sampling procedure from such a GRBM, where sample points generated by $vec{w}_1$ likely elicit $vec{w}_2$ as well as $vec{w}_1$ itself (and vice versa), resulting co-activation of two filters that generates a component at their composition. In cases where hundreds of visible units and even larger number of hidden units are involved, this effect can be quite severe because the number of GRBM Gaussian components is exponential in the number of (non-dead) hidden units, and the number of orthogonal filters is limited by the number of visible units and thus, most of the filters become linear independent. The highest data-likelihood, therefore, is achieved by representations which assign such exponential number of components to data points. Such representations, in general, are not a copy of data points. \r\n\r\nA supplement explanation is also needed for our conclusion that the number of effective GRBM filters matches the data dimensionality. We drew our conclusion from an assertion that the number of GRBM components is at the same order of the volume of space where data points lies. Because the volume is $O(exp(data_dimensionality))$ and the number of GRBM components is $O(exp(number_of_effective_filters))$, we concluded that $number_of_effective_filters approx data_dimensionality$ by taking the exponents. Let us note that these arguments were included in our initial draft but later removed to keep the draft within 9 pages. We are planing to add the argument again in the next revision. \r\n\r\nOn computation of mutual information, we have realized that our explanation in the current draft is misleading. First of all, we computed mutual information with a joint $p_GRBM(h_i|v) p_data(v)$ where $p_GRBM(v)$ in $p_GRBM(h_i,v)$ is replaced with $p_data(v)$ as you mentioned. However, we hesitated to call it an approximation because we are only interested in measuring statistics from this joint, not from $p_GRBM(h_i,v)$. $p_GRBM(h_i|v) p_data(v)$ can be considered as a joint distribution over $h_i, v$ when a GRBM is used to encode data generated from $p_data(v)$. From this joint distribution, we can know how efficient a GRBM represents data with its hidden units. We didn't mention this clearly in the draft. Second, $S_D(H_i)$ is computed as $S_D(H_i) = -p_D(H_i=0)log(p_D(H_i=0)) -p_D(H_i=1)log(p_D(H_i=1)) = S(p_D(H_i))$ where $p_D(cdot)$ is in Eq.(4) and $S(cdot)$ is the entropy functional. The difference between $S_D(H)$ and $S_D(H|D)$ is the order of the summation over data points and entropy functional. We have found these points are not obvious as well, and are going to correct the draft. \r\n\r\nWe admit that our empirical evidences are limited. However, we consider that some extent of generality of our reports can be provided through expensive exploration of the GRBM parametric effects on the phenomenon, and the discussion (with the supplement above) on the mechanism of the phenomenon along with the results of an abstract toy data experiment. \r\n\r\nThe limitation over performance improvements is mainly due to the shallowness of the architecture. If we apply our method to deeper models, we believe that much larger performance leverage can be obtained. \r\n\r\nFinally, we agree that the other measures might be used instead of our proposed measure. We find this possibility should be examined in the future."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392741960000, "tcdate": 1392741960000, "number": 2, "id": "llAo1wLFVa1Fy", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "bb7SwHahSUpiq", "replyto": "Q-BGJoM3VsJ32", "signatures": ["Taichi Kiwaki"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Annonymous e42c:\r\n\r\nThe reply above is for the first reviewer. Please skip this.\r\n\r\n=====summary of the review=====\r\nThe reviewer first suggested a possibility that the filter attenuation phenomenon is caused by CD approximation. He then pointed out our discussion on overfitting is obscure. He finally pointed out that the benefits of our criterion over validation error are limited to cases where unsupervised training is used to pretrain discriminative models. \r\n===============================\r\n\r\nTo begin with, our experimental results do not indicate that the phenomenon is due to CD approximation. First in experiments on CIFAR, GRBMs trained by not only CD but also by PCD showed the phenomenon. Because the PCD gradient is regarded as an unbiased estimate of the true gradient, this result indicates CD approximation is less likely responsible for the phenomenon. Second in the toy data experiment, where we used the true gradient without any approximation, the phenomenon is also observed. This supports that the phenomenon is not caused by the approximation as well. \r\n\r\nWe now find that the discussion on overfitting is not clear, therefore we here supplement an explanation. As you pointed out, GRBM overfitting is not directly related to the classification performance of SVMs. However, it can be still expected that if a GRBM overfits training data, test data accuracy can be degraded because the GRBM representation is specifically tuned for training data and loses generality. However, this explanation cannot be applied to the decline in training data accuracy in our observation. Therefore, we sought another hypothesis which we described in section 3 (Supplement discussion on section 3 is at the response to the third reviewer (f20c), and this will be added to the final draft). \r\n\r\nFinally, it is true that our claim on computational efficiency only holds in cases where unsupervised learning is used to pretrain supervised models. However, combination of unsupervised and supervised training is still under active research and is potentially important because this learning model can be readily applied to to semi-supervised learning problems with enormous unlabeled data. We therefore consider our research maintains sufficient significance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392741180000, "tcdate": 1392741180000, "number": 1, "id": "44ip4258KGxNA", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "bb7SwHahSUpiq", "replyto": "nB6IBHqgjJ9Px", "signatures": ["Taichi Kiwaki"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Dr. KyungHyun (Anonymous dd7e):\r\n\r\n=====summary of the review=====\r\nThe reviewer first pointed out that he and his colleague are working on a resembling measure for the usefulness of neural representations. He further questioned whether the concept sketched in Fig 2 (d) has quantitative evidences. He also questioned the relationship between MI/AMI and sparsity. He next suggested that 1-step reconstruction error might be used for early stopping in place of AMI. He finally pointed out the word 'overcomplete' is used without a clear definition in our draft. \r\n===============================\r\n\r\nThank you for your kind comments/reviews. Upon the connection between your study and ours, we are correcting our draft so that the relationship between these studies is clarified. \r\n\r\nOn Fig 2 (d) (and section 3), we have rewrite the section and introduced an experiment on a small, tractable GRBM with which a quantitative analysis is possible. We are also going to introduce an overcomplete measure, which we describe in the end of this response. \r\n\r\nOn the relationship between MI/AMI and sparsity, our understanding is currently limited to give a clear answer. However, experiments on Bernoulli-RBMs (not included in the draft) suggest that the filter attenuation phenomenon only occurs when RBMs gain sparse representations (including cases where such representations are naturally gained without regularization). \r\n\r\nThe idea of early stopping using 1-shot reconstruction error is interesting. However, through our experiments, we have observed that 1-shot reconstruction error only decreases during GRBM training and does not show a feature that once increase and then decrease, unlike AMI does. We consider this reflects less-sensitivity of MI to the overcompleteness than AMI, as we discussed in section 4. \r\n\r\nFinally, we are going to define a quantitative measure of overcompleteness in the current revision. We hesitate to use AMI as an overcomplete measure because we regard that AMI reflects both data-likelihood and overcompleteness. Instead of AMI, we are introducing a measure based on counting the number of possible hidden configurations that reproduce data samples. This count represents overcompleteness of a representation because there are multiple configurations that reconstruct a data point when representation is overcomplete. We have analyzed the measure and shown that computation of the measure is tractable under approximation where hidden configurations are relaxed into a continuous space."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392741000000, "tcdate": 1392741000000, "number": 1, "id": "H9BQsORED6H4X", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "bb7SwHahSUpiq", "replyto": "Q-BGJoM3VsJ32", "signatures": ["Taichi Kiwaki"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Dr. KyungHyun (Anonymous dd7e):\r\n\r\n=====summary of the review=====\r\nThe reviewer first pointed out that he and his colleague are working on a resembling measure for the usefulness of neural representations. He further questioned whether the concept sketched in Fig 2 (d) has quantitative evidences. He also questioned the relationship between MI/AMI and sparsity. He next suggested that 1-step reconstruction error might be used for early stopping in place of AMI. He finally pointed out the word 'overcomplete' is used without a clear definition in our draft. \r\n===============================\r\n\r\nThank you for your kind comments/reviews. Upon the connection between your study and ours, we are correcting our draft so that the relationship between these studies is clarified. \r\n\r\nOn Fig 2 (d) (and section 3), we have rewrite the section and introduced an experiment on a small, tractable GRBM with which a quantitative analysis is possible. We are also going to introduce an overcomplete measure, which we describe in the end of this response. \r\n\r\nOn the relationship between MI/AMI and sparsity, our understanding is currently limited to give a clear answer. However, experiments on Bernoulli-RBMs (not included in the draft) suggest that the filter attenuation phenomenon only occurs when RBMs gain sparse representations (including cases where such representations are naturally gained without regularization). \r\n\r\nThe idea of early stopping using 1-shot reconstruction error is interesting. However, through our experiments, we have observed that 1-shot reconstruction error only decreases during GRBM training and does not show a feature that once increase and then decrease, unlike AMI does. We consider this reflects less-sensitivity of MI to the overcompleteness than AMI, as we discussed in section 4. \r\n\r\nFinally, we are going to define a quantitative measure of overcompleteness in the current revision. We hesitate to use AMI as an overcomplete measure because we regard that AMI reflects both data-likelihood and overcompleteness. Instead of AMI, we are introducing a measure based on counting the number of possible hidden configurations that reproduce data samples. This count represents overcompleteness of a representation because there are multiple configurations that reconstruct a data point when representation is overcomplete. We have analyzed the measure and shown that computation of the measure is tractable under approximation where hidden configurations are relaxed into a continuous space."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391729160000, "tcdate": 1391729160000, "number": 4, "id": "KKryduAqwvKqj", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bb7SwHahSUpiq", "replyto": "bb7SwHahSUpiq", "signatures": ["anonymous reviewer f20c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "review": "In this paper the authors observe an interesting training dynamics when training GRBMs on CIFAR patches: filters first become selective to the usual Gabor looking features but then many faint and decay to 0 towards the end of training. From this observation they propose to measure an approximation to the mutual information between input and hidden units to detect the time when most features are used. They demonstrate that this is an effective method to automatically select the best checkpoint to use for discrimination on CIFAR dataset.\r\n\r\nPros\r\n- sufficient novelty: I am not aware of other works making this observation and proposing this measurement of goodness of a trained GRBM\r\n- relevance: the problem of automatically selecting good features is very relevant many working on unsupervised learning (one of the main themes of this conference).\r\n\r\nCons\r\n- the paper is not very clear. I find the authors' use of words like 'overcomplete', 'useful', etc. very vague. I will explain better below what I did not quite understand.\r\n- technically I am not sure the paper is correct; I did not follow how the authors measure mutual information exactly. More details below.\r\n- the impact of this work seems rather limited: the empirical validation is done on CIFAR only showing modest improvements.\r\n\r\nI will expand here on what I did not understand or did not agree.\r\nFirst of all, the fluency of English should be improved. The choice of words is often too vague. For instance, in the abstract the authors say:\r\n\u201c gain non-overcomplete representations which include uniform \ufb01lters that do not represent useful image features\u201d.\r\nWhat does this mean? What are 'uniform filters' or \u201c useful \ufb01lters\u201c? It would be nice if every part of the paper was self-explanatory, concise yet precise in its meaning.\r\n\r\nSecond, I am not convinced by the arguments about why filters decay to zero. From fig. 1, it seems to me that filters do not decay to zero at all. The range of values of filters that become low-pass shrinks, which is rather different and perhaps expected. Most energy is in the low frequencies, however hidden units are forced to be in {0,1} which forces the low-pass (less localized) filters to rescale their values to a smaller range. If the authors rescaled each filter independently they may see a different picture (please, verify if this is correct).\r\nBesides, it would be useful to get an estimate of the log-probability of the training data during training: is that steadily increasing? The monotonic behavior of FED seems to show an overfitting effect. I am not sure the hypothesis chosen by the authors in the introduction is the only one that can explain their observations.\r\nSimilarly in sec. 3, I do not understand the authors' conclusions. The number of hidden units need not to match the dimensionality of the data. Without any regularization, the highest likelihood on the training set would be achieved by having each hidden unit represent a single training sample, and therefore, the highest likelihood can be achieved by having as many hidden units as training samples (although this would be horrible on the test set).\r\nFrom this perspective, I did not understand what happened in the example of fig. 2A.\r\nDid the authors use any regularization? How did the authors measured 'overcompleteness' and 'fitness'? \r\n\r\nThird, I did not understand how the authors computed mutual information.\r\nFirst, mutual information assumes a joint distribution over data and hidden units. Are the authors replacing p_GRBM(v) with  p_data(v)? \r\nMoreover, I did not follow how the authors compute S_D(H). Shouldn't it be the entropy of the hidden units? which means marginalizing the visibles to get p_GRBM(h) and then computing the entropy. The authors should report the formula they used and explicitly mention the approximations used. In the current draft, I do not understand how S_D(H) is computed or differs from the conditional entropy term.\r\nFor the sake of clarity, I would recommend to include in the main body of the paper the definition of bar{AMI} and tilda{AMI} as well.\r\n\r\nFinally, empirical evidence is shown only on the CIFAR dataset. It is not obvious this is a general finding/method. \r\n\r\nOverall, this is an interesting paper. Unfortunately, I did not fully understand it nor I was fully convinced by its claims. I recommend a good rewrite of the paper to address these concerns. In general, the authors mention that this method is able to make GRBM a better model of natural images and contrast it to other models like SS-RBM, gMRF, etc. But actually, what they propose has little to do with these other methods (and it could be applied to these methods as well). The proposed method is about extracting discriminative features by measuring a quantity that correlates with number of non-dead filters. The question is then: are there even simpler measures (like variance across samples of the corresponding filter responses or variance of the coefficients in the filters)?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390949220000, "tcdate": 1390949220000, "number": 3, "id": "Q-BGJoM3VsJ32", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bb7SwHahSUpiq", "replyto": "bb7SwHahSUpiq", "signatures": ["anonymous reviewer e42c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "review": "This paper is in the domain of using GRBMs to train filters which can then be convolved and pooled before an L2-SVM is used for classification. The overall idea of finding degraded classification results after prolonged RBM training is an interesting one.  The authors then proposes an information criterion for early stopping. The paper is well written and makes link between AMI and filter quality and classification performance.\r\n\r\nThe fact that GRBM lose useful filters might be due to contrastive divergence training. Also, the concept of overfitting is hard to decipher here because learning GRBM filters is not for directly reducing classification error.\r\n\r\nThe argument that validation error is too expensive to compute is only unique to a framework where a probabilistic model is used to pretrain filters, which are then used in a discriminative pipeline.  For majority of discriminative based models, this argument would not apply and early stopping could simply be performed based on validation error."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390886160000, "tcdate": 1390886160000, "number": 2, "id": "nB6IBHqgjJ9Px", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bb7SwHahSUpiq", "replyto": "bb7SwHahSUpiq", "signatures": ["anonymous reviewer dd7e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "review": "Dear authors,  \r\n\r\nLet me reveal my identity before I continue. I'm Kyunghyun Cho who wrote the earlier review (by an unexpected coincidence). The previous comments reflect what I wanted/want to say as an official reviewer, and I will not write another separate review.  \r\n\r\n- Cho"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389041100000, "tcdate": 1389041100000, "number": 1, "id": "eh41hfmOBvUHN", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "bb7SwHahSUpiq", "replyto": "bb7SwHahSUpiq", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This is an interesting paper on Gaussian RBMs. The authors' explanation in Sec. 3 sounds convincing to me. The illustration in Fig. 2 (d) agrees well with my (and probably others') observation that the classification performance obtained by the features extracted by an RBM (either binary or Gaussian) does not correlate well with the test log-probabilities. Fig. 1 (a)-(d) agree as well with my own experience of training GRBMs on image patches. \r\n\r\nA similar idea of using the mutual information (or its upper-bound) to measure the usefulness of hidden neurons of an RBM/DBM was proposed by me and co-authors recently at ICONIP 2013 (Berglund et al., 2013). We were able to observe a similar trend of decreasing mutual information of quite a number of hidden neurons as training continues, which was also shown by the authors in Fig. 3 (B). The authors' approach of using this measure as an early-stopping criterion is very clever and seems to work well for the purpose of training a GRBM as a feature extractor. As the authors mention in Sec. 5.2.1, it is possible to use this measure for more complicated models such as DBM (which we tried in our paper), which makes this approach more interesting.\r\n\r\nOne potential point for improvement I can think of is to actually measure how other statistical quantities of a GRBM correlate with MI (or AI) as well. If the log-likelihood of a GRBM is measured, will one actually see the trend in Fig. 2 (d) (if we replace the overcompleteness with the MI or AI)? Also, since it's known that sparse RBM performs somewhat better as a feature extractor, how would the sparsity correlate with the MI or AI?\r\n\r\nAnother interesting/important point to discuss in the paper is the relationship to the autoencoders. Vincent et al. (2010) explains that 'training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between input X and learnt representation Y'. Does it mean that it is possible to simply use the 1-step reconstruction error as an early-stopping criterion instead of the proposed MI? If not, what would be some differences?\r\n\r\nOne minor point to consider In my opinion is that the term 'overcompleteness' is being somewhat abused without any precise definition. From the paper, I understood that the authors mean the (normalized) number of 'close-to-orthogonal' filters by 'overcompleteness', but this seems a bit too loose a definition (without any actual definition in the paper). Though, I guess the proposed measure itself can be used to measure the overcompleteness. \r\n\r\n= Refs = \r\nBerglund, M., Raiko, T. and Cho, K. Measuring the Usefulness of Hidden Units in Boltzmann Machines with Mutual Information. ICONIP 2013.\r\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y. and Manzagol, P. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. JMLR. 2010."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387532700000, "tcdate": 1387532700000, "number": 15, "id": "bb7SwHahSUpiq", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "bb7SwHahSUpiq", "signatures": ["qqm377p9k@gmail.com"], "readers": ["everyone"], "content": {"title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images", "decision": "submitted, no decision", "abstract": "We pursue early stopping that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent sharp edges. We have recently found that GRBMs once gain and subsequently lose sharp edge filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose approximated infomax early stopping for GRBMs. The proposed method enables huge performance boosts of classifiers trained on GRBM representations.", "pdf": "https://arxiv.org/abs/1312.5412", "paperhash": "kiwaki|approximated_infomax_early_stopping_revisiting_gaussian_rbms_on_natural_images", "keywords": [], "conflicts": [], "authors": ["Taichi Kiwaki", "Takaki Makino", "Kazuyuki Aihara"], "authorids": ["qqm377p9k@gmail.com", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 9}