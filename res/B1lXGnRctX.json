{"notes": [{"id": "B1lXGnRctX", "original": "SyxGNb09YQ", "number": 1252, "cdate": 1538087947369, "ddate": null, "tcdate": 1538087947369, "tmdate": 1545355411015, "tddate": null, "forum": "B1lXGnRctX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Classification in the dark using tactile exploration", "abstract": "Combining information from different sensory modalities to execute goal directed actions is a key aspect of human intelligence. Specifically, human agents are very easily able to translate the task communicated in one sensory domain (say vision) into a representation that enables them to complete this task when they can only sense their environment using a separate sensory modality (say touch). In order to build agents with similar capabilities, in this work we consider the problem of a retrieving a target object from a drawer. The agent is provided with an image of a previously unseen object and it explores objects in the drawer using only tactile sensing to retrieve the object that was shown in the image without receiving any visual feedback. Success at this task requires close integration of visual and tactile sensing. We present a method for performing this task in a simulated environment using an anthropomorphic hand. We hope that future research in the direction of combining sensory signals for acting will find the object retrieval from a drawer to be a useful benchmark problem", "keywords": ["tactile sensing", "multimodal representations", "vision", "object identification"], "authorids": ["mudigonda@berkeley.edu", "btickell@berkeley.edu", "pulkitag@berkeley.edu"], "authors": ["Mayur Mudigonda", "Blake Tickell", "Pulkit Agrawal"], "TL;DR": "In this work, we study the problem of learning representations to identify novel objects by exploring objects using tactile sensing. Key point here is that the query is provided in image domain.", "pdf": "/pdf/f45a4ba2821dc271070e94959752b3fb440e5af8.pdf", "paperhash": "mudigonda|classification_in_the_dark_using_tactile_exploration", "_bibtex": "@misc{\nmudigonda2019classification,\ntitle={Classification in the dark using tactile exploration},\nauthor={Mayur Mudigonda and Blake Tickell and Pulkit Agrawal},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lXGnRctX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1gqbqAC1V", "original": null, "number": 1, "cdate": 1544641025544, "ddate": null, "tcdate": 1544641025544, "tmdate": 1545354503245, "tddate": null, "forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1252/Meta_Review", "content": {"metareview": "The paper describes the use of tactile sensors for exploration.  An important topic which has been addressed in various previous publications, but is unsolved to date.\n\nThe research and the paper are unfortunately in a raw state.  Rejected unanimously by the reviewers, without rebuttal chances used by the authors.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "unfinished"}, "signatures": ["ICLR.cc/2019/Conference/Paper1252/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1252/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification in the dark using tactile exploration", "abstract": "Combining information from different sensory modalities to execute goal directed actions is a key aspect of human intelligence. Specifically, human agents are very easily able to translate the task communicated in one sensory domain (say vision) into a representation that enables them to complete this task when they can only sense their environment using a separate sensory modality (say touch). In order to build agents with similar capabilities, in this work we consider the problem of a retrieving a target object from a drawer. The agent is provided with an image of a previously unseen object and it explores objects in the drawer using only tactile sensing to retrieve the object that was shown in the image without receiving any visual feedback. Success at this task requires close integration of visual and tactile sensing. We present a method for performing this task in a simulated environment using an anthropomorphic hand. We hope that future research in the direction of combining sensory signals for acting will find the object retrieval from a drawer to be a useful benchmark problem", "keywords": ["tactile sensing", "multimodal representations", "vision", "object identification"], "authorids": ["mudigonda@berkeley.edu", "btickell@berkeley.edu", "pulkitag@berkeley.edu"], "authors": ["Mayur Mudigonda", "Blake Tickell", "Pulkit Agrawal"], "TL;DR": "In this work, we study the problem of learning representations to identify novel objects by exploring objects using tactile sensing. Key point here is that the query is provided in image domain.", "pdf": "/pdf/f45a4ba2821dc271070e94959752b3fb440e5af8.pdf", "paperhash": "mudigonda|classification_in_the_dark_using_tactile_exploration", "_bibtex": "@misc{\nmudigonda2019classification,\ntitle={Classification in the dark using tactile exploration},\nauthor={Mayur Mudigonda and Blake Tickell and Pulkit Agrawal},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lXGnRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1252/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352905617, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1252/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1252/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1252/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352905617}}}, {"id": "HkgGeGbTn7", "original": null, "number": 3, "cdate": 1541374441771, "ddate": null, "tcdate": 1541374441771, "tmdate": 1541533293330, "tddate": null, "forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1252/Official_Review", "content": {"title": "exciting research problem, but there are problems in the level of detail in the paper, and the approach taken is not convincing", "review": "This is an exciting research problem, and could be of broad interest in robotics.  The problem posed, and associated data sets and simulation code, if shared, could be an interesting and novel source of challenge to machine learning researchers. \n\nHowever, the paper appears to be a fairly early work in progress, with missing detail in many areas, and making some odd approximations. One concern is the use of averaged haptic readings over a series of explorations, rather than the haptic reading for a specific pose. The approach of averaging seems certain to blur and confound things unnecessarily, making it harder for the system to learn the relationship between pose, object form and sensation.\n\nThe paper itself has weaknesses, e.g. on p5 you say \"When employing the predicted means, this accuracy was a bit lower.\" when you actually have a drop from 99% to 54%! You do not specify which objects are used for this experiment. and in Section 4.2, you do not specify the exploration strategy used. \n\nCan you clarify the explanation of the images in Figure 3 - you say that the image is as in Figure 3, but are you really giving the image of the object AND hand, or just the object itself (if so, you need to change the explanation). \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1252/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification in the dark using tactile exploration", "abstract": "Combining information from different sensory modalities to execute goal directed actions is a key aspect of human intelligence. Specifically, human agents are very easily able to translate the task communicated in one sensory domain (say vision) into a representation that enables them to complete this task when they can only sense their environment using a separate sensory modality (say touch). In order to build agents with similar capabilities, in this work we consider the problem of a retrieving a target object from a drawer. The agent is provided with an image of a previously unseen object and it explores objects in the drawer using only tactile sensing to retrieve the object that was shown in the image without receiving any visual feedback. Success at this task requires close integration of visual and tactile sensing. We present a method for performing this task in a simulated environment using an anthropomorphic hand. We hope that future research in the direction of combining sensory signals for acting will find the object retrieval from a drawer to be a useful benchmark problem", "keywords": ["tactile sensing", "multimodal representations", "vision", "object identification"], "authorids": ["mudigonda@berkeley.edu", "btickell@berkeley.edu", "pulkitag@berkeley.edu"], "authors": ["Mayur Mudigonda", "Blake Tickell", "Pulkit Agrawal"], "TL;DR": "In this work, we study the problem of learning representations to identify novel objects by exploring objects using tactile sensing. Key point here is that the query is provided in image domain.", "pdf": "/pdf/f45a4ba2821dc271070e94959752b3fb440e5af8.pdf", "paperhash": "mudigonda|classification_in_the_dark_using_tactile_exploration", "_bibtex": "@misc{\nmudigonda2019classification,\ntitle={Classification in the dark using tactile exploration},\nauthor={Mayur Mudigonda and Blake Tickell and Pulkit Agrawal},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lXGnRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1252/Official_Review", "cdate": 1542234270581, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1252/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335906408, "tmdate": 1552335906408, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1252/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJerAD2j3X", "original": null, "number": 2, "cdate": 1541289933286, "ddate": null, "tcdate": 1541289933286, "tmdate": 1541533293129, "tddate": null, "forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1252/Official_Review", "content": {"title": "Not clear what's the contribution.", "review": "The authors propose a task of classifying objects from tactile signals. To do this, the image and haptic data are collected for each object. Then, image-to-haptic and haptic-to-label predictors are trained by supervised learning. In the experiment, prediction accuracy on unseen object class is studied. \n\nThe paper is clearly written although it contains several typos. The proposed task of cross-modal inference is an interesting task. I however hardly find any significance of the proposed method. The proposed method is simple non-end-to-end predictors trained by supervised learning. So, the proposed model seems more like a naive baseline. It is not clear what scientific challenge the paper is solving and what is the contribution. Also, the performance seems not impressive. I\u2019m not sure why the authors average the haptic features. Lots of information will be lost during the averaging, why not RNNs. Overall, the paper seems to require a significant improvement.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1252/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification in the dark using tactile exploration", "abstract": "Combining information from different sensory modalities to execute goal directed actions is a key aspect of human intelligence. Specifically, human agents are very easily able to translate the task communicated in one sensory domain (say vision) into a representation that enables them to complete this task when they can only sense their environment using a separate sensory modality (say touch). In order to build agents with similar capabilities, in this work we consider the problem of a retrieving a target object from a drawer. The agent is provided with an image of a previously unseen object and it explores objects in the drawer using only tactile sensing to retrieve the object that was shown in the image without receiving any visual feedback. Success at this task requires close integration of visual and tactile sensing. We present a method for performing this task in a simulated environment using an anthropomorphic hand. We hope that future research in the direction of combining sensory signals for acting will find the object retrieval from a drawer to be a useful benchmark problem", "keywords": ["tactile sensing", "multimodal representations", "vision", "object identification"], "authorids": ["mudigonda@berkeley.edu", "btickell@berkeley.edu", "pulkitag@berkeley.edu"], "authors": ["Mayur Mudigonda", "Blake Tickell", "Pulkit Agrawal"], "TL;DR": "In this work, we study the problem of learning representations to identify novel objects by exploring objects using tactile sensing. Key point here is that the query is provided in image domain.", "pdf": "/pdf/f45a4ba2821dc271070e94959752b3fb440e5af8.pdf", "paperhash": "mudigonda|classification_in_the_dark_using_tactile_exploration", "_bibtex": "@misc{\nmudigonda2019classification,\ntitle={Classification in the dark using tactile exploration},\nauthor={Mayur Mudigonda and Blake Tickell and Pulkit Agrawal},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lXGnRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1252/Official_Review", "cdate": 1542234270581, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1252/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335906408, "tmdate": 1552335906408, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1252/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlMo5m9nm", "original": null, "number": 1, "cdate": 1541188249785, "ddate": null, "tcdate": 1541188249785, "tmdate": 1541533292929, "tddate": null, "forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1252/Official_Review", "content": {"title": "Poorly written paper, with lots of confusing statements and incomplete description of the concepts and references introduced. Very weak contribution.", "review": "This paper is poorly written, and looks like it was not proof-read. \nPresentation of the problem at hand is presented over so many times that it becomes confusing.\nAuthors ought to better define the image description space of the objects and the haptic space. \nMore interesting would have been a good explanation of the different sensors used in the anthropomorphic hand  and the vector built to represent the different sensed objects.\nThe most expected contribution of this work is barely explained: how the haptic sensors' values/object labels vectors were built and fed to the predictor network, what their values looked like for the various objects, how these vectors clustered for the various objects etc.\n\nAmong the many evident weaknesses:\n- Domain specific concepts and procedures of most importance to this work are not explained: \"... measure various physical properties of objects using the bio-tac sensor using five different exploration procedures (EP)\".  Page 3, Paragraph 1. Bio-tac sensor and most importantly exploration procedures (EP) should be presented more clearly.\n- Incomplete and out of nowhere sentences are common: \"The SHAP procedure\nwas established for evaluating prosthetic hands and arms. With this idea in mind, prior work (?)\nbuilt a prosthetic arm which could ...\" Page 4, Paragraph 1.\n- Many references are not well introduced and justified: \"We then trained the network using\nADAM (Kingma & Ba (2014)) with an initial learning rate set to 1e-4.\" Page 4, Paragraph 6. In the same paragraph,  authors explain using \"The groundtruth predictions were per-channel averaged haptic forces\" without having defined those channels (that one can guess but shouldn't). Concepts have to be clearly defined prior to their use.\n\n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1252/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification in the dark using tactile exploration", "abstract": "Combining information from different sensory modalities to execute goal directed actions is a key aspect of human intelligence. Specifically, human agents are very easily able to translate the task communicated in one sensory domain (say vision) into a representation that enables them to complete this task when they can only sense their environment using a separate sensory modality (say touch). In order to build agents with similar capabilities, in this work we consider the problem of a retrieving a target object from a drawer. The agent is provided with an image of a previously unseen object and it explores objects in the drawer using only tactile sensing to retrieve the object that was shown in the image without receiving any visual feedback. Success at this task requires close integration of visual and tactile sensing. We present a method for performing this task in a simulated environment using an anthropomorphic hand. We hope that future research in the direction of combining sensory signals for acting will find the object retrieval from a drawer to be a useful benchmark problem", "keywords": ["tactile sensing", "multimodal representations", "vision", "object identification"], "authorids": ["mudigonda@berkeley.edu", "btickell@berkeley.edu", "pulkitag@berkeley.edu"], "authors": ["Mayur Mudigonda", "Blake Tickell", "Pulkit Agrawal"], "TL;DR": "In this work, we study the problem of learning representations to identify novel objects by exploring objects using tactile sensing. Key point here is that the query is provided in image domain.", "pdf": "/pdf/f45a4ba2821dc271070e94959752b3fb440e5af8.pdf", "paperhash": "mudigonda|classification_in_the_dark_using_tactile_exploration", "_bibtex": "@misc{\nmudigonda2019classification,\ntitle={Classification in the dark using tactile exploration},\nauthor={Mayur Mudigonda and Blake Tickell and Pulkit Agrawal},\nyear={2019},\nurl={https://openreview.net/forum?id=B1lXGnRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1252/Official_Review", "cdate": 1542234270581, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1lXGnRctX", "replyto": "B1lXGnRctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1252/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335906408, "tmdate": 1552335906408, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1252/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}