{"notes": [{"id": "r1lQQeHYPr", "original": "Hkxk-VxKwr", "number": 2204, "cdate": 1569439770812, "ddate": null, "tcdate": 1569439770812, "tmdate": 1577168226311, "tddate": null, "forum": "r1lQQeHYPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hl-r2zcfr", "original": null, "number": 1, "cdate": 1576798743156, "ddate": null, "tcdate": 1576798743156, "tmdate": 1576800893045, "tddate": null, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "invitation": "ICLR.cc/2020/Conference/Paper2204/-/Decision", "content": {"decision": "Reject", "comment": "This paper offers a new approach to cross-modal embodied learning that aims to overcome limited vocabulary and other issues.  Reviews are mixed.  I concur with the two reviewers who say the work is interesting but the contribution is not sufficiently clear for acceptance at this time.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725293, "tmdate": 1576800277145, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2204/-/Decision"}}}, {"id": "HJxnPOdRYr", "original": null, "number": 3, "cdate": 1571879011994, "ddate": null, "tcdate": 1571879011994, "tmdate": 1574287717472, "tddate": null, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "invitation": "ICLR.cc/2020/Conference/Paper2204/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #3", "review": "I thank the authors for their detailed response and appreciate their hard work in bringing us this paper. \n\nI think that my main point is that this work relies too much on the extra information/constraints in the synthetic env. E.g., 1. since the vocab size is small, thus the feature map could be designed 'equal to the vocabulary size' 2. The bag-of-words representation is effective but it is not the case for natural language. Although the authors kindly point me to some recent works on sim2real, I am still not convinced whether this proposed method could be transferred to real setups based on the referenced papers.\n\nHowever, it is a personal research taste that I always take real setup into considerations, because I have worked on both synthetic and real setup (on both lang and visn sides) for years and observed a large gap. My opinion is that methods of synthetic setups are not naturally convertible to the real ones. If AC/meta-reviewer considers the ability of vision-and-language interactions could be effectively studied through this setup with synthetic language and simulated-unrealistic images, I am OK with acceptance. I have downgraded my confidence scores (but kept my overall score) for this purpose.\n\n\n-----------------------------------------------------------------------------------\n\nPros: \n(1) The proposed model makes sense to me, which tries to have two attention layers to extract the information related to the questions. It seems to have the ability to deal with \"and\"/\"or\" logical relationships as well. \n\n(2) Fig. 4 is impressive. It is clear and well-designed. \n\n(3) The results in Table 2 are convincing. They show that both the proposed dual-attention method and multi-task learning would contribute to the performance. \n\nCons:\n(1) It seems that the two main contributions are related to the language. Thus the synthetic language might not be proper to study. For example, in Eqn. 2, the first GA multiplies the BOW vector with the vision feature map, which could filter out unrelated instruction. This method could not be directly transferred to a real setup where natural language and natural images are involved.\n\n(2) The designed attention modules is lack of generalizability. It implements a two-step attention module, while the first step selects the related visual regions w.r.t the words and the second step gathers the information regarding these attended regions. However, it might not be aware of the spatial relationships and thus be limited to simple questions. For example, if the question is \"What is the object on top of the apple?\". To my understanding, the current module would not explicitly handle this one-hop spatial relationship. \n\nComments:\n(1) According to Sec. 3, 70 instructions and 29 questions are involved in this task. Using GRU to encoder these questions seems to be redundant. A simple one-hot embedding for these instructions might already be enough to encode the information.\n\n(2) I am not sure why the visual attention map x_S could be used as the state of the module.\n\n(3) After Eqn. 3, the paper says that \"ReLU activations ... make all elements positive, ensuring ...\". I am confused about the intuition behind this argument because of the softmax activation. Softmax will projects 0 to 1. So the sum of the all-zero vector would still be non-zero after softmax. \n\nTypo:\n- In Sec. 4, X_{BoW} \\in \\{0, 1\\}^V.\n- In Sec. 4.1, \"this matrix is multiplied ...\" --> this tensor.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2204/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2204/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916804491, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2204/Reviewers"], "noninvitees": [], "tcdate": 1570237726212, "tmdate": 1575916804509, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2204/-/Official_Review"}}}, {"id": "B1g45ZScor", "original": null, "number": 3, "cdate": 1573699980375, "ddate": null, "tcdate": 1573699980375, "tmdate": 1573699980375, "tddate": null, "forum": "r1lQQeHYPr", "replyto": "HJgz_vSntH", "invitation": "ICLR.cc/2020/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thanks for the review and helpful feedback. We address your concerns below:\n\n> First, the paper uses a new environment to evaluate the SGN and EQA task instead of the benchmark environments for these two tasks, making it difficult to compare performance to previous work\n\nWe agree with you that reproducibility and benchmarking is important. And this is the reason we did not use the House3D EQA dataset as it requires the SUNCG dataset which is no longer available. For SGN, we use the same dataset as used by Chaplot et. al. 2018 [1]. \n\n\n> Also, the paper only compares to relatively out-of-date approaches on EQA and SGN, instead of the state-of-the-art approaches on them.\n\nPlease note that our baselines aren't weak -- as shown by the multi-task training performance of our baselines in Table 2, they achieve nearly 100% performance on both SGN and EQA during training. Testing a newer method will not improve this performance. The problem is not that these baselines are ineffective at SGN or EQA but that these models are designed for a single task and hence do not perform well when tested for cross-task knowledge transfer. And this issue remains with the state-of-the-art single-task approaches for EQA and SGN. In fact, no prior work has proposed a model for cross-task knowledge transfer for embodied multimodal learning, so we cannot easily compare to prior work (we do construct reasonable baselines and ablations, as described in section 5.1 and 5.2). Having said that, if there are any specific recommendations for a baseline to add that we may have missed, we will be happy to add it in the revised version.\n\n\n> In addition, the paper should also discuss its connections to other multi-task learning approaches in the related work section.\n\nWe did not discuss multitask learning in the related work as we are not aware of any multitask learning approaches specific to embodied multimodal learning. We will add a discussion about multitask learning in non-embodied multimodal settings in the revised version.\n\n[1] Gated-Attention Architectures for Task-Oriented Language Grounding\nDevendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lQQeHYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference/Paper2204/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2204/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2204/Reviewers", "ICLR.cc/2020/Conference/Paper2204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2204/Authors|ICLR.cc/2020/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144808, "tmdate": 1576860556728, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference/Paper2204/Reviewers", "ICLR.cc/2020/Conference/Paper2204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2204/-/Official_Comment"}}}, {"id": "BJehkWrcjS", "original": null, "number": 2, "cdate": 1573699811875, "ddate": null, "tcdate": 1573699811875, "tmdate": 1573699811875, "tddate": null, "forum": "r1lQQeHYPr", "replyto": "HyxBKMA6KB", "invitation": "ICLR.cc/2020/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thanks for the helpful review and feedback. We address your concerns below:\n\n> However, the generality of the proposed method, i.e., dual attention, is still ambiguous \u2026\n\nWe argue that the proposed dual-attention is generally applicable to any multimodal task which requires grounding words in visual concepts. It provides a general way of aligning textual and visual representations in any multimodal task such that they can be reused for other tasks. Although it is evaluated on two tasks in a specific environment, the design of the dual-attention unit itself is not specific to the environment or the task.\n\n\n> Even though the title has the phrase \"multitask learning,\" what the system copes with is just two specific tasks. If the system is designed to solve the two specific tasks simultaneously, it's better to change the title. The title seems to be misleading.\n\nNote that we called semantic goal navigation as a single task in the paper for easier understanding, whereas prior work [1, 2, 3] has called each instruction as a different task and handling multiple instructions as multi-task learning. In our work, we not only handle multiple instructions but also multiple questions. Thus, under the notation of past work, we are solving multiple tasks. Having said that, we see your concern, and we are happy to change the title to \u201cEmbodied Multimodal Learning and Knowledge Transfer between Semantic Goal Navigation and Embodied Question Answering\u201d or just \u201cEmbodied Multimodal Learning and Knowledge Transfer\u201d if the reviewers and the Area Chair find this more suitable. We are also open to other suggestions from you.\n\n\n> Some of the main contributions, e.g., \"modularity and interpretability\" and \"transfer to new concepts,\" are not evaluated quantitatively.\n\nWe believe these contributions are evaluated quantitatively. In Section 5.4: \"Transfer to new concepts\"', we evaluate the model's capability of transferring to new concepts with quantitative results in Table 4. The results show that our model achieves a success rate of 0.97 on average over different types of instructions involving new object types and attributes. These results also demonstrate not only our model\u2019s ability to handle new concepts but also to combine the knowledge of existing concepts with a new concept without any additional policy training. The above results are possible because of the modularity and interpretability of the model, as it allows us to add the output of external object detectors as intermediate representation in our model. \n\nFurthermore, in Section 5.3 \u201cHandling relational tasks\u201d we show that the modularity and interpretability of our model also allow us to use trainable neural modules to handle relational tasks involving negation and spatial relationships and also tackle relational instructions involving new concepts. \n\nThanks for pointing out the typo. We will correct it in the revised version.\n\n[1] Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning\nJunhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli\n\n[2] Grounded Language Learning in a Simulated 3D World\nKarl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, Phil Blunsom\n\n[3] Gated-Attention Architectures for Task-Oriented Language Grounding\nDevendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lQQeHYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference/Paper2204/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2204/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2204/Reviewers", "ICLR.cc/2020/Conference/Paper2204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2204/Authors|ICLR.cc/2020/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144808, "tmdate": 1576860556728, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference/Paper2204/Reviewers", "ICLR.cc/2020/Conference/Paper2204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2204/-/Official_Comment"}}}, {"id": "Hkl2Ber9jS", "original": null, "number": 1, "cdate": 1573699652193, "ddate": null, "tcdate": 1573699652193, "tmdate": 1573699652193, "tddate": null, "forum": "r1lQQeHYPr", "replyto": "HJxnPOdRYr", "invitation": "ICLR.cc/2020/Conference/Paper2204/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks for the review and helpful feedback. We address your concerns and answer your questions below:\n\nRegarding the use of synthetic language: The focus of this submission is not tackling natural language but transferring the knowledge of grounded concepts (words grounded to their visual properties) across different embodied multimodal tasks and handling new concepts never seen during training. In our opinion, handling natural language is important but a separate problem in itself. For example, a parallel submission in ICLR (https://openreview.net/forum?id=rklraTNFwB) studies this problem specifically and shows that even embodied agents trained only with synthetic language can be transferred to natural language by using word representations learned by language models trained on large text corpora. Another work [1] shows that embodied instruction-following models can be transferred to unseen natural language synonymous words using GLoVe [2] word embeddings. These approaches could be used to transfer our model to natural language as well.\n\nIn order to provide evidence for the above, we conducted additional experiments. We constructed a new test set using synonymous words given by [1] such that each question and instruction in this new test set contains at least one unseen word never seen in any task during training. In order to handle this new test set containing unseen natural language words, we map each unseen word to the closest seen word in our synthetic data using the GLoVe [2] word vector space similar to [1]. The Dual-Attention model achieved a performance of 0.81/0.48 SGN/EQA as compared to the best performance of 0.27/0.19 SGN/EQA (GA) among the baselines. Clearly, we understand that natural language has much more complexity than previously unseen synonyms, but these results and the papers referenced above indicate that models trained with synthetic language can be used with natural language as well.\n\n\n> it might not be aware of the spatial relationships and thus be limited to simple questions. For example, if the question is \"What is the object on top of the apple?\". To my understanding, the current module would not explicitly handle this one-hop spatial relationship.\n\nThe current model can handle relational questions including one-hop spatial relationships as shown in Section 5.3: \"Handling Relational Tasks\". In this section, we show how a simple extension of the model can address questions and instructions containing \"left of\", \"right of\" and \"not\". We also show visualizations of the learnt representations in Figure 5. We do not tackle \u2018top of\u2019 specifically, but \u2018left of\u2019 and \u2018right of\u2019 are analogous to `top of\u2019 and tackle one-hop spatial relationship as the review mentions.\n\n\n> I am not sure why the visual attention map x_S could be used as the state of the module.\n\nThe visual attention map is passed to the navigation policy because the information in the visual attention map is sufficient for successful navigation. For example, for the instruction, `Go to the red torch\u2019 if the visual attention map identifies the location of red and torch things, that information is sufficient for navigating to the red torch.\n\n\n> After Eqn. 3, the paper says that \"ReLU activations ... make all elements positive, ensuring ...\". I am confused about the intuition behind this argument because of the softmax activation. Softmax will projects 0 to 1. So the sum of the all-zero vector would still be non-zero after softmax. \n\nThe purpose of ReLU activations is not to zero-out the prediction after softmax. In fact, ReLU activations were chosen independently of the subsequent softmax operation. The purpose of ReLU activations is to have only positive activations during summation. Positive activations ensure that they aggregate during summation. If there were negative activations, they could potentially cancel out positive activations during summation. \n\nThanks for pointing out the typos. We will correct them in the revised version.\n\n[1] ACTRCE: Augmenting Experience via Teacher's Advice For Multi-Goal Reinforcement Learning\nHarris Chan, Yuhuai Wu, Jamie Kiros, Sanja Fidler, Jimmy Ba\n\n[2] Glove: Global vectors for word representation. \nJeffrey Pennington, Richard Socher, and Christopher Manning\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2204/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1lQQeHYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference/Paper2204/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2204/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2204/Reviewers", "ICLR.cc/2020/Conference/Paper2204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2204/Authors|ICLR.cc/2020/Conference/Paper2204/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144808, "tmdate": 1576860556728, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2204/Authors", "ICLR.cc/2020/Conference/Paper2204/Reviewers", "ICLR.cc/2020/Conference/Paper2204/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2204/-/Official_Comment"}}}, {"id": "HyxBKMA6KB", "original": null, "number": 2, "cdate": 1571836541263, "ddate": null, "tcdate": 1571836541263, "tmdate": 1572972369525, "tddate": null, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "invitation": "ICLR.cc/2020/Conference/Paper2204/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "*Summary \n\nThe paper describes a Dual-Attention model using Gated- and Spatial-Attention for disentanglement of attributes in feature representations for visually-grounded multitask learning. It has been shown that these models are capable of learning navigational instructions and answering questions. However, they addressed two limitations of previous works about visually-grounded embodied language learning models.  The first is the inability to transfer grounded knowledge across different\ntasks, and the other is the inability to transfer to new words and concepts not seen during the training phase. To overcome the problem, a multitask model is introduced. The model can transfer knowledge across tasks via learning disentanglement of the knowledge of words and visual attributes. The paper shows that the proposed model outperforms a range of baselines in simulated 3D environments. \n\n\n*Decision and supporting arguments\n\nI think the paper is on the borderline. The reason is as follows. \nThe motivation of the study is described appropriately, and the performance is quantitatively evaluated, as shown while Table 2. \nHowever, the generality of the proposed method, i.e., dual attention, is still ambiguous. Though the devised module performs effectively in this specific simulation environment and specific two tasks, an explanation of the theoretical basis and generality of dual attention seem to be missing.\nEven though the title has the phrase \"multitask learning,\" what the system copes with is just two specific tasks.  If the system is designed to solve the two specific tasks simultaneously, it's better to change the title. The title seems to be misleading.\nSome of the main contributions, e.g., \"modularity and interpretability\" and \"transfer to new concepts,\" are not evaluated quantitatively.\n\n\n*Additional feedback\nIn conclusion, \"interpretablew\" -> \"interpretable\""}, "signatures": ["ICLR.cc/2020/Conference/Paper2204/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2204/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916804491, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2204/Reviewers"], "noninvitees": [], "tcdate": 1570237726212, "tmdate": 1575916804509, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2204/-/Official_Review"}}}, {"id": "HJgz_vSntH", "original": null, "number": 1, "cdate": 1571735402233, "ddate": null, "tcdate": 1571735402233, "tmdate": 1572972369433, "tddate": null, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "invitation": "ICLR.cc/2020/Conference/Paper2204/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThe paper explores multi-task learning in embodied environments and proposes a Dual-Attention Model that disentangles the knowledge of words and visual attributes in the intermediate representations. It addresses two tasks, namely Semantic Goal Navigation (SGN) and Embodied Question Answering (EQA), using a simple synthetic environment. The paper compares against a few simple baselines and baselines adapted from models in each task.\n\nI would recommend for acceptance, as the experimental results show that the proposed approach successfully transfers knowledge across tasks.\n\nHowever, I would also like to note that the paper has a few drawbacks.\n\nFirst, the paper uses a new environment to evaluate the SGN and EQA task instead of the benchmark environments for these two tasks, making it difficult to compare performance to previous work. The environment in the paper is small (compared to e.g., House3D for EQA) and has a limited variety. Also, the paper only compares to relatively out-of-date approaches on EQA and SGN, instead of the state-of-the-art approaches on them.\n\nIn addition, the paper should also discuss its connections to other multi-task learning approaches in the related work section."}, "signatures": ["ICLR.cc/2020/Conference/Paper2204/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2204/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["chaplot@cs.cmu.edu", "lslee@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Embodied Multimodal Multitask Learning", "authors": ["Devendra Singh Chaplot", "Lisa Lee", "Ruslan Salakhutdinov", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "TL;DR": "We propose a multitask model which facilitates knowledge transfer across multimodal tasks by disentangling the knowledge of words and visual concepts in the intermediate representations. ", "abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "keywords": ["Visual Grounding", "Semantic Goal Navigation", "Embodied Question Answering"], "paperhash": "chaplot|embodied_multimodal_multitask_learning", "original_pdf": "/attachment/ae8db272968e6485697872bd0a3334d301b1271c.pdf", "_bibtex": "@misc{\nchaplot2020embodied,\ntitle={Embodied Multimodal Multitask Learning},\nauthor={Devendra Singh Chaplot and Lisa Lee and Ruslan Salakhutdinov and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lQQeHYPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lQQeHYPr", "replyto": "r1lQQeHYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2204/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916804491, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2204/Reviewers"], "noninvitees": [], "tcdate": 1570237726212, "tmdate": 1575916804509, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2204/-/Official_Review"}}}], "count": 8}