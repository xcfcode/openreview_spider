{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124282290, "tcdate": 1518111207951, "number": 35, "cdate": 1518111207951, "id": "SJexcZc8G", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SJexcZc8G", "signatures": ["~Xavier_Bresson2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "An Experimental Study of Neural Networks for Variable Graphs", "abstract": "Graph-structured data such as social networks, functional brain networks, chemical molecules have brought the interest in generalizing deep learning techniques to graph domains. In this work, we propose an empirical study of neural networks for graphs with variable size and connectivity. We rigorously compare several graph recurrent neural networks (RNNs) and graph convolutional neural networks (ConvNets) to solve two fundamental and representative graph problems, subgraph matching and graph clustering. Numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Interestingly, graph ConvNets are also 36% more accurate than non-learning (variational) techniques. The benefit of such study is to show that complex architectures like LSTM is not useful in the context of graph neural networks, but one should favour architectures with minimal inner structures, such as locality, weight sharing, index invariance, multi-scale, gates and residuality, to design efficient novel neural network models for applications like drugs design, genes analysis and particle physics.", "paperhash": "bresson|an_experimental_study_of_neural_networks_for_variable_graphs", "keywords": ["Graph neural networks", "ConvNets", "RNNs", "subgraph matching", "semi-supervised clustering"], "_bibtex": "@misc{\n  bresson2018an,\n  title={An Experimental Study of Neural Networks for Variable Graphs},\n  author={Xavier Bresson and Thomas Laurent},\n  year={2018},\n  url={https://openreview.net/forum?id=SJexcZc8G}\n}", "authorids": ["xbresson@ntu.edu.sg", "thomas.laurent@lmu.edu"], "authors": ["Xavier Bresson", "Thomas Laurent"], "TL;DR": "We propose an empirical study of RNN and ConvNet architectures for graphs with variable size and connectivity. Graph ConvNets combined with gated edges and residuality offer the best performance.", "pdf": "/pdf/635867926ea38f706f4823149331a1907868694a.pdf"}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1521705058476, "tcdate": 1521705058476, "number": 1, "cdate": 1521705058476, "id": "HycDxJ-qz", "invitation": "ICLR.cc/2018/Workshop/-/Paper35/Official_Comment", "forum": "SJexcZc8G", "replyto": "HJPTRC0YM", "signatures": ["ICLR.cc/2018/Workshop/Paper35/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper35/Authors"], "content": {"title": "ICLR", "comment": "We would like to thank the area chair and the referees for their time and the review of our paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Experimental Study of Neural Networks for Variable Graphs", "abstract": "Graph-structured data such as social networks, functional brain networks, chemical molecules have brought the interest in generalizing deep learning techniques to graph domains. In this work, we propose an empirical study of neural networks for graphs with variable size and connectivity. We rigorously compare several graph recurrent neural networks (RNNs) and graph convolutional neural networks (ConvNets) to solve two fundamental and representative graph problems, subgraph matching and graph clustering. Numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Interestingly, graph ConvNets are also 36% more accurate than non-learning (variational) techniques. The benefit of such study is to show that complex architectures like LSTM is not useful in the context of graph neural networks, but one should favour architectures with minimal inner structures, such as locality, weight sharing, index invariance, multi-scale, gates and residuality, to design efficient novel neural network models for applications like drugs design, genes analysis and particle physics.", "paperhash": "bresson|an_experimental_study_of_neural_networks_for_variable_graphs", "keywords": ["Graph neural networks", "ConvNets", "RNNs", "subgraph matching", "semi-supervised clustering"], "_bibtex": "@misc{\n  bresson2018an,\n  title={An Experimental Study of Neural Networks for Variable Graphs},\n  author={Xavier Bresson and Thomas Laurent},\n  year={2018},\n  url={https://openreview.net/forum?id=SJexcZc8G}\n}", "authorids": ["xbresson@ntu.edu.sg", "thomas.laurent@lmu.edu"], "authors": ["Xavier Bresson", "Thomas Laurent"], "TL;DR": "We propose an empirical study of RNN and ConvNet architectures for graphs with variable size and connectivity. Graph ConvNets combined with gated edges and residuality offer the best performance.", "pdf": "/pdf/635867926ea38f706f4823149331a1907868694a.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450278, "id": "ICLR.cc/2018/Workshop/-/Paper35/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SJexcZc8G", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper35/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper35/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper35/Reviewers", "ICLR.cc/2018/Workshop/Paper35/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450278}}, "tauthor": "xbresson@ntu.edu.sg"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582973814, "tcdate": 1519996358308, "number": 1, "cdate": 1519996358308, "id": "rkR6aT8dz", "invitation": "ICLR.cc/2018/Workshop/-/Paper35/Official_Review", "forum": "SJexcZc8G", "replyto": "SJexcZc8G", "signatures": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer1"], "content": {"title": "interesting results", "rating": "6: Marginally above acceptance threshold", "review": "SUMMARY.\n\nThe paper presents an experimental study of several Graph Neural network architectures on two graph problems: subgraph matching and graph clustering.\nThe experiments are carried out on artificial data.\n\n\n----------\n\nOVERALL JUDGMENT\nThe paper is clearly written but it is quite dense.\nThere is no much novelty in the proposed approach but the experiments are well carried out, and the conclusions, although not very surprising are interesting.\nSince this is a workshop paper I would suggest the authors focus more on the descriptions of architectural differences and maybe remove the paragraph about Learning vs. non-learning that seems a bit out of context.\n\n----------\n\nPROS\nGood experimental setting\nInteresting experimental results\n\nCONS\nThe paper is too dense (Figures are too small)\nExperiments only on artificial data", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Experimental Study of Neural Networks for Variable Graphs", "abstract": "Graph-structured data such as social networks, functional brain networks, chemical molecules have brought the interest in generalizing deep learning techniques to graph domains. In this work, we propose an empirical study of neural networks for graphs with variable size and connectivity. We rigorously compare several graph recurrent neural networks (RNNs) and graph convolutional neural networks (ConvNets) to solve two fundamental and representative graph problems, subgraph matching and graph clustering. Numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Interestingly, graph ConvNets are also 36% more accurate than non-learning (variational) techniques. The benefit of such study is to show that complex architectures like LSTM is not useful in the context of graph neural networks, but one should favour architectures with minimal inner structures, such as locality, weight sharing, index invariance, multi-scale, gates and residuality, to design efficient novel neural network models for applications like drugs design, genes analysis and particle physics.", "paperhash": "bresson|an_experimental_study_of_neural_networks_for_variable_graphs", "keywords": ["Graph neural networks", "ConvNets", "RNNs", "subgraph matching", "semi-supervised clustering"], "_bibtex": "@misc{\n  bresson2018an,\n  title={An Experimental Study of Neural Networks for Variable Graphs},\n  author={Xavier Bresson and Thomas Laurent},\n  year={2018},\n  url={https://openreview.net/forum?id=SJexcZc8G}\n}", "authorids": ["xbresson@ntu.edu.sg", "thomas.laurent@lmu.edu"], "authors": ["Xavier Bresson", "Thomas Laurent"], "TL;DR": "We propose an empirical study of RNN and ConvNet architectures for graphs with variable size and connectivity. Graph ConvNets combined with gated edges and residuality offer the best performance.", "pdf": "/pdf/635867926ea38f706f4823149331a1907868694a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582973638, "id": "ICLR.cc/2018/Workshop/-/Paper35/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper35/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper35/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper35/AnonReviewer3"], "reply": {"forum": "SJexcZc8G", "replyto": "SJexcZc8G", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582973638}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582929085, "tcdate": 1520348577852, "number": 2, "cdate": 1520348577852, "id": "H15iaX2uM", "invitation": "ICLR.cc/2018/Workshop/-/Paper35/Official_Review", "forum": "SJexcZc8G", "replyto": "SJexcZc8G", "signatures": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer2"], "content": {"title": "Nice experimental comparison, not well-suited for the workshop", "rating": "5: Marginally below acceptance threshold", "review": "An experimental comparison of several graph neural networks w.r.t. subgraph matching and clustering is presented. The authors distinguish graph CNNs and graph RNNs. The result is that CNNs are faster and more accurate than RNNs.\n\nIn my opinion, authoritative experimental comparisons in this domain are very important. Although the authors claim to provide an \"in-depth experimental comparison\", my feeling is that this is not possible within the scope of a short paper. Many questions remain open, e.g., details on parameters and network architecture. Moreover, the relevance of the two tasks ist not clear.\n\nStrong points:\n  * Important subject.\n  * Clear summary of the findings.\n\nWeak points:\n * Due to the nature of an experimental study, the novelty and originality is limited.\n * Details of the experimental setup are missing.\n * Only two tasks/datasets.\n\nIn summary, I think that the article does not fit well into the format of a workshop paper and, therefore, cannot recommend its acceptance.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Experimental Study of Neural Networks for Variable Graphs", "abstract": "Graph-structured data such as social networks, functional brain networks, chemical molecules have brought the interest in generalizing deep learning techniques to graph domains. In this work, we propose an empirical study of neural networks for graphs with variable size and connectivity. We rigorously compare several graph recurrent neural networks (RNNs) and graph convolutional neural networks (ConvNets) to solve two fundamental and representative graph problems, subgraph matching and graph clustering. Numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Interestingly, graph ConvNets are also 36% more accurate than non-learning (variational) techniques. The benefit of such study is to show that complex architectures like LSTM is not useful in the context of graph neural networks, but one should favour architectures with minimal inner structures, such as locality, weight sharing, index invariance, multi-scale, gates and residuality, to design efficient novel neural network models for applications like drugs design, genes analysis and particle physics.", "paperhash": "bresson|an_experimental_study_of_neural_networks_for_variable_graphs", "keywords": ["Graph neural networks", "ConvNets", "RNNs", "subgraph matching", "semi-supervised clustering"], "_bibtex": "@misc{\n  bresson2018an,\n  title={An Experimental Study of Neural Networks for Variable Graphs},\n  author={Xavier Bresson and Thomas Laurent},\n  year={2018},\n  url={https://openreview.net/forum?id=SJexcZc8G}\n}", "authorids": ["xbresson@ntu.edu.sg", "thomas.laurent@lmu.edu"], "authors": ["Xavier Bresson", "Thomas Laurent"], "TL;DR": "We propose an empirical study of RNN and ConvNet architectures for graphs with variable size and connectivity. Graph ConvNets combined with gated edges and residuality offer the best performance.", "pdf": "/pdf/635867926ea38f706f4823149331a1907868694a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582973638, "id": "ICLR.cc/2018/Workshop/-/Paper35/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper35/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper35/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper35/AnonReviewer3"], "reply": {"forum": "SJexcZc8G", "replyto": "SJexcZc8G", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582973638}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582804546, "tcdate": 1520621577400, "number": 3, "cdate": 1520621577400, "id": "r1-zOLxYG", "invitation": "ICLR.cc/2018/Workshop/-/Paper35/Official_Review", "forum": "SJexcZc8G", "replyto": "SJexcZc8G", "signatures": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer3"], "content": {"title": "Useful Insights", "rating": "7: Good paper, accept", "review": "The paper demonstrates an empirical study to compare graph recurrent networks and convolutional networks in various applications. The results show a general advantage of graph convolutional networks.\n\nThe conclusion provides useful insights between comparisons between architectures for graphs. It certainly suffices as a workshop paper.\n\nOne problem is that the paper only selects a few possible instantiations for both recurrent and convolutional networks. There are a lot more ways to use either recurrent or convolutional networks for variable-size graphs, and the paper should include a discussion, even if experiments could not have be included for the workshop submission.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Experimental Study of Neural Networks for Variable Graphs", "abstract": "Graph-structured data such as social networks, functional brain networks, chemical molecules have brought the interest in generalizing deep learning techniques to graph domains. In this work, we propose an empirical study of neural networks for graphs with variable size and connectivity. We rigorously compare several graph recurrent neural networks (RNNs) and graph convolutional neural networks (ConvNets) to solve two fundamental and representative graph problems, subgraph matching and graph clustering. Numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Interestingly, graph ConvNets are also 36% more accurate than non-learning (variational) techniques. The benefit of such study is to show that complex architectures like LSTM is not useful in the context of graph neural networks, but one should favour architectures with minimal inner structures, such as locality, weight sharing, index invariance, multi-scale, gates and residuality, to design efficient novel neural network models for applications like drugs design, genes analysis and particle physics.", "paperhash": "bresson|an_experimental_study_of_neural_networks_for_variable_graphs", "keywords": ["Graph neural networks", "ConvNets", "RNNs", "subgraph matching", "semi-supervised clustering"], "_bibtex": "@misc{\n  bresson2018an,\n  title={An Experimental Study of Neural Networks for Variable Graphs},\n  author={Xavier Bresson and Thomas Laurent},\n  year={2018},\n  url={https://openreview.net/forum?id=SJexcZc8G}\n}", "authorids": ["xbresson@ntu.edu.sg", "thomas.laurent@lmu.edu"], "authors": ["Xavier Bresson", "Thomas Laurent"], "TL;DR": "We propose an empirical study of RNN and ConvNet architectures for graphs with variable size and connectivity. Graph ConvNets combined with gated edges and residuality offer the best performance.", "pdf": "/pdf/635867926ea38f706f4823149331a1907868694a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582973638, "id": "ICLR.cc/2018/Workshop/-/Paper35/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper35/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper35/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper35/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper35/AnonReviewer3"], "reply": {"forum": "SJexcZc8G", "replyto": "SJexcZc8G", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper35/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582973638}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573566946, "tcdate": 1521573566946, "number": 105, "cdate": 1521573566600, "id": "HJPTRC0YM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SJexcZc8G", "replyto": "SJexcZc8G", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Experimental Study of Neural Networks for Variable Graphs", "abstract": "Graph-structured data such as social networks, functional brain networks, chemical molecules have brought the interest in generalizing deep learning techniques to graph domains. In this work, we propose an empirical study of neural networks for graphs with variable size and connectivity. We rigorously compare several graph recurrent neural networks (RNNs) and graph convolutional neural networks (ConvNets) to solve two fundamental and representative graph problems, subgraph matching and graph clustering. Numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Interestingly, graph ConvNets are also 36% more accurate than non-learning (variational) techniques. The benefit of such study is to show that complex architectures like LSTM is not useful in the context of graph neural networks, but one should favour architectures with minimal inner structures, such as locality, weight sharing, index invariance, multi-scale, gates and residuality, to design efficient novel neural network models for applications like drugs design, genes analysis and particle physics.", "paperhash": "bresson|an_experimental_study_of_neural_networks_for_variable_graphs", "keywords": ["Graph neural networks", "ConvNets", "RNNs", "subgraph matching", "semi-supervised clustering"], "_bibtex": "@misc{\n  bresson2018an,\n  title={An Experimental Study of Neural Networks for Variable Graphs},\n  author={Xavier Bresson and Thomas Laurent},\n  year={2018},\n  url={https://openreview.net/forum?id=SJexcZc8G}\n}", "authorids": ["xbresson@ntu.edu.sg", "thomas.laurent@lmu.edu"], "authors": ["Xavier Bresson", "Thomas Laurent"], "TL;DR": "We propose an empirical study of RNN and ConvNet architectures for graphs with variable size and connectivity. Graph ConvNets combined with gated edges and residuality offer the best performance.", "pdf": "/pdf/635867926ea38f706f4823149331a1907868694a.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 6}