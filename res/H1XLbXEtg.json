{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028578400, "tcdate": 1490028578400, "number": 1, "id": "SJcm_FTie", "invitation": "ICLR.cc/2017/workshop/-/paper65/acceptance", "forum": "H1XLbXEtg", "replyto": "H1XLbXEtg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028578920, "id": "ICLR.cc/2017/workshop/-/paper65/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1XLbXEtg", "replyto": "H1XLbXEtg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028578920}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489758481186, "tcdate": 1489165476970, "number": 1, "id": "BJai2Ugse", "invitation": "ICLR.cc/2017/workshop/-/paper65/official/review", "forum": "H1XLbXEtg", "replyto": "H1XLbXEtg", "signatures": ["ICLR.cc/2017/workshop/paper65/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper65/AnonReviewer2"], "content": {"title": "Raising the score to 6 after considering the authors' response", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes an online method for training reinforcement learning agents to perform several tasks at once. The method samples a task to train on after each episode. The main idea is to assume that a reference score is available for each task and to use the agent's relative performance with respect to the reference score to determine the probability of sampling the task.  The result is that tasks with lower relative performance are sampled more frequently.\n\nThe experiments show that the proposed strategy outperforms uniform sampling of tasks during online training. While the idea is simple it is very ad hoc and has a number of unexplored parameters (temperature, number of recent scores, number of training steps with uniform sampling). There are more principled and well understood methods that could be applied to this problem. For example, why not apply a UCB-style non-stationary bandit algorithm that attempts to balance exploration and exploitation? Such a method would keep periodically going back to tasks with high relative performance, which could help mitigate catastrophic forgetting. The proposed method seems to ignore such potential interference between tasks.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489277447053, "id": "ICLR.cc/2017/workshop/-/paper65/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper65/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper65/AnonReviewer2", "ICLR.cc/2017/workshop/paper65/AnonReviewer1"], "reply": {"forum": "H1XLbXEtg", "replyto": "H1XLbXEtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper65/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper65/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489277447053}}}, {"tddate": null, "tmdate": 1489636077518, "tcdate": 1489636077518, "number": 5, "id": "ByresFPje", "invitation": "ICLR.cc/2017/workshop/-/paper65/public/comment", "forum": "H1XLbXEtg", "replyto": "H1XLbXEtg", "signatures": ["~Sahil_Sharma1"], "readers": ["everyone"], "writers": ["~Sahil_Sharma1"], "content": {"title": "UCB experiments", "comment": "\nThanks to AnonReviewer 2 again, for pointing out the missing comparisons with UCB-style algorithms. We are currently running the discounted-UCB-tuned experiments on multi tasking instance MT1. One of the UCB agents has been trained for 126 million steps (close to half the training time).\n\nWhile the q_am performance of this UCB agent matches that of our A3CSH agent (both are near 0.4), the UCB-based approach almost completely ignores 2 out of the 6 games. As a result of this the q_hm performance of UCB agent is at 0.124.\nIn comparison, after the same number of training steps, performance (q_hm) of our A3CSH agent was at 0.349.  While the results are definitely preliminary, this close to triple initial performance exhibited by A3CSH supports our general belief that A3CSH agents are better suited for multi-tasking than UCB-based approaches."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487315275540, "tcdate": 1487315275540, "id": "ICLR.cc/2017/workshop/-/paper65/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper65/reviewers"], "reply": {"forum": "H1XLbXEtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487315275540}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489400252460, "tcdate": 1489277446299, "number": 2, "id": "SkAZMGzjg", "invitation": "ICLR.cc/2017/workshop/-/paper65/official/review", "forum": "H1XLbXEtg", "replyto": "H1XLbXEtg", "signatures": ["ICLR.cc/2017/workshop/paper65/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper65/AnonReviewer1"], "content": {"title": "Simple but effective - final review", "rating": "7: Good paper, accept", "review": "I have updated my rating to a 7 after reading the authors' response.\n\nThis paper uses active sampling to select tasks to train on in a multi-task, deep RL setting. Their multi-task baseline, in comparison, chooses the next task using uniform sampling. The active sampling is done by comparing the current score on each task with its 'aspirational' high score - a score that could come from human performance or from single-task training or from published results. Tasks that are underperforming with respect to the aspirational high score are more likely to be sampled. \n\nCompared to the uniform sampling, active sampling yields much higher scores. In fact, uniform sampling causes some tasks to not get off the ground at all, where as all are able to learn with active sampling. This demonstrates the challenge of multi-task deep RL, where tasks may be adversarial and thus prevent others from learning.\n\nPros: Active sampling is a well-known approach, but it has not been tried for multi-task deep RL, which is a much harder problem than, e.g., supervised learning. This paper is simple, but the approach is quite effective. It will be of interest to those in the community that are studying continual or transfer learning in RL.  \n\nCons: The results are limited. The authors only used 6 Atari games, and this type of result is highly variable and might not hold for a different set. However, for a workshop paper, it is understandable. More sophisticated methods, such as a multi-armed bandit formulation, might have been tried. My one complaint is that the authors did not show how often the different games were chosen, nor show any visualisation of the game selection over time. This would have been quite interesting. \n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489277447053, "id": "ICLR.cc/2017/workshop/-/paper65/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper65/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper65/AnonReviewer2", "ICLR.cc/2017/workshop/paper65/AnonReviewer1"], "reply": {"forum": "H1XLbXEtg", "replyto": "H1XLbXEtg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper65/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper65/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489277447053}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489305443387, "tcdate": 1489304444183, "number": 1, "id": "HyVKjdzsl", "invitation": "ICLR.cc/2017/workshop/-/paper65/public/comment", "forum": "H1XLbXEtg", "replyto": "SkAZMGzjg", "signatures": ["~Sahil_Sharma1"], "readers": ["everyone"], "writers": ["~Sahil_Sharma1"], "content": {"title": "Thanks for the review", "comment": "Thanks for the largely positive review! We would like to point out some clarifying points regarding the cons:\n\n---- Results are limited.  The authors only used 6 Atari games.\nWe would like to make two important points here.\n    1. This is not the case. We have demonstrated results on 3 different sets of 6 games. In fact our paper is the first to report performance of a multi-tasking algorithm with hyper-parameters tuned on one multi-tasking instance (MT1 in our paper) on other multi-tasking instances (MT2 and MT3). The details are indeed scanty in this manuscript and we apologize for that. The 3 page limit really limits the amount of material which can be presented. Please have a look at the expanded version of the paper if you\u2019d like to : https://arxiv.org/abs/1702.06053 We have presented results on 3 different multi-tasking instances as well as two different sensible architectures. We have also reported on what happens when we double the \u201cbaseline\u201d scores used by our method. \n    2. We have also run experiments on 12-task instances and we observe similar performance levels (Similar to MT2 and MT3) on 12-task instance as well. The reason we haven\u2019t included it in manuscript yet is that we are still running the baseline uniform sampling method on this instance. Note that the highest number of dis-similar tasks on which multi-tasking instance results have been published (without increasing the network size significantly) is 8 by actor mimic. This is a 50% improvement on the number of games and also consumes 50% lesser data and compute (since we train for only half the time required to train all the experts).\n\n---- and this type of result is highly variable and might not hold for a different set\n\nWe agree that this is indeed a problem with Multi Tasking learning literature. To address this, we tuned the hyper-parameters on MT1 and then also demonstrated performance on MT2 and MT3. \n\n---- More sophisticated methods, such as a multi-armed bandit formulation, might have been tried\n\nThis is something Reviewer2 pointed out as well. We agree that this is the logical second step. We are currently trying out Discounted-UCB-tuned on the multi-tasking instance MT1.  Our preliminary experiments indicate that the bandit formulation isn't able to perform as well as the active sampling formulation. But we do agree that this is the logical next step and are actively pursuing it. In fact we are also in parallel pursuing the meta-learning problem of learning an actor critic as the meta-agent which dictates the next game to train on. \n\n---- did not show how often the different games were chosen, nor show any visualisation of the game selection over time\n\nApologies for not including it in this manuscript. The 3 page limit really restricts what we could and could not include in this paper. We chose to include the full learning algorithm so that the active sampling procedure  is clear. We have already included  all of these and many more interesting analyses in the arxiv version of the paper : https://arxiv.org/abs/1702.06053 which is slightly longer at 13 pages. Among the analyses included are those of the hidden neuron activations. We have shown that our method learns more task-agnostic neurons and this is perhaps the reason that it does so well on the multi-tasking problem.\n\nIn conclusion the main contributions of this work are:\na) We have shown that a simple active sampling approach in and of itself is a valid technique for multi-task learning.\nb) We show results on 3 sets of Multi Tasking instances. Previous works in the area show results on at most one instance.\nc) Hyper-Parameter tuning was done on only  one instance (MT1). We demonstrate how the method generalizes to two other MT instances and thus show robustness.\nd) We propose sensible evaluation metrics for the multi tasking problem which help in separating  the good multi tasking algorithms from those that are great on a narrow set of tasks but do not perform well on others.\ne) Previous works in the area do not perform any analyses on why their methods work well vis-a-vis baselines. We also perform analyses on why we think our method performs much better than baseline methods. We apologize for not including these analyses in the workshop paper; it was hard given the 3 page limit. The arxiv version of the paper (https://arxiv.org/pdf/1702.06053.pdf) contains all of these analyses. We will definitely present these analyses and all the experimental results at ICLR, if our paper gets accepted. \nf) While previous approaches perform experiments only on a single multi-task instance with up to 8 games, we perform experiments on 12-game multi-task instances as well. We have obtained very promising results. Same  hyper-parameters found by tuning on MT1 were used for these experiments. \n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487315275540, "tcdate": 1487315275540, "id": "ICLR.cc/2017/workshop/-/paper65/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper65/reviewers"], "reply": {"forum": "H1XLbXEtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487315275540}}}, {"tddate": null, "tmdate": 1489305160143, "tcdate": 1489305160143, "number": 4, "id": "SJx8Rufox", "invitation": "ICLR.cc/2017/workshop/-/paper65/public/comment", "forum": "H1XLbXEtg", "replyto": "SkxZAuGse", "signatures": ["~Sahil_Sharma1"], "readers": ["everyone"], "writers": ["~Sahil_Sharma1"], "content": {"title": "Part 2 of the above comment", "comment": "[Part 2/2]\n\n---- why not apply a UCB-style non-stationary bandit algorithm that attempts to balance exploration and exploitation?\n\nWe agree that it makes a lot of sense to experiment with UCB-style non stationary algorithms in this setting. We are currently running experiments for discounted-UCB-tuned but have not achieved very promising results. This is the reason that we haven't included it in the manuscript yet. We will definitely include UCB baselines in the final version of the paper if it gets accepted. In fact we are  also in parallel working on an MDP formulation of the meta-learner which decides the next task to train on. Experiments are in a very preliminary stage, however.\n\n----  Such a method would keep periodically going back to tasks with high relative performance, which could help      mitigate catastrophic forgetting. The proposed method seems to ignore such potential interference between tasks.\n\nThis is not true. In fact our method is \u201cdesigned\u201d to beat catastrophic forgetting/destructive interference. If the learning of task 1 causes task 2 performance to fall down, we would immediately start actively sampling task 2 more and get better at it. \n\nIn conclusion I\u2019d like to say that while other approaches to the problem of multi-task learning using active sampling: like UCB are valid; it does not invalidate simpler approaches, especially since these simpler approaches work well on a wide variety of different multi-tasking instances. The main contributions of this work are:\na) We have shown that a simple active sampling approach in and of itself is a valid technique for multi-task learning.\nb) We show results on 3 sets of Multi Tasking instances. Previous works in the area show results on at most one instance.\nc) Hyper-Parameter tuning was done on only  one instance (MT1). We demonstrate how the method generalizes to two other MT instances and thus show robustness.\nd) We propose sensible evaluation metrics for the multi tasking problem which help identify the good multi tasking algorithms from those that are great on a narrow set of tasks but do not perform well on others.\ne) Previous works in the area do not perform any analyses on why their methods work well vis-a-vis baselines. We also perform analyses on why we think our method performs much better than baseline methods. We apologize for not including these analyses in the workshop paper; it was hard given the 3 page limit. The arxiv version of the paper (https://arxiv.org/pdf/1702.06053.pdf) contains all of these analyses. We will definitely present these analyses and all the experimental results at ICLR, if our paper were to get accepted.\nf) While previous approaches perform experiments only on a single multi-task instance with up to 8 games, we perform experiments on 12-game multi-task instances as well. We have obtained very promising results. Same  hyper-parameters found by tuning on MT1 were used for these experiments. No additional hyper-parameter tuning was done."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487315275540, "tcdate": 1487315275540, "id": "ICLR.cc/2017/workshop/-/paper65/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper65/reviewers"], "reply": {"forum": "H1XLbXEtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487315275540}}}, {"tddate": null, "tmdate": 1489305080062, "tcdate": 1489305080062, "number": 2, "id": "SkxZAuGse", "invitation": "ICLR.cc/2017/workshop/-/paper65/public/comment", "forum": "H1XLbXEtg", "replyto": "BJai2Ugse", "signatures": ["~Sahil_Sharma1"], "readers": ["everyone"], "writers": ["~Sahil_Sharma1"], "content": {"title": "Thank you for the informative review", "comment": "[Part 1/2]\nThank you for the very informative review. We agree with your general point that Multi-Arm bandits is certainly an algorithm that we should try out in this multi-task setting of active sampling. In fact we are running experiments on this formulation right now (with Discounted-UCB-tuned) and will try to include the experiments in this workshop paper, if the paper gets accepted. Since this comment is kind of long, i am breaking it into two parts.\n\nWe would like to clarify some of the points raised by you:\n\n---- While  the idea is simple it is very ad hoc\n\nWe would like to point out that active sampling is a very active area of research in machine learning. What we have presented is the \u201csimplest\u201d form of active sampling. While multi arm bandit formulations of our idea are certainly more sophisticated, they also fall in the same domain of active sampling. The reason we were so excited by this simple idea is because the improvements we were able to get over the baselines are staggering. With just a simple change in sampling frequency, performance improves from close to 30% to close to 80%. \n\n---- and has a number of unexplored parameters (temperature, number of recent scores, number of training steps with uniform sampling).\n\nWe would like to make three important points here. They are related to the basic idea that some of the hyper-parameters in a model are perhaps not very important whereas others are. \n1- We did hyper-parameter tune for the temperature hyper-parameter. Apologies for not including this information in the paper because of the 3 page limit. We have described the complete experimental setup in detail in the arxiv version of this paper : https://arxiv.org/abs/1702.06053 \n2- We believe that the other hyper-parameters the reviewer has mentioned are not important  for the performance. One could make the same argument against almost any state of the art method. Many deep RL algorithms have frame stacking (to convert a POMDP problem into a more markovian version) replay memory (for breaking correlations in updates)  and convolutional layers (for doing well on visual control problems). There are usually at least several dozen hyper-parameter choices one must make including size of convolutional filters, stride sizes, number of filters, number of layers for the convolutions, the number of frames to be stacked, or the size of replay memory.I think the reason most DRL algorithms rightly choose to make arbitrary choices in such situations is that it isn't necessary to fine tune every  hyper-parameter in the model, only the important ones are tuned. It is for this reason that we decided not to tune for number of training steps with uniform sampling and number of recent scores for average calculation. \n3- While UCB style non stationary bandit algorithms are certainly more sophisticated than our simple active sampling method, they do have more tunable important hyper-parameters. This makes the task of hyper-parameter tuning much harder. What we have shown in our work is that a much simpler method with fewer important tunable hyper-parameters (i) performs exceedingly well in the online multi task setting. The q_am metric performance on MT1 is at 0.80 and is comparable to actor mimic performance. (ii) We are able to match offline expert-supervision based performance with only half the data which is required for training the experts. (iii) the multi tasking algorithm with hyper-parameters tuned on one multi-tasking instance works well on other multi tasking instances as well. (iv) We have also run experiments on 12-task instances and we observe similar performance levels (To MT2 and MT3) on the 12-task instance as well. The reason we haven\u2019t included it in manuscript yet is that we are still running the baseline uniform sampling method on it. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487315275540, "tcdate": 1487315275540, "id": "ICLR.cc/2017/workshop/-/paper65/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper65/reviewers"], "reply": {"forum": "H1XLbXEtg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487315275540}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487737404762, "tcdate": 1487315274951, "number": 65, "id": "H1XLbXEtg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "H1XLbXEtg", "signatures": ["~Sahil_Sharma1"], "readers": ["everyone"], "content": {"title": "Online Multi-Task Learning Using Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning  for  goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require  extensive training.\nWe propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or  active learning setup without the need for expert supervision.\n", "pdf": "/pdf/06d0f058e36a3920d22f7679c6229541b25654ed.pdf", "TL;DR": "A novel online multi-task reinforcement learning approach which does not require expert supervision", "paperhash": "sharma|online_multitask_learning_using_active_sampling", "conflicts": ["cse.iitm.ac.in"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Sahil Sharma", "Balaraman Ravindran"], "authorids": ["sahil@cse.iitm.ac.in", "ravi@cse.iitm.ac.in"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 8}