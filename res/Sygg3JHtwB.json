{"notes": [{"id": "Sygg3JHtwB", "original": "SkxQLe1KvS", "number": 1938, "cdate": 1569439655661, "ddate": null, "tcdate": 1569439655661, "tmdate": 1577168294167, "tddate": null, "forum": "Sygg3JHtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DA8TJOjW4U", "original": null, "number": 1, "cdate": 1576798736403, "ddate": null, "tcdate": 1576798736403, "tmdate": 1576800899970, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Decision", "content": {"decision": "Reject", "comment": "The paper is rejected based on unanimous reviews.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725932, "tmdate": 1576800277946, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Decision"}}}, {"id": "Bkecwky9KH", "original": null, "number": 1, "cdate": 1571577698453, "ddate": null, "tcdate": 1571577698453, "tmdate": 1573892183059, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "First, I would like to point out that there has not been a conclusion or discussion section included, therefore the paper appears to be incomplete.\nAside from this the main contribution of the paper is a study on optimising the step size in gradient methods. They achieve this through the use of alternating direction method of multipliers. Given all the formulations provided, it appears as if this method does not rely on second order information or any probabilistic method.\nAn extension of the proposed method covers stochastic environments.\nThe results demonstrate some promising properties, including convergence and improvements on MNIST, SVHN, Cifar-10 and Cifar-100, albeit marginal improvements.\nAlthough the results appear to be promising the overall structure of the paper and the method presented are based upon established techniques, therefore the technical contribution is rather limited.\nI have read the rebuttal and answered to some of the concerns of the authors.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1938/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1938/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633865901, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1938/Reviewers"], "noninvitees": [], "tcdate": 1570237730116, "tmdate": 1575633865915, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Official_Review"}}}, {"id": "S1xEkJKysr", "original": null, "number": 6, "cdate": 1572994779745, "ddate": null, "tcdate": 1572994779745, "tmdate": 1573776678072, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "Bkecwky9KH", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment", "content": {"title": "Response to review #2", "comment": "\nWe agree with your comment, so we will append a conclusion or discussion section.\n\nThe performance improvement on CIFAR-10 and CIFAR-100 datasets is not marginal. Could you give a reference that achieves similar performance using RMSProp and Adam on ResNet-18?\n\nWe first defined step size adaptation as a constrained optimization problem and converted it into a solvable problem by applying linearization and introducing slack variables. Then, we analyzed convergence of the proposed method with L2 regularization that is the most common regularization technique. To alleviate bad convergence problem, we developed the upper bound decay that is a generalized technique of the step size decay. Furthermore, we extended the proposed method into the stochastic learning environments. Thus, this paper is not just a list of existing methods. In this work, is there really no technical contribution? You should provide some references to criticize the performance improvement and technical contributions."}, "signatures": ["ICLR.cc/2020/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1938/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1938/Authors|ICLR.cc/2020/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148687, "tmdate": 1576860529483, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment"}}}, {"id": "ryeaLodyoS", "original": null, "number": 5, "cdate": 1572993877444, "ddate": null, "tcdate": 1572993877444, "tmdate": 1573708902471, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "HkxuABPC5S", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment", "content": {"title": "Response to review #4", "comment": "\n(1, 2) We don't know what you're pointing out. SSO always showed faster convergence speed than RMSProp and Adam. In addition, SSO consistently showed the performance improvement with relatively large initial learning rate (e.g., 0.5). Note that RMSProp and Adam commonly use very small initial learning rate (e.g., 0.001`). Thus, your comments are incorrect. Furthermore, SSO showed comparable convergence speed with L4-Adam and AdaBound while improving the generalization significantly.\n\n(3) We tuned hyperparameters of the competitors with the grid search and achieved the experimental results similar to other papers and GitHub repositories on CNN and ResNet-18. Especially, for L4-Adam and AdaBound, we used the best hyperparameters suggested in their original papers.\n\nYou should provide clearer and more understandable review."}, "signatures": ["ICLR.cc/2020/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1938/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1938/Authors|ICLR.cc/2020/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148687, "tmdate": 1576860529483, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment"}}}, {"id": "HkxuABPC5S", "original": null, "number": 2, "cdate": 1572922831681, "ddate": null, "tcdate": 1572922831681, "tmdate": 1572972404182, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a new step size adaptation in first-order gradient methods. The proposed method establishes a new optimization problem with the first-order expansion of loss function and the regularization, where the step size is treated as a variable.  ADMM is adopted to solve the optimization problem.\n\nThis paper should be rejected because (1) the proposed method does not show the convergence rate improvement of the gradient method with other step sizes adaptation methods. (2) the linearization of the objective function leads the step size to be small ($0<\\eta<\\epsilon$), which could slow down the convergence in some cases. (3) the experiments generally do not support a significant contribution. In table 1, the results of the competitor are not with the optimal step sizes. The limit grid search range could not verify the empirical superiority of the proposed method.\n\n\nMinor comments:\nThe y-axis label of (a) panel in each figure is wrong. I guess it should be \"Training loss \"."}, "signatures": ["ICLR.cc/2020/Conference/Paper1938/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1938/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633865901, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1938/Reviewers"], "noninvitees": [], "tcdate": 1570237730116, "tmdate": 1575633865915, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Official_Review"}}}, {"id": "Syx5cIUHFS", "original": null, "number": 2, "cdate": 1571280529724, "ddate": null, "tcdate": 1571280529724, "tmdate": 1571982810731, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "Syg0LyARdH", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment", "content": {"title": "New update rule and experiment results", "comment": "\nI am sorry for the late reply. I realized that I forgot to change the inequality constraints into the equality constraints in derivation of the Lagrangian. Thus, the nonnegative constraints on the dual variables should be removed. For this reason, I conducted the experiments again using a new update rule for the dual variables without nonnegative constraints. The experiments results are here:\n------------------------------------------------------------------------------------------------\n|\t               |     MNIST       |      SVHN       |    CIFAR-10  |   CIFAR-100  |\n===========================================================\n| SSO-SGD    | 99.32+=0.05 | 96.45+=0.13 | 94.42+=0.19 | 75.54+=0.14 |\n------------------------------------------------------------------------------------------------\n| SSO-Adam | 99.28+=0.05 | 95.75+=0.07 | 92.43+=0.06 | 71.23+=0.18 |\n------------------------------------------------------------------------------------------------\nThe results are generally similar to the results in the paper, and I got further improvement on CIFAR-100 dataset. I really appreciate to your comment. If I have a change to revise the paper, I will modify the update rule of the dual variables and the experiment results.\n\nThe convergence of SSO with L2 regularization is guaranteed because its objective function consists of two convex functions and one strongly convex function [1]. However, if the regularization term is not strongly convex, the convergence is not guaranteed as you mentioned. To answer this general situation, I further studied the convergence of the multi-block ADMM and found an advanced ADMM called RP-ADMM [2]. It randomly permutes the order of the update rules for primal variables and empirically showed the convergence of ADMM on multi-block objective functions. I will also append your concern and RP-ADMM for SSO in the appendix of the paper.\n\n\nThanks.\n\n\n[1] Lin, T., Ma, S., Zhang, S. Global Convergence of Unmodified 3-block ADMM for a Class of Convex Minimization Problems. J. Sci. Comput 76, 69-88 (2018).\n[2] http://www.iciam2015.cn/Yinyu%20Ye.html"}, "signatures": ["ICLR.cc/2020/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1938/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1938/Authors|ICLR.cc/2020/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148687, "tmdate": 1576860529483, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment"}}}, {"id": "S1l1vLzkqB", "original": null, "number": 4, "cdate": 1571919447436, "ddate": null, "tcdate": 1571919447436, "tmdate": 1571919447436, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "rkx1fl00YH", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment", "content": {"title": "Unrealistic training environments", "comment": "\nI agree with your concern because eta is trivial as epsilon for g^T v > 0 and zero for g^T v < 0 on the loss function without the regularization term. Nonetheless, this solution is mathematically correct due to the linearity of the simplified loss function.\n\nFurthermore, if the regularization term is added or minibatch is used, the solution is no longer trivial. Your concern is about the training environments that the training dataset perfectly represents the test dataset (non-overfitting) and the size of the training dataset is tiny (non-minibatch). These training environments are unrealistic in deep learning, so I did not consider them in designing the method.\n\nThanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1938/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1938/Authors|ICLR.cc/2020/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148687, "tmdate": 1576860529483, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment"}}}, {"id": "SkeABj_TKH", "original": null, "number": 3, "cdate": 1571814214443, "ddate": null, "tcdate": 1571814214443, "tmdate": 1571901475973, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment", "content": {"title": "How do we explained why just expanding J(\u03b8) but not \u2126(\u03b8)?", "comment": "An excellent job but still some confusions.\n\nWhy we just expanding J(\u03b8 \u2212 \u03b7v) as  J(\u03b8)\u2212\u03b7 g^T v, but not  \u2126(\u03b8 \u2212 \u03b7v) as  \u2126(\u03b8)\u2212\u03b7 g^T v?\n\nI know you may want to get a closed form like eq.(14), but it is not a sufficient reason in my opinion. I think we must demonstrate that ignoring higher order of J(\u03b8 \u2212 \u03b7v) is reasonable.\n\nMeanwhile, can we do it while the loss has no regularizer term?"}, "signatures": ["~Jianlin_Su1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jianlin_Su1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187425, "tmdate": 1576860563248, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment"}}}, {"id": "rkx1fl00YH", "original": null, "number": 4, "cdate": 1571901447154, "ddate": null, "tcdate": 1571901447154, "tmdate": 1571901447154, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "HJx3p4tTYH", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment", "content": {"title": "trivial if no regularization term", "comment": "if without regularization term, the optimal \u03b7 of eq.(3) is just \u03f5, which is just a trivial result and makes no sense."}, "signatures": ["~Jianlin_Su1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Jianlin_Su1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187425, "tmdate": 1576860563248, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment"}}}, {"id": "HJx3p4tTYH", "original": null, "number": 3, "cdate": 1571816644350, "ddate": null, "tcdate": 1571816644350, "tmdate": 1571816644350, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "SkeABj_TKH", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment", "content": {"comment": "\nLinearization was not applied to obtain a closed-form solution, but to simplify the severely complex and nonlinear loss function. In this process, the approximation error inevitably occurs, so there is no reason to unnecessarily linearize the regularization term when it is simple (e.g., convex).\n\nSSO can be derived without regularization term, but we did not consider this training environment because most objective functions for training deep neural networks include the regularization term to improve training or testing performances.\n\nThanks.", "title": "Linearization on the objective function"}, "signatures": ["ICLR.cc/2020/Conference/Paper1938/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1938/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1938/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1938/Authors|ICLR.cc/2020/Conference/Paper1938/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148687, "tmdate": 1576860529483, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Official_Comment"}}}, {"id": "Syg0LyARdH", "original": null, "number": 2, "cdate": 1570852693665, "ddate": null, "tcdate": 1570852693665, "tmdate": 1570852693665, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "rJl_tE60dS", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment", "content": {"comment": "Dear Author:\n           Thank you for your answer.\n1.   The dual variables lambda1 lambda2 correspond to two linear equality constraints eta-s1=0 and epsilon-eta-s2=0, respectively. So in this case, lambda1 and lambda2 can be any real number to my knowledge. If lambda1 and lambda2 correspond to any inequality constraint, this means that lambda1 lambda2 should be nonnegative [1].\n2.   Equation 4 actually has three decision variables eta, s1, and s2, even though s1 and s2 are auxiliary. So it should be multi-block ADMM. By the way, there is no single ADMM to my knowledge. The goal of the ADMM is to split a problem into multiple subproblems. So ADMM has at least two variables[2]. If there is only one variable, that is called the augmented Lagrangian method (ALM)[2].\nPlease point out my mistakes if necessary. Thanks.\n[1]. Boyd, Stephen, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n[2]. Boyd, Stephen, et al. \"Distributed optimization and statistical learning via the alternating direction method of multipliers.\" Foundations and Trends\u00ae in Machine learning 3.1 (2011): 1-122.\n", "title": "A more clear description of confusion"}, "signatures": ["~Junxiang_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Junxiang_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187425, "tmdate": 1576860563248, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment"}}}, {"id": "r1xG0jhCuB", "original": null, "number": 1, "cdate": 1570847689689, "ddate": null, "tcdate": 1570847689689, "tmdate": 1570847689689, "tddate": null, "forum": "Sygg3JHtwB", "replyto": "Sygg3JHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment", "content": {"comment": "Dear author:\n             Thank you for your interesting work. Step size optimization is an important topic. However, I find it difficult to understand some points in the paper.\n1.  In page 2, why the dual variables lambda1 and lambda2 must be nonnegative? This may explain why there is a max operation in Equations 8 and 9.\n2.  The convergence analysis of the proposed ADMM is confusing. As far as I know, Equation 4 is a multi-block ADMM (i.e., with more than two variables), and the multi-block ADMM is not guaranteed to converge. See the following paper for reference. \nThe direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent\nhttps://link.springer.com/article/10.1007/s10107-014-0826-5.\n               Thanks.", "title": "Interesting approach some points are confusing"}, "signatures": ["~Junxiang_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Junxiang_Wang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ngs0726@gmail.com", "dmhyeon@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "title": "Step Size Optimization", "authors": ["Gyoung S. Na", "Dongmin Hyeon", "Hwanjo Yu"], "pdf": "/pdf/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "TL;DR": "We propose an efficient and effective step size adaptation method for the gradient methods.", "abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.", "keywords": ["Deep Learning", "Step Size Adaptation", "Nonconvex Optimization"], "paperhash": "na|step_size_optimization", "original_pdf": "/attachment/70b268c295747e06120638d5ce4d6c3ff77e67ab.pdf", "_bibtex": "@misc{\nna2020step,\ntitle={Step Size Optimization},\nauthor={Gyoung S. Na and Dongmin Hyeon and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Sygg3JHtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sygg3JHtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187425, "tmdate": 1576860563248, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1938/Authors", "ICLR.cc/2020/Conference/Paper1938/Reviewers", "ICLR.cc/2020/Conference/Paper1938/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1938/-/Public_Comment"}}}], "count": 13}