{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730166454, "tcdate": 1509134049078, "number": 747, "cdate": 1518730166444, "id": "B1KJJf-R-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "B1KJJf-R-", "original": "SkOkkGZ0b", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Neural Program Search: Solving Data Processing Tasks from Description and Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "pdf": "/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf", "TL;DR": "Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model", "paperhash": "polosukhin|neural_program_search_solving_data_processing_tasks_from_description_and_examples", "_bibtex": "@misc{\npolosukhin2018neural,\ntitle={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\nauthor={Illia Polosukhin and Alexander Skidanov},\nyear={2018},\nurl={https://openreview.net/forum?id=B1KJJf-R-},\n}", "keywords": ["Deep learning", "Structured Prediction", "Natural Language Processing", "Neural Program Synthesis"], "authors": ["Illia Polosukhin", "Alexander Skidanov"], "authorids": ["illia@near.ai", "alex@near.ai"]}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": ["BJTtWDyPM"], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260091616, "tcdate": 1517249624518, "number": 366, "cdate": 1517249624504, "id": "SJeDEJ6BG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "B1KJJf-R-", "replyto": "B1KJJf-R-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "the reviewers all found the problem to be important, the proposed approach to be interesting, but the manuscript to be preliminary. i agree with them.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Program Search: Solving Data Processing Tasks from Description and Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "pdf": "/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf", "TL;DR": "Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model", "paperhash": "polosukhin|neural_program_search_solving_data_processing_tasks_from_description_and_examples", "_bibtex": "@misc{\npolosukhin2018neural,\ntitle={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\nauthor={Illia Polosukhin and Alexander Skidanov},\nyear={2018},\nurl={https://openreview.net/forum?id=B1KJJf-R-},\n}", "keywords": ["Deep learning", "Structured Prediction", "Natural Language Processing", "Neural Program Synthesis"], "authors": ["Illia Polosukhin", "Alexander Skidanov"], "authorids": ["illia@near.ai", "alex@near.ai"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642502581, "tcdate": 1511599382626, "number": 1, "cdate": 1511599382626, "id": "SkJQ6oUgG", "invitation": "ICLR.cc/2018/Conference/-/Paper747/Official_Review", "forum": "B1KJJf-R-", "replyto": "B1KJJf-R-", "signatures": ["ICLR.cc/2018/Conference/Paper747/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Promising direction, but too preliminary", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a seq2Tree model to translate a problem statement in natural \nlanguage to the corresponding functional program in a DSL. The model uses\nan RNN encoder to encode the problem statement and uses an attention-based\ndoubly recurrent network for generating tree-structured output. The learnt model is \nthen used to perform Tree-beam search using a search algorithm that searches \nfor different completion of trees based on node types. The evaluation is performed\non a synthetic dataset and shows improvements over seq2seq baseline approach.\n\nOverall, this paper tackles an important problem of learning programs from \nnatural language and input-output example specifications. Unlike previous\nneural program synthesis approaches that consider only one of the specification \nmechanisms (examples or natural language), this paper considers both of them \nsimultaneously. However, there are several issues both in the approach and the \ncurrent preliminary evaluation, which unfortunately leads me to a reject score,\nbut the general idea of combining different specifications is quite promising.\n\nFirst, the paper does not compare against a very similar approach of Parisotto et al.\nNeuro-symbolic Program Synthesis (ICLR 2017) that uses a similar R3NN network\nfor generating the program tree incrementally by decoding one node at a time.\nCan the authors comment on the similarity/differences between the approaches?\nWould it be possible to empirically evaluate how the R3NN performs on this dataset?\n\nSecond, it seems that the current model does not use the input-output examples at \nall for training the model. The examples are only used during the search algorithm.\nSeveral previous neural program synthesis approaches (DeepCoder (ICLR 2017), \nRobustFill (ICML 2017)) have shown that encoding the examples can help guide \nthe decoder to perform efficient search. It would be good to possibly add another \nencoder network to see if encoding the examples as well help improve the accuracy.\n\nSimilar to the previous point, it would also be good to evaluate the usefulness of\nencoding the problem statement by comparing the final model against a model in which\nthe encoder only encodes the input-output examples.\n\nFinally, there is also an issue with the synthetic evaluation dataset. Since the \nproblem descriptions are generated syntactically using a template based approach, \nthe improvements in accuracy might come directly from learning the training templates\ninstead of learning the desired semantics. The paper mentions that it is prohibitively \nexpensive to obtain human-annotated set, but can it be possible to at least obtain a \nhandful of real tasks to evaluate the learnt model? There are also some recent \ndatasets such as WikiSQL (https://github.com/salesforce/WikiSQL) that the authors\nmight consider in future.\n\nQuestions for the authors:\n\nWhy was MAX_VISITED only limited to 100? What happens when it is set to 10^4 or 10^6?\n\nThe Search algorithm only shows an accuracy of 0.6% with MAX_VISITED=100. What would\nthe performance be for a simple brute-force algorithm with a timeout of say 10 mins?\n\nTable 3 reports an accuracy of 85.8% whereas the text mentions that the best result\nis 90.1% (page 8)?\n\nWhat all function names are allowed in the DSL (Figure 1)? \n\nCan you clarify the contributions of the paper in comparison to the R3NN?\n\nMinor typos:\n\npage 2: allows to add constrains --> allows to add constraints\npage 5: over MAX_VISITED programs has been --> over MAX_VISITED programs have been\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Program Search: Solving Data Processing Tasks from Description and Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "pdf": "/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf", "TL;DR": "Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model", "paperhash": "polosukhin|neural_program_search_solving_data_processing_tasks_from_description_and_examples", "_bibtex": "@misc{\npolosukhin2018neural,\ntitle={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\nauthor={Illia Polosukhin and Alexander Skidanov},\nyear={2018},\nurl={https://openreview.net/forum?id=B1KJJf-R-},\n}", "keywords": ["Deep learning", "Structured Prediction", "Natural Language Processing", "Neural Program Synthesis"], "authors": ["Illia Polosukhin", "Alexander Skidanov"], "authorids": ["illia@near.ai", "alex@near.ai"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642502450, "id": "ICLR.cc/2018/Conference/-/Paper747/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper747/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper747/AnonReviewer2", "ICLR.cc/2018/Conference/Paper747/AnonReviewer1", "ICLR.cc/2018/Conference/Paper747/AnonReviewer3"], "reply": {"forum": "B1KJJf-R-", "replyto": "B1KJJf-R-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper747/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642502450}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642502544, "tcdate": 1511747546723, "number": 2, "cdate": 1511747546723, "id": "rJmyxltgz", "invitation": "ICLR.cc/2018/Conference/-/Paper747/Official_Review", "forum": "B1KJJf-R-", "replyto": "B1KJJf-R-", "signatures": ["ICLR.cc/2018/Conference/Paper747/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Decent execution, but not super new or exciting", "rating": "5: Marginally below acceptance threshold", "review": "This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples. The approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured outputs. In addition, the paper assembles a template-based synthetic dataset of task descriptions and programs.  Results show that a Seq2Tree model outperforms a Seq2Seq model, that adding search to Seq2Tree improves results, and that search without any training performs worse, although the experiments assume that only a fixed number of programs are explored at test time regardless of the wall time that it takes a technique.\n\nStrengths:\n\n- Reasonable approach, quality is good\n\n- The DSL is richer than that of previous related work like Balog et al. (2016).\n\n- Results show a reasonable improvement in using a Seq2Tree model over a Seq2Seq model, which is interesting.\n\nWeaknesses:\n\n- There are now several papers on using a trained neural network to guide search, and this approach doesn't add too much on top of previous work. Using beam search on tree outputs is a bit of a minor contribution.\n\n- The baselines are just minor variants of the proposed method. It would be stronger to compare against a range of different approaches to the problem, particularly given that the paper is working with a new dataset.\n\n- Data is synthetic, and it's hard to get a sense for how difficult the presented problem is, as there are just four example problems given.\n\nQuestions:\n\n- Why not compare against Seq2Seq + Search?\n\n- How about comparing wall time against a traditional program synthesis technique (i.e., no machine learning), ignoring the descriptions. I would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth 3, which makes me skeptical that Figure 4 is a fair representation of how well a non neural network-based search could do.\n\n- Are there plans to release the dataset? Could you provide a large sample of the data at an anonymized link? I'd re-evaluate my rating after looking at the data in more detail.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Program Search: Solving Data Processing Tasks from Description and Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "pdf": "/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf", "TL;DR": "Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model", "paperhash": "polosukhin|neural_program_search_solving_data_processing_tasks_from_description_and_examples", "_bibtex": "@misc{\npolosukhin2018neural,\ntitle={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\nauthor={Illia Polosukhin and Alexander Skidanov},\nyear={2018},\nurl={https://openreview.net/forum?id=B1KJJf-R-},\n}", "keywords": ["Deep learning", "Structured Prediction", "Natural Language Processing", "Neural Program Synthesis"], "authors": ["Illia Polosukhin", "Alexander Skidanov"], "authorids": ["illia@near.ai", "alex@near.ai"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642502450, "id": "ICLR.cc/2018/Conference/-/Paper747/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper747/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper747/AnonReviewer2", "ICLR.cc/2018/Conference/Paper747/AnonReviewer1", "ICLR.cc/2018/Conference/Paper747/AnonReviewer3"], "reply": {"forum": "B1KJJf-R-", "replyto": "B1KJJf-R-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper747/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642502450}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642502463, "tcdate": 1511818572391, "number": 3, "cdate": 1511818572391, "id": "BJNUHb5lz", "invitation": "ICLR.cc/2018/Conference/-/Paper747/Official_Review", "forum": "B1KJJf-R-", "replyto": "B1KJJf-R-", "signatures": ["ICLR.cc/2018/Conference/Paper747/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Paper offers a novel approach to a tricky problem and does really well", "rating": "7: Good paper, accept", "review": "This paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network. This work to my knowledge is the first to use a DSL closer to a full language.\n\nThe paper is very clear and easy to follow. One way it could be improved is if it were compared with another system. The results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Program Search: Solving Data Processing Tasks from Description and Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "pdf": "/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf", "TL;DR": "Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model", "paperhash": "polosukhin|neural_program_search_solving_data_processing_tasks_from_description_and_examples", "_bibtex": "@misc{\npolosukhin2018neural,\ntitle={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\nauthor={Illia Polosukhin and Alexander Skidanov},\nyear={2018},\nurl={https://openreview.net/forum?id=B1KJJf-R-},\n}", "keywords": ["Deep learning", "Structured Prediction", "Natural Language Processing", "Neural Program Synthesis"], "authors": ["Illia Polosukhin", "Alexander Skidanov"], "authorids": ["illia@near.ai", "alex@near.ai"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642502450, "id": "ICLR.cc/2018/Conference/-/Paper747/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper747/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper747/AnonReviewer2", "ICLR.cc/2018/Conference/Paper747/AnonReviewer1", "ICLR.cc/2018/Conference/Paper747/AnonReviewer3"], "reply": {"forum": "B1KJJf-R-", "replyto": "B1KJJf-R-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper747/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642502450}}}, {"tddate": null, "ddate": null, "tmdate": 1515443022730, "tcdate": 1513727072476, "number": 2, "cdate": 1513727072476, "id": "SJdDNQvGf", "invitation": "ICLR.cc/2018/Conference/-/Paper747/Official_Comment", "forum": "B1KJJf-R-", "replyto": "SkJQ6oUgG", "signatures": ["ICLR.cc/2018/Conference/Paper747/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper747/Authors"], "content": {"title": "Response to AnonReviewer2", "comment": "Thanks for the detailed review and pointing out typos.\n\nResponse to questions:\n\nMAX_VISITED is limited to 100 mostly due to inefficient implementation of search and DSL interpreter in Python and striving to run experiments and evaluation relatively fast. For Seq2Tree + Search it currently takes 1-4s per example or ~4 hours for full dev set. We have run model on smaller subset of dev set with MAX_VISITED 10^4 and accuracy was within the noise margin of reported, which most probably due to fact that if Seq2Tree model doesn\u2019t know right subspace of programs, search won\u2019t be able to recover even with large number of checks.\n\nIf we let search run for 10 minutes per task, with current size of dev set it would take ~2 months to evaluate on single machine. And given there are roughly 10^25 programs with depth = 3, even 10 minutes of very optimized search would not be enough to actually find correct program of depth = 3 with brute-force search. \n(note, we consider depth = 0 just constant / function, depth = 1 a call to function with arguments, and so on. Depth = 2 already has 17,518,345,206 various programs in our DSL)\n\nFull implementation of DSL is here: https://paste.ofcode.org/SNzgEQzFAL8sVSQrrhBtRA\nWe are going also add an Appendix with details on DSL. You can also find dataset here:\nhttps://www.dropbox.com/s/wep81pcrar5fttl/metaset3.train.jsonl.gz:\nhttps://www.dropbox.com/s/h3mn0abeiqy6foz/metaset3.dev.jsonl.gz\n\nComparing our work with \u201cNeuro-Symbolic Program Synthesis\u201d paper:\n * Our DSL is way more expressive: conditions, map/reduce/filter array operations, lambda functions and recursion.\n * R3NN paper due to origin of their DSL were limited to string->string transformation. We have inputs and outputs of next types: integers, arrays of integers, strings, booleans. And we don\u2019t have any limitations in our model to expand to any other type of inputs/outputs (for R3NN it will require to adapt IO encoder, which right now only consumes characters via LSTM). \n * For non trivial transformations, like integer array -> boolean transformation, even 100s of examples can be not enough to understand what transformation is done, and natural language will work better. Though we agree that we should compare with just IO and IO/text combined in the encoder. Preliminary results show ~12% accuracy from just IO on our dataset for IO2SEQ.\n * R3NN decoder iteratively adds a node to tree, thus claiming to not require an explicit search. They have also shown improving results via backtracking search. From our observations, backtracking search can get stuck in wrong space as some much earlier decision was wrong (i.e. choosing \"+\" with probability of 0.51 instead of \"-\" with 0.49). Our breadth first search in program space guided by neural model is more principled approach to do search that allows to evaluate globally most probable programs.\n\nWe are working on evaluating R3NN approach on our dataset and will update here with results. Additionally, we can run our search on top of R3NN decoding strategy. \n\nPreliminary results applying IO2Seq (from Parisotto et al 2017):\nBEAM_SIZE = 1     0.02\nBEAM_SIZE = 100 0.12\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Program Search: Solving Data Processing Tasks from Description and Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "pdf": "/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf", "TL;DR": "Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model", "paperhash": "polosukhin|neural_program_search_solving_data_processing_tasks_from_description_and_examples", "_bibtex": "@misc{\npolosukhin2018neural,\ntitle={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\nauthor={Illia Polosukhin and Alexander Skidanov},\nyear={2018},\nurl={https://openreview.net/forum?id=B1KJJf-R-},\n}", "keywords": ["Deep learning", "Structured Prediction", "Natural Language Processing", "Neural Program Synthesis"], "authors": ["Illia Polosukhin", "Alexander Skidanov"], "authorids": ["illia@near.ai", "alex@near.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728396, "id": "ICLR.cc/2018/Conference/-/Paper747/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1KJJf-R-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper747/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper747/Authors|ICLR.cc/2018/Conference/Paper747/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper747/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper747/Authors|ICLR.cc/2018/Conference/Paper747/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper747/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper747/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper747/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper747/Reviewers", "ICLR.cc/2018/Conference/Paper747/Authors", "ICLR.cc/2018/Conference/Paper747/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728396}}}, {"tddate": null, "ddate": null, "tmdate": 1515443011879, "tcdate": 1513723094718, "number": 1, "cdate": 1513723094718, "id": "SyyJBfwGz", "invitation": "ICLR.cc/2018/Conference/-/Paper747/Official_Comment", "forum": "B1KJJf-R-", "replyto": "rJmyxltgz", "signatures": ["ICLR.cc/2018/Conference/Paper747/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper747/Authors"], "content": {"title": "Response to AnonReviewer1", "comment": "Thanks for the comments!\n\n1. Good point, here are Seq2Seq + Beam Search results. Will update paper accordingly.\n\nBEAM_SIZE = 10      0.719\nBEAM_SIZE = 100    0.728\n\n2.  On your comment to make a more efficient enumerative search, it\u2019s indeed limitation of our setup that our executor is in Python and limits how many programs we can find and execute in reasonable time (also reason why MAX_VISITED for beam search is relatively small to be able to evaluate on our dev set in reasonable time).\n\nFor example in our DSL, given just one array as input, total number of syntactically programs of depth = 2 is 17,518,345,206. Which in our current implementation in Python took about ~4.5 hour to enumerate 1B of programs (without evaluating). It\u2019s indeed true that more efficient implementation will be able to iterate over it may be 10-100 times faster. But for depth = 3 the total number of programs is roughly 10^25 which makes it impossible to apply regular enumerative search.\n\nIdeally, would be great to compare with traditional program synthesis techniques. But to run state-of-the-art traditional techniques (like PROSE) requires a large amount of work to build out heuristics and given our DSL is almost full LISP (with lambda functions and recursion) some things would be extremely hard to make work. I may be wrong, but my understanding is that functions like \u201creduce\u201d or recursive calls can not be implemented in the PROSE\u2019s backpropagation setup (https://microsoft.github.io/prose/documentation/prose/backpropagation/).\n\n3.  Please find train/dev data here:\nhttps://www.dropbox.com/s/wep81pcrar5fttl/metaset3.train.jsonl.gz:\nhttps://www.dropbox.com/s/h3mn0abeiqy6foz/metaset3.dev.jsonl.gz\n\nAlso our DSL full implementation can be found here: https://paste.ofcode.org/SNzgEQzFAL8sVSQrrhBtRA"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Program Search: Solving Data Processing Tasks from Description and Examples", "abstract": "We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms sequence-to-sequence model with attention baseline.", "pdf": "/pdf/bc1044fec2e73009b88961c9a346a6fb0cfbf658.pdf", "TL;DR": "Program synthesis from natural language description and input / output examples via Tree-Beam Search over Seq2Tree model", "paperhash": "polosukhin|neural_program_search_solving_data_processing_tasks_from_description_and_examples", "_bibtex": "@misc{\npolosukhin2018neural,\ntitle={Neural Program Search: Solving Data Processing Tasks from Description and Examples},\nauthor={Illia Polosukhin and Alexander Skidanov},\nyear={2018},\nurl={https://openreview.net/forum?id=B1KJJf-R-},\n}", "keywords": ["Deep learning", "Structured Prediction", "Natural Language Processing", "Neural Program Synthesis"], "authors": ["Illia Polosukhin", "Alexander Skidanov"], "authorids": ["illia@near.ai", "alex@near.ai"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728396, "id": "ICLR.cc/2018/Conference/-/Paper747/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "B1KJJf-R-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper747/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper747/Authors|ICLR.cc/2018/Conference/Paper747/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper747/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper747/Authors|ICLR.cc/2018/Conference/Paper747/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper747/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper747/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper747/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper747/Reviewers", "ICLR.cc/2018/Conference/Paper747/Authors", "ICLR.cc/2018/Conference/Paper747/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728396}}}], "count": 7}