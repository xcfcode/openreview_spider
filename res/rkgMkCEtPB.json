{"notes": [{"id": "H1g5i-Zc9r", "original": null, "number": 2, "cdate": 1572635042360, "ddate": null, "tcdate": 1572635042360, "tmdate": 1593665106705, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Public_Comment", "content": {"title": "Some recent related work", "comment": "Thank you for this nice paper. I really enjoyed reading it. \n\nI just wanted to point to our work -- Javed and Martha, 2019 [1] -- which proposes a meta-learning framework similar to ANIL. More concretely, ANIL is a special case of our method (See Figure 1 in the paper: https://arxiv.org/pdf/1905.12588.pdf). For a linear RLN, our method is equivalent to ANIL (In our experiments, however, we used a single hidden layer RLN). \n\nI want to be clear to the reviewers that I'm not suggesting this paper is not adding to the meta-learning discourse. Our work and this paper -- even though propose a similar solution method -- look at the problem from a very different perspective. This paper shows that even when the meta-parameters represent a model initialization, MAML is essentially doing feature reuse (in other words, it is just learning a representation which allows for quick adaptation). \n\nWe, on the other hand, directly define our meta-parameters as a representation learning network; we DO NOT investigate if MAML is already implicitly doing this (which is the main contribution of this paper). In fact, the results in this paper came as a surprise to me and added to my understanding of meta-learners that learn a model initialization. \n\n[1] Javed and White, Meta-Learning Representations for Continual Learning, NeurIPS19. \nLink: https://arxiv.org/pdf/1905.12588.pdf"}, "signatures": ["~Khurram_Javed1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Khurram_Javed1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgMkCEtPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202799, "tmdate": 1576860574153, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper884/-/Public_Comment"}}}, {"id": "rkgMkCEtPB", "original": "HyxooBzOvS", "number": 884, "cdate": 1569439193675, "ddate": null, "tcdate": 1569439193675, "tmdate": 1583912054088, "tddate": null, "forum": "rkgMkCEtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "u9SwzOjXYb", "original": null, "number": 1, "cdate": 1576798708727, "ddate": null, "tcdate": 1576798708727, "tmdate": 1576800927652, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Paper received mixed reviews: WR (R1), A (R2 and R3). AC has read reviews/rebuttal and examined paper. AC agrees that R1's concerns are misplaced and feels the paper should be accepted. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728214, "tmdate": 1576800280583, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper884/-/Decision"}}}, {"id": "r1lsg0nptH", "original": null, "number": 2, "cdate": 1571831282711, "ddate": null, "tcdate": 1571831282711, "tmdate": 1574507120207, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper is exploring the importance of the inner loop in MAML. It shows that using the inner loop only for the classifier head (ANIL) results are comparable to MAML. It also shows that using no inner loop at all (NIL) is okay for test time but not for training time.\n\nIt is indeed interesting to understand the effect of the inner loop. But, as the authors noted (\u201cOur work is complementary to methods extending MAML, and our simplification and insights could be applied to such extensions also\u201d), for it to be useful I\u2019d like to see whether these insights can be extended to SOTA models. MAML is less than 50% accuracy on 1-shot mini-imagenet while current SOTS models achieve 60-65%.\n\nThe NIL experiment that shows low performance when no inner loop is used in training time doesn\u2019t make sense. This is basically the same as the nearest-neighbour family of methods, e.g. ProtoNet (Snell et al., 2017), which have been shown to perform similarly to (or even better than) MAML.\n\n\nAfter rebuttal:\nI do think it's important to also have that kind of analysis works. My main concern is with how ANIL and NIL are introduced as new algorithms and not just an ablation of the MAML method. Presented as new algorithms I tend to compare them against the leaderboard where they are very far from the top. I am keeping my previous rating.  ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper884/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575404498998, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper884/Reviewers"], "noninvitees": [], "tcdate": 1570237745582, "tmdate": 1575404499009, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Review"}}}, {"id": "H1xctUU2oB", "original": null, "number": 3, "cdate": 1573836418132, "ddate": null, "tcdate": 1573836418132, "tmdate": 1574259280513, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "After  rebuttal period: I recommend accepting  this  paper.\n======================================\nSummary:\n\nThis paper attempts to understand if the success of MAML is due to rapid learning or feature reuse. The analysis shows that MAML is performing better mainly due to feature reuse. Authors use this result to derive a simpler version of MAML called ANIL. ANIL does not update the non-final layers of the network during inner loop training and still has similar performance to MAML.\n\nMy comments:\n\nOverall I think this is an interesting analysis paper which sheds some light on how MAML works, However, I see these analysis not just as a criticism towards MAML. I also see these analysis as a criticism against the meta-learning datasets that we use. All these datasets are artifically created from the same dataset and hence it might be very easy to reuse features to get good performance. I am not sure if the same analysis will hold if we consider a dataset where tasks are not this similar (like Meta-dataset, Triantafillou et al 2019). I encourage the authors to have this disclaimer in the end of the paper so that the community does not falsely conclude that MAML cannot do rapid learning.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper884/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575404498998, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper884/Reviewers"], "noninvitees": [], "tcdate": 1570237745582, "tmdate": 1575404499009, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Review"}}}, {"id": "r1gL7hcnjB", "original": null, "number": 8, "cdate": 1573854237946, "ddate": null, "tcdate": 1573854237946, "tmdate": 1573854237946, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "H1xctUU2oB", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment", "content": {"title": "Author Response to Review #4", "comment": "Thank you very much for your review and comments on our paper. We will update the latest version of our paper with the disclaimer you have mentioned. We think that exploring how our analysis applies to other datasets from Meta-dataset (Triantafillou et al 2019), or across more diverse tasks, will be interesting future work.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgMkCEtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper884/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper884/Authors|ICLR.cc/2020/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164728, "tmdate": 1576860540658, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment"}}}, {"id": "H1eF5J9GjB", "original": null, "number": 6, "cdate": 1573195665334, "ddate": null, "tcdate": 1573195665334, "tmdate": 1573195665334, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "r1lUaFTfFH", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment", "content": {"title": "Author Response to Review #2", "comment": "Thank you for the thorough reading of our paper and the positive feedback. We will add these edits you suggested in the next revision! \n\nSince submission we have also added additional interesting experiments to test how important the features that layers learn in the meta-initialization is to performance. We do this by resetting contiguous blocks of layers at meta-initialization to the very start of training, finding that resetting the lowest layer leads to the greatest performance drop. These experiments further support the feature reuse paradigm and also show that features in the earliest layers are most important for good performance: \n\n__________________________________________________________________\nLayers Reset           MiniImageNet-5way-1shot               MiniImageNet-5way-5shot\n__________________________________________________________________\n  None                                  46.7                                                  61.5\n  1                                         31.3                                                  39.4\n  1,2                                      28.8                                                  36.8\n  1,2,3                                   29.3                                                  37.3\n  1,2,3,4                                27.5                                                  35.9\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgMkCEtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper884/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper884/Authors|ICLR.cc/2020/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164728, "tmdate": 1576860540658, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment"}}}, {"id": "Byl4wk9GoB", "original": null, "number": 5, "cdate": 1573195612242, "ddate": null, "tcdate": 1573195612242, "tmdate": 1573195612242, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "r1lsg0nptH", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment", "content": {"title": "Author Response to Review #1", "comment": "Thank you for your review. We respectfully disagree with your overall assessment of our paper, and offer here some justification.\n\nFirstly, the focus of this paper is understanding why the highly popular and influential MAML algorithm works, and *not* just about removing the inner loop. The key message is that somewhat surprisingly, there is significant feature reuse, and not much large adaptation.  ANIL and NIL arise as a consequence of this, providing useful insights into further understanding the meta-learning field.\n\nIn line with the goal of gaining *fundamental insights* into these algorithms, the scope of this paper was not to look at SOTA methods, but perform a thorough and detailed study of the main MAML algorithm, and have **entirely reproducible** experiments. To do this, we stuck to using the following standard, easy to use and open source repositories:\nhttps://github.com/cbfinn/maml for the few shot classification results (Code for the few-shot learning experiments from the original MAML paper, open-sourced by the original authors).\nhttps://github.com/tristandeleu/pytorch-maml-rl for RL experiments (Code to reproduce the RL results from the original MAML paper).\n\nWe show how these insights on feature reuse connect to how other metalearning algorithms e.g. (Matching Networks, Vinyals et al) work, in Section 6. Note that feature reuse is a key factor behind the performance of other metalearning methods that achieve near SOTA , e.g. (Meta-learning with differentiable convex optimization, Kwonjoon et al), which does no \u2018adaptation\u2019 at test time, and still achieves excellent performance on the benchmarks. By demonstrating the importance of feature reuse in MAML (which is not at all immediately evident, due to the inner loop optimization), and identifying that other metalearning methods also employ feature reuse effectively, we link understanding about how MAML works to how other algorithms work. This contribution goes beyond just removing the inner loop in MAML.\n\nRegarding NIL at training time and ProtoNets: firstly, we again emphasize that NIL at training time was **not at all** the main focus of the paper. Additionally, there are **several very important differences** between NIL and Protonet:\n\u2014 ProtoNets compute prototypes (averages over representations from the same class in the support set), which we do not do, taking the raw cosine similarity between test examples and all support set examples.\n\u2014 ProtoNets use euclidean distance, instead of cosine distance, which is explicitly stated in the paper to improve performance.\n\u2014 ProtoNets use a *learning rate scheduler* which also helps performance, while for using NIL during training we simply use the Adam optimizer with the default settings used in the original MAML paper, as we are seeking to compare NIL as closely as possible with the original MAML paper, not search over potential optimization curricula to improve its performance.\n\u2014 ProtoNets have a different training process, with a *larger* number of classes during training compared to testing. E.g for testing 5-shot performance, they perform 20-way classification during the meta-training stage.\nThese key differences make it very hard to compare performance when training with NIL to  performance from ProtoNets.\n\nSince submission we have also added additional interesting experiments to test how important the features learned in the meta-initialization are. We reset contiguous blocks of layers at meta-initialization to the very start of training. These experiments further support the feature reuse paradigm and also show that features in the earliest layers are most important for good performance:\n__________________________________________________________________\nLayers Reset           MiniImageNet-5way-1shot               MiniImageNet-5way-5shot\n__________________________________________________________________\n  None                                  46.7                                                  61.5\n  1                                         31.3                                                  39.4\n  1,2                                      28.8                                                  36.8\n  1,2,3                                   29.3                                                  37.3\n  1,2,3,4                                27.5                                                  35.9\n\nThe MAML algorithm has been extensively developed in recent literature, and has been applied to a wide range of problems. Our work offers fundamental insight into why this highly popular algorithm is effective, thereby answering a central open question. In investigating this question, we conducted detailed, reproducible experiments, that are of interest to the community, as seen by public comments. Our paper provides an excellent foundation for future work, which could leverage our insights to both understand few-shot learning algorithms better and develop improved few-shot learning algorithms."}, "signatures": ["ICLR.cc/2020/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgMkCEtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper884/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper884/Authors|ICLR.cc/2020/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164728, "tmdate": 1576860540658, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment"}}}, {"id": "Hkx5nntGjr", "original": null, "number": 4, "cdate": 1573194930045, "ddate": null, "tcdate": 1573194930045, "tmdate": 1573194930045, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "H1g5i-Zc9r", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment", "content": {"title": "Thank you for your interest and the reference!", "comment": "Thank you very much for your comment and for highlighting this related work -- it is very interesting to see the parallels between the method you have proposed and the ANIL algorithm, especially seeing how ANIL-like ideas apply to continual learning and catastrophic forgetting. We will include a reference to this work in the updated version of our paper. "}, "signatures": ["ICLR.cc/2020/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgMkCEtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper884/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper884/Authors|ICLR.cc/2020/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164728, "tmdate": 1576860540658, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment"}}}, {"id": "r1lUaFTfFH", "original": null, "number": 1, "cdate": 1571113405925, "ddate": null, "tcdate": 1571113405925, "tmdate": 1572972540209, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper claims to examine the reasons for the success of MAML---an influential meta-learning algorithm to tackle few-shot learning. It thoroughly investigated the importance of the two optimization loops, and found that feature reuse is the dominant factor for MAML\u2019s success. Moreover, the authors proposed new algorithms---ANIL and NIL---which spend much less computation on the inner loop of MAML. They also discussed their findings in a broader meta-learning context. \n\nI think the paper should be accepted for the following reasons: \n\n1. The experimental study is thorough. \n\nThe experiments follow a rigorous design of hypothesis-checking style and the conclusions are supported by extensive results under various evaluations. \n\nThe findings are potentially helpful for many future works in this field. \n\n2. The paper is clearly written. \n\nIt is generally enjoyable to read, except for some minor things to improve: (1) Evaluation metrics in table-2, table-4 and table-5 had better be explicitly clarified in the captions (2) No subsection seems needed in section-6. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper884/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575404498998, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper884/Reviewers"], "noninvitees": [], "tcdate": 1570237745582, "tmdate": 1575404499009, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Review"}}}, {"id": "H1e8wWvQFr", "original": null, "number": 1, "cdate": 1571152222131, "ddate": null, "tcdate": 1571152222131, "tmdate": 1571152229084, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "Sygws6-GKS", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment", "content": {"comment": "Thank you for your interest in our paper, the helpful outline of CAVIA, and the pointer to the blog post. We have updated the description of CAVIA in a newer version of our paper.\n\nWe feel that there are exciting open directions to explore relating the context parameter-main network split in CAVIA and the head-body split we analyze; for example, how adding a context vector to hidden layer(s) affects what is learned by the head of the network, and whether there is a variant of the NIL algorithm that can incorporate the context vector. From an analysis point of view, we also think it is interesting to understand the representations learned by the higher dimensional context vectors using analysis tools such as CCA/CKA (as in our paper).", "title": "Thank you for the comments on related work -- we have updated our description accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper884/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgMkCEtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper884/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper884/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper884/Authors|ICLR.cc/2020/Conference/Paper884/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164728, "tmdate": 1576860540658, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper884/-/Official_Comment"}}}, {"id": "Sygws6-GKS", "original": null, "number": 1, "cdate": 1571065246770, "ddate": null, "tcdate": 1571065246770, "tmdate": 1571065246770, "tddate": null, "forum": "rkgMkCEtPB", "replyto": "rkgMkCEtPB", "invitation": "ICLR.cc/2020/Conference/Paper884/-/Public_Comment", "content": {"comment": "\nDear authors, \n\nI read your paper with great interest and am excited to see more insights into understanding how gradient-based meta-learning methods work. Our work [1] from this year\u2019s ICRL comes to similar conclusions. I wanted to take this opportunity to clarify our work, since this is currently not represented accurately in your paper.\n\nIn your paper you say that [1] \u201chas looked at having outer/inner loop specific parameters, but does this in a more complex fashion, partitioning parameters within each layer, and for specific layers, contrasting with the simple head/body separation in ANIL\u201d. That\u2019s not true. We do not partition the parameters within each layer, and there is only a single (additional input) vector which is updated in the inner loop. So in fact, we also do a very simple partition, which could be described as a tail/body separation compared to the body/head separation of ANIL.\n\nIn short: Our method CAVIA separates the network into two parts, a single input vector (\u201ctail\u201d or\u201ccontext parameter\u201d) and the main network (\u201cbody\u201d). In the inner loop we only adapt the context parameter, and in the outer loop we adapt the main body of the network. The intuition behind this idea is that for many tasks, if we had access to a true task description (e.g., in the HalfCheetah environments the direction/velocity) we could just condition the network on this and there would be no need for meta-learning. See also the simplified architecture visualisation in our blog post: http://whirl.cs.ox.ac.uk/blog/cavia/. \n\nCan you please rectify the description of our work? Please do let me know if you have any questions.\n\n[1] Fast context adaptation via meta-learning. Luisa Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, Shimon Whiteson. ICLR 2019.", "title": "Clarification of Related Work"}, "signatures": ["~Luisa_M_Zintgraf1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Luisa_M_Zintgraf1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Samy Bengio", "Oriol Vinyals"], "authorids": ["aniruddhraghu@gmail.com", "maithrar@gmail.com", "bengio@google.com", "vinyals@google.com"], "keywords": ["deep learning analysis", "representation learning", "meta-learning", "few-shot learning"], "TL;DR": "The success of MAML relies on feature reuse from the meta-initialization, which also yields a natural simplification of the algorithm, with the inner loop removed for the network body, as well as other insights on the head and body.", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "pdf": "/pdf/f0530e2cf88af3b74bf61bc8591b7a5a1339c49e.pdf", "paperhash": "raghu|rapid_learning_or_feature_reuse_towards_understanding_the_effectiveness_of_maml", "_bibtex": "@inproceedings{\nRaghu2020Rapid,\ntitle={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},\nauthor={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgMkCEtPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/36be6c8e943e59a47af8e489b2970b162c59f1f4.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgMkCEtPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504202799, "tmdate": 1576860574153, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper884/Authors", "ICLR.cc/2020/Conference/Paper884/Reviewers", "ICLR.cc/2020/Conference/Paper884/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper884/-/Public_Comment"}}}], "count": 12}