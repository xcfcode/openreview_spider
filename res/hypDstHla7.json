{"notes": [{"id": "hypDstHla7", "original": "vpT6K1ArQj", "number": 3484, "cdate": 1601308386642, "ddate": null, "tcdate": 1601308386642, "tmdate": 1614985775380, "tddate": null, "forum": "hypDstHla7", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning", "authorids": ["~Benedikt_Feldotto1", "heiko.lengenfelder@tum.de", "~Alois_Knoll1"], "authors": ["Benedikt Feldotto", "Heiko Lengenfelder", "Alois Knoll"], "keywords": ["Reinforcement Learning", "Machine Learning", "Robot Motion Learning", "DQN", "Robot Manipulator", "Target Reaching", "Network Pruning"], "abstract": "Recent experiments indicate that pre-training of end-to-end Reinforcement Learning neural networks on general tasks can speed up the training process for specific robotic applications. However, it remains open if these networks form general feature extractors and a hierarchical organization that are reused as apparent e.g. in Convolutional Neural Networks. In this paper we analyze the intrinsic neuron activation in networks trained for target reaching of robot manipulators with increasing joint number in a vertical plane. We analyze the individual neuron activity distribution in the network, introduce a pruning algorithm to reduce network size keeping the performance, and with these dense network representations we spot correlations of neuron activity patterns among networks trained for robot manipulators with different joint number. We show that the input and output network layers have more distinct neuron activation in contrast to inner layers. Our pruning algorithm reduces the network size significantly, increases the distance of neuron activation while keeping a high performance in training and evaluation. Our results demonstrate that neuron activity can be mapped among networks trained for robots with different complexity. Hereby, robots with small joint difference show higher layer-wise projection accuracy whereas more different robots mostly show projections to the first layer.", "one-sentence_summary": "We analyze the neuron activation in neural networks trained for robot target reaching with Reinforcement Learning, prune the network and highlight correlations between networks trained for robots with different joint count.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feldotto|neuron_activation_analysis_for_multijoint_robot_reinforcement_learning", "pdf": "/pdf/dfa95930949d84e2d82cd610b6ad04688cd32bd6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Pmm51zRMNO", "_bibtex": "@misc{\nfeldotto2021neuron,\ntitle={Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning},\nauthor={Benedikt Feldotto and Heiko Lengenfelder and Alois Knoll},\nyear={2021},\nurl={https://openreview.net/forum?id=hypDstHla7}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5H730Hl_UOt", "original": null, "number": 1, "cdate": 1610040355907, "ddate": null, "tcdate": 1610040355907, "tmdate": 1610473945435, "tddate": null, "forum": "hypDstHla7", "replyto": "hypDstHla7", "invitation": "ICLR.cc/2021/Conference/Paper3484/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper analyzes neuron activations for neural networks trained via RL to perform reaching with planar robot arms. This analysis includes an evaluation of the correlation between neurons of different models trained to control arms with different degrees-of-freedom. In performing these evaluations, the paper proposes a heuristic pruning algorithm that reduces the size of the network and increases information density. Correlation is assessed based on a projection of the source network on the target network.\n\nThe paper is well written and considers a challenging problem of interest to the community. The proposed pruning strategy as a means of maximizing information content is reasonable and seems to perform well. However, the significance of the contributions is limited by the experimental evaluation. The experiments consider a large number of models, however the scope of problems on which the method is evaluated is narrow, making it difficult to draw conclusions about the merits and significance of the work. The authors are encouraged to extend the analysis to a more diverse set of problems."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning", "authorids": ["~Benedikt_Feldotto1", "heiko.lengenfelder@tum.de", "~Alois_Knoll1"], "authors": ["Benedikt Feldotto", "Heiko Lengenfelder", "Alois Knoll"], "keywords": ["Reinforcement Learning", "Machine Learning", "Robot Motion Learning", "DQN", "Robot Manipulator", "Target Reaching", "Network Pruning"], "abstract": "Recent experiments indicate that pre-training of end-to-end Reinforcement Learning neural networks on general tasks can speed up the training process for specific robotic applications. However, it remains open if these networks form general feature extractors and a hierarchical organization that are reused as apparent e.g. in Convolutional Neural Networks. In this paper we analyze the intrinsic neuron activation in networks trained for target reaching of robot manipulators with increasing joint number in a vertical plane. We analyze the individual neuron activity distribution in the network, introduce a pruning algorithm to reduce network size keeping the performance, and with these dense network representations we spot correlations of neuron activity patterns among networks trained for robot manipulators with different joint number. We show that the input and output network layers have more distinct neuron activation in contrast to inner layers. Our pruning algorithm reduces the network size significantly, increases the distance of neuron activation while keeping a high performance in training and evaluation. Our results demonstrate that neuron activity can be mapped among networks trained for robots with different complexity. Hereby, robots with small joint difference show higher layer-wise projection accuracy whereas more different robots mostly show projections to the first layer.", "one-sentence_summary": "We analyze the neuron activation in neural networks trained for robot target reaching with Reinforcement Learning, prune the network and highlight correlations between networks trained for robots with different joint count.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feldotto|neuron_activation_analysis_for_multijoint_robot_reinforcement_learning", "pdf": "/pdf/dfa95930949d84e2d82cd610b6ad04688cd32bd6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Pmm51zRMNO", "_bibtex": "@misc{\nfeldotto2021neuron,\ntitle={Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning},\nauthor={Benedikt Feldotto and Heiko Lengenfelder and Alois Knoll},\nyear={2021},\nurl={https://openreview.net/forum?id=hypDstHla7}\n}"}, "tags": [], "invitation": {"reply": {"forum": "hypDstHla7", "replyto": "hypDstHla7", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040355894, "tmdate": 1610473945418, "id": "ICLR.cc/2021/Conference/Paper3484/-/Decision"}}}, {"id": "qpRRH9UvWqu", "original": null, "number": 1, "cdate": 1603710769629, "ddate": null, "tcdate": 1603710769629, "tmdate": 1605023991977, "tddate": null, "forum": "hypDstHla7", "replyto": "hypDstHla7", "invitation": "ICLR.cc/2021/Conference/Paper3484/-/Official_Review", "content": {"title": "Interesting idea worth exploring, however, it needs to be developed further.", "review": "###  Summary\n\nThe authors investigate individual neuron activations over time, and compare the neuron activations within individual networks all-to-all and layer wise. \n\nA distance metric is introduced and utilized to set up a pruning procedure to maximize the information density in learned neural networks and reduce redundancy as well as unused network nodes.\n\nFinally, neuron activations are used to assess the correlations between learned policy networks for manipulators with a varying number of degrees of freedom. A projection mapping between different policy networks is implemented and analysed, as a type of transfer learning between different robot morphologies.\n\n\n###  Review\n\nI believe that this is an interesting work which tries to understand the inner-workings of a robot-control policy network by examining the network activations, and further using this information to prune the unnecessary neurons. Transferring learned network policies between robot morphologies is very useful and preliminary insights seem interesting.\n\nThere are some important flaws that need to be addressed regarding the clarity of the methodology and contributions, as well as the significance of the experimental evaluations. \n\nMy impression is that although the work tackles an important problem with a good idea, this is still an incomplete work, as the presented experimental evaluation is insufficient to draw significant conclusions.\n\nBelow are some of the comments organised by sections, including concrete suggestions for improving the work.\n\n1. INTRODUCTION\n\n The main motivation and goal should be presented explicitly in a separate paragraph. There seems to be a missing link between the neuron activation estimations for pruning and correlation analysis for transfer mapping.\n\n2. RELATED WORK\n\n I do not see the relevance of Bellemare et al. (2013), Mnih et al. (2015), Chess Silver et al. (2017), Go Silver et al. (2016) and Lillicrap et al. (2015) to the specific problems investigated in this paper.\n\n It would be useful to consider other work investigating NN complexity, and adding a discussion on how it relates to this work:\n - Gaier, Adam, and David Ha. \"Weight agnostic neural networks.\" Advances in Neural Information Processing Systems. 2019.\n - Li, Chunyuan, et al. \"Measuring the intrinsic dimension of objective landscapes.\" International Conference on Learning Representations. 2018.\n\n3. EXPERIMENTAL SETUP\n\n \u201cplanar space robot manipulator that represents a multitude of real world applications\u201d Do you mean that this task is a surrogate for examining many applications? This seems like a strong statement as it represents a small subset of potential applications.\n\n Moreover, planar space usually refers to having a horizontal plane as a task space. In this case it would be more clear to say \u201coperating in a vertical plane\u201d.\n\n \u201cA neural network is trained with end-to-end Reinforcement Learning\u201d this usually means from input images to output torques, but in the presented approach position control is used, so this should be emphasised.\n\n \u201cphysical robotic simulation\u201d, usually it is said \u201cphysical robot\u201d referring to a real robot experiment, or a \u201crealistic robot simulation\u201d which refers to a simulation that takes into account the real robot component values (dimensions, mass, inertia\u2026).\n\n Equation 1 is not clear as the text says that $\\textbf{x}$ consists of joint angles $\\hat{\\theta}_i$ but eq 1 shows the sin and cos projections of the angles. Moreover, the index $i$ seems to refer to both the time-step index $t_i$, as well as the joint index $\\hat{\\theta}_i$. Also, $n$ is not introduced as the number of joints.\n\n What is the reason behind mapping the target distance into a (0,1] range?\n\n Another important aspect which I believe should be addressed, is what is the effect of the task which is learned on the activation correlations? Basically, different tasks would have a different state distribution seen at the input of the policy? One simple example would be examining different types of control - position vs velocity vs torque.\n\n\n4. NEURON ACTIVATION ANALYSIS\n\n This section provides an analysis for the 3DOF robot only and should be emphasised.\n\n \u201cWe define a distance metric between neurons that is based on the neuron activation history in scope of every episode in order to account for the dynamics in motion trajectory learning.\u201d It is a bit unclear how the activation history is evaluated. This is one of the most important parts of the paper and should be made very clear.\n\n I am not 100% sure that the distance metric should be referred to as Euclidean distance, as it does not operate on euclidean space, so I think it would be sufficient to say \u201ca proposed neural activation distance metric\u201d.\n\n \u201cFor a set of sample episodes E representing the agents action space\u201d How do episodes represent the action space exactly, I do not understand this part completely.\n\n The notation is a bit confusing, as $n$ refers both to the number of joints and the number of neurons. Also, what does the superscript $T$ in $R^T$ stand for?\n\n It seems to me that there is another summation missing in equation 2, as the indices of neurons should be more generic like $n_i$ and $n_j$ (unless there are only 2 neurons?). Also, are these neurons form the same layer or all the neurons in the network? This should be clarified in the text and formulated in Eq 2.\n\n What is $C$ in equation 3? I assume it is the cluster and cluster size, but this should be explained explicitly.\n\n Some of the distributions in Fig 2 are a bit skewed and look more like Beta distribution rather than Gaussian. Why is having a Gaussian distribution of distances relevant? \n\n There might be some other visualisation method that could be used here to shed light on the findings. Because currently, it is difficult to see any significant differences between the plots.\n\n The reference to Fig 2 should be improved, the definition of the trained, random and pruned lines is missing (both in the figure and the text). \n\n What does \u201call-to-all distribution of trained networks\u201d mean (all-to-all neuron comparison)? \n\n The findings for the clustering (Fig 2 bottom) are very interesting! Could you maybe elaborate on these more?\n\n\n5. HEURISTIC NETWORK PRUNING\n\n What is the motivation for retraining after pruning? If neurons that have similar activations are pruned, what would happen if one of them is kept with the corresponding weights? How would this affect the performance? This could be an insightful baseline comparison.\n\n Moreover, what is the advantage of reusing the network weights for initialisation, instead of randomly initialising them? This would also be an interesting experiment to conduct.\n\n Please introduce what are \u201cdead neurons\u201d.\n\n Wrong reference to Equation (5) should be (4)\n\n $\\tau > 2$ \u2192 $\\tau > 0.2$ \n\n The accuracy in Fig 3 left, is given in [%], is this a mistake, because then it seems that the initial accuracy is only 0.8%. If this is actually 80%, why is the optimal $\\tau = 0.2$ as the corresponding accuracy has fallen to 20%. How is this evaluated? Does the green label correspond to pruned or initial network size?\n\n\n6. CORRELATIONS IN NETWORKS TRAINED FOR MULTI-JOINT ROBOTS\n\n It would be useful to start with a high level overview of what is the goal of finding these correlations in addition to how they are calculated. It seems that the correlations between networks are not examined, rather the mappings between them. Therefore this is slightly misrepresentative of what is actually being done.\n\n How are activation matrices A and B related to P? Also, what are $\\alpha_m$ and $\\beta_k$ ? \n\n What is the motivation behind making $\\bar{P}^g_{km}$ sparse according to minimal distance in Greedy mapping, or applying L1 regularisation in Linear mapping? I assume your goal is to map joints 1-1 rather than combining them? Please explain this in a more clear way if possible.\n\n Equation 5 does not show the variable which is optimised. What does $\\alpha^{\\downarrow}_m$ stand for?\n\n Equations 6 and 7 are not referred to in the text.\n\n The quantities defined in equations 6, 7, 8 and 9 should be properly introduced as evaluation metrics and named accordingly, in a separate paragraph.\n\n Figure 5 correlations (top right) would be more impactful if represented with a heatmap matrix in addition to the numbers. The graphs on the bottom are not very clear.\n\n The discussion of the mapping results is very interesting and should emphasise the mapping between different robot morphologies. For example the difference between 4 -> 2 and 2 -> 4, where the latter has a higher error which could be expected as there is not enough information stored which can be decoded. Having additional comparisons of robot manipulators with larger differences in DOF would probably emphasise this and support the given conclusions better. This would also strengthen the paper significantly.\n\n Another metric which would be necessary to evaluate the transfer procedure, is to evaluate the mapped network on a test set of the reaching task.\n\n\nOTHER COMMENTS:\n - Figure captions should be larger\n - Several typos\n - Consistency in using \u201c3 joint manipulation task\u201d or \u201c3-DOF manipulator\u201d\n - In the conclusion it is stated: \u201cIn this paper we analyzed individual neuron activation and correlations between neural networks trained for goal reaching of a variety of planar space robot manipulators.\u201d Having the same robot structure with 3 different DOFs is not sufficient to be considered as a variety of manipulators.  \n - It would significantly help the clarity of the paper to split certain sections into thematic paragraphs.\n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3484/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3484/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning", "authorids": ["~Benedikt_Feldotto1", "heiko.lengenfelder@tum.de", "~Alois_Knoll1"], "authors": ["Benedikt Feldotto", "Heiko Lengenfelder", "Alois Knoll"], "keywords": ["Reinforcement Learning", "Machine Learning", "Robot Motion Learning", "DQN", "Robot Manipulator", "Target Reaching", "Network Pruning"], "abstract": "Recent experiments indicate that pre-training of end-to-end Reinforcement Learning neural networks on general tasks can speed up the training process for specific robotic applications. However, it remains open if these networks form general feature extractors and a hierarchical organization that are reused as apparent e.g. in Convolutional Neural Networks. In this paper we analyze the intrinsic neuron activation in networks trained for target reaching of robot manipulators with increasing joint number in a vertical plane. We analyze the individual neuron activity distribution in the network, introduce a pruning algorithm to reduce network size keeping the performance, and with these dense network representations we spot correlations of neuron activity patterns among networks trained for robot manipulators with different joint number. We show that the input and output network layers have more distinct neuron activation in contrast to inner layers. Our pruning algorithm reduces the network size significantly, increases the distance of neuron activation while keeping a high performance in training and evaluation. Our results demonstrate that neuron activity can be mapped among networks trained for robots with different complexity. Hereby, robots with small joint difference show higher layer-wise projection accuracy whereas more different robots mostly show projections to the first layer.", "one-sentence_summary": "We analyze the neuron activation in neural networks trained for robot target reaching with Reinforcement Learning, prune the network and highlight correlations between networks trained for robots with different joint count.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feldotto|neuron_activation_analysis_for_multijoint_robot_reinforcement_learning", "pdf": "/pdf/dfa95930949d84e2d82cd610b6ad04688cd32bd6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Pmm51zRMNO", "_bibtex": "@misc{\nfeldotto2021neuron,\ntitle={Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning},\nauthor={Benedikt Feldotto and Heiko Lengenfelder and Alois Knoll},\nyear={2021},\nurl={https://openreview.net/forum?id=hypDstHla7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hypDstHla7", "replyto": "hypDstHla7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074980, "tmdate": 1606915760054, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3484/-/Official_Review"}}}, {"id": "yxS0PGeDbRI", "original": null, "number": 2, "cdate": 1603807073688, "ddate": null, "tcdate": 1603807073688, "tmdate": 1605023991910, "tddate": null, "forum": "hypDstHla7", "replyto": "hypDstHla7", "invitation": "ICLR.cc/2021/Conference/Paper3484/-/Official_Review", "content": {"title": "Potentially a good paper requiring more conclusive results", "review": "# Summary\nThe paper presents a technique to compare networks trained to solve similar tasks trained in different context. The considered task is reaching with a robotic planar arm; the considered context is varied varying the robot degrees of freedom. The goal of the paper is to find correlations across neural activity patterns across networks trained to solve the same task in different contexts.\n\nTo achieve their goals, authors propose an heuristic network pruning algorithm to reduce the network size while keeping performance in training and evaluation. To correlate different networks, authors propose a technique to project a source network onto a target network.\n#### Clarity\nThe paper is well written and easy to read. \n#### Originality \nThe paper follows a main stream research which aims at pre-training neural networks on general task to speed up learning of specific tasks. As far as I know (I am not an expert in the field), the proposed analysis is original.\n#### Significance \nThe significance of this work is relatively low. The results could be more conclusive with further analysis and experiments.\n#### Major comments\n* **Kinematic redundancy**. Authors have chosen a specific task, nominally reaching a target with a planar manipulator. Within this context, in presence of more than two degrees of freedom (i.e. with kinematic redundancy), the solution of the task itself is non-unique. Therefore fixing the context (i.e. fixing the robot kinematics, the robot geometry, etc) doesn't guarantee that the RL algorithm will find similar solutions across different runs. Actually, given the random training procedure each solution should come up with completely different strategy, exploiting the kinematic redundancy. If so, how do authors compare networks which exploit differently the kinematic redundancy? Are the proposed metrics (greedy mapping, linear mapping, etc) invariant with respect to different solutions to the same task (i.e. invariant to kinematic redundancies)?\n* **Results and conclusions**. Goal of the paper (mentioned in the first two sentences of the abstract) is to progress in understanding if pre-training of end-to-end RL can be used as feature extractors and hierarchical organizations. Despite what claimed in the conclusions (\"Networks trained for robots with only small joint number difference show a good correlation of neuron activation, for small differences this correlation can be found layer-wise.\"), authors fail short in giving a sound explanation of why this is the case.\n#### Major comments\n* **Page 7, line 8 from the top.** \"[..] the reflexive mapping\". This was not mentioned before, authors should give more details.\n* **Page 7, line 1 from the bottom.** \"Are joint numbers very different a proper input transformation is crucial to find correlations\". Please check this sentence. \n* **Page 8, caption of figure 5.** \"balanced mapping \u03b81\u2032 = \u03b81 , \u03b82\u2032 = \u03b83 (4b) we apply in contrast to the naive mapping \u03b82\u2032 =\u03b82,\u03b82\u2032 =\u03b82(4a).\" It's unclear what these mappings refer to and how they have been used. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3484/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3484/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning", "authorids": ["~Benedikt_Feldotto1", "heiko.lengenfelder@tum.de", "~Alois_Knoll1"], "authors": ["Benedikt Feldotto", "Heiko Lengenfelder", "Alois Knoll"], "keywords": ["Reinforcement Learning", "Machine Learning", "Robot Motion Learning", "DQN", "Robot Manipulator", "Target Reaching", "Network Pruning"], "abstract": "Recent experiments indicate that pre-training of end-to-end Reinforcement Learning neural networks on general tasks can speed up the training process for specific robotic applications. However, it remains open if these networks form general feature extractors and a hierarchical organization that are reused as apparent e.g. in Convolutional Neural Networks. In this paper we analyze the intrinsic neuron activation in networks trained for target reaching of robot manipulators with increasing joint number in a vertical plane. We analyze the individual neuron activity distribution in the network, introduce a pruning algorithm to reduce network size keeping the performance, and with these dense network representations we spot correlations of neuron activity patterns among networks trained for robot manipulators with different joint number. We show that the input and output network layers have more distinct neuron activation in contrast to inner layers. Our pruning algorithm reduces the network size significantly, increases the distance of neuron activation while keeping a high performance in training and evaluation. Our results demonstrate that neuron activity can be mapped among networks trained for robots with different complexity. Hereby, robots with small joint difference show higher layer-wise projection accuracy whereas more different robots mostly show projections to the first layer.", "one-sentence_summary": "We analyze the neuron activation in neural networks trained for robot target reaching with Reinforcement Learning, prune the network and highlight correlations between networks trained for robots with different joint count.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feldotto|neuron_activation_analysis_for_multijoint_robot_reinforcement_learning", "pdf": "/pdf/dfa95930949d84e2d82cd610b6ad04688cd32bd6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Pmm51zRMNO", "_bibtex": "@misc{\nfeldotto2021neuron,\ntitle={Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning},\nauthor={Benedikt Feldotto and Heiko Lengenfelder and Alois Knoll},\nyear={2021},\nurl={https://openreview.net/forum?id=hypDstHla7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hypDstHla7", "replyto": "hypDstHla7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074980, "tmdate": 1606915760054, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3484/-/Official_Review"}}}, {"id": "SQ9AiSN-LuA", "original": null, "number": 3, "cdate": 1603826390508, "ddate": null, "tcdate": 1603826390508, "tmdate": 1605023991838, "tddate": null, "forum": "hypDstHla7", "replyto": "hypDstHla7", "invitation": "ICLR.cc/2021/Conference/Paper3484/-/Official_Review", "content": {"title": "Thorough analysis in a limited scope", "review": "#### Summary\nThe authors present a method for analysing neuron activity in neural networks trained via RL on a multi-joint planar reaching task, as well as correlating neurons between different models trained on tasks with potentially a different number of joints. The methods consists of three steps:\n1. Compare different neurons within a model using normalised activation traces over a number of episodes, and cluster hierarchically based on similar activity.\n2. Use said clusters to prune the networks based on merging neurons within a cluster with an intra-cluster distance below some threshold, alternated with retraining.\n3. Compare different models by optimising a linear projection between neurons, and evaluate reconstruction error, coverage and saturation.\nResults indicate the proposed pruning method is effective in reducing the number of neurons without affecting accuracy, as well as showing correlations between corresponding layers of different models, though these reduce with larger difference in number of joints.\n\n#### Pros\n- The authors perform a sufficiently thorough evaluation, with a large number of models compared and reasonable ablations, baselines and metrics.\n- While descriptions are brief, the method is generally well described and mathematical notation consistent.\n- The proposed heuristic pruning approach seems to perform well in this case, as evident by all model sizes converging to the same size in Fig. 3.\n- The approach of first pruning networks to maximise information content in the activations before correlating different models makes a lot of sense.\n\n#### Cons\n- The authors frame their work within the context of feature reuse and explainability, however the presented work is limited to showing correlations between features learned on identical or very similar tasks. It is unclear how this enables either reuse or explainability and perhaps not surprising that these correlations for very similar or identical tasks exists per se, more interesting would be to see how these can exploited. These correlations also degrade very rapidly with an increasing number of joints. I hypothesise that one would perhaps see stronger correlations between more different tasks if not the morphology was changed but rather the objective / reward. The scope of a planar reacher may also be too limited to draw more general conclusions for other control tasks.\n- Related to task differentiation, a potential weakness in the proposed methods is how the activation traces are generated for source and target model when optimising the projection. Effectively only the target model is evaluated within distribution, after which the inputs observed there are then remapped to the source model. It's unclear what the effect is of potentially evaluating the source model out of its training distribution. I was hoping to see a way to correlate trajectories collected with the respective models independently. Perhaps the type of problem considered only allows a singular solution even across different number of joints, but it would be good to verify this.\n- While the authors do evaluate a large combination of models, only averages are reported. Given how close the results seem to be to random in Fig. 5, it's hard to gauge the significance of the results. Some variance or error metric would be very valuable.\n- While the idea of pruning the networks before correlating intuitively seems like a good idea, this is not experimentally validated. It would be good to add a comparison with and between unpruned models as well.\n- While okay to follow, the text could use a bit more polish.\n\n#### Questions\n- There's currently no mention of how all these models were trained. One caption hints at DQN? Please provide more details.\n- It's unclear how to interpret training duration in Fig. 3. Is this the time required to \"pass\" the validation set again after pruning?\n- What do the values in the table in Fig. 5 represent? Sums of weights in the projection matrix?\n\n#### Conclusion\nWhile overall the method presented makes sense, and the evaluation is relatively thorough, the scope of the problems evaluated is considerably limited to draw any general conclusions of its validity, and some of the framing and details raise questions. As such I'd consider this submission marginally below acceptance.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3484/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3484/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning", "authorids": ["~Benedikt_Feldotto1", "heiko.lengenfelder@tum.de", "~Alois_Knoll1"], "authors": ["Benedikt Feldotto", "Heiko Lengenfelder", "Alois Knoll"], "keywords": ["Reinforcement Learning", "Machine Learning", "Robot Motion Learning", "DQN", "Robot Manipulator", "Target Reaching", "Network Pruning"], "abstract": "Recent experiments indicate that pre-training of end-to-end Reinforcement Learning neural networks on general tasks can speed up the training process for specific robotic applications. However, it remains open if these networks form general feature extractors and a hierarchical organization that are reused as apparent e.g. in Convolutional Neural Networks. In this paper we analyze the intrinsic neuron activation in networks trained for target reaching of robot manipulators with increasing joint number in a vertical plane. We analyze the individual neuron activity distribution in the network, introduce a pruning algorithm to reduce network size keeping the performance, and with these dense network representations we spot correlations of neuron activity patterns among networks trained for robot manipulators with different joint number. We show that the input and output network layers have more distinct neuron activation in contrast to inner layers. Our pruning algorithm reduces the network size significantly, increases the distance of neuron activation while keeping a high performance in training and evaluation. Our results demonstrate that neuron activity can be mapped among networks trained for robots with different complexity. Hereby, robots with small joint difference show higher layer-wise projection accuracy whereas more different robots mostly show projections to the first layer.", "one-sentence_summary": "We analyze the neuron activation in neural networks trained for robot target reaching with Reinforcement Learning, prune the network and highlight correlations between networks trained for robots with different joint count.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feldotto|neuron_activation_analysis_for_multijoint_robot_reinforcement_learning", "pdf": "/pdf/dfa95930949d84e2d82cd610b6ad04688cd32bd6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Pmm51zRMNO", "_bibtex": "@misc{\nfeldotto2021neuron,\ntitle={Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning},\nauthor={Benedikt Feldotto and Heiko Lengenfelder and Alois Knoll},\nyear={2021},\nurl={https://openreview.net/forum?id=hypDstHla7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hypDstHla7", "replyto": "hypDstHla7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074980, "tmdate": 1606915760054, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3484/-/Official_Review"}}}], "count": 5}