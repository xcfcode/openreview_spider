{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396527227, "tcdate": 1486396527227, "number": 1, "id": "S1POhGLdg", "invitation": "ICLR.cc/2017/conference/-/paper343/acceptance", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers unanimously recommend rejecting the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396527707, "id": "ICLR.cc/2017/conference/-/paper343/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396527707}}}, {"tddate": null, "tmdate": 1481907369958, "tcdate": 1481907369958, "number": 3, "id": "rJznn9ZNx", "invitation": "ICLR.cc/2017/conference/-/paper343/official/review", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["ICLR.cc/2017/conference/paper343/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper343/AnonReviewer3"], "content": {"title": "Minor variant on existing regularization methods", "rating": "4: Ok but not good enough - rejection", "review": "The proposed regularizer seems to be a particular combination of existing methods. Though the implied connection between nonlinearities and stochastic regularizers is intriguing, in my opinion the empirical performance does not exceed the performance achieved by similar methods by a large enough margin to arrive at a meaningful conclusion. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512619115, "id": "ICLR.cc/2017/conference/-/paper343/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper343/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper343/AnonReviewer2", "ICLR.cc/2017/conference/paper343/AnonReviewer1", "ICLR.cc/2017/conference/paper343/AnonReviewer3"], "reply": {"forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512619115}}}, {"tddate": null, "tmdate": 1481897459515, "tcdate": 1481897459515, "number": 2, "id": "H1jgL_WVx", "invitation": "ICLR.cc/2017/conference/-/paper343/official/review", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["ICLR.cc/2017/conference/paper343/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper343/AnonReviewer1"], "content": {"title": "Official review.", "rating": "5: Marginally below acceptance threshold", "review": "The method proposed essential trains neural networks without a traditional nonlinearity, using multiplicative gating by the CDF of a Gaussian evaluated at the preactivation; this is motivated as a relaxation of a probit-Bernoulli stochastic gate. Experiments are performed with both.\n\nThe work is somewhat novel and interesting. Little is said about why this is preferable to other similar parameterizations of the same (sigmoidal? softsign? etc.) It would be stronger with more empirical interrogation of why this works and exploration of the nearby conceptual space. The CIFAR results look okay by today's standards but the MNIST results are quite bad, neural nets were doing better than 1.5% a decade ago and the SOI map results (and the ReLU baseline) are above 2%. (TIMIT results on frame classification also aren't that interesting without evaluating word error rate within a speech pipeline, but this is a minor point.)\n\nThe idea put forth that SOI map networks without additional nonlinearities are comparable to linear functions is rather misleading as they are, in expectation, nonlinear functions of their input. Varying an input example by multiplying or adding a constant will not be linearly reflected in the expected output of the network. In this sense they are more nonlinear than ReLU networks which are at least locally linear.\n\nThe plots are very difficult to read in grayscale,", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512619115, "id": "ICLR.cc/2017/conference/-/paper343/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper343/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper343/AnonReviewer2", "ICLR.cc/2017/conference/paper343/AnonReviewer1", "ICLR.cc/2017/conference/paper343/AnonReviewer3"], "reply": {"forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512619115}}}, {"tddate": null, "tmdate": 1481835915170, "tcdate": 1481835244927, "number": 4, "id": "H1Bl7KeVx", "invitation": "ICLR.cc/2017/conference/-/paper343/public/comment", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "A Brief Comment on the SOI Map", "comment": "It is worth mentioning that the SOI map is not a proposed dropout replacement--we only conclude that it is \"comparable to nonlinearities plus dropout,\" as our experiments show. In the current draft, we even call it \"an Adaptive Dropout variant without any nonlinearity.\" We mention the SOI map only because it aids in our motivation of the GELU, shows that traditional nonlinearities are not necessary for training, and because it is an endpoint of a bridge from a stochastic regularizer to a nonlinearity. We do not intend for it to be construed as a proposed dropout replacement, and I am sorry if it was."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616476, "id": "ICLR.cc/2017/conference/-/paper343/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk0MRI5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper343/reviewers", "ICLR.cc/2017/conference/paper343/areachairs"], "cdate": 1485287616476}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481833419469, "tcdate": 1478286869905, "number": 343, "id": "Bk0MRI5lg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bk0MRI5lg", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "content": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481803986384, "tcdate": 1481803986376, "number": 1, "id": "B15RublVe", "invitation": "ICLR.cc/2017/conference/-/paper343/official/review", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["ICLR.cc/2017/conference/paper343/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper343/AnonReviewer2"], "content": {"title": "The proposed approach seems similar to other existing approaches in literature (eg. adaptive dropout). Experimental validation not adequate for evaluation.", "rating": "5: Marginally below acceptance threshold", "review": "Approaches like adaptive dropout also have the binary mask as a function of input to a neuron very similar to the proposed approach. It is not clear, even from the new draft, how the proposed approach differs to Adaptive dropout in terms of functionality. The experimental validation is also not extensive since comparison to SOTA is not included. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512619115, "id": "ICLR.cc/2017/conference/-/paper343/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper343/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper343/AnonReviewer2", "ICLR.cc/2017/conference/paper343/AnonReviewer1", "ICLR.cc/2017/conference/paper343/AnonReviewer3"], "reply": {"forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512619115}}}, {"tddate": null, "tmdate": 1481600466326, "tcdate": 1481600466320, "number": 3, "id": "SkqC6yTmg", "invitation": "ICLR.cc/2017/conference/-/paper343/public/comment", "forum": "Bk0MRI5lg", "replyto": "ryBRQxymx", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "RE: comparison to literature", "comment": "Thank you for the questions. Adaptive dropout multiplies the nonlinearity output by a mask (page 3 of Ba et al.), but with the SOI map, a mask is multiplied by the neuron input. Consequently, the SOI map trains without any nonlinearity, while adaptive dropout modifies the output of a nonlinearity. Imagine implementing adaptive dropout, changing the probability estimate from a sigmoid to a Gaussian CDF, and replacing all nonlinearities with an identity transformation: this gives us SOI map, a stochastic regularizer showing that traditional nonlinearities may not be essential for neural networks since there are no differentiable, deterministic nonlinearities in training. Moreover, the composition of adaptive dropout and a ReLU only results in nonnegative outputs, so it is unlike the GELU. We now contrast with adaptive dropout in an updated manuscript thanks to your question.\n\nYou raise an interesting connection to annealed dropout. A difference between the SOI Map and (annealed) dropout is that the variance of the output monotonically increases as the input increases with (annealed) dropout, but this is not necessarily the case with the SOI map (page 6 of the manuscript). Another difference is that the composition of a ReLU and annealed dropout will threshold above zero, not around zero. This is because the ReLU will certainly threshold at 0, and dropout will randomly mask the ReLU output again regardless of the input. Yet the SOI map may not threshold around 0 depending on the input or it may threshold below 0, which cannot happen with a ReLU and (annealed) dropout."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616476, "id": "ICLR.cc/2017/conference/-/paper343/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk0MRI5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper343/reviewers", "ICLR.cc/2017/conference/paper343/areachairs"], "cdate": 1485287616476}}}, {"tddate": null, "tmdate": 1481600378881, "tcdate": 1481600378876, "number": 2, "id": "S1mYT16mg", "invitation": "ICLR.cc/2017/conference/-/paper343/public/comment", "forum": "Bk0MRI5lg", "replyto": "HysQDQCfl", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "RE: Architectural choices and comparisons to the literature", "comment": "Thank you for the questions. Our MNIST Classification experiment is from the original ELU paper [1]. The primary difference is that we also show how nonlinearities perform if we add dropout, and we also use a better optimizer.\nLikewise, the MNIST autoencoding architecture also appears in the ELU paper to showcase the strength of the ELU. The same deep autoencoder architecture appears in a Deepmind paper [2], but the nonlinearities are of course different.\nNext, for our small dataset experiment we use a popular NLP dataset [3], and we use a fully connected network with accuracy approaching 90%, which is a reasonable accuracy for a non-structured approach without external resources (the original paper reports better accuracies when adding many features derived from annotated and curated resources). \nFor the automatic speech recognition dataset, we use the architecture described in the field-shifting paper [4]. In Srivastava's thesis on dropout [5], he found using dropout on deep neural networks increased performance on TIMIT, so we use dropout too.\nOur CIFAR-100 classifier uses Wide Residual Networks [6] which held the state of the art on several vision tasks for part of this summer. We also used a better learning rate schedule for WideResNets to increase accuracy [7] and dropout. These modifications were helpful because the 40-4 WideResNet in the original paper obtained 22.89% error with a ReLU, but under our modifications we obtain 21.77% with a ReLU and 20.74% with a GELU.\nOur CIFAR-10 classifier uses the exact code for the convnet in [8], which held state of the art on CIFAR-10 without data augmentation for much of this year.\n\nWe show the learning curves instead of just the final accuracy because the speed of convergence is important for running experiments quickly, and for neural networks in reinforcement learning convergence speed can be a critical factor [9].\n\nAdaptive dropout multiplies the nonlinearity output by a mask (page 3 of Ba et al.), but with the SOI map, a mask is multiplied by the neuron input. Consequently, the SOI map trains without any nonlinearity, while adaptive dropout modifies the output of a nonlinearity. Imagine implementing adaptive dropout, changing the probability estimate from a sigmoid to a Gaussian CDF, and replacing all nonlinearities with an identity transformation: this gives us SOI map, a stochastic regularizer showing that traditional nonlinearities may not be essential for neural networks since there are no differentiable, deterministic nonlinearities in training. Moreover, the composition of adaptive dropout and a ReLU only results in nonnegative outputs, so it is unlike the GELU.\n\n\n[1] Clevert et al. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n[2] Desjardins et al. Natural Neural Networks\n[3] Gimpel et al. Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments\n[4] Mohamed, Hinton et al. Acoustic Modeling Using Deep Belief Networks\n[5] Srivastava. Improving Neural Networks with Dropout\n[6] Zagoruyko and Komodakis. Wide Residual Networks\n[7] Loshchilov and Hutter. SGDR: Stochastic Gradient Descent with Restarts\n[8] Salimans and Kingma. Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\n[9] Schulman. Deep Reinforcement Learning at the Machine Learning Summer School"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616476, "id": "ICLR.cc/2017/conference/-/paper343/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk0MRI5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper343/reviewers", "ICLR.cc/2017/conference/paper343/areachairs"], "cdate": 1485287616476}}}, {"tddate": null, "tmdate": 1481600291535, "tcdate": 1481600291530, "number": 1, "id": "ryjmTyTXg", "invitation": "ICLR.cc/2017/conference/-/paper343/public/comment", "forum": "Bk0MRI5lg", "replyto": "SyfAoQ1ml", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "RE: hyper-parameters", "comment": "Thank you for the question. Introducing learnable hyperparameters such that a competing nonlinearity is a special case could be unfair.\nHowever, we kept the hyperparameters fixed for all experiments (\\mu=0, \\sigma=1), so the nonlinearity could not have learned to become a ReLU.\nWe highlight this choice on page 2 more than previous versions of the manuscript thanks to your comment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287616476, "id": "ICLR.cc/2017/conference/-/paper343/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bk0MRI5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper343/reviewers", "ICLR.cc/2017/conference/paper343/areachairs"], "cdate": 1485287616476}}}, {"tddate": null, "tmdate": 1480698826253, "tcdate": 1480698826249, "number": 3, "id": "SyfAoQ1ml", "invitation": "ICLR.cc/2017/conference/-/paper343/pre-review/question", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["ICLR.cc/2017/conference/paper343/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper343/AnonReviewer3"], "content": {"title": "hyper-parameters", "question": "Considering that the GELU introduces extra hyper-parameters (\\mu and \\sigma), \ndo the authors think it is a fair to compare against ReLU which is merely a \nspecial case of GELU where \\mu and \\sigma are both zero? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959333074, "id": "ICLR.cc/2017/conference/-/paper343/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper343/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper343/AnonReviewer1", "ICLR.cc/2017/conference/paper343/AnonReviewer2", "ICLR.cc/2017/conference/paper343/AnonReviewer3"], "reply": {"forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959333074}}}, {"tddate": null, "tmdate": 1480684493034, "tcdate": 1480684493028, "number": 2, "id": "ryBRQxymx", "invitation": "ICLR.cc/2017/conference/-/paper343/pre-review/question", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["ICLR.cc/2017/conference/paper343/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper343/AnonReviewer2"], "content": {"title": "comparison to literature ", "question": "Can the authors please elaborate how the presented approach is different from works like Adaptive dropout?\n\nWhile ReLU has a fixed threshold at 0 the presented approach places the threshold randomly around zero. This might result in effect similar to decreasing noise levels over training epochs due to saturation of activations, a concept previously proposed in the literature (\"Annealed dropout training of deep networks\"). Can you please comment on this?  \n\nI also have similar questions as that of AnonReviewer1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959333074, "id": "ICLR.cc/2017/conference/-/paper343/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper343/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper343/AnonReviewer1", "ICLR.cc/2017/conference/paper343/AnonReviewer2", "ICLR.cc/2017/conference/paper343/AnonReviewer3"], "reply": {"forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959333074}}}, {"tddate": null, "tmdate": 1480632099224, "tcdate": 1480632099221, "number": 1, "id": "HysQDQCfl", "invitation": "ICLR.cc/2017/conference/-/paper343/pre-review/question", "forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "signatures": ["ICLR.cc/2017/conference/paper343/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper343/AnonReviewer1"], "content": {"title": "Architectural choices and comparisons to the literature", "question": "- Your manuscript relies exclusively on baselines you've run yourself. Can you please elaborate on how your method compares to results already presented in the literature on your tasks of interest? \n- Can you explain what it is you're trying to illustrate with the loss curves? These account for 4 of 7 figures but it's not clear why they're being displayed, or why they convey more information than a table of final results at convergence.\n- Can you contrast your method with other work on adaptive dropout masks, e.g. Ba et al's \"standout\"?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "pdf": "/pdf/a171eb69077774da944c6b59bfc7ea95b2286029.pdf", "TL;DR": "A Competitor of ReLUs and ELUs with a Probabilistic Underpinning", "paperhash": "hendrycks|bridging_nonlinearities_and_stochastic_regularizers_with_gaussian_error_linear_units", "keywords": [], "conflicts": ["uchicago.edu", "ttic.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959333074, "id": "ICLR.cc/2017/conference/-/paper343/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper343/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper343/AnonReviewer1", "ICLR.cc/2017/conference/paper343/AnonReviewer2", "ICLR.cc/2017/conference/paper343/AnonReviewer3"], "reply": {"forum": "Bk0MRI5lg", "replyto": "Bk0MRI5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper343/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959333074}}}], "count": 12}