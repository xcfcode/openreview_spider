{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363751520000, "tcdate": 1363751520000, "number": 1, "id": "L9s74sx8Ka9cP", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "rOvg47Txgprkn", "replyto": "6tLOt5yk_I6cd", "signatures": ["Mateusz Malinowski"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "As Table 1 shows our method gives similar results to Jia's method (79.6% and 80.17% accuracy). If we allow transfer between datasets, our method gives slightly better results (Table 5 reports 80.35% test accuracy for our method).\r\n\r\nWe could weight features with real-valued weights constrained to unit cube, and next use max-operator."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363741140000, "tcdate": 1363741140000, "number": 4, "id": "6tLOt5yk_I6cd", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["anonymous reviewer 45d8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I'm not sure why the authors are claiming state of the art on CIFAR-10 in their response, because the paper doesn't make this claim and I don't see any update to the paper. The method does not actually have state of the art on CIFAR-10 even under the constraint that it follow the architecture considered in the paper. It's nearly as good as Jia and Huang's method but not quite as good.\r\n\r\nBack-propagation over the max operator may be possible, but how would you parameterize the max to include or exclude different input features? Each max pooling unit needs to take the max over some subset of the detector layer features. Since including or excluding a feature in the max is a hard 0/1 decision it's not obvious how to learn those subsets using your gradient based method.\r\n\r\nRegarding the competitiveness of CIFAR-100: This is not a very important point because CIFAR-100 being competitive or not doesn't enter much into my evaluation of the paper. It's still true that the proposed method beats Jia and Huang on that dataset. However, I do think that my opinion of CIFAR-100 as being less competitive than CIFAR-10 is justified. I'm aware that CIFAR-100 has fewer examples per class and that this explains why the error rates published on that dataset are higher. My reason for considering it less competitive is that the top two papers on CIFAR-100 right now both say that they didn't even bother optimizing their hyperparameters for that dataset. Presumably, anyone could easily get a better result on that dataset just by downloading the code for one of those papers and playing with the hyperparameters for a day or two."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363737660000, "tcdate": 1363737660000, "number": 7, "id": "bYfTY-ABwrbB2", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["Mateusz Malinowski, Mario Fritz"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank all the reviewers for their comments.\r\nWe will include suggested papers on related work and origins of pooling architectures as well as improvement on the state of the art that occurred in the meanwhile.\r\nThe reviewers acknowledge our analysis of regularization schemes to learn weighted pooling units together with a regularizer that promotes spatial smoothness.\r\n\r\nOur work aims at replacing hand-crafted pooling stage in computer vision architectures ([1], [2], [3] and [4]) where the pooling is a way to reduce dimensionality of the features while preserving spatial information. Handcrafted spatial pooling schemes that operate on an image-level are still part of many state of the art architectures. In particular, recent approaches that aim at higher-level semantic representations (e.g. [3], [6]) follow this paradigm and are within the scope of our method. We therefore believe that our method will find wide applicability in those scenarios.\r\n\r\nAnonymous 45d8:\r\nWe don't agree that CIFAR-100 is less-competitive as the state-of-the-art results are lower than CIFAR-10, moreover CIFAR-100 contains fewer examples per class for training and 10x more classes.\r\nWe are not restricted to sum pooling as back-propagation over the max operator is possible. \r\nWe use non-negativity constraint for the weights as Formula 5 shows.\r\nSparsity constraint on the weights has no computational benefits at test time as the weighted sum ranges over the whole image. \r\nConcerning the remarks about increased computation time, we would like to point out that computational costs are dominated by the coding procedure. The pooling stage - hand-crafted or learnt - is on the order of milliseconds per image.\r\nThe connection between the matrix factorization of the weights of the softmax classifier and pooling stage is an interesting additional observation, however, the paper analyzes the regularization terms of the pooling operator and therefore regularization of the factorized weight matrix.\r\nIn our work we want to make our architecture consistent with other computer vision architectures that use image-level pooling stage ([1], [2], [3], [4] and [5]) exploiting the shared representation among classes and computational benefits of this method.\r\n\r\nAnonymous 2426:\r\nThe method produces state-of-the-art results, at the time of submission, on CIFAR-100 and state-of-the-art on both CIFAR-10 and CIFAR-100 given SPM architecture ([1], [2], [4]).\r\nAs our results show, the smoothness constrain/regularization is the most crucial (Table 3), non-negativity constraint though increases the interpretability of the results. We use lbfgs with projection onto a unit box after every weights update. \r\nAlthough some of our speed-ups to make the system more scalable are heuristic, they are appreciated e.g. by 'Anonymous c1a0' and share similarities with recently proposed approaches for scalable learning as we reference in the paper.\r\n\r\nAnonymous c1a0:\r\nIncreasing number of classification parameters in the SPM architecture ([1], [2], [4]) requires a bigger codebooks which increases the complexity of encoding step as every image patch has to be assigned to a cluster via triangle coding [4]. This would lead to a significant increase at test time. On the other hand, our architecture adds little overhead compared to SPM architectures at the test time.\r\n\r\nAnonymous 45d8 & Anonymous 2426:\r\nThe pre-pooling step is pooling over a small neighborhood (over a 3x3 neighboring pixels), and therefore can be seen as form of weight sharing. This is a technical detail in order to reduce memory consumption and training time. This doesn't defy the main argument given in the paper as pooling is learnt over larger areas."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362196620000, "tcdate": 1362196620000, "number": 3, "id": "0IOVI1hnXH0m-", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["anonymous reviewer c1a0"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learnable Pooling Regions for Image Classification", "review": "The paper presents a method for training pooling regions in image classification pipelines (similar to those that employ bag-of-words or spatial pyramid models).  The system uses a linear pooling matrix to parametrize the pooling units and follows them with a linear classifier.  The pooling units are then trained jointly with the classifier.  Several strategies for regularizing the training of the pooling parameters are proposed in addition to several tricks to increase scalability.  Results are presented on the CIFAR10 and CIFAR100 datasets.\r\n\r\nThe main idea here appears to be to replace the 'hard coded' average pooling stage + linear classifier with a trainable linear pooling stage + linear classifier.  Though I see why this is natural, it is not clear to me why using two linear stages is advantageous here since the combined system is no more powerful than connecting the linear classifier directly to all the features.  The two main advantages of competing approaches are that they can dramatically reduce dimensionality or identify features to combine with nonlinear pooling operations.  It could be that the performance advantage of this approach (without regularization) comes from directly learning the linear classifier from all the feature values (and thus the classifier has lower bias).\r\n\r\nThe proposed regularization schemes applied to the pooling units potentially change the picture.  Indeed the authors found that a 'smoothness' penalty (which enforces some spatial coherence on the pooling weights) was useful to regularize the system, which is quite similar to what is achieved using hand-coded pooling areas.  The advantage is that the classifier is given the flexibility to choose other weights for all of the feature values while retaining regularization that is similar to hand-coded pooling.  How useful this effect is in general seems worth exploring in more detail.\r\n\r\nPros:\r\n(1)  Potentially interesting analysis of regularization schemes to learn weighted pooling units.\r\n(2)  Tricks for pre-training the pooling units in batches and transferring the results to other datasets.\r\n\r\nCons:\r\n(1)  The method does not appear to add much power beyond the ability to specify prior knowledge about the smoothness of the weights along the spatial dimensions.\r\n(2)  The results show some improvement on CIFAR-100, but it is not clear that this could not be achieved simply due to the greater number of classifier parameters (as opposed to the pooling methods proposed in the paper.)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362138060000, "tcdate": 1362138060000, "number": 5, "id": "4w1kwHXszr4D8", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["anonymous reviewer 2426"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learnable Pooling Regions for Image Classification", "review": "This paper proposes a method to jointly train a pooling layer and a classifier in a supervised way. \r\nThe idea is to first extract some features and then train a 2 layer neural net by backpropagation (although in practice they use l-bfgs). The first layer is linear and the parameters are box constrained and regularized to be spatially smooth. The authors propose also several little tricks to speed up training (divide the space into smaller pools, partition the features, etc.).\r\n\r\nMost relevant work related to this method is cited but some references are missing.\r\nFor instance, learning pooling (and unappealing) regions was also proposed by Zeiler et al. in an unsupervised setting:\r\nDifferentiable Pooling for Hierarchical Feature Learning\r\nMatthew D. Zeiler and Rob Fergus\r\narXiv:1207.0151v1 (July 3, 2012)\r\nSee below for other missing references.\r\n\r\nThe overall novelty is limited but sufficient. In my opinion the most novel piece in this work is the choice of the regularizer that enforces smoothness in the weights of the pooling. This regularization term is not new per se, but its application to learning filters certainly is.\r\nThe overall quality is fair. The paper lacks clarity in some parts and the empirical validation is ok but not great.\r\nI wish the authors stressed more the importance of the weight regularization and analyzed that part a bit more in depth instead of focussing on other aspects of their method which seem less exciting actually.\r\n\r\nPROS\r\n+ nice idea to regularize weights promoting spatial smoothness\r\n+ nice visualization of the learned parameters\r\n\r\nCONS\r\n- novelty is limited and the overall method relies on heuristics to improve its scalability\r\n- empirical validation is ok but not state of the art as claimed\r\n- some parts of the paper are not clear\r\n- some references are missing\r\n\r\nDetailed comments:\r\n- The notation in sec. 2.2 could be improved. In particular, it seems to me that pooling is just a linear projection subject to constraints in the parameterization. The authors mentions that constraints are used just for interpretability but I think they are actually important to make the system 'less unidentifiable' (since it is the composition of two linear stages). \r\nRegarding the box  constraints, I really do not understand how the authors modified l-bfgs to account for these box constraints since this is an unconstrained optimization method. A detailed explanation is required for making this method reproducible. Besides, why not making the weights non-negative and sum to one instead?\r\n- The pre-pooling step is unsatisfying because it seems to defeat the whole purpose of the method. Effectively, there seem to be too many other little tricks that need to be in place to make this method competitive.\r\n- Other people have reported better accuracy on these datasets. For instance, \r\nPractical Bayesian Optimization of Machine Learning Algorithms\r\nJasper Snoek, Hugo Larochelle and Ryan Prescott Adams\r\nNeural Information Processing Systems, 2012\r\n- There are lots of imprecise claims:\r\n  - convolutional nets before HMAX and SPM used pooling and they actually learned weights in the average pooling/subsampling step\r\n  - 'logistic function' in pag. 3 should be 'softmax function'\r\n  - the contrast with the work by Le et al. on pag.4 is weak since although pooling regions can be trained in parallel but the classifier trained on the top of them has to be done afterwards. This sequential step makes the whole procedure less parallelizable.\r\n  - second paragraph of sec. 3.2 about 'transfer pooling regions' is not clear."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361927280000, "tcdate": 1361927280000, "number": 9, "id": "uEhruhQZrGeZw", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["anonymous reviewer 45d8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "PS. After reading some of the other comments, I see that I was wrong about the weights in the linear layer being possibly negative. I actually wasn't able to find the part of the paper that specifies this. I think in general the paper could be improved by being a little bit more straightforward. The method is very simple but it's difficult to tell exactly what the method is from reading the paper.\r\n\r\nI definitely agree with Yann LeCun that the smoothness prior is interesting and should be explored in more detail."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361922660000, "tcdate": 1361922660000, "number": 6, "id": "ddaBUNcnvHrLK", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This is a follow-up to Yoshua Bengio's comment. I'm lead author on the paper that he linked to.\r\n\r\nOne reason that Zeiler & Fergus got good results on CIFAR-100 with stochastic max pooling and my co-authors and I got good results on CIFAR-100 with maxout is that we were both using deep architectures. I think there's room to ask the scientific question 'how well can we do with one layer, just by being more clever about how to do the pooling?' even if this doesn't immediately lead to better answers to the engineering question, 'how can we get the best possible numbers on CIFAR-100?' So it's important to evaluate Malinowski and Fritz's method in the context of it being constrained to using a single-layer architecture.\r\n\r\nOn the other hand, it's not obvious to me that Malinowski and Fritz's training procedure would generalize to deeper achitectures, since the current implementation assumes that the output of the pooling layer is connected directly to the classification layer. It would be interesting to investigate whether this strategy (and Jia and Huang's strategy) works for deeper architectures."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361914920000, "tcdate": 1361914920000, "number": 8, "id": "xEdmrekMJsvCj", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["anonymous reviewer 45d8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learnable Pooling Regions for Image Classification", "review": "Summary:\r\nThe paper proposes to replace the final stages of Coates and Ng's CIFAR-10 classification pipeline. In place of the hand-designed 3x3 mean pooling layer, the paper proposes to learn a pooling layer. In place of the SVM, the paper proposes to use softmax regression jointly trained with the pooling layer.\r\n\r\nThe most similar prior work is Jia and Huang's learned pooling system. Jia and Huang use a different means of learning the pooling layer, and train a separate logistic regression classifier for each class instead of using one softmax model.\r\n\r\nThe specific method proposed here for learning the pooling layer is to make the pooling layer a densely connected linear layer in an MLP and train it jointly with the softmax layer.\r\n\r\nThe proposed method doesn't work quite as well as Jia and Huang's on the CIFAR-10 dataset, but does beat them on the less-competitive CIFAR-100 benchmark.\r\n\r\nPros:\r\n-The method is fairly simple and straightforward\r\n-The method improves on the state of the art of CIFAR-100 (at the time of submission, there are now two better methods known to this reviewer)\r\n\r\nCons:\r\n-I think it's somewhat misleading to call this operation pooling, for the following reasons:\r\n1) It doesn't allow learning how to max-pool, as Jia and Huang's method does. It's sort of like mean pooling, but since the weights can be negative it's not even really a weighted average.\r\n2) Since the weights aren't necessarily sparse, this loses most of the computational benefit of pooling, where each output is computed as a function of just a few inputs. The only real computational benefit is that you can set the hyperparameters to make the output smaller than the input, but that's true of convolutional layers too.\r\n-A densely connected linear layer followed by a softmax layer is representationally equivalent to a softmax layer with a factorized weight matrix. Any improvements in performance from using this method are therefore due to regularizing a softmax model better. The paper doesn't explore this connection at all.\r\n-The paper doesn't do proper controls. For example, their smoothness prior might explain their entire success. Just applying the smoothness prior to the softmax model directly might work just as well as factoring the softmax weights and applying the smoothness prior to one factor.\r\n-While the paper says repeatedly that their method makes few assumptions about the geometry of the pools, their 'pre-pooling' step seems to make most of the same assumptions as Jia and Huang, and as far as I can tell includes Coates and Ng's method as a special case."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361903280000, "tcdate": 1361903280000, "number": 1, "id": "DtAvRX423kRIf", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This is an interesting investigation and I only have remarks to make regarding the CIFAR-10 and CIFAR-100 results and the rapidly moving state-of-the-art (SOTA).  In particular, on CIFAR-100, the 56.29% accuracy is not state-of-the-art anymore (thankfully, our field is moving fast!). There was first the result by Zeiler & Fergus using stochastic pooling, bringing the SOTA to 57.49% accuracy. Then, using another form of pooling innovation (max-linear pooling units with dropout, which we call maxout units), we brought the SOTA on CIFAR-100 to 61.43% accuracy. On CIFAR-10, maxout networks also beat the SOTA, bringing it to 87.07% accuracy. All these are of course without using any deformations.\r\n\r\nYou can find these in this arxiv paper (which appeared after your submission): http://arxiv.org/abs/1302.4389\r\n\r\nMaxout units also use linear filters pooled with a max, but without the positivity constraint. We found that using dropout on the max output makes a huge difference in performance, so you may want to try that as well."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360973580000, "tcdate": 1360973580000, "number": 10, "id": "mdD47o8J4hmr1", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["Mateusz Malinowski"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Our paper addresses the shortcomings of fixed and data-independent pooling regions in architectures such as Spatial Pyramid Matching [Lazebnik et. al., 'Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories', CVPR 2006], where dictionary-based features are pooled over large neighborhood. In our work we propose an alternative data-driven approach for the pooling stage, and there are three main novelties of our work.\r\n \r\nFirst of all, we base our work on the popular Spatial Pyramid Matching architectures and generalize the pooling operator allowing for joint and discriminative training of both the classifier together with the pooling operator. The realization of the idea necessary for training is essentially an Artificial Neural Network with dense connections between pooling units with the classifier, and the pooling units connected with the high-dimensional dictionary-based features. Therefore, back-propagation and the Neural Network interpretation should rather be considered here as a tool to achieve joint and data-dependent training of the parameters of the pooling operator and the classifier. Moreover, our parameterization allows for the interpretation in terms of spatial regions. The proposed architecture is an alternative to another discriminatively trained architecture presented by Jia et. al.  ['Beyond spatial pyramids: Receptive field learning for pooled image features' CVPR 2012 and NIPS workshop 2011] outperforming the latter on the CIFAR-100 dataset.\r\n\r\nSecondly, as opposed to the previous Spatial Pyramid Matching schemes, we don't constrain the pooling regions to be the identical for all coordinates of the code.\r\n \r\nLastly, as you've said, we investigate regularization terms. The popular spatial pyramid matching architectures which we generalize in this paper are typically used to pool over large spatial regions. In combination with our code-specific pooling scheme this leads to a large number of parameters that call for regularization. In our investigations of different regularizers it turns out that a smoothness regularizer is key to strong performance for this type of architecture on CIFAR-10 and CIFAR-100 datasets.\r\n\r\nConcerning LBFGS vs SGD: We have chosen LBFGS out of convenience, as it tends to have fewer parameters.\r\n\r\nThanks for pointing out missing references."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360139640000, "tcdate": 1360139640000, "number": 2, "id": "ttaRtzuy2NtjF", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "rOvg47Txgprkn", "replyto": "rOvg47Txgprkn", "signatures": ["Yann LeCun"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "As far as I can tell, the algorithm in section 2.2 (pooling + linear classifier) is essentially a 2-layer neural net trained with backprop, except that the hidden layer is linear with positive weights.\r\nThe only innovation seems to be the weight spatial smoothness regularizer of section 2.3. I think this should be emphasized.\r\n\r\nQuestion: why use LBFGS when a simple stochastic gradient would have been simpler and probably faster?\r\n\r\nThe introduction seems to suggest that pooling appeared with [Riesenhuber and Poggio 2009] and [Koenderink and van Doorn 1999], but models of vision with pooling (even multiple levesl of pooling) can be found in the neo-cognitron model [Fukushima 1980] and in convolutional networks [LeCun et al. 1990, and pretty much every subsequent paper on convolutional nets].\r\nThe origin of the idea can be traced to the 'complex cell' model from Hubel and Wiesel's classic work on the cat's primary visual cortex [Hubel and Wiesel 1962].\r\n\r\nYou might also be interested in [Boureau et al. ICML 2010] 'A theoretical analysis of feature pooling in vision algorithms'."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358430300000, "tcdate": 1358430300000, "number": 21, "id": "rOvg47Txgprkn", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "rOvg47Txgprkn", "signatures": ["mmalinow@mpi-inf.mpg.de"], "readers": ["everyone"], "content": {"title": "Learnable Pooling Regions for Image Classification", "decision": "conferencePoster-iclr2013-workshop", "abstract": "From the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms used in the proposed model together with an efficient method to train them. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter.", "pdf": "https://arxiv.org/abs/1301.3516", "paperhash": "malinowski|learnable_pooling_regions_for_image_classification", "keywords": [], "conflicts": [], "authors": ["Mateusz Malinowski", "Mario Fritz"], "authorids": ["mmalinow@mpi-inf.mpg.de", "mario.j.fritz@googlemail.com"]}, "writers": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 12}