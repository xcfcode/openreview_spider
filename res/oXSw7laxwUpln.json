{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392786240000, "tcdate": 1392786240000, "number": 1, "id": "11PgjmvJdz2kc", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "COqd1FIP0RONW", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We're working on this, but as you said we won't have it in time for the rebuttal."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392786180000, "tcdate": 1392786180000, "number": 4, "id": "rWtJWM_B7bqIc", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "UU9xOhFI5MUib", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'The authors state that the issue was 'well-studied in the past' but provide no references or summary of that past work. I am not familiar with that literature.'\r\nA reasonably good review is available here: http://ox.no/files/catastrophic_forgetting_in_neural_networks.pdf\r\nWe can cite this paper and / or write more of an overview in our own introduction for the final copy if desired.\r\n\r\n'The suggestion that the first layer of a neural network is not rewired in response to a remapping of MNIST pixels, but rather upper layers handle the new task is counter-intuitive and intriguing. The paper would be stronger if the authors tested this hypothesis.'\r\n\r\nThanks for asking us to check this. It looks like our visual inspection of the weights was incorrect:\r\nTask 1 test error, using all parameters trained on task 1: 1.44%\r\nTask 1 test error, restoring the layer 0 parameters to those trained on task 0: 78.97%\r\nTask 1 test error, restoring the layer 1 parameters to those trained on task 0: 1.65%\r\nThe network does appear to be doing the desired behavior after all. We'll change the claim and include these new experiments in the final copy.\r\n\r\n\r\n'Figures 1 and 3: A better caption would help these intriguing figures to stand on their own. Is a lower-envelope on a scatter plot of randomly drawn models being drawn here? Is there a modelling reason that motivates connecting the dots, or is it simply a visual aide? Did you consider leaving all of the scatter-plots on the axes (though it might be too busy)?'\r\n\r\nWe will attempt to improve the caption for the final copy. Yes, it is a lower envelope. The idea of drawing the envelope is just to indicate the best performance we were able to demonstrate is obtainable by varying the hyper parameters. The existence of a 'bad' point in the scatterplot is not necessarily meaningful, since we can easily reach bad points for any method just by setting the hyper parameters badly. The existence of 'good points' is meaningful though, since they demonstrate that each method can perform at least as well as that point indicates. The scatterplot could be somewhat useful for understanding how robust the various methods are to hyper parameter choices, though the full scatterplot is very busy. We can add the full scatterplots as an appendix."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392786180000, "tcdate": 1392786180000, "number": 2, "id": "daI9sS8PVBdhc", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "ff2Ufvs1UolkP", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'I am mystified about the comment on dropout having a \u201cknown\u201d range of values that works well (0.5 or 0.2, depending on whether it\u2019s a hidden or visible unit). This may very well be true in the context of obtaining a good performance on a given set of tasks that the authors consider. But since dropout can be seen as having a regularization effect: this effect can manifest itself by less (or more) catastrophic forgetting.'\r\n\r\nThe performance curve is actually pretty flat over a broad region surrounding 0.5. See fig 2.6 of http://www.cs.toronto.edu/~nitish/msc_thesis.pdf\r\nThe use of 0.5 is so standard that many descriptions of dropout, even by its inventors, omit the possibility of using a value other than 0.5. See for example section 4.2 of https://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\r\nKeep in mind that we found that dropout performed better than no-dropout on every single task, so this was a reasonably good setting. Further searchers can only improve the performance, at least on the validation set.\r\n\r\n'Other than that, it would be nice if the authors proposed hypotheses for why maxout seems to perform better -- it is not obvious from their analysis that this should be the case.'\r\n\r\nWe didn't really have any analysis, but we do have a hypothesis we could include in the paper. Each maxout unit contains multiple filters. Only the filter that wins the max receives any gradient at all. This means if filter i learns to be useful on task 0 but is orthogonal to most of the inputs seen in task 1, then probably filter j will be more active than filter i on task 1. Filter i thus won't be modified much during training on task 1. The final network will have filter i take on maximal activations on task 0 while filter j takes on maximal activations on task 1.\r\n\r\nHard LWTA has the same property, but it lacks pooling. Pooling has an additional benefit. Suppose that first layer unit k represents a concept that is useful for both task 0 and task 1. If maxout unit k contains filters i and j, with filter i working well on task 0 and filter j working well on task 1, then the second layer weight coming out of unit k doesn't need to change much in order to solve task 1. For LWTA, during the training of the second layer, there is no pooling and no concept of a 'unit k'. Filters i and j drive two separate units, which are both in block k, but the next layer has no way of knowing about the blocks a priori and must learn that filter j on task 1 means the same thing that filter i meant on task 0.\r\n\r\n\r\n'A general comment: it seems rather odd that catastrophic forgetting seems to be somehow tied to the choice of an activation function in this paper'\r\n\r\nThis idea was put forward in a paper at NIPS this year (Srivastava 2013), which argued that using the hard LWTA activation function was a means of combating catastrophic forgetting. One of our main goals for this paper was to further test the idea from that paper.\r\n\r\n'Arguably, things like the optimization and, generally speaking, the regularization strategies employed, are equally as important. This paper touches very little upon those issues, glossing over anything other than SGD + dropout.'\r\n\r\nOur main objective was to determine how prone the 'modern' approaches are to catastrophic forgetting. That's why we used the max norm regularization--this was used by Geoff Hinton's group to set several state of the art results with their paper on dropout, and we also used it ourselves to set several state of the art results in our paper on maxout units."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392780540000, "tcdate": 1392780540000, "number": 3, "id": "VIv0IzYmZ5-zX", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "C2_L2xJZKTCxq", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'The problem of catastrophic forgetting seems irrelevant to most of the current research in deep learning, and seems to be a somewhat constructed thought experiment. The paper does not mention a practical task that requires training the same neural network on two problems in sequence. Indeed, I can\u2019t think of such a task, and the authors should significantly rewrite the introduction to make clear why this problem is important.'\r\n\r\nThe purpose of this work isn't to solve any specific task. It's not an engineering paper, it's a basic science paper. The goal of this paper is simply to characterize a phenomenon. We don't attempt to capitalize on the understanding of that phenomenon directly all in one paper. The purpose of publishing is to spread what we have learned about this phenomenon so that other people have an opportunity to capitalize on it. \r\n\r\nThere are many reasons to believe that an improved understanding of catastrophic forgetting could lead to advances in engineering applications. Here are two reasons that are of immediate commercial relevance:\r\n\r\n1) Underfitting on large datasets: Catastrophic forgetting is a property of stochastic gradient descent, which is currently the state of the art training method for many important problems such as object recognition, speech recognition, and drug activity prediction. It is well known that for large datasets, stochastic gradient descent can suffer from under fitting on large datasets: http://arxiv.org/abs/1301.3583 This may be because different examples have very different task profiles and the net can forget how to classify an example if it is not re-presented frequently enough. Since large datasets are the main driving force behind the recent industrial interest in deep learning, this seems especially important to understand.\r\n\r\n2) Reinforcement learning: When neural nets are used for reinforcement learning, catastrophic forgetting is an important issue because it is not computationally feasible to store all of the agent's experiences. Moreover, as the agent learns, the kinds of situations it has to deal with will change, but it is good if the agent remembers how to deal with a variety of situations. For example, a neural net that is first learning how to drive a car may frequently drive too aggressively and go into a skid. A net early in training will thus spend a lot of its capacity learning to get out of a skid. Later, a fully trained net will skid only very rarely because it has a good controller, but we would like it to remember how to get out of a skid in case one happens. Some experimental evidence suggests that catastrophic forgetting is in fact a serious issue for reinforcement learning. For example, Deep Mind recently demonstrated a neural reinforcement learning system that can play Atari games: http://arxiv.org/pdf/1312.5602v1.pdf The main algorithmic advance making their system perform acceptably was an experience replay mechanism.\r\n\r\nIn this work we don't try to completely solve either of the above issues, but rather we construct a test-bed to understand smaller scale issues that may be involved in either of the more complex scenarios.\r\n\r\nCatastrophic forgetting is also interesting not from an engineering perspective but from a neuroscience perspective. Neuroscience and machine learning frequently inform each other. An improved understanding of how catastrophic forgetting behaves in artificial neural networks can lead to advances in understanding of biological neural networks.\r\n\r\nFinally, a large part of our motivation for this paper is to correct what we see as limitations to a recent study of catastrophic forgetting that was published in NIPS this year. The inclusion of work on catastrophic forgetting in NIPS suggests that the community views it as a relevant topic.\r\n\r\n'The ability of a network to \u201cremember\u201d a previous task is purely an artifact of poor optimization. If the neural network cost function could be better optimized, neural networks, like SVMs, would completely forget any previous task. This is actually a good thing! Neural networks are function approximators and any \u201cremembering\u201d of a previous task exhibits poor performance on fitting the function of interest. This observation again points to doubts as to why catastrophic forgetting is a problem worth studying.'\r\n\r\nWe're never going to have a perfect optimization algorithm for neural networks. Optimizing them is NP-complete. Catastrophic forgetting is a property of the training algorithms we have for specific neural networks, and it is those algorithms that we study in this paper.\r\n\r\n'In terms of experiments, the authors do a relatively good job of exploring possible network architecture choices. My concern with the experimental setup is the contrived nature of the dataset pairs. The pairs of tasks used don\u2019t seem representative of realistic task pairs, but again I can\u2019t think of a task pairs which actually require sequential training.'\r\n\r\nThe task pairs are chosen to elicit characteristics of the training algorithms we use, not to be commercially interesting.\r\n\r\n'The paper seems incomplete without comparing sequential task training to jointly training on the two tasks. I suspect joint training should perform at least as well.'\r\n\r\nYes, obviously joint training will perform better, but I'm not sure why we would be interested in how it performs. Our goal isn't to obtain the best possible performance on two tasks, but rather to determine what happens to the performance on the first task when we train on a second task.\r\n\r\n'When discussion dropout, the authors should be careful to not describe it as an alternative to SGD. Dropout is a regularization technique and can be written into the cost function of network training. SGD is an optimization technique used to optimize that cost function, so for example SGD can be replaced with conjugate gradient to train a model with dropout regularization.'\r\n\r\nWe can rewrite the paper to make this clear.\r\n\r\n'In dropout training some papers have found significant performance impact from the setting of dropout probability. The authors argue that setting the dropout probability to 0.5 is usually best, but it would be nice to have an experimental validation of this claim.'\r\n\r\nThis has been experimentally validated before, see for example fig 2.6 of http://www.cs.toronto.edu/~nitish/msc_thesis.pdf . We can add this pointer to the paper.\r\nTo our knowledge, the only time that a deviation from 0.5 has been valuable is on the input to the network or on convolutional layers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392775620000, "tcdate": 1392775620000, "number": 1, "id": "COqd1FIP0RONW", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "O1zu1tHAs_OtY", "signatures": ["anonymous reviewer 6ba5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Even simply re-running the experiments for one of these datasets, with n coarsely-spaced different values of dropout, would be interesting (if not in time for the rebuttal, just in general).  I feel this would be an important addition to the results of the paper, and further strengthen your claims re: dropout being somehow beneficial."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392178140000, "tcdate": 1392178140000, "number": 3, "id": "iBz7iIpI6PMKO", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "UU9xOhFI5MUib", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'I think the experimental protocol used here is not quite the same as the one used to label catastrophic forgetting in neural networks: there, task A is trained to mastery, then B, then A again. The question is how fast someone can go from A to B and back, without regard for how good they are at A or B in absolute terms.'\r\n\r\nCould you give us a reference for one of these papers?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392177960000, "tcdate": 1392177960000, "number": 2, "id": "3LOact4IaBp6u", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "UU9xOhFI5MUib", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "' Experiments intro: a surprising omission was that prior to dropout L2 and L1 weight regularization '\r\n\r\nIt looks like your sentence got cut off, would you mind re-completing it?\r\n\r\nI'm assuming you're asking why the beginning of section 4 doesn't talk about L2 or L1 weight regularization. The reason is that the goal of section X isn't to provide a general history of the means of regularizing neural nets, but rather to describe the methods that we used in our experiments. We chose these methods based on which methods currently obtain state of the art on challenging neural network tasks. Currently, L1/L2 penalties on neural network weights are not popular. However, we do not see dropout as the replacement for these penalties. Instead, these penalties have been replaced by a constraint on the maximum norm of the weights for each hidden unit. Penalties can be seen as a means of implementing constraints using Lagrange multipliers, but penalties and constraints are not fully equivalent in the case of non-convex optimization with multiple local minima. Penalties affect the search direction regardless of the current point in parameter space, while constraints only affect the search direction at the edge of constraint space. The penalty can have some negative effects such as trapping hidden units with small weights near the origin, resulting in 'dead units.' Experimentally, we've consistently found that max-norm constraints perform significantly better than L2 penalties, in our work on both maxout (ICML 2013) and the MP-DBM (NIPS 2013). We got the idea of using the max-norm constraint instead of weight decay from Geoffrey Hinton's original arxiv paper on dropout, where it was also used to great effect. For these reasons, we felt that an empirical investigation of modern state of the art methods would do better to focus on weight norm constraints rather than weight norm penalties.\r\n\r\nLet me know if that was not what your question was going to be. Also, if you don't mind, please, let me know how much of this clarification should be added to the next version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392176940000, "tcdate": 1392176940000, "number": 1, "id": "z60J9r5MxX6tp", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "UU9xOhFI5MUib", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'The paper was a joy to read. As a reviewer, I want to thank the authors for taking the time to write so clearly.'\r\nThanks, we're glad you appreciated our efforts."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392176640000, "tcdate": 1392176640000, "number": 1, "id": "O1zu1tHAs_OtY", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "ff2Ufvs1UolkP", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'I encourage the authors to come up with a single number than can represent well the entirety of the curves shown in Figs. 1,3,5 and then plot out the influence of dropout vs. catastrophic forgetting, one curve per activation function (and per dataset, of course).'\r\n\r\nI'm not sure I understand this suggestion, can you elaborate? Are you saying to re-run the experiments done here but with n different values of the dropout rate? Please keep in mind that that each of these figures required 400 runs of deep net training, so generating them for multiple values of the dropout rate would be considerably expensive. The rate at which we could do this depends on the availability of shared computational resources, but we probably could not add even one point on this curve before the rebuttal period ends."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392175920000, "tcdate": 1392175920000, "number": 1, "id": "s2Ke2gPbSkMja", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "X85EX3gs7xrDS", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Please delete this comment"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392175920000, "tcdate": 1392175920000, "number": 1, "id": "DUdZvpNtZ_0Ox", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "7RzcD30jSEDfe", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Please delete this comment"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392175560000, "tcdate": 1392175560000, "number": 1, "id": "X85EX3gs7xrDS", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "C2_L2xJZKTCxq", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "In dropout training some papers have found significant performance impact from the setting of dropout probability"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392175560000, "tcdate": 1392175560000, "number": 2, "id": "3PpH3F-_2JyW1", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "C2_L2xJZKTCxq", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'In dropout training some papers have found significant performance impact from the setting of dropout probability.' Could you specify which papers? To my knowledge, the deviation from p=0.5 is only significantly beneficial on input units or on convolutional layers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392175500000, "tcdate": 1392175500000, "number": 2, "id": "gI_cXQ6AcliQ7", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "oXSw7laxwUpln", "replyto": "7RzcD30jSEDfe", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'In dropout training some papers have found significant performance impact from the setting of dropout probability.'\r\nCould you specify which papers? To my knowledge, the deviation from p=0.5 is only significantly beneficial on input units or on convolutional layers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392064140000, "tcdate": 1392064140000, "number": 5, "id": "UU9xOhFI5MUib", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "oXSw7laxwUpln", "replyto": "oXSw7laxwUpln", "signatures": ["anonymous reviewer 5894"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "review": "The paper 'An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks' investigates the robustness of 'modern' feed-forward networks trained on dropout and a variety of activation functions to task-switching during training. Biological neural networks have the property that after being trained for a task A, and then trained for a different task B, they can be re-trained for A more quickly than networks that have never mastered A before.  This paper investigates the capacity of various sorts of feed-forward neural networks to exhibit this property.                                                                  \r\n \r\n \r\nNovelty:                                                                                                                            \r\n \r\nThe authors state that the issue was 'well-studied in the past' but provide no references or summary of that past work. I am not familiar with that literature.\r\n\r\nQuality:                                                                                                                            \r\n\r\nThe paper was a joy to read. As a reviewer, I want to thank the authors for taking the time to write so clearly.                    \r\n\r\nThe large parenthetical remark before section 4.1 should probably be the caption on the page with Figures 1 and 3.\r\n\r\n\r\nPro:\r\n\r\nThe issue of how deep networks might transfer patterns learned from one data set to another is important and topical.\r\n\r\nThe experiments are presented clearly and reveal an interesting new property of dropout training.\r\n\r\n\r\nCon:\r\n\r\nThe suggestion that the first layer of a neural network is not rewired in response to a remapping of MNIST pixels, but rather upper layers handle the new task is counter-intuitive and intriguing. The paper would be stronger if the authors tested this hypothesis.\r\n\r\nComments:\r\n \r\nExperiments intro: a surprising omission was that prior to dropout L2 and L1 weight regularization\r\n \r\nFigures 1 and 3: A better caption would help these intriguing figures to stand on their own. Is a lower-envelope on a scatter plot of randomly drawn models being drawn here? Is there a modelling reason that motivates connecting the dots, or is it simply a visual aide? Did you consider leaving all of the scatter-plots on the axes (though it might be too busy)?                                \r\n \r\nI think the experimental protocol used here is not quite the same as the one used to label catastrophic forgetting in neural networks: there, task A is trained to mastery, then B, then A again. The question is how fast someone can go from A to B and back, without regard for how good they are at A or B in absolute terms. Here, an early stopping criterion was used during B-training so that the question of how fast A degrades may be confounded with the networks' raw ability to do tasks A and B."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391938440000, "tcdate": 1391938440000, "number": 4, "id": "ff2Ufvs1UolkP", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "oXSw7laxwUpln", "replyto": "oXSw7laxwUpln", "signatures": ["anonymous reviewer 6ba5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "review": "The paper discusses the problem of catastrophic forgetting, in the context of deep neural networks. While this is the purported goal of it, an equally interesting outcome of this analysis is the in-depth comparison between a variety of common activation functions used with or without dropout. This analysis reveals that training with dropout is always beneficial in terms of adapting to the new task, but that the various activation functions studied can perform quite differently depending on the task in question. The authors\u2019 analysis does suggests the maxout activation function is consistently a good choice.\r\n\r\nI am mystified about the comment on dropout having a \u201cknown\u201d range of values that works well (0.5 or 0.2, depending on whether it\u2019s a hidden or visible unit). This may very well be true in the context of obtaining a good performance on a given set of tasks that the authors consider. But since dropout can be seen as having a regularization effect: this effect can manifest itself by less (or more) catastrophic forgetting. I encourage the authors to come up with a single number than can represent well the entirety of the curves shown in Figs. 1,3,5 and then plot out the influence of dropout vs. catastrophic forgetting, one curve per activation function (and per dataset, of course). One good single number may be the area under the curve, perhaps discounting for the left-most and right-most parts? Such an analysis might elucidate the aspects related to the increased number of parameters that dropout seems to favor in the \u201csimilar\u201d tasks scenario.\r\n\r\nOther than that, it would be nice if the authors proposed hypotheses for why maxout seems to perform better -- it is not obvious from their analysis that this should be the case.\r\n\r\nA general comment: it seems rather odd that catastrophic forgetting seems to be somehow tied to the choice of an activation function in this paper -- this (plus dropout) cannot possibly be the only aspect that influences the magnitude of the catastrophic forgetting problem. Arguably, things like the optimization and, generally speaking, the regularization strategies employed, are equally as important. This paper touches very little upon those issues, glossing over anything other than SGD + dropout.\r\n\r\nSmall comment: there\u2019s a dangling \u201cMcClelland, James L\u201d reference."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391845920000, "tcdate": 1391845920000, "number": 3, "id": "C2_L2xJZKTCxq", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "oXSw7laxwUpln", "replyto": "oXSw7laxwUpln", "signatures": ["anonymous reviewer d827"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "review": "Summary\r\nThis paper explores the behavior of a neural network when the cost function changes from an initial task to a second subsequent task. The authors construct pairs of tasks from standard benchmarks in textual sentiment classification and MNIST digit classification. For constructed pairs of tasks the authors analyze performance of networks trained with four different standard hidden unit activation functions, and dropout regularization. The neural networks used are standard architectures, and there is not anything new from a modeling perspective. The authors conclude that dropout regularization is beneficial on the datasets they evaluate, but there is not a consistent best choice of activation function.\r\n\r\nDetailed Review\r\nThe problem of catastrophic forgetting seems irrelevant to most of the current research in deep learning, and seems to be a somewhat constructed thought experiment. The paper does not mention a practical task that requires training the same neural network on two problems in sequence. Indeed, I can\u2019t think of such a task, and the authors should significantly rewrite the introduction to make clear why this problem is important. \r\n\r\nThe ability of a network to \u201cremember\u201d a previous task is purely an artifact of poor optimization. If the neural network cost function could be better optimized, neural networks, like SVMs, would completely forget any previous task. This is actually a good thing! Neural networks are function approximators and any \u201cremembering\u201d of a previous task exhibits poor performance on fitting the function of interest. This observation again points to doubts as to why catastrophic forgetting is a problem worth studying. \r\n\r\nIn terms of experiments, the authors do a relatively good job of exploring possible network architecture choices. My concern with the experimental setup is the contrived nature of the dataset pairs. The pairs of tasks used don\u2019t seem representative of realistic task pairs, but again I can\u2019t think of a task pairs which actually require sequential training. The paper seems incomplete without comparing sequential task training to jointly training on the two tasks. I suspect joint training should perform at least as well.\r\n\r\nWhen discussion dropout, the authors should be careful to not describe it as an alternative to SGD. Dropout is a regularization technique and can be written into the cost function of network training. SGD is an optimization technique used to optimize that cost function, so for example SGD can be replaced with conjugate gradient to train a model with dropout regularization. \r\n\r\nIn dropout training some papers have found significant performance impact from the setting of dropout probability. The authors argue that setting the dropout probability to 0.5 is usually best, but it would be nice to have an experimental validation of this claim.\r\n\r\nReview Summary\r\n- Forgetting seems like a contrived problem not relevant to current deep learning research\r\n- Evaluation is on toy datasets which do not require sequential task training and could easily be trained jointly\r\n- No comparison is given to jointly training \r\n+ Reasonable evaluation of various hidden unit activation function choices"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389060840000, "tcdate": 1389060840000, "number": 1, "id": "7RzcD30jSEDfe", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "oXSw7laxwUpln", "replyto": "oXSw7laxwUpln", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In response to David's comments:\r\n\r\n-The version of the paper you saw was just an incomplete placeholder we uploaded to be able to enter the openreview submission system. We just barely uploaded a new version to ArXiv that is complete and ready to be reviewed. The new version addresses your comments, and should be visible as soon as ArXiv approves it.\r\n-In particular, we added some more explanation of how the possibilities frontiers curves are generated. To be clear, they don't show performance on the new task worsening over time, as you thought. What they show is performance on the old task task worsening as performance on the new task improves. We do in fact use early stopping, so any overfitting is intrinsic to methods themselves. The points that are most overfit are probably bad at both tasks so they lie above and to the right of the frontier and have no effect on the plot.\r\n-It sounds to me like you assumed one of the axes of the plots was a time axis, but I'm not sure if that was your mistake. If you read the axis labels you can see there is no time axis, both are error. You can't tell for sure which points came from what point in time, but presumably the ones on the upper left are from later in time, and the ones on the lower right are from earlier in time.\r\n-We don't have any control over the author order display bug but we've asked the ICLR organizers to fix it."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389001920000, "tcdate": 1389001920000, "number": 2, "id": "CCdGC5h4O_Z5L", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "oXSw7laxwUpln", "replyto": "oXSw7laxwUpln", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Main comment: I would add 'dropout' to the abstract and even the title.  It seems like the main result is that dropout helps ameliorate catastrophic forgetting.\r\n\r\nI also don't understand why the production possibility frontier curves don't decrease monotonically.  \r\n\r\nMy impression is that you create these curves by training the network (initialized via training for the old task) on the new task and then at various points during training, examining test performance on both tasks.  So then why does test error go up on the new task when you are training the network to perform this task? Is this just overfitting? If so, why not do early stopping?  Or am I incorrect about how these plots are generated?\r\n\r\n\r\nLittle things:\r\nThe authors are listed in a different order on this site vs. \r\n\r\non the paper.\r\nHyperparameters is misspelled a few times.  \r\n3. I would add citation for hard LWTA (at least), I think \r\n\r\nthis stands for local winner takes all, but it took me a bit \r\n\r\nto figure that out.  \r\n3. 'and being training on the 'new task'' -> 'and BEGIN \r\n\r\ntraining on the new task'\r\n\r\n3.3 '...two cases that are NOT semantically similar' \r\n\r\n(missing not)\r\n\r\nFor the graphs, I would color match the dropout vs. SGD \r\n\r\nmodels with the same conditions.\r\n\r\nAlso, did you train the dropout nets using something other \r\n\r\nthan SGD?  If not, I would clarify this and rename the 'SGD' \r\n\r\nmodels as 'no dropout' or something like that."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387867080000, "tcdate": 1387867080000, "number": 42, "id": "oXSw7laxwUpln", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "oXSw7laxwUpln", "signatures": ["yoshua.bengio@gmail.com"], "readers": ["everyone"], "content": {"title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks", "decision": "submitted, no decision", "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models 'forget'' how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting.", "pdf": "https://arxiv.org/abs/1312.6211", "paperhash": "bengio|an_empirical_investigation_of_catastrophic_forgeting_in_gradientbased_neural_networks", "keywords": [], "conflicts": [], "authors": ["Yoshua Bengio", "Mehdi Mirza", "Ian Goodfellow", "Aaron Courville", "Xia Da"], "authorids": ["yoshua.bengio@gmail.com", "mirzamom@iro.umontreal.ca", "goodfellow.ian@gmail.com", "aaron.courville@gmail.com", "xiada99@gmail.com"]}, "writers": [], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 20}