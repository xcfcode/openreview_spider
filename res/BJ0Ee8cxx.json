{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396479504, "tcdate": 1486396479504, "number": 1, "id": "HJPShzU_g", "invitation": "ICLR.cc/2017/conference/-/paper283/acceptance", "forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396480071, "id": "ICLR.cc/2017/conference/-/paper283/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396480071}}}, {"tddate": null, "tmdate": 1484848662866, "tcdate": 1484848662866, "number": 6, "id": "SJyQA_RIx", "invitation": "ICLR.cc/2017/conference/-/paper283/public/comment", "forum": "BJ0Ee8cxx", "replyto": "B1tyd0EVg", "signatures": ["~Sarath_Chandar1"], "readers": ["everyone"], "writers": ["~Sarath_Chandar1"], "content": {"title": "Response for AnonReviewer3", "comment": "Thanks for your valuable feedback.\n\n>>> The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. \n\nIt is true that the k-mips algorithm forces the memories to be fixed. However, it is not the issue with k-mips algorithm only. We have this issue with nearest neighbor search algorithms and maximum cosine similarity search algorithms as well. When we need to update the memory, then one can do periodic memory reorganization after every few epochs as done by Vijayanarasimhan et al., 2014 or Rae et al., 2016.\n\nHowever, this is not a limiting constraint on problems which will require multiple hops. One can design a separate inference module (like the episodic memory in end-to-end memory nets) that will take care of multiple hops and the main memory can still be fixed. So we believe that this technique is still useful in the setting mentioned by the reviewer.\n\n>>> Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. \n\nYes true. Exact k-mips has the sample complexity of full attention. Our k-mips experiments serve two purposes: 1. It is a proof of concept that approximate k-mips if done correctly would help us achieve scalability as well as performance. 2. It is suggestive to use k-mips instead of full attention even in situations where full attention is cheap. We think that both are important messages. Our approximate k-mips experiments benchmark the existing state-of-the-art methods for approximate k-mips and show that they are not good enough for this task. This demands more research on better approximate k-mips methods.\n\n>>> The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned.\n\nWe agree with the reviewer. As the reviewer rightly pointed out, we replaced the dataset-specific heuristics with dataset-independent heuristics which is a research direction worth exploring since it will be applicable to any kind of datasets.\n\n >>> The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN.\n\nFLANN is a standard library that people use for nearest neighbor search. However, please note that PCA-tree is a better method than FLANN and is a state-of-the-art in tree based methods. We benchmark three search algorithms each state-of-the-art in clustering-based, tree-based, and hashing-based approaches respectively.\nSpeed comparison of different approximate k-mips algorithms used in this paper has already been done in an extensive setup in Auvolat et al., 2015."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641538, "id": "ICLR.cc/2017/conference/-/paper283/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ0Ee8cxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper283/reviewers", "ICLR.cc/2017/conference/paper283/areachairs"], "cdate": 1485287641538}}}, {"tddate": null, "tmdate": 1484624679054, "tcdate": 1484624679054, "number": 5, "id": "Sky4mzoLe", "invitation": "ICLR.cc/2017/conference/-/paper283/public/comment", "forum": "BJ0Ee8cxx", "replyto": "SyMkFcfVl", "signatures": ["~Sarath_Chandar1"], "readers": ["everyone"], "writers": ["~Sarath_Chandar1"], "content": {"title": "Response for AnonReviewer1", "comment": "Thanks for your valuable feedback.\n\n>>> The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable. However, this was precisely the point of Rae et al.\n\nYou are correct that the basic idea is to use fast nearest neighbor search and do k-softmax instead of softmax for memory access. This can be considered as a way of hard attention followed by soft attention and the whole process is still differentiable. While this is precisely the point of Rae et al., 2016, we would like to highlight the fact that our work and Rae et al.\u2019s work happened around same time independently. In fact, our arxiv version predates that of Rae et al. So we feel that it is not fair to discard the main contributions of this paper since it has been proposed in parallel in another independent paper.\n\n>>> There are a number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).\n\nWhile there are several commonly used nearest neighbor searchers like FLANN, we would like to highlight that we have used recent state-of-the-art methods for maximum inner product search. We chose three algorithms k-means clustering, PCA-Tree, and WTA-Hash each being the current state-of-the-art clustering-based, tree-based, and hashing-based approaches for MIPS respectively. These have already been benchmarked against standard libraries and hence state-of-the-art.\n\n>>> Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.\n\nWe agree that simple-questions is not a suitable problem for the proposed model. We in fact state this in the paper and mention that we use this task mainly for demonstration purpose and acknowledge that keyword hashing based approach would perform much better in this task. Our goal is to design a general memory access mechanism which does not take into account any dataset specific priors (like keyword hashing).\n\n>>> I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified to switch between the problems; indeed the authors do this when they convert to the \"mcss\" problem.\n\nThanks for this feedback. We will fix this issue.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641538, "id": "ICLR.cc/2017/conference/-/paper283/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ0Ee8cxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper283/reviewers", "ICLR.cc/2017/conference/paper283/areachairs"], "cdate": 1485287641538}}}, {"tddate": null, "tmdate": 1484621641521, "tcdate": 1484621641521, "number": 4, "id": "HkbIvZiUx", "invitation": "ICLR.cc/2017/conference/-/paper283/public/comment", "forum": "BJ0Ee8cxx", "replyto": "HJhbVGU4e", "signatures": ["~Sarath_Chandar1"], "readers": ["everyone"], "writers": ["~Sarath_Chandar1"], "content": {"title": "Response for AnonReviewer2", "comment": "Thanks for your valuable feedback.\n\n>>> The hierarchical memory is fixed, not learned.\n\nIt is true that the hierarchical memory is fixed and not learned. Quoting our reply for the same question in pre-review: \u201cSince the memory is read only, we obtain the hierarchical memory in an unsupervised way. The main idea is to have a fixed memory access scheme and train the reader to learn this fixed memory access scheme. In other words, the reader is learning to do search. While updating the memory hierarchy during training is an interesting problem by itself, it might not be necessary in applications where the external memory is explicit and fact-based rather than implicit memory like in NTMs.\u201d\n \n>>> There is no hierarchical in the experimental section, only one layer for softmax layer.\n\nThis is not true. Except for the full-softmax experiments, all other experiments use two levels of hierarchy.\n\n>>> It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?\n\nNo. We do not claim that 1-mips is the best one we should adopt. To clarify the definition, \u201c1-mips means a softmax over the correct fact and one incorrect fact who has maximum inner product with the query\u201d. 1-mips can be considered as a probabilistic version of max-margin learning. While it is tempting to set k=1 seeing that smaller k results in better performance, k=1 will affect the training time since new maximum inner product candidate will pop up every time the old one is pushed down the rank. However this problem is not significant when k=10 since we already push all top-10 candidates down and hence this will converge faster than k=1 setting. Thus there is a trade-off in choosing the value of k.  \n\t\n>>> Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.\n\nIt is true that approximate k-mips is worse than full softmax. However we would like to highlight that the main message of the paper is to use k-softmax instead of softmax. This can be considered as a way of hard attention followed by soft attention and the whole process is still differentiable. Very similar idea is also proposed in Rae et al., 2016 independently and our arxiv version predates that of Rae et al., 2016. We do not propose a novel approximate k-mips algorithm. Instead we benchmark all the state-of-the-art approximate k-mips algorithms in this newly proposed setting.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641538, "id": "ICLR.cc/2017/conference/-/paper283/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ0Ee8cxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper283/reviewers", "ICLR.cc/2017/conference/paper283/areachairs"], "cdate": 1485287641538}}}, {"tddate": null, "tmdate": 1482200067569, "tcdate": 1482200067569, "number": 3, "id": "HJhbVGU4e", "invitation": "ICLR.cc/2017/conference/-/paper283/official/review", "forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "signatures": ["ICLR.cc/2017/conference/paper283/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper283/AnonReviewer2"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer.\n2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt?\n3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636935, "id": "ICLR.cc/2017/conference/-/paper283/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper283/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper283/AnonReviewer1", "ICLR.cc/2017/conference/paper283/AnonReviewer3", "ICLR.cc/2017/conference/paper283/AnonReviewer2"], "reply": {"forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636935}}}, {"tddate": null, "tmdate": 1482119137382, "tcdate": 1482119137382, "number": 2, "id": "B1tyd0EVg", "invitation": "ICLR.cc/2017/conference/-/paper283/official/review", "forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "signatures": ["ICLR.cc/2017/conference/paper283/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper283/AnonReviewer3"], "content": {"title": "Not a very convincing proposal for dealing with very large memories in memory networks", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients. The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied. Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance. An artifact of using k-mips is that one cannot learn the memory slots. Hence they are pre-trained and kept fixed during entire training. The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset. The exact k-mips results in the same performance as the full attention. The approximate k-mips results in deterioration in performance. The paper is quite clearly written and easy to understand. \n\nI think the ideas proposed in the paper are not super convincing. I have a number of issues with this paper. \n\n1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. \n2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. \n3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned. \n4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636935, "id": "ICLR.cc/2017/conference/-/paper283/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper283/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper283/AnonReviewer1", "ICLR.cc/2017/conference/paper283/AnonReviewer3", "ICLR.cc/2017/conference/paper283/AnonReviewer2"], "reply": {"forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636935}}}, {"tddate": null, "tmdate": 1481971929580, "tcdate": 1481971929580, "number": 1, "id": "SyMkFcfVl", "invitation": "ICLR.cc/2017/conference/-/paper283/official/review", "forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "signatures": ["ICLR.cc/2017/conference/paper283/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper283/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636935, "id": "ICLR.cc/2017/conference/-/paper283/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper283/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper283/AnonReviewer1", "ICLR.cc/2017/conference/paper283/AnonReviewer3", "ICLR.cc/2017/conference/paper283/AnonReviewer2"], "reply": {"forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636935}}}, {"tddate": null, "tmdate": 1481683570076, "tcdate": 1481683569948, "number": 3, "id": "HkcOz4Cmx", "invitation": "ICLR.cc/2017/conference/-/paper283/public/comment", "forum": "BJ0Ee8cxx", "replyto": "r1h39PkXg", "signatures": ["~Sarath_Chandar1"], "readers": ["everyone"], "writers": ["~Sarath_Chandar1"], "content": {"title": "Response", "comment": ">> Any reason to not learn the memories, as in the case of standard memory networks? Is the model hard to learn, given that it also has to learn MIPS?\n\nYes it is hard to learn both the memory and k-MIPS. Because, all approximate k-MIPS algorithms assume that the memory is fixed. If we start updating the memory, all these algorithms will break. However, this can be addressed by designing efficient dynamic approximate k-MIPS ('dynamic' since memory is dynamic), which itself is a fundamental and challenging problem by itself.\n\nWe think that this might not be necessary for situations where memory is an explicit set of facts for which we can already learn good representations in an unsupervised way (as is the case in the application considered in this paper).\n \n>> Why not report the wall clock time in the experiments to get a sense of speedup? Comparing the number of epochs across models is not fair because each epoch could take different amount of time for different models. \n\nWe agree with you that wall clock is a better measure of speedup. We will add wall clock time comparisons in the next update to the paper.\n\nPS: I am sorry for the delay in reply. It was a NIPS week :)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641538, "id": "ICLR.cc/2017/conference/-/paper283/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ0Ee8cxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper283/reviewers", "ICLR.cc/2017/conference/paper283/areachairs"], "cdate": 1485287641538}}}, {"tddate": null, "tmdate": 1481678763014, "tcdate": 1481678763008, "number": 2, "id": "rJXn1XR7e", "invitation": "ICLR.cc/2017/conference/-/paper283/public/comment", "forum": "BJ0Ee8cxx", "replyto": "rkBbFOJXg", "signatures": ["~Sarath_Chandar1"], "readers": ["everyone"], "writers": ["~Sarath_Chandar1"], "content": {"title": "Response", "comment": ">> It seems the hierarchical memory is not learned, it is obtained by unsupervised method such as clustering.\n\nYes you are right. Since the memory is read only, we obtain the hierarchical memory in an unsupervised way. The main idea is to have a fixed memory access scheme and train the reader to learn this fixed memory access scheme. In other words, the reader is learning to do search. While updating the memory hierarchy during training is an interesting problem by itself, it might not be necessary in applications where the external memory is explicit and fact-based rather than implicit memory like in NTMs. \n\n>> The key in the model is to find the Exact K-Nearest neighbor, because in table 1, the approximate k-nearest neighbor methods are much worse than full-softmax method.\n\nThe key in the model is to use k-softmax instead of softmax over the entire memory. This can be done using either exact or approximate k-NN.\n\n>> In training time, when you run the K-MIPS method and backpropagate the loss based on these K-outputs, but if the groundtruth is not in these set of K-outputs, what is the loss? \n\nQuoting from section 3: \"To avoid this issue, we always include the correct candidate to the top-K candidates retrieved by the K-MIPS algorithm, effectively performing a fully supervised form of learning.\"\n\nPS: I am sorry for the delay in reply. It was a NIPS week :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641538, "id": "ICLR.cc/2017/conference/-/paper283/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ0Ee8cxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper283/reviewers", "ICLR.cc/2017/conference/paper283/areachairs"], "cdate": 1485287641538}}}, {"tddate": null, "tmdate": 1481676509369, "tcdate": 1481676509362, "number": 1, "id": "H1ByPf0Qg", "invitation": "ICLR.cc/2017/conference/-/paper283/public/comment", "forum": "BJ0Ee8cxx", "replyto": "BkFaKBpQl", "signatures": ["~Sarath_Chandar1"], "readers": ["everyone"], "writers": ["~Sarath_Chandar1"], "content": {"title": "Response", "comment": "Thanks for the suggestion. If we use FLANN to compute exact K-MIPS solution (by setting target precision to 1), then the performance should be similar to the exact K-MIPS performance reported in the paper. However, running time might be slightly better than the linear search based exact K-MIPS used in our experiments. We will add this result in the next update to the paper.\n\nThe main message of the paper is to use top-K memory cells based on K-MIPS instead of full softmax and FLANN or any other approximate search methods can be used to do that."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641538, "id": "ICLR.cc/2017/conference/-/paper283/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ0Ee8cxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper283/reviewers", "ICLR.cc/2017/conference/paper283/areachairs"], "cdate": 1485287641538}}}, {"tddate": null, "tmdate": 1481624001250, "tcdate": 1481624001243, "number": 3, "id": "BkFaKBpQl", "invitation": "ICLR.cc/2017/conference/-/paper283/pre-review/question", "forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "signatures": ["ICLR.cc/2017/conference/paper283/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper283/AnonReviewer1"], "content": {"title": "FLANN?  other standard ANN libraries?", "question": "It seems natural to compare against standard fast-nearest neighbor approaches.  How does using exact 10-MIPS with FLANN, say, compare to your approximate K-MIPS approaches?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481624001821, "id": "ICLR.cc/2017/conference/-/paper283/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper283/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper283/AnonReviewer3", "ICLR.cc/2017/conference/paper283/AnonReviewer2", "ICLR.cc/2017/conference/paper283/AnonReviewer1"], "reply": {"forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481624001821}}}, {"tddate": null, "tmdate": 1480718588872, "tcdate": 1480718588869, "number": 2, "id": "rkBbFOJXg", "invitation": "ICLR.cc/2017/conference/-/paper283/pre-review/question", "forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "signatures": ["ICLR.cc/2017/conference/paper283/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper283/AnonReviewer2"], "content": {"title": "Few questions", "question": "1. It seems the hierarchical memory is not learned, it is obtained by unsupervised method such as clustering.\n2. The key in the model is to find the Exact K-Nearest neighbor, because in table 1, the approximate k-nearest neighbor methods are much worse than full-softmax method.\n3. In training time, when you run the K-MIPS method and backpropagate the loss based on these K-outputs, but if the groundtruth is not in these set of K-outputs, what is the loss? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481624001821, "id": "ICLR.cc/2017/conference/-/paper283/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper283/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper283/AnonReviewer3", "ICLR.cc/2017/conference/paper283/AnonReviewer2", "ICLR.cc/2017/conference/paper283/AnonReviewer1"], "reply": {"forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481624001821}}}, {"tddate": null, "tmdate": 1480714932086, "tcdate": 1480714932081, "number": 1, "id": "r1h39PkXg", "invitation": "ICLR.cc/2017/conference/-/paper283/pre-review/question", "forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "signatures": ["ICLR.cc/2017/conference/paper283/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper283/AnonReviewer3"], "content": {"title": "Model and timing issues", "question": "1. Any reason to not learn the memories, as in the case of standard memory networks? Is the model hard to learn, given that it also has to learn MIPS? \n2. Why not report the wall clock time in the experiments to get a sense of speedup? Comparing the number of epochs across models is not fair because each epoch could take different amount of time for different models. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481624001821, "id": "ICLR.cc/2017/conference/-/paper283/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper283/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper283/AnonReviewer3", "ICLR.cc/2017/conference/paper283/AnonReviewer2", "ICLR.cc/2017/conference/paper283/AnonReviewer1"], "reply": {"forum": "BJ0Ee8cxx", "replyto": "BJ0Ee8cxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper283/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481624001821}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478454864726, "tcdate": 1478283318552, "number": 283, "id": "BJ0Ee8cxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJ0Ee8cxx", "signatures": ["~Sarath_Chandar1"], "readers": ["everyone"], "content": {"title": "Hierarchical Memory Networks", "abstract": "Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.\n", "pdf": "/pdf/cc931474b5c9d578c9e5e4543ba03ead784ffd50.pdf", "TL;DR": "We propose a hierarchical memory organization strategy for efficient memory access in memory networks with large memory.", "paperhash": "chandar|hierarchical_memory_networks", "keywords": ["Deep learning", "Natural language processing"], "conflicts": ["umontreal.ca", "twitter.com", "iitm.ac.in", "usherbrooke.ca", "google.com"], "authors": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio"], "authorids": ["apsarathchandar@gmail.com", "sjn.ahn@gmail.com", "hugo@twitter.com", "vincentp@iro.umontreal.ca", "gtesauro@us.ibm.com", "yoshua.bengio@umontreal.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 14}