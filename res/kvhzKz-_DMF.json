{"notes": [{"id": "kvhzKz-_DMF", "original": "OXF_9qPgTUb", "number": 3366, "cdate": 1601308373352, "ddate": null, "tcdate": 1601308373352, "tmdate": 1615997518882, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QDKN4JLrMu", "original": null, "number": 1, "cdate": 1610040514409, "ddate": null, "tcdate": 1610040514409, "tmdate": 1610474122536, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper shows the success of a relatively simple idea -- fine tune a pretrained BERT Model using Variational Information Bottleneck method of Alemi to improve transfer learning in low resource scenarios.\n\nI agree with the reviewers that novelty is low -- one would like to use any applicable method for controlling overfitting when doing transfer learning, and of the suite of good candidates, VIB is an obvious one -- but at the same time, I'm moved by the results because of: the improvements and the success on a wide range of tasks and the surprising success of VIB over other alternatives like dropout etc, and hence I'm breaking the tie in the reviews by supporting acceptance.  Its a nice trick that the community could use, if the results of the paper are an indication of its potential."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040514395, "tmdate": 1610474122519, "id": "ICLR.cc/2021/Conference/Paper3366/-/Decision"}}}, {"id": "sSbPLswLVMs", "original": null, "number": 8, "cdate": 1606164888189, "ddate": null, "tcdate": 1606164888189, "tmdate": 1606165583777, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "dyV_chrp-P", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment", "content": {"title": "RE: Clarifications", "comment": "Thank you for the response, especially regarding the random seeds.\nRegarding the IB Curve, it does make sense to just use the CE-Loss in context of targeting a more broad audience.\n\nRegarding the novelty:\n\nI acknowledge that the proposed method of using a VIB in this specific setting has not been done before, and as you noted, the paper never claimed a novel contribution to the VIB itself.\n \nHowever, I stand by my opinion that it is limited in its novelty, as I would expect either more methodical or more formal  contributions aside from empirically showcasing improvements in terms of accuracy and robustness in a specific setting.  \n\n(As a Side Note: The robustness was mentioned in the Contributions part of my review)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kvhzKz-_DMF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3366/Authors|ICLR.cc/2021/Conference/Paper3366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment"}}}, {"id": "dyV_chrp-P", "original": null, "number": 7, "cdate": 1605609638245, "ddate": null, "tcdate": 1605609638245, "tmdate": 1605609638245, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "xK-Kp1WtSsl", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment", "content": {"title": "Clarifications", "comment": "We thank the reviewer for their helpful comments.  We will try to clarify these points in any future version of our paper.\n\nReviewer comment:\n\"the methodical contribution is rather marginal, as it boils down to adding a pre-trained BERT to the encoder part of a DVIB.\"\n\nThis portrayal of our contribution is false.  We never compare models with BERT to models without BERT, and we use the scientific method to evaluate all our contributions.  Also, as discussed in the general comments above, we do not claim a novel contribution to VIB itself; our contribution is in identifying the ways in which VIB is useful for fine-tuning.  Please review the paper which we actually submitted.\n\nReviewer comment:\n\"In my opinion the random seed should not be a tunable hyperparameter.\"\n\nWe'd like to clarify a misunderstanding: We did not tune on the random seeds. In tables 1, 2, and 6, where we deal with low-resource datasets, we trained all methods for 3 random seeds, and report the averaged results and the std across all three seeds. \n\nBut tuning the random seed is a training methodology which many developers use in practice.  In Figure 2, we followed the methodology proposed in Dodge et al, 2019, whose goal is to study how much random seeds impact the results. For this, we considered the selected VIBERT model and baseline, train both models for 50 random seeds, and plot the expected test performance when selecting the best seed from a budget of X random seeds. The results on Figure 2 demonstrate that our VIBERT model consistently obtains better performance than BERT on all datasets, and makes it very clear that our higher performance is not a result of the choice of random seeds.\n\nReviewer comment:\n\"I think for the effect of the Lagrange parameter on the losses (Figure 3), an IB curve plotting the two mutual information terms against each other for different betas would be more suitable.\"\n\nThis would also make sense, but it would make no difference to the conclusion.  CE-Loss and I(Z,Y) are monotonically related, and Beta and I(Z,X) are monotonically related.  We chose the version which we thought would be more easily understood by most readers to show the impact of reducing over-fitting.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kvhzKz-_DMF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3366/Authors|ICLR.cc/2021/Conference/Paper3366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment"}}}, {"id": "DYBxebFtAxy", "original": null, "number": 6, "cdate": 1605609104013, "ddate": null, "tcdate": 1605609104013, "tmdate": 1605609104013, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "R5uNbQ3CRx", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment", "content": {"title": "Clarifications", "comment": "We thank the reviewer for their helpful comments.  We will try to clarify these points in any future version of our paper.\n\nReviewer comment:\n\"the VIB framework is general and if additional task/fine-tuning specific insights were identified and shown to necessary when applying to the low-resource fine-tuning, novelty is also justified. However, with the current set up of plainly applying VIB to fine-tune a PLM, I find novelty rather limited.\"\n\nAs discussed in the general comments, we do not claim a novel contribution to VIB itself; our contribution is in identifying the ways in which VIB is useful for fine-tuning.  Please see our comment above.\n\nReviewer comment:\n\"would one imagine the framework to work even better with large pretrained model pretrained on a much larger corpus (like BERT-large compared to BERT-base)? The main results in the paper seem to suggest otherwise,\"\n\nThis is a good question, but we leave it for future work.  Our guess is that this is due to the difficulty of optimization with BERT_large and VIBERT_large, as suggested by the higher variance of results with BERT_large and VIBERT_large compared to BERT_base and VIBERT_base.  We note that in both settings, our method obtains higher performance compared to the previous state-of-the-art regularization techniques like Mixout for finetuning large-scale language models. \n\nReviewer question:\n\"How would the VIB framework work with a different PLM, e.g., XLM-Roberta, XLNet or T5?\"\n\nThank you, this is a good suggestion for future work. However, as confirmed by reviewers, this paper already includes extensive experiments.  We do our experiments with BERT because it is the most widely used pretrained model for finetuning.  For this reason, we consider experimenting with other language models to be out of the scope of this paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kvhzKz-_DMF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3366/Authors|ICLR.cc/2021/Conference/Paper3366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment"}}}, {"id": "5iblQpT36n4", "original": null, "number": 5, "cdate": 1605608706251, "ddate": null, "tcdate": 1605608706251, "tmdate": 1605608706251, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "QFlyPRIvuiw", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment", "content": {"title": "Clarification", "comment": "We thank the reviewer for their helpful comments. \n\nReviewer comment:\n\"Basic idea is already demonstrated by Li and Eisner (2019)\"\n\nLi and Eisner(2019) use VIB to learn to compress contextualized word embeddings for parsing tasks.  They do not consider the low-resource scenario nor fine-tuning, and do not address any of the three contributions we claim in our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kvhzKz-_DMF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3366/Authors|ICLR.cc/2021/Conference/Paper3366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment"}}}, {"id": "4jBNLlDlwdQ", "original": null, "number": 4, "cdate": 1605607977767, "ddate": null, "tcdate": 1605607977767, "tmdate": 1605607977767, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "MQZkybuGqSp", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment", "content": {"title": "Answers to questions", "comment": "We thank the reviewer for their helpful comments. \n\nReviewer question:\n\"Questions: In Section 3.3, the authors methon the following:\n> Following prior work (Belinkov et al., 2019a;Mahabadi et al., 2020), we select hyper-parameters based on the development set of each target dataset and report the results on the test set.\n\nWhere is the hyperparameter selection required while evaluating the model on OOD data? I guess I am missing something here...\"\n\nSimilar to other regularization techniques, VIB also requires hyper-parameters to define the strength of regularization. As explained in Section 2, below equation (2), K and \\beta are hyper-parameters for the VIB method, specifying the strength of regularization.  The reason hyperparameter selection needs to be done on OOD data and not on in-domain data is that the strength of regularization needs to take into consideration how similar the training data is to the OOD testing data.\n\nReviewer comment:\n\"Since the paper deals with low-resource scenarios, I would have really appreciated if the experiment section also included some experiments on multilingual datasets while focusing on low-resource languages. The general usefulness of the proposed method might have been more apparent if the paper could also cover a few additional tasks beyond text classification (E.g. NER, Translation e.t.c.)\"\n\nThank you, this is a nice suggestion, and it would be good for future work. However, as confirmed by reviewers, this paper already includes extensive experiments on various tasks and is already heavy on experimental sections, so we consider including additional experiments out of scope of this submission. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kvhzKz-_DMF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3366/Authors|ICLR.cc/2021/Conference/Paper3366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment"}}}, {"id": "fjHLmKF6hK2", "original": null, "number": 3, "cdate": 1605607176984, "ddate": null, "tcdate": 1605607176984, "tmdate": 1605607176984, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment", "content": {"title": "Simplicity does not imply a lack of novelty if it is not at all obvious", "comment": "Two of four reviewers claim a lack of novelty.  It is true that we are not proposing a novel version of VIB; VIB is just a tool for us.  Our novel contributions are the proposal and evaluation of VIB as a method for avoiding both overfitting and dataset biases during fine-tuning.  VIB has never been proposed for fine-tuning, and VIB has never been proposed as a bias reduction method.  So clearly these contributions are novel, and the only remaining question is whether they are too obvious to be considered a contribution.\n\nThe use of VIB for fine-tuning is not at all obvious.  The BERT paper currently has over 12,000 citations.  Most of those are doing fine-tuning, but none of them use VIB for fine-tuning.  Some of this work is in a low-resource setting, where we have shown VIB is clearly effective.  Why have hundreds of top-level researchers never done this before?  It must not be at all obvious.  Several papers (see Baselines in Section 3) have specifically looked at overfitting during fine-tuning, but none of those have used VIB either.\n\nThe use of VIB for bias reduction is not at all obvious.  Reducing susceptibility to dataset biases is an active area of research in NLP (see references in Section 3.3), but none of this work has used VIB.  We think that the conclusion that VIB can distinguish superficial dataset correlations from deep semantic correlations is an extremely exciting development in this sub-field.  One of these reviews does not even mention the contribution of robustness to dataset biases (contribution 3) as a contribution.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kvhzKz-_DMF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3366/Authors|ICLR.cc/2021/Conference/Paper3366/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Comment"}}}, {"id": "xK-Kp1WtSsl", "original": null, "number": 1, "cdate": 1603653845585, "ddate": null, "tcdate": 1603653845585, "tmdate": 1605024014235, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review", "content": {"title": "Recommendation to reject due to lack of novelty and exclusively empirical contributions.  ", "review": "**Short summary of the paper**:\nThe authors apply the Deep Variational Information Bottleneck (DVIB) to a NLP setting, using pretrained BERT\nas a fixed part of the encoder and fine-tune subsequent MLP layers of the encoder as well as an MLP decoder.\nThe proposed architecture shows state-of-the-art results compared to other recent regularization methods, especially in low-resource and out-of-domain benchmarks.\n\n**Contributions**:\n- Proposal of the use of DVIB with large-scale pretrained models such as BERT in a NLP setting (low significance)\n- Extensive experiments showing higher generalization and robustness to bias compared to other SOTA regularization methods in NLI benchmarks and low-resource transfer learning (medium significance)\n\n**Pros**:\n- The shown results show SOTA results in terms of generalization for a wide range of benchmarks with only marginal increase of model complexity (in terms of # of parameters & training-time).\n\n\n**Cons**:\nLimited novelty & incremental contribution:\n- Although SOTA results are shown in very extensive experiments, the methodical contribution is rather marginal,\nas it boils down to adding a pre-trained BERT to the encoder part of a DVIB.\n- Aside from the pre-trained BERT part, no contributions or changes to a vanilla DVIB architecture were made.\n- The novelty mainly stems from applying the DVIB to a new specific setting (\"fine-tuning large-scale language models on low-resource scenarios\").\n\n**Style**:\nOverall, the paper is well written and structured.\n\n**Experiments**:\nIn principal, the experimental setup seems well reasoned, comprehensible & extensive.\nHowever, I'm rather concerned about the general concept of \"fine-tuning across random seeds\".\nIn my opinion the random seed should not be a tunable hyperparameter.\n\n**Minor Comments**:\nI think for the effect of the Lagrange parameter on the losses (Figure 3), an IB curve plotting the two mutual information terms against each other for different betas would be more suitable. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077261, "tmdate": 1606915804158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review"}}}, {"id": "R5uNbQ3CRx", "original": null, "number": 2, "cdate": 1603861553348, "ddate": null, "tcdate": 1603861553348, "tmdate": 1605024014175, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review", "content": {"title": "Well written paper with good results, but limited novelty", "review": "This paper studies fine-tuning BERT-like pretrained language models (PLMs) on low resource target tasks. The authors hypothesize that the general-purpose knowledge obtained by the PLMs from pre-training might be irrelevant and redundant for a given target task. When fine-tuned onto a low resource target task, overfitting is likely to happen. To this end, a fine-tuning framework based on variational information bottleneck (VIB) is proposed to address these challenges. Specifically, the sentence representation will be mapped to a latent Gaussian variable  which compresses information in the sentence and also suppress irrelevant and redundant features, and a reconstructed version of the representation is used for task prediction. Empirical evaluations on sever datasets demonstrates the effectiveness of the method over previous research.\n\nThe paper is presented well, and it's a good read. However, my major concern is on the novelty of the proposed method. As cited by the paper, VIB has been proposed and explored in various different settings, including supervised learning, semi-supervised learning, etc., and in a similar sense, variational encoder decoders have also been thoroughly explored. The proposed method is a direct application of VIB and/or variational encoder decoder. Apart from the competitive experimental results shown on the GLUE benchmark and a set of other tasks over standard baselines including Dropout, mixout and weight decay, I find it hard to justify the novelty of the proposed method. In other words, the VIB framework is general and if additional task/fine-tuning specific insights were identified and shown to necessary when applying to the low-resource fine-tuning, novelty is also justified. However, with the current set up of plainly applying VIB to fine-tune a PLM, I find novelty rather limited. \n\nA minor question: as hypothesized if the pretrained LM contains many general purpose features, thus those irrelevant and redundant features needs to be suppressed, would one imagine the framework to work even better with large pretrained model pretrained on a much larger corpus (like BERT-large compared to BERT-base)? The main results in the paper seem to suggest otherwise, i.e., with a larger model, VIBERT actually has much less room to improve. How would the VIB framework work with a different PLM, e.g., XLM-Roberta, XLNet or T5?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077261, "tmdate": 1606915804158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review"}}}, {"id": "QFlyPRIvuiw", "original": null, "number": 3, "cdate": 1603954009420, "ddate": null, "tcdate": 1603954009420, "tmdate": 1605024014107, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review", "content": {"title": "Nice work on using information bottleneck for fine-tuning pre-trained models", "review": "This work applies information bottleneck as a way to compress the pre-trained representation so that only meaningful features are employed for the target task. It is applied for the number of GLUE tasks especially focusing on low resource settings and show consistent gains over previously known strong baselines, e.g., Mixout and L2-of-difference. This work also demonstrates that the learned model has generalization capacity so that the tuned model works on out-of-domain data.\n\n# Pros\n\n* An elegant solution to the fine tuning settings especially for the low-resource settings.\n\n* Experiments are performed extensively on various tasks and demonstrates its effectiveness in generalization for out-of-domain settings.\n\n* Interesting analysis of the experimental results.\n\n# Cons\n\n* Basic idea is already demonstrated by Li and Eisner (2019), and I was not very surprised by this results.\n\n# Details\n\nIt is a very sophisticated way of avoiding overfitting especially when the data size is limited, and it might have an impact of broader application when exploiting pre-trained models. Thus, I'd recommend acceptance for this submission.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077261, "tmdate": 1606915804158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review"}}}, {"id": "MQZkybuGqSp", "original": null, "number": 4, "cdate": 1604263861019, "ddate": null, "tcdate": 1604263861019, "tmdate": 1605024014049, "tddate": null, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "invitation": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review", "content": {"title": "The paper proposes a method to avoid overfitting while finetuning the large pretrained models for downstream tasks on small scale datasets. ", "review": "The paper proposes a method to avoid overfitting while finetuning the large pretrained models for downstream tasks on small scale datasets. It has been shown that many SOTA models usually overfit w.r.t. spurious correlations in the data and as a result fail miserably when tested for generalization on the out of domain datasets. The proposed method tries to maximally filter out task-irrelevant information in the feature vectors by minimizing the mutual information between the original features and the bottleneck features while simultaneously optimizing for performance. Experiments on several datasets show improved performance on both in-domain and out-of-domain datasets.\n\nStrong Points:\nSimple to implement the method and strong empirical results and analysis. In Section 3, Table 2 clearly shows that the method provides significant improvements under the low-data regimes and the model also achieves significant improvements in most of the datasets when tested for out of domain generalization. Analysis in section 4 shows that the method is indeed able to avoid overfitting to spurious correlations. \n\nWeak Points:\nSince the paper deals with low-resource scenarios, I would have really appreciated if the experiment section also included some experiments on multilingual datasets while focusing on low-resource languages.  The general usefulness of the proposed method might have been more apparent if the paper could also cover a few additional tasks beyond text classification (E.g. NER, Translation e.t.c.)\n\nQuestions:\nIn Section 3.3, the authors methon the following:\n> Following prior work (Belinkov et al., 2019a;Mahabadi et al., 2020), we select hyper-parameters based on the development set of each target dataset and report the results on the test set.\n\nWhere is the hyperparameter selection required while evaluating the model on OOD data? I guess I am missing something here...", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3366/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3366/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning", "authorids": ["~Rabeeh_Karimi_mahabadi2", "~Yonatan_Belinkov1", "~James_Henderson1"], "authors": ["Rabeeh Karimi mahabadi", "Yonatan Belinkov", "James Henderson"], "keywords": ["Transfer learning", "NLP", "large-scale pre-trained language models", "over-fitting", "robust", "biases", "variational information bottleneck"], "abstract": "While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.", "one-sentence_summary": "We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahabadi|variational_information_bottleneck_for_effective_lowresource_finetuning", "supplementary_material": "/attachment/6f630aa52d0d71a691a4c4b1f477fd435a52a785.zip", "pdf": "/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahabadi2021variational,\ntitle={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},\nauthor={Rabeeh Karimi mahabadi and Yonatan Belinkov and James Henderson},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kvhzKz-_DMF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kvhzKz-_DMF", "replyto": "kvhzKz-_DMF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3366/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077261, "tmdate": 1606915804158, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3366/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3366/-/Official_Review"}}}], "count": 12}