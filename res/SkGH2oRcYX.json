{"notes": [{"id": "SkGH2oRcYX", "original": "H1xPKYqqF7", "number": 710, "cdate": 1538087853458, "ddate": null, "tcdate": 1538087853458, "tmdate": 1545355386088, "tddate": null, "forum": "SkGH2oRcYX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "DEEP ADVERSARIAL FORWARD MODEL", "abstract": "Learning world dynamics has recently been investigated as a way to make reinforcement\nlearning (RL) algorithms to be more sample efficient and interpretable.\nIn this paper, we propose to capture an environment dynamics with a novel forward\nmodel that leverages recent works on adversarial learning and visual control. Such\na model estimates future observations conditioned on the current ones and other\ninput variables such as actions taken by an RL-agent. We focus on image generation\nwhich is a particularly challenging topic but our method can be adapted to\nother modalities. More precisely, our forward model is trained to produce realistic\nobservations of the future while a discriminator model is trained to distinguish\nbetween real images and the model\u2019s prediction of the future. This approach overcomes\nthe need to define an explicit loss function for the forward model which is currently\nused for solving such a class of problem. As a consequence, our learning protocol\ndoes not have to rely on an explicit distance such as Euclidean distance which\ntends to produce unsatisfactory predictions. To illustrate our method, empirical\nqualitative and quantitative results are presented on a real driving scenario, along\nwith qualitative results on Atari game Frostbite.", "keywords": ["forward model", "adversarial learning"], "authorids": ["morgan.funtowicz@naverlabs.com", "tomi.silander@naverlabs.com", "arnaud.sors@naverlabs.com", "julien.perez@naverlabs.com"], "authors": ["Morgan Funtowicz", "Tomi Silander", "Arnaud Sors", "Julien Perez"], "pdf": "/pdf/1a9692a0f310b0072e87e82f8724319ca006834e.pdf", "paperhash": "funtowicz|deep_adversarial_forward_model", "_bibtex": "@misc{\nfuntowicz2019deep,\ntitle={{DEEP} {ADVERSARIAL} {FORWARD} {MODEL}},\nauthor={Morgan Funtowicz and Tomi Silander and Arnaud Sors and Julien Perez},\nyear={2019},\nurl={https://openreview.net/forum?id=SkGH2oRcYX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkeDLTW-lV", "original": null, "number": 1, "cdate": 1544785230682, "ddate": null, "tcdate": 1544785230682, "tmdate": 1545354524187, "tddate": null, "forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper710/Meta_Review", "content": {"metareview": "The paper presents an action conditioned video prediction method that combines previous losses in the literature, such as, perceptual, adversarial and infogan type of losses. The reviewers point out the lack of novelty in the formulation, as well as the lack of experiments that would verify its usefulness in model based RL. There is no rebuttal thus no ground for discussion or acceptance.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "novelty not well justified"}, "signatures": ["ICLR.cc/2019/Conference/Paper710/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper710/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP ADVERSARIAL FORWARD MODEL", "abstract": "Learning world dynamics has recently been investigated as a way to make reinforcement\nlearning (RL) algorithms to be more sample efficient and interpretable.\nIn this paper, we propose to capture an environment dynamics with a novel forward\nmodel that leverages recent works on adversarial learning and visual control. Such\na model estimates future observations conditioned on the current ones and other\ninput variables such as actions taken by an RL-agent. We focus on image generation\nwhich is a particularly challenging topic but our method can be adapted to\nother modalities. More precisely, our forward model is trained to produce realistic\nobservations of the future while a discriminator model is trained to distinguish\nbetween real images and the model\u2019s prediction of the future. This approach overcomes\nthe need to define an explicit loss function for the forward model which is currently\nused for solving such a class of problem. As a consequence, our learning protocol\ndoes not have to rely on an explicit distance such as Euclidean distance which\ntends to produce unsatisfactory predictions. To illustrate our method, empirical\nqualitative and quantitative results are presented on a real driving scenario, along\nwith qualitative results on Atari game Frostbite.", "keywords": ["forward model", "adversarial learning"], "authorids": ["morgan.funtowicz@naverlabs.com", "tomi.silander@naverlabs.com", "arnaud.sors@naverlabs.com", "julien.perez@naverlabs.com"], "authors": ["Morgan Funtowicz", "Tomi Silander", "Arnaud Sors", "Julien Perez"], "pdf": "/pdf/1a9692a0f310b0072e87e82f8724319ca006834e.pdf", "paperhash": "funtowicz|deep_adversarial_forward_model", "_bibtex": "@misc{\nfuntowicz2019deep,\ntitle={{DEEP} {ADVERSARIAL} {FORWARD} {MODEL}},\nauthor={Morgan Funtowicz and Tomi Silander and Arnaud Sors and Julien Perez},\nyear={2019},\nurl={https://openreview.net/forum?id=SkGH2oRcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper710/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353114445, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper710/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper710/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper710/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353114445}}}, {"id": "Byl3zi5K2X", "original": null, "number": 3, "cdate": 1541151507529, "ddate": null, "tcdate": 1541151507529, "tmdate": 1541533752984, "tddate": null, "forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper710/Official_Review", "content": {"title": "Review for Deep Adversarial Forward Model", "review": "Summary: Model-based RL that work on pixel-based environments tend to use forward models trained with pixel-wise loss. Rather than using pixel-wise loss for an action-conditioned video prediction model (\"Forward Model\"), they use an adversarial loss combined with mutual-information loss (from InfoGAN) and content loss (based on difference in convnet features of VGG network, rather than pixels). They run experiments on video-action sequences collected from an Atari game (Frostbite), and on a Udacity driving dataset.\n\nPros: The introduction and related work section is very well written, and motivation of why one should try adversarial loss for forward models is clear.\n\nWhile I think this work has potential, this paper is clearly not ready for publication, and below are a few suggestions on what I think the authors need to do to improve the work:\n\n(1) The authors emphasize novelty, and being \"first\" a few times in the paper, but fail to mention the large existing work done on video prediction (i.e. [1]), many of which also used these triplet loss or adversarial losses. Sure, those works focus on video prediction, while this work focus on building a \"forward model\"and is supposed to be for model-based RL, but this work has not performed any model-based RL experiments, so from my point of view, it is a video-prediction model contingent on an action input. Regardless, I believe the approach and results should be compared to existing work on video prediction, and similarities and differences to existing approaches should be highlighted. Adding an action-conditioned element to existing video-prediction techniques is also fairly simple.\n\n(2) From reading the intro/related work section, this work is clearly motivated in the direction of model-based RL, and the authors has already used this model for Frostbite. If this method is useful for model-based RL, I would expect to see experimental results for RL, at least for Frostbite (rather than just the training loss in Table 1). Rather than focusing on saying this method is the first to use triplet loss, or the first to use adversarial loss for forward models, I am much more interested in seeing a forward model that works well for RL tasks, since, that's the point right?\n\nAlthough the work is promising, I can only give it a score of 4 at the moment. If the author fixes the writing to include detailed discussion with video prediction literature, with good quantitative and qualitative comparison to existing methods, that is worth 1 extra point. If the author has good results on using this forward model on environments that have previously used older forward models (such as Atari environments in [2] or CarRacing/VizDoom in [3]), and presents those results in a satisfactory way, that may increase my score by another 1-2 points depending on the depth of the experiments. Currently the paper is only < 7 pages, so I believe there is room for more substance.\n\nMinor points:\n- in related work section, should be f_{theta} not f_theta\n\n[1] Denton et al., \"Unsupervised Learning of Disentangled Representations from Video\", (NIPS 2017). https://arxiv.org/abs/1705.10915\n[2] https://arxiv.org/abs/1704.02254\n[3] https://arxiv.org/abs/1803.10122\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper710/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP ADVERSARIAL FORWARD MODEL", "abstract": "Learning world dynamics has recently been investigated as a way to make reinforcement\nlearning (RL) algorithms to be more sample efficient and interpretable.\nIn this paper, we propose to capture an environment dynamics with a novel forward\nmodel that leverages recent works on adversarial learning and visual control. Such\na model estimates future observations conditioned on the current ones and other\ninput variables such as actions taken by an RL-agent. We focus on image generation\nwhich is a particularly challenging topic but our method can be adapted to\nother modalities. More precisely, our forward model is trained to produce realistic\nobservations of the future while a discriminator model is trained to distinguish\nbetween real images and the model\u2019s prediction of the future. This approach overcomes\nthe need to define an explicit loss function for the forward model which is currently\nused for solving such a class of problem. As a consequence, our learning protocol\ndoes not have to rely on an explicit distance such as Euclidean distance which\ntends to produce unsatisfactory predictions. To illustrate our method, empirical\nqualitative and quantitative results are presented on a real driving scenario, along\nwith qualitative results on Atari game Frostbite.", "keywords": ["forward model", "adversarial learning"], "authorids": ["morgan.funtowicz@naverlabs.com", "tomi.silander@naverlabs.com", "arnaud.sors@naverlabs.com", "julien.perez@naverlabs.com"], "authors": ["Morgan Funtowicz", "Tomi Silander", "Arnaud Sors", "Julien Perez"], "pdf": "/pdf/1a9692a0f310b0072e87e82f8724319ca006834e.pdf", "paperhash": "funtowicz|deep_adversarial_forward_model", "_bibtex": "@misc{\nfuntowicz2019deep,\ntitle={{DEEP} {ADVERSARIAL} {FORWARD} {MODEL}},\nauthor={Morgan Funtowicz and Tomi Silander and Arnaud Sors and Julien Perez},\nyear={2019},\nurl={https://openreview.net/forum?id=SkGH2oRcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper710/Official_Review", "cdate": 1542234397376, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper710/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335785926, "tmdate": 1552335785926, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper710/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklJKsfYnQ", "original": null, "number": 2, "cdate": 1541118839469, "ddate": null, "tcdate": 1541118839469, "tmdate": 1541533752737, "tddate": null, "forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper710/Official_Review", "content": {"title": "Straightforward application of existing techniques to forward modeling; experiments & writing could be improved", "review": "This paper proposed to train a forward model used in reinforcement learning (RL) by task-independent losses. The idea is to use the adversarial loss, infoGAN, and perception loss to replace the task-specific losses in RL. \n\nHowever, the experiments did not show any benefits for the RL tasks. While it is possible that the improved prediction in terms of the Euclidean distance could lead to better results for RL, it is better to directly verify it. \n\nMany style transfer methods can be modified to solve the problem considered in the paper. Some works on conditional GAN can also be employed. However, there is no baseline compared in the experiments. \n\nThe notations in Section 3 change from one sub-section to another. It is hard to obtain a coherent understanding about the proposed approach. \n\nOverall, the paper identifies a key component, forward modeling, in RL and aims to improve the solution to that component. However, the proposed approach is a straightforward application of existing techniques to this problem. Both the writing and the experiments could be strengthened, per the suggestions above.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper710/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP ADVERSARIAL FORWARD MODEL", "abstract": "Learning world dynamics has recently been investigated as a way to make reinforcement\nlearning (RL) algorithms to be more sample efficient and interpretable.\nIn this paper, we propose to capture an environment dynamics with a novel forward\nmodel that leverages recent works on adversarial learning and visual control. Such\na model estimates future observations conditioned on the current ones and other\ninput variables such as actions taken by an RL-agent. We focus on image generation\nwhich is a particularly challenging topic but our method can be adapted to\nother modalities. More precisely, our forward model is trained to produce realistic\nobservations of the future while a discriminator model is trained to distinguish\nbetween real images and the model\u2019s prediction of the future. This approach overcomes\nthe need to define an explicit loss function for the forward model which is currently\nused for solving such a class of problem. As a consequence, our learning protocol\ndoes not have to rely on an explicit distance such as Euclidean distance which\ntends to produce unsatisfactory predictions. To illustrate our method, empirical\nqualitative and quantitative results are presented on a real driving scenario, along\nwith qualitative results on Atari game Frostbite.", "keywords": ["forward model", "adversarial learning"], "authorids": ["morgan.funtowicz@naverlabs.com", "tomi.silander@naverlabs.com", "arnaud.sors@naverlabs.com", "julien.perez@naverlabs.com"], "authors": ["Morgan Funtowicz", "Tomi Silander", "Arnaud Sors", "Julien Perez"], "pdf": "/pdf/1a9692a0f310b0072e87e82f8724319ca006834e.pdf", "paperhash": "funtowicz|deep_adversarial_forward_model", "_bibtex": "@misc{\nfuntowicz2019deep,\ntitle={{DEEP} {ADVERSARIAL} {FORWARD} {MODEL}},\nauthor={Morgan Funtowicz and Tomi Silander and Arnaud Sors and Julien Perez},\nyear={2019},\nurl={https://openreview.net/forum?id=SkGH2oRcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper710/Official_Review", "cdate": 1542234397376, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper710/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335785926, "tmdate": 1552335785926, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper710/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygIV9GUnQ", "original": null, "number": 1, "cdate": 1540921902228, "ddate": null, "tcdate": 1540921902228, "tmdate": 1541533752531, "tddate": null, "forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "invitation": "ICLR.cc/2019/Conference/-/Paper710/Official_Review", "content": {"title": "Interesting but not novel; could be better evaluated.", "review": "This paper describes an approach for training conditional future frame prediction models, where the conditioning is with respect to the current frame and additional inputs - specifically actions performed in a reinforcement learning (RL) setting. \n\nThe authors suggest that one can predict future frames from a vector comprised of an observation encoding and an action. To train the model, they suggest using a linear combination of three different losses: (1) an adversarial loss that encourages the generated sample to look similarly to training data, (2) an InfoGAN-inspired loss that is supposed to maximise mutual information between the conditioning (e.g. action) and the generated sample, and (3) a content loss, taken to be the mean-squared error of the prediction and ground-truth in the VGG feature space.\n\nThe major contribution of this work seems to be using these three losses in conjunction, while doing conditional frame prediction at the same time. While interesting, there exist very similar approaches that also use adversarial losses [1] as well as approaches using different means to reach the same goal [2, 3]. None of these are mentioned in the text, nor evaluated against. It is true that [1] is not action-conditional, but adding actions as conditioning could be a simple extension.\n\nExperimental section consists of an ablation study, which evaluates importance of different components of the loss, and a qualitative study of model predictions. With no comparison to state of the art (e.g. [1, 3]), it is hard to gauge how valuable this particular approach is. \nThe qualitative evaluation starts with \u00a74.4\u00b61 \u201cwe follow the customary GAN literature to include some qualitative results for illustration\u201d, as if there was no other reason for including samples than to follow the custom. Since the paper is about action-conditional prediction, it would be interesting to see predictions conditioned on the same initial sequence but different actions, which are not present, however. Moreover, this work is developed in the context of RL applications, and since prior art [4] has shown that better predictive models do not necessarily lead to better RL results, it would be interesting to evaluate the proposed approach against baselines in an RL setting.\n\nThe paper is clearly written, but some claims in the text are not supported by any citations (e.g. \u00a71\u00b62 \u201cMore recently, several papers have shown that forward modelling\u2026\u201d without a citation).  Some claims are misleading (e.g. \u00a71\u00b63 says that by using adversarial training we don\u2019t need to use task-specific losses and it does not put constraints on input modality. While true, using MSE loss is equally general). Some other claims are not supported at all or may not be true (e.g. \u00a73.2\u00b61 \u201cResNet \u2026 aims at compressing the information in the raw observation\u201d - to the best of my knowledge, there is no evidence for this).\n\nTo conclude, the suggested approach is not novel, the experimental evaluation is lacking, and the text contains a number of unsupported statements. I recommend to reject this paper.\n\n[1] Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., & Levine, S. (2018). Stochastic Adversarial Video Prediction. CoRR, abs/1804.01523.\n[2] Eslami, S.M., Rezende, D.J., Besse, F., Viola, F., Morcos, A.S., Garnelo, M., Ruderman, A., Rusu, A.A., Danihelka, I., Gregor, K., Reichert, D.P., Buesing, L., Weber, T., Vinyals, O., Rosenbaum, D., Rabinowitz, N.C., King, H., Hillier, C., Botvinick, M.M., Wierstra, D., Kavukcuoglu, K., & Hassabis, D. (2018). Neural scene representation and rendering. Science, 360, 1204-1210.\n[3] Denton, E.L., & Fergus, R. (2018). Stochastic Video Generation with a Learned Prior. ICML.\n[4] Buesing, L., Weber, T., Racani\u00e8re, S., Eslami, S.M., Rezende, D.J., Reichert, D.P., Viola, F., Besse, F., Gregor, K., Hassabis, D., & Wierstra, D. (2018). Learning and Querying Fast Generative Models for Reinforcement Learning. CoRR, abs/1802.03006.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper710/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DEEP ADVERSARIAL FORWARD MODEL", "abstract": "Learning world dynamics has recently been investigated as a way to make reinforcement\nlearning (RL) algorithms to be more sample efficient and interpretable.\nIn this paper, we propose to capture an environment dynamics with a novel forward\nmodel that leverages recent works on adversarial learning and visual control. Such\na model estimates future observations conditioned on the current ones and other\ninput variables such as actions taken by an RL-agent. We focus on image generation\nwhich is a particularly challenging topic but our method can be adapted to\nother modalities. More precisely, our forward model is trained to produce realistic\nobservations of the future while a discriminator model is trained to distinguish\nbetween real images and the model\u2019s prediction of the future. This approach overcomes\nthe need to define an explicit loss function for the forward model which is currently\nused for solving such a class of problem. As a consequence, our learning protocol\ndoes not have to rely on an explicit distance such as Euclidean distance which\ntends to produce unsatisfactory predictions. To illustrate our method, empirical\nqualitative and quantitative results are presented on a real driving scenario, along\nwith qualitative results on Atari game Frostbite.", "keywords": ["forward model", "adversarial learning"], "authorids": ["morgan.funtowicz@naverlabs.com", "tomi.silander@naverlabs.com", "arnaud.sors@naverlabs.com", "julien.perez@naverlabs.com"], "authors": ["Morgan Funtowicz", "Tomi Silander", "Arnaud Sors", "Julien Perez"], "pdf": "/pdf/1a9692a0f310b0072e87e82f8724319ca006834e.pdf", "paperhash": "funtowicz|deep_adversarial_forward_model", "_bibtex": "@misc{\nfuntowicz2019deep,\ntitle={{DEEP} {ADVERSARIAL} {FORWARD} {MODEL}},\nauthor={Morgan Funtowicz and Tomi Silander and Arnaud Sors and Julien Perez},\nyear={2019},\nurl={https://openreview.net/forum?id=SkGH2oRcYX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper710/Official_Review", "cdate": 1542234397376, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkGH2oRcYX", "replyto": "SkGH2oRcYX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper710/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335785926, "tmdate": 1552335785926, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper710/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}