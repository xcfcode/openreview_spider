{"notes": [{"id": "rkg3kRNKvH", "original": "HyevQaMuDB", "number": 907, "cdate": 1569439204015, "ddate": null, "tcdate": 1569439204015, "tmdate": 1577168226666, "tddate": null, "forum": "rkg3kRNKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities", "authors": ["Nancy Fulda"], "authorids": ["nfulda@byu.edu"], "keywords": ["knowledge representation", "word embeddings", "sentence embeddings", "common-sense knowledge"], "TL;DR": "This paper presents a paradigm and methodology for using learned sentence representations as emergent, flexible knowledge bases that can be queried using linear algebra.", "abstract": "Many applications of linguistic embedding models rely on their value as pre-trained inputs for end-to-end tasks such as dialog modeling, machine translation, or question answering. This position paper presents an alternate paradigm: Rather than using learned embeddings as input features, we instead treat them as a common-sense knowledge repository that can be queried via simple mathematical operations within the embedding space. We show how linear offsets can be used to (a) identify an object given its description, (b) discover relations of an object given its label, and (c) map free-form text to a set of action primitives. Our experiments provide a valuable proof of concept that language-informed common sense reasoning, or `reasoning in the linguistic domain', lies within the grasp of the research community. In order to attain this goal, however, we must reconsider the way neural embedding models are typically trained an evaluated. To that end, we also identify three empirically-motivated evaluation metrics for use in the training of future embedding models.", "pdf": "/pdf/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "paperhash": "fulda|linguistic_embeddings_as_a_commonsense_knowledge_repository_challenges_and_opportunities", "original_pdf": "/attachment/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "_bibtex": "@misc{\nfulda2020linguistic,\ntitle={Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities},\nauthor={Nancy Fulda},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg3kRNKvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "mtJtiJNB_u", "original": null, "number": 1, "cdate": 1576798709357, "ddate": null, "tcdate": 1576798709357, "tmdate": 1576800926960, "tddate": null, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "invitation": "ICLR.cc/2020/Conference/Paper907/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents an analysis of the kind of knowledge captured by pre-trained word embeddings. The authors show various kinds of properties like relation between entities and their description, mapping high-level commands to discrete commands etc. The problem with the paper is that almost all of the properties shown in this work has already been established in existing literature. In fact, the methods presented here are the baseline algorithms to the identification of different properties presented in the paper.\n\nThe term common-sense which is used often in the paper is mischaracterized. In NLP literature, common-sense is something that is implicitly understood by humans but which is not really captured by language. For example, going to a movie means you need parking is something that is well-understood by humans but is not implied by the language of going to the movie. The phenomenon described by the authors is general language processing.\n\nTowards the end the evaluation criteria for embedding proposed is also a well-established concept, its just that these metrics are not part of the training mechanism as yet. So if the contribution was on showing how those metrics can be integrated in training the embeddings, that would be a great contribution.\n\nI agree with the reviewer's critics and recommend a rejection as of now.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities", "authors": ["Nancy Fulda"], "authorids": ["nfulda@byu.edu"], "keywords": ["knowledge representation", "word embeddings", "sentence embeddings", "common-sense knowledge"], "TL;DR": "This paper presents a paradigm and methodology for using learned sentence representations as emergent, flexible knowledge bases that can be queried using linear algebra.", "abstract": "Many applications of linguistic embedding models rely on their value as pre-trained inputs for end-to-end tasks such as dialog modeling, machine translation, or question answering. This position paper presents an alternate paradigm: Rather than using learned embeddings as input features, we instead treat them as a common-sense knowledge repository that can be queried via simple mathematical operations within the embedding space. We show how linear offsets can be used to (a) identify an object given its description, (b) discover relations of an object given its label, and (c) map free-form text to a set of action primitives. Our experiments provide a valuable proof of concept that language-informed common sense reasoning, or `reasoning in the linguistic domain', lies within the grasp of the research community. In order to attain this goal, however, we must reconsider the way neural embedding models are typically trained an evaluated. To that end, we also identify three empirically-motivated evaluation metrics for use in the training of future embedding models.", "pdf": "/pdf/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "paperhash": "fulda|linguistic_embeddings_as_a_commonsense_knowledge_repository_challenges_and_opportunities", "original_pdf": "/attachment/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "_bibtex": "@misc{\nfulda2020linguistic,\ntitle={Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities},\nauthor={Nancy Fulda},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg3kRNKvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728164, "tmdate": 1576800280524, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper907/-/Decision"}}}, {"id": "rkljjJJ0tS", "original": null, "number": 1, "cdate": 1571839907171, "ddate": null, "tcdate": 1571839907171, "tmdate": 1572972537240, "tddate": null, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "invitation": "ICLR.cc/2020/Conference/Paper907/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper provides an overview of methods to use embeddings for texts as common-sense knowledge. It mentions many aspects where embeddings can be used as common-sense knowledge. However, the paper lacks both novelty and in-depth analysis.  Most methods proposed are basically computing the cosine distances between language embeddings pairs and find the closest one. It's hard to imagine how to scale up this process in real applications. And most of the analyses are based on cherry-picked examples rather than evaluation with quantitative metrics. "}, "signatures": ["ICLR.cc/2020/Conference/Paper907/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper907/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities", "authors": ["Nancy Fulda"], "authorids": ["nfulda@byu.edu"], "keywords": ["knowledge representation", "word embeddings", "sentence embeddings", "common-sense knowledge"], "TL;DR": "This paper presents a paradigm and methodology for using learned sentence representations as emergent, flexible knowledge bases that can be queried using linear algebra.", "abstract": "Many applications of linguistic embedding models rely on their value as pre-trained inputs for end-to-end tasks such as dialog modeling, machine translation, or question answering. This position paper presents an alternate paradigm: Rather than using learned embeddings as input features, we instead treat them as a common-sense knowledge repository that can be queried via simple mathematical operations within the embedding space. We show how linear offsets can be used to (a) identify an object given its description, (b) discover relations of an object given its label, and (c) map free-form text to a set of action primitives. Our experiments provide a valuable proof of concept that language-informed common sense reasoning, or `reasoning in the linguistic domain', lies within the grasp of the research community. In order to attain this goal, however, we must reconsider the way neural embedding models are typically trained an evaluated. To that end, we also identify three empirically-motivated evaluation metrics for use in the training of future embedding models.", "pdf": "/pdf/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "paperhash": "fulda|linguistic_embeddings_as_a_commonsense_knowledge_repository_challenges_and_opportunities", "original_pdf": "/attachment/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "_bibtex": "@misc{\nfulda2020linguistic,\ntitle={Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities},\nauthor={Nancy Fulda},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg3kRNKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper907/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper907/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575134902022, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper907/Reviewers"], "noninvitees": [], "tcdate": 1570237745243, "tmdate": 1575134902036, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper907/-/Official_Review"}}}, {"id": "HkeJHnHy5r", "original": null, "number": 2, "cdate": 1571933238936, "ddate": null, "tcdate": 1571933238936, "tmdate": 1572972537197, "tddate": null, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "invitation": "ICLR.cc/2020/Conference/Paper907/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "- This paper claims to provide a \u201calternate\u201d view of pretrained embedding as commonsense repository. But this is not a new view at all. It is well known embedding can encode commonsense knowledge, which is a reason it helps a wide variety of recent commonsense related tasks (and there has been considerable analysis on what kind of coomonsense is learned and helpful).\n\n- The paper also claims to propose three training criteria, which are not new as well. In fact, much work has been performed to learn representation for different semantic orientation, dimensions and relations (e.g., polarity like full/empty is one special case of contrasting meaning, which has been studied a lot in the distributed representation paradigm for years).\n\n- The paper has not been carefully written yet; e.g., even in the abstract, there is a typo like \u201ctypically trained an evaluated\u201d. \n\nI do not think this paper, claimed as a position paper, adds any new positions to the existing literature. I do not recommend it for the conference.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper907/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper907/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities", "authors": ["Nancy Fulda"], "authorids": ["nfulda@byu.edu"], "keywords": ["knowledge representation", "word embeddings", "sentence embeddings", "common-sense knowledge"], "TL;DR": "This paper presents a paradigm and methodology for using learned sentence representations as emergent, flexible knowledge bases that can be queried using linear algebra.", "abstract": "Many applications of linguistic embedding models rely on their value as pre-trained inputs for end-to-end tasks such as dialog modeling, machine translation, or question answering. This position paper presents an alternate paradigm: Rather than using learned embeddings as input features, we instead treat them as a common-sense knowledge repository that can be queried via simple mathematical operations within the embedding space. We show how linear offsets can be used to (a) identify an object given its description, (b) discover relations of an object given its label, and (c) map free-form text to a set of action primitives. Our experiments provide a valuable proof of concept that language-informed common sense reasoning, or `reasoning in the linguistic domain', lies within the grasp of the research community. In order to attain this goal, however, we must reconsider the way neural embedding models are typically trained an evaluated. To that end, we also identify three empirically-motivated evaluation metrics for use in the training of future embedding models.", "pdf": "/pdf/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "paperhash": "fulda|linguistic_embeddings_as_a_commonsense_knowledge_repository_challenges_and_opportunities", "original_pdf": "/attachment/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "_bibtex": "@misc{\nfulda2020linguistic,\ntitle={Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities},\nauthor={Nancy Fulda},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg3kRNKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper907/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper907/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575134902022, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper907/Reviewers"], "noninvitees": [], "tcdate": 1570237745243, "tmdate": 1575134902036, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper907/-/Official_Review"}}}, {"id": "BygwytnZ5H", "original": null, "number": 3, "cdate": 1572092126906, "ddate": null, "tcdate": 1572092126906, "tmdate": 1572972537151, "tddate": null, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "invitation": "ICLR.cc/2020/Conference/Paper907/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This is a position paper - it discusses overall about multi-word embedding (sentence) level and how it can be leveraged in multiple applications. Also, it discusses some open opportunities on how the embeddings can be used as a base knowledge source and some challenges in them\n\nPros:\n1. The examples and analysis of embeddings involved in the paper is detailed\n2. The authors have run lots of experiments to position their idea correctly.\n\nCons:\n1. I find many of their intuitions well established in the literature - the intuition of geometric structures in the embedding space, the intuition of incidence angle. The ideas from section 4,5,6 seem to be rather obvious extension of what we know\n\n2. Especially, in Section 4,5 the idea of finding indexes, is nothing but the projection of x on the mean of (A,B) - which is more of a common practice. I find both these ideas of finding indexes and extrapolation, pretty much recurrent and existing in literature.\n\n3. What I would be really interested is, a new training algorithm for learning embeddings, that would be able to answer all linear algebra based questions in the embedding space. Rather, this paper analyses the pros and cons of 6 existing models. In my personal opinion, this is half work done for a very good paper.\n\n4. Personally, I am not very appreciative of the use of  \"Common Sense\" in this paper. There is a set of papers in the literature (atleast in vision domain), where they actually try to learn the common sense from the underlying data. The use of common sense in this paper does not align with that usage and hence, is misleading for me.\n\n5. From reading a position paper, I would ideally want to see ideas worth of atleast 2-3 PhD thesis. I do not find such compelling ideas in this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper907/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper907/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities", "authors": ["Nancy Fulda"], "authorids": ["nfulda@byu.edu"], "keywords": ["knowledge representation", "word embeddings", "sentence embeddings", "common-sense knowledge"], "TL;DR": "This paper presents a paradigm and methodology for using learned sentence representations as emergent, flexible knowledge bases that can be queried using linear algebra.", "abstract": "Many applications of linguistic embedding models rely on their value as pre-trained inputs for end-to-end tasks such as dialog modeling, machine translation, or question answering. This position paper presents an alternate paradigm: Rather than using learned embeddings as input features, we instead treat them as a common-sense knowledge repository that can be queried via simple mathematical operations within the embedding space. We show how linear offsets can be used to (a) identify an object given its description, (b) discover relations of an object given its label, and (c) map free-form text to a set of action primitives. Our experiments provide a valuable proof of concept that language-informed common sense reasoning, or `reasoning in the linguistic domain', lies within the grasp of the research community. In order to attain this goal, however, we must reconsider the way neural embedding models are typically trained an evaluated. To that end, we also identify three empirically-motivated evaluation metrics for use in the training of future embedding models.", "pdf": "/pdf/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "paperhash": "fulda|linguistic_embeddings_as_a_commonsense_knowledge_repository_challenges_and_opportunities", "original_pdf": "/attachment/f6e80525ce7a70e3f0e9e8b2f0a82d5e8bf12936.pdf", "_bibtex": "@misc{\nfulda2020linguistic,\ntitle={Linguistic Embeddings as a Common-Sense Knowledge Repository: Challenges and Opportunities},\nauthor={Nancy Fulda},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg3kRNKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg3kRNKvH", "replyto": "rkg3kRNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper907/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper907/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575134902022, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper907/Reviewers"], "noninvitees": [], "tcdate": 1570237745243, "tmdate": 1575134902036, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper907/-/Official_Review"}}}], "count": 5}