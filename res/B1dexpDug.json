{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1490292072965, "tcdate": 1486503920232, "number": 7, "id": "B1dexpDug", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "B1dexpDug", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Early Methods for Detecting Adversarial Images", "abstract": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.", "pdf": "/pdf/8308714f50d2db04ff0e30eec2b76efd437d4fd1.pdf", "paperhash": "hendrycks|early_methods_for_detecting_adversarial_images", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["ttic.edu", "uchicago.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028550981, "tcdate": 1490028550981, "number": 1, "id": "HJkfuFajl", "invitation": "ICLR.cc/2017/workshop/-/paper7/acceptance", "forum": "B1dexpDug", "replyto": "B1dexpDug", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Early Methods for Detecting Adversarial Images", "abstract": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.", "pdf": "/pdf/8308714f50d2db04ff0e30eec2b76efd437d4fd1.pdf", "paperhash": "hendrycks|early_methods_for_detecting_adversarial_images", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["ttic.edu", "uchicago.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028551531, "id": "ICLR.cc/2017/workshop/-/paper7/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1dexpDug", "replyto": "B1dexpDug", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028551531}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489607764871, "tcdate": 1489607592948, "number": 1, "id": "r1b2oMDjl", "invitation": "ICLR.cc/2017/workshop/-/paper7/public/comment", "forum": "B1dexpDug", "replyto": "Skw4FzWix", "signatures": ["~Dan_Hendrycks1"], "readers": ["everyone"], "writers": ["~Dan_Hendrycks1"], "content": {"title": "Response", "comment": "Thank you for your analysis of our paper.\n\n> The authors address this by adding log barriers to the loss, and find that 92% of cifar images could not be perturbed to satisfy the new constraints (meaning, 8% can).\n\nWe should also note that we had 100% of images perturbed by the fast gradient sign method _fail_ to satisfy the constraints, which is one of the only few adversarial image generation techniques that works in the physical world.\n\n> particularly since it is evaluated only on relatively small images\nWe consider 64x64x3 Tiny-ImageNet images in two experiments in addition to CIFAR-10 and MNIST, and this differs from much adversarial images research which at most uses CIFAR-10.\nExamples:\nhttps://arxiv.org/pdf/1703.00410v2.pdf (recent)\nhttps://arxiv.org/pdf/1412.5068.pdf (seminal)\nPerhaps this concern is related to your question about convolution, but I do not understand that question."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Early Methods for Detecting Adversarial Images", "abstract": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.", "pdf": "/pdf/8308714f50d2db04ff0e30eec2b76efd437d4fd1.pdf", "paperhash": "hendrycks|early_methods_for_detecting_adversarial_images", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["ttic.edu", "uchicago.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486503922133, "tcdate": 1486503922133, "id": "ICLR.cc/2017/workshop/-/paper7/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper7/reviewers"], "reply": {"forum": "B1dexpDug", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486503922133}}}, {"tddate": null, "tmdate": 1489427992325, "tcdate": 1489427992325, "number": 2, "id": "HygmALVsg", "invitation": "ICLR.cc/2017/workshop/-/paper7/official/review", "forum": "B1dexpDug", "replyto": "B1dexpDug", "signatures": ["ICLR.cc/2017/workshop/paper7/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper7/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "Authors provide a method to detect if an image has been modified using one of standard algorithms for generating adversarial images.\n\nLooking at representation of an image in a whitened representation can provide information whether the image has been modified. This is expected for any sort of linear transformation of the images.\n\nAn important missing piece of information is how matrices U/V are generated. It would be a stronger result if matrices were generated using subset of the data, and then evaluated on the remaining set.\n\nIt is not clear that adversarial generation method couldn't be modified to satisfy the spectrum constraints. However, the logarithmic barrier experiment on MNIST provides some evidence that this method is hard to circumvent.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Early Methods for Detecting Adversarial Images", "abstract": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.", "pdf": "/pdf/8308714f50d2db04ff0e30eec2b76efd437d4fd1.pdf", "paperhash": "hendrycks|early_methods_for_detecting_adversarial_images", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["ttic.edu", "uchicago.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489427993072, "id": "ICLR.cc/2017/workshop/-/paper7/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper7/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper7/AnonReviewer1", "ICLR.cc/2017/workshop/paper7/AnonReviewer2"], "reply": {"forum": "B1dexpDug", "replyto": "B1dexpDug", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper7/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper7/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489427993072}}}, {"tddate": null, "tmdate": 1489213743268, "tcdate": 1489213743268, "number": 1, "id": "Skw4FzWix", "invitation": "ICLR.cc/2017/workshop/-/paper7/official/review", "forum": "B1dexpDug", "replyto": "B1dexpDug", "signatures": ["ICLR.cc/2017/workshop/paper7/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper7/AnonReviewer1"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents a method to distinguish images that are perturbed to adversarially affect the classification output of a network.  The main method relies on observation that PCA coefficients of adversarial examples are larger and higher variance for high-frequency components.  Images are classified as clean vs adversarial by fitting two gaussians to use in a likelihood comparison, one for clean examples and another for adversarial.\n\nAn immediate question is whether the approach can be defeated by constraining image perturbation method not to cause the image to fall outside the bounds of the detector.  The authors address this by adding log barriers to the loss, and find that 92% of cifar images could not be perturbed to satisfy the new constraints (meaning, 8% can).  Unfortunately, this experiment was not performed on the tiny-imagenet benchmark.\n\nAnother question I have is whether this method might be applied to larger images, perhaps in a convolutional fashion?\n\nThere are some rough bits to the paper -- for example, the introduction mentions three methods, while only one is presented in the main text (another two are described in the appendix, though they are evaluated in less detail, and do no include tiny-imagenet here).\n\nAppendix C (Saliency Map) looks irrelevant to this paper, as well:  I don't see how it relates to the goal of detecting adversarial manipulations.\n\nOverall, this paper shows a basic method that appears to work well in some limited settings.  However, the one method presented feels a bit light, even for a workshop paper, particularly since it is evaluated only on relatively small images.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Early Methods for Detecting Adversarial Images", "abstract": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.", "pdf": "/pdf/8308714f50d2db04ff0e30eec2b76efd437d4fd1.pdf", "paperhash": "hendrycks|early_methods_for_detecting_adversarial_images", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["ttic.edu", "uchicago.edu"], "authors": ["Dan Hendrycks", "Kevin Gimpel"], "authorids": ["dan@ttic.edu", "kgimpel@ttic.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489427993072, "id": "ICLR.cc/2017/workshop/-/paper7/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper7/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper7/AnonReviewer1", "ICLR.cc/2017/workshop/paper7/AnonReviewer2"], "reply": {"forum": "B1dexpDug", "replyto": "B1dexpDug", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper7/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper7/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489427993072}}}], "count": 5}