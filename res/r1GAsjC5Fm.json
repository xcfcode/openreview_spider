{"notes": [{"id": "r1GAsjC5Fm", "original": "S1xZjmxcY7", "number": 669, "cdate": 1538087846103, "ddate": null, "tcdate": 1538087846103, "tmdate": 1550696631236, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 32, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SylMd7nMlV", "original": null, "number": 1, "cdate": 1544893290462, "ddate": null, "tcdate": 1544893290462, "tmdate": 1546874946170, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Meta_Review", "content": {"title": "meta-review", "metareview": "The authors have described a navigation method that uses co-grounding between language and vision as well as an explicit self-assessment of progress. The method is used for room 2 room navigation and is tested in unseen environments. On the positive side, the approach is well-analyzed, with multiple ablations and baseline comparisons. The method is interesting and could be a good starting point for a more ambitious grounded language-vision agent. The approach seems to work well and achieves a high score using the metric of successful goal acquisition. On the negative side, the method relies on beam search, which is certainly unrealistic for real-world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community. There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers' conclusion is that the paper should be accepted. ", "recommendation": "Accept (Poster)", "confidence": "4: The area chair is confident but not absolutely certain"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper669/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353132489, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353132489}}}, {"id": "HJeZ0QS9nm", "original": null, "number": 2, "cdate": 1541194697424, "ddate": null, "tcdate": 1541194697424, "tmdate": 1543613823406, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Review", "content": {"title": "Good idea, unclear results", "review": "This submission introduces a new method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module. The method is shown to perform well on a standard benchmark. Ablation tests indicate the importance of each component of the model. Qualitative examples show that the proposed method attends to different parts of the instruction as the agent moves. \n\nHere are some comments/questions:\n- I like the underlying idea behind the method. The manuscript is written well for most parts.\n- The qualitative examples and Figure 2 are really helpful in understanding the reasons behind the improved performance.\n- There is a lot of confusion regarding the use of beam search. It's unclear from the current manuscript which results are with and without beam search. It seems like beam search was added from Ours 1 to Ours 2 in Table 2. It's not clear which rows involve beam search in Table 1. Some concerns about beam width were raised in the comments which I agree with. Please modify the submission to clearly indicate the use of beam search for each result and specify the beam width.\n- The use of beam search seems unrealistic to me as I can not think of any way a navigational model using beam search can be transferred or applied to real-world. I understand that one of the baselines uses beam search, so it's fair for performance comparison purposes, but could you provide any justification of how it might be useful in real-world? If there's no reasonable justification, could you also provide all the results (along with SPL metric) without beam search, including ablation, comparing with only methods without beam search? \n- I do not understand why the OSR in the submission is 0.64 and 0.70 for Speaker-Follower and proposed method and 0.96 and 0.97 in the comments.\n- It seems like the proposed method is tailored for the VLN task. In many real-world scenarios, an agent might be given an instruction which only describes the goal (such as in Chaplot et al. 2017 and Hermann et al. 2017) and not the path to the goal, could the authors provide their thoughts on whether the proposed would work well for such instructions? What would the progress monitor and textual attention distribution learn in such a scenario?\n\nDue to confusion about results and concerns about beam search, I give a rating of 5. I am willing to increase the rating if the authors address the above concerns.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Review", "cdate": 1542234406947, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335776560, "tmdate": 1552335776560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eCDpX114", "original": null, "number": 24, "cdate": 1543613798373, "ddate": null, "tcdate": 1543613798373, "tmdate": 1543613798373, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "Bkg3NyOmAm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Updating score", "comment": "Most of my questions were answered by the authors. I raise my rating to 6 based on the response and revised paper. However, I am not convinced by the argument for using beam search. I do not believe something unrealistic should be used to get better numbers just because the task is challenging. The authors mention that beam search can be used in robotics but do not provide any reference. I highly doubt the effectiveness of beam-search in an imperfect mapping of an environment, with realistic fine-grained motion as compared to discretized motion and perfect forward model in the simulated environment used. Nevertheless, the proposed model provides better performance even without beam search.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "HJliJgopAX", "original": null, "number": 23, "cdate": 1543512035137, "ddate": null, "tcdate": 1543512035137, "tmdate": 1543512035137, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "Bkxytl_7CQ", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "update", "comment": "Dear authors,\n\nThanks for the detailed response! The new revision addresses my presentation concerns and answers my questions, so I've increased my score to a 7. I still think it would be useful to present a slightly more targeted set of ablations in the main paper rather than the kitchen sink in table 2: e.g. just \n- \"co-grounding\" / nothing\n- progress monitor / progress inference / both / neither\n- best model + data augmentation / best model (no data aug)\n\nHaven't proofread the new draft carefully but \"state of the arts\" in Table 1 is wrong, so you should do another copyediting pass before the final version if the paper is accepted."}, "signatures": ["ICLR.cc/2019/Conference/Paper669/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "S1g2c3eVn7", "original": null, "number": 1, "cdate": 1540783252288, "ddate": null, "tcdate": 1540783252288, "tmdate": 1543511461240, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Review", "content": {"title": "Review", "review": "This paper describes a model for vision-and-language navigation. The proposed\nmodel adds two components to the baseline model proposed by Fried et al. (2018):\n\n- a panoramic visual attention (referred to in this paper as \"visual--textual\n  co-grounding\"), in which the full scene around the agent's current position is\n  attended to prior to selecting a direction to follow\n\n- an auxiliary \"progress monitoring\" loss which encourages the agent to to\n  produce textual attentions from which the distance to the goal can be directly\n  inferred\n\nThe two components combine to give state-of-the-art results on the Room2Room\ndataset: small improvements over existing approaches on the \"-seen\" evaluation\nset and larger improvements on the \"-unseen\" evaluation sets. These improvements\nalso stack with the data-augmentation approach of Fried et al.\n\nI think this is a reasonable submission and should probably be accepted. However, I\nhave some concerns about presentation and a number of specific questions about\nmodel implementation and evaluation.\n\nPRESENTATION AND NAMING\n\nFirst off: I implore the authors to find some descriptor other than \"self-aware\"\nfor the proposed model. \"Self-aware\" is an imprecise description of the agent in\nthis paper---the agent is specifically \"aware\" of its visual surroundings and\nits distance from the goal, neither of which is meaningfully an aspect of\n\"self\". Moreover, self-awareness means something quite different in adjacent\nareas of cognitive science and philosophy; overloading the term in the specific\n(and comparatively mundane) way used here creates confusion. See section 3.4 of\nhttps://arxiv.org/abs/1807.03341 for broader discussion. Perhaps something\nlike \"visual / temporal context-sensitivity\" to describe what's new here? A bit\nclunky, but I think it makes the contributions of this work much clearer.\n\nAs suggested in the summary above, I also think \"visual--textual co-attention\"\nis also an unhelpfully vague description of this aspect of the contribution. The\ntextual attention mechanism used in this paper is the same as in all previous\nwork on the task. Representations of language don't even interact with the\nvisual attention mechanism except by way of the hidden state, and the salient\nnew feature of the visual attention is the fact that it considers the full\npanoramic context before choosing a direction.\n\nMODELING QUESTIONS\n\n- p4: $y_t^{pm}$ is defined as the \"normalized distance from the current\n  viewpoint to the goal\". Is this distance in units of length (as defined by the\n  simulator) or units of time (i.e. the number of discrete \"steps\" needed to\n  reach the goal)?\n\n  The authors have already clarified on OpenReview that the progress monitor\n  objective uses an MSE loss rather than a likelihood loss. Do I understand\n  correctly that ground-truth distances are in [0, 1] but model predictions are\n  in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search\n  candidates as $p_t^{pm} \\times p_{k,t}$ work if $p_t^{pm}$ can flip the sign?\n\n- The input to the progress monitor is formed by concatenating the attention\n  vector $\\alpha_t$ to a vector of state features, and then multiplying by a\n  fixed weight matrix. How is this possible? The size of $\\alpha_t$ varies\n  depending on the length of the instruction sequence. Are attentions padded out\n  to the length of the longest instruction in the training set? If so, how can\n  the model learn when it's reached the end of a short instruction sequence?\n  What would happen if the agent encountered a sequence that was too long?\n\nEVALUATION QUESTIONS\n\n- The progress monitor is used both as an auxiliary training objective and as a\n  beam search heuristic. Is it possible to disentangle these two contributions?\n  (E.g. by ignoring the scores during beam search, or by doing augmented beam\n  search in a model that was trained without the auxiliary objective.)\n\n- Not critical, but it would be nice to know if the contributions here stack\n  with the pragmatic inference procedure in Fried et al.\n\n- While, as pointed out on OpenReview, it is not required to include SPL\n  evaluations, I think it would be informative to do so---the preliminary\n  results with no beam search look good!\n\nMISCELLANEOUS\n\np1: \"without a map\" If you can do beam search, you effectively have a map.\n\np1: \"...smoothly\" What does \"smoothly\" mean in this context?\n\np2: \"the position of grounded instruction can follow past and future\n    instructions\". Is the claim here that if instructions are of the form \"ACB\"\n    and the agent is supposed to do \"ABC\", that the proposed model will execute\n    these instructions successfully and the baseline will not? This claim does\n    not appear to be evaluated anywhere in the body of the paper.\n\np4: \"intelligently prunes\" \"Intelligently\" is unnecessary.\n\np4: \"for empirical reasons\" What does this mean?\n\np5: \"Intuitively, an instruction-following agent is required...\" The existence\n    of non-attentive models that do reasonably well at these\n    instruction-following tasks suggest that this is not actually a requirement.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Review", "cdate": 1542234406947, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335776560, "tmdate": 1552335776560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bklhwhtm07", "original": null, "number": 19, "cdate": 1542851683554, "ddate": null, "tcdate": 1542851683554, "tmdate": 1542851683554, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "BJee1Q403m", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Thanks for reproducing our results", "comment": "Hi, \n\nThanks for reproducing our results using the co-grounding module. \n\n1. \nThe 46% reported in the comment is without data augmentation. Originally, we only use data augmentation for test server submission, and all ablation study settings are without data augmentation unless specified. Please see the updated ablation study table in our revision for further details on the performance reported. \n\n2. The Eq. 5 seems to be correct. Can you please elaborate more if you have any concerns? \n\n3. The losses across time step are summed together. Please note that the loss should be zero for the samples which are ended. Also, if the distance to the goal is lower than 3, we set the target y^{pm}_t to 1 (we have clarified this in the revision). \n\n4. Yes, it is roughly 10:1 during training. "}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "BklgYjvXRm", "original": null, "number": 11, "cdate": 1542843256311, "ddate": null, "tcdate": 1542843256311, "tmdate": 1542850660101, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "HJeZ0QS9nm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "SOTA and ablation study tables updated, SPL metric included, comparison without beam search added", "comment": "Hi,\n\nWe would like to thank the reviewer for the thoughtful and constructive feedback.\n\n1. Regarding \u201cconfusion regarding the result of beam search\u201d:\n\nThe updated ablation study table is shown below, and the same table has been added to the revised paper. In this table, we show the performance improvement of each component with different inference modes (which are described in the revised paper). We have also added the recently introduced SPL metric under all settings to table. From the results, we can again see that the proposed components improved the performance across different evaluation metrics. \n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                     Inference Mode\t\t\t                                Validation-Seen           Validation-Unseen\n\t       Co-\t              Progress      Greedy      Progress     Beam\t        Data \t\n\t#    Grounding    Monitor      Decoding   Inference    Search\tAug.\tNE\u2193  SR\u2191  OSR\u2191  SPL\u2191     NE\u2193       SR\u2191  OSR\u2191  SPL\u2191\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nBaseline \t\t\t\t\t\t\t\t                                                        4.36  0.54  0.68       -       7.22        0.27  0.39       -\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t        1      \u2714 \t\t\t                  \u2714\t\t\t\t                                        3.65  0.65  0.75    0.56    6.07 \t0.42   0.57    0.28\nOurs\t2      \u2714 \t            \u2714 \t\t  \u2714\t\t\t\t                                        3.72  0.63  0.75    0.56    5.98 \t0.44   0.58    0.30\n\t        3      \u2714\t            \u2714\t\t          \u2714\t\t\t                                   \u2714 \t        3.22  0.67  0.78    0.58    5.52       0.45   0.56    0.32\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nOurs\t4      \u2714 \t            \u2714 \t\t\t              \u2714\t\t\t                        3.56  0.65  0.75    0.58    5.89 \t0.46   0.60    0.32\n\t        5      \u2714 \t            \u2714 \t\t\t              \u2714\t\t                   \u2714\t        3.18  0.68  0.77    0.58    5.41 \t0.47   0.59    0.34\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t        6      \u2714 \t             \t\t\t\t                             \u2714\t\t                3.66  0.66  0.76    0.62    5.70 \t0.49   0.68    0.42\nOurs\t7      \u2714 \t            \u2714 \t\t\t\t                     \u2714\t\t                3.23  0.70  0.78    0.66    5.04 \t0.57   0.70    0.51\n\t        8      \u2714 \t            \u2714 \t\t\t\t                     \u2714\t           \u2714\t        3.04  0.71  0.78    0.67    4.62 \t0.58   0.68    0.52\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n2. Regarding \u201cThe use of beam search and SPL metric\u201d:\n\nWe have provided all the results without beam search in the updated ablation study table (see table above or from the revised paper), and the comparison with methods without beam search is shown in the table below (for Speaker-Follower, the Pragmatic Inference which relies on beam search is removed for comparison purpose). This table is also included in the revised paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "S1gQk2P70X", "original": null, "number": 12, "cdate": 1542843355464, "ddate": null, "tcdate": 1542843355464, "tmdate": 1542847060510, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "BklgYjvXRm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "(Cont\u2019d)", "comment": "\n------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t                                            Val-seen\t   \t                    Val-unseen\t\t                  Test\n                                                    NE\u2193    SR\u2191    OSR\u2191    SPL\u2191\t    NE\u2193    SR\u2191      OSR\u2191    SPL\u2191         NE\u2193    SR\u2191    OSR\u2191    SPL\u2191\n------------------------------------------------------------------------------------------------------------------------------------------------------------\nStudent-forcing\t\t           6.01    0.39    0.53      -\t    7.81    0.22    0.28       -\t           7.85    0.20    0.27       0.18\nRPA\t\t\t                           5.56    0.43    0.53      -\t    7.65    0.25    0.32       -\t           7.53    0.25    0.33       0.23\nSpeaker-Follower\t\t           3.36    0.66    0.74      -\t    6.62    0.36    0.45       -\t           6.62    0.35    0.44       0.28\n------------------------------------------------------------------------------------------------------------------------------------------------------------\nOurs (Greedy Decoding)\t   3.22    0.67    0.78      0.58\t    5.52    0.45    0.56       0.32\t   5.99    0.43    0.55       0.32\nOurs (Progress Inference)\t   3.18    0.68    0.77      0.58\t    5.41    0.47    0.59       0.34\t   5.67    0.48    0.59       0.35\n------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nIn addition to the comparison without beam search provided above, we would like to elaborate on the reason beam search is important given the research progress in this direction. \n\nAdmittedly, beam search exhaustively searches a much larger space so that the agent can better decide which direction to go and when to stop. The VLN task along with the R2R dataset was recently introduced less than 1 year ago with a more than 60% SR gap between the best-known model and human performance. Each work including ours has step by step brought this gap down to 25%. We agree that in the long-term, ideally, the common goal of the research community is to develop an agent that achieves a high success rate while maintaining a low trajectory length. We argue that given the complexity of the VLN task which requires the agent to simultaneously achieve visual grounding, textual reasoning, temporal memorization/reasoning, and intelligent action selection to navigate, the need to relax the task along multiple directions in order to make progress is important and essential. There is no current best model for both metrics (SR and SPL), and beam search typically differentiates the two regimes. Also, whether the beam search is realistic or not depends on the application. For example, in robotics, it is not atypical to have some exploration and/or mapping of the environment as well, after which beam search can be utilized. In fairness and future comparison, we have provided SR and SPL metrics for our proposed method and all settings in the ablation study table.\n\n3. Regarding \u201cOSR differences in the submission and OpenReview comment\u201d:\n\nThe OSR were 0.96 and 0.97 respectively due to the fact that, when submitting the results to the test server, it is strictly required that all submissions need to include all locations that the agent traversed. Since beam search explores the environment, the trajectory length is usually significantly higher. The chance of passing/reaching the goal is also higher, hence the OSR is close to 1. In order to be consistent with how the existing work reports their performance (see the recently updated Speaker-Follower paper for example), we follow the same convention: when using beam search, only the performance on the test set includes all viewpoints traversed. The performance reported on the validation set use only the highest ranked trajectory after beam search.\n\n4. Regarding \u201cscenarios of instructions describe the goal, not the path\u201d:\n\nOur proposed agent learns to infer and leverage the progress monitor to constrain and regularize the textual grounding module. We believe that the high-level concept of the progress monitor will work as long as inferring progress made towards the goal can be done for the given task, i.e. there is some information in the grounding or visual information to accurately estimate it. Using textual attention distribution is just one instantiation that we explore on leveraging the progress monitor for the VLN task."}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "BkelrYdXCQ", "original": null, "number": 18, "cdate": 1542846775596, "ddate": null, "tcdate": 1542846775596, "tmdate": 1542846775596, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "rkgCnbU-6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Please see the updated ablation study table in our revision", "comment": "Hi, \n\n1. \nAlthough removing visual grounding may seem to produce a slightly higher performance on unseen SR, We chose to use both visual and textual grounding since the training is more stable and the model is less prone to overfitting. \n\n2. \nThe test server result for \"without beam search\" is using \"co-grounding + progress monitor\". Please kindly see our updated ablation study table in our revision for further details."}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "Bkxytl_7CQ", "original": null, "number": 16, "cdate": 1542844535105, "ddate": null, "tcdate": 1542844535105, "tmdate": 1542846220360, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "SJxOQedmC7", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "(Cont\u2019d)", "comment": "\n11. Regarding \u201cthe existence of non-attentive models\u201d:\n\nTo the best of our knowledge, all existing methods use attention models on the VLN task, but we have shown in Figure 3 that the baseline method (using attention on textual grounding) was not able to successfully track the instruction. Our proposed progress monitor made this possible and demonstrated superior performance across difference metrics."}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "SJxOQedmC7", "original": null, "number": 15, "cdate": 1542844448211, "ddate": null, "tcdate": 1542844448211, "tmdate": 1542845763699, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "H1x9gg_70m", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "(Cont\u2019d)", "comment": "\n5. Regarding \u201cusing beam search without auxiliary objective\u201d:\n\nYes, and we have provided the result of using beam search with the co-grounding model (without progress monitor) in the updated ablation study table as shown below. We have also included this updated ablation study table in the revision.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                     Inference Mode\t\t\t                                Validation-Seen           Validation-Unseen\n\t       Co-\t              Progress      Greedy      Progress     Beam\t        Data \t\n\t#    Grounding    Monitor      Decoding   Inference    Search\tAug.\tNE\u2193  SR\u2191  OSR\u2191  SPL\u2191     NE\u2193       SR\u2191  OSR\u2191  SPL\u2191\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nBaseline \t\t\t\t\t\t\t\t                                                        4.36  0.54  0.68       -       7.22        0.27  0.39     -\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t        1      \u2714 \t\t\t                  \u2714\t\t\t\t                                        3.65  0.65  0.75    0.56    6.07 \t0.42   0.57    0.28\nOurs\t2      \u2714 \t            \u2714 \t\t  \u2714\t\t\t\t                                        3.72  0.63  0.75    0.56    5.98 \t0.44   0.58    0.30\n\t        3      \u2714\t            \u2714\t\t          \u2714\t\t\t                                   \u2714 \t        3.22  0.67  0.78    0.58    5.52       0.45   0.56    0.32\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nOurs\t4      \u2714 \t            \u2714 \t\t\t              \u2714\t\t\t                        3.56  0.65  0.75    0.58    5.89 \t0.46   0.60    0.32\n\t        5      \u2714 \t            \u2714 \t\t\t              \u2714\t\t                   \u2714\t        3.18  0.68  0.77    0.58    5.41 \t0.47   0.59    0.34\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t        6      \u2714 \t             \t\t\t\t                             \u2714\t\t                3.66  0.66  0.76    0.62    5.70 \t0.49   0.68    0.42\nOurs\t7      \u2714 \t            \u2714 \t\t\t\t                     \u2714\t\t                3.23  0.70  0.78    0.66    5.04 \t0.57   0.70    0.51\n\t        8      \u2714 \t            \u2714 \t\t\t\t                     \u2714\t           \u2714\t        3.04  0.71  0.78    0.67    4.62 \t0.58   0.68    0.52\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n6. Regarding \u201cif contribution stacks with the pragmatic inference\u201d:\n\nWe believe that the proposed method would stack with the Pragmatic Inference in Fried et al with some performance improvement since the underlying methods for \u201cranking\u201d the candidate routes are different. Pragmatic Inference ranks the routes by learning a mapping from a complete sequence of visual inputs to textual output, whereas the Progress Monitor learns a mapping from incomplete visual-textual grounding output and actions to distance. We expect there will be performance improvement by further using pragmatic inference since these two methods are orthogonal. \n\n7. Regarding \u201cadding SPL metric\u201d:\n\nWe completely agree with the reviewers that reporting SPL will be beneficial for future research work along this direction. As suggested, the new SPL metric is added along with ALL settings in the updated ablation study table and the original table 1 for comparing with state of the arts as well.\n\n8. Regarding the comments in MISCELLANEOUS:\n\nWe thank the reviewer for paying extra attention to the details of the paper. We have revised the paper according to the suggestions in the MISCELLANEOUS section and clarified some confusion by rephrasing some sentences. We now provide answers to some of the questions asked. \n\n9. Regarding \u201cthe position of grounded instruction can follow past and future instructions\u201d:\n\nWe meant to indicate that (also as we demonstrated from qualitative results), the positions of grounded instructions at each step reflects the action that is required at this step and the action that was completed from last or previous steps (see Figure 5 and 6 for examples). \nAlso, to the best of our knowledge, the order of the instruction will be the same as the order of actions required from the agent because ground-truth actions are computed from the shortest path on the navigation graph. \n\n10. Regarding \u201cfor empirical reasons\u201d:\n\nWe empirically found that using concatenation leads to slightly higher performance and stable training over using element-wise addition. We have made the change in the revision to make this clear. "}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "SkxO6muXC7", "original": null, "number": 17, "cdate": 1542845375523, "ddate": null, "tcdate": 1542845375523, "tmdate": 1542845375523, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "SJeQkmIR3m", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Additional results without positional encoding and related work added.", "comment": "\nHi,\n\nWe would like to thank the reviewer for the thoughtful and constructive feedback.\n\nWe thank the reviewer for bringing the additional literature from related fields to our attention. We have included and discussed them in the revised paper (please see both introduction and related work sections for the changes we made).\n\n1. Regarding \u201cimportance of reasoning over completed or next instruction\u201d:\n\nWe rewrote the sentences in the second paragraph in the introduction and try to make the reason why reasoning over both past and present instructions is important and essential. Please see the revised paper for the changes we made. We emphasize that the transition between past and next part of the instructions is a soft boundary, in order to determine when to transit and to follow the instruction correctly the agent is required to keep track of both grounded instructions.  \n\n2. Regarding \u201cvisual grounding over visual features\u201d:\n\nIn order to provide a fair comparison with prior arts, we chose to use the image feature vector provided directly with this task. Our current visual grounding module performs attention over different parts of the panoramic image and grounds the located instruction to a part of the panoramic image. To further provide fine-grained visual grounding, we agree that it would be interesting to use panoramic images directly as input and perform visual grounding on feature maps or object-level bounding boxes.\n\n3. Regarding \u201ceffect of positional encoding\u201d:\n\nIn our early experiments, we found that, although removing positional encoding can achieve better results on val-seen, the agent overfits quickly on val-unseen. We believe that the agent\u2019s ability to generalize to unseen environments is more important than achieving good results on val-seen. Thus, we use positional encoding for ablation study and produce the final result.\n-----------------------------------------------------------------------------------------------------------------------------------\n\t                            Val-seen\t   \t                   Val-unseen\n                                    NE\u2193    SR\u2191    OSR\u2191    SPL\u2191    NE\u2193     SR\u2191     OSR\u2191    SPL\u2191\n-----------------------------------------------------------------------------------------------------------------------------------\nOurs (No PE)  \t    3.37    0.69    0.78     0.61\t    6.04    0.42    0.55      0.30\nOurs\t\t\t    3.72    0.63    0.75     0.56     5.98    0.44    0.58      0.30\n-----------------------------------------------------------------------------------------------------------------------------------\n\n4. Regarding \u201cwhat if the orderings of the instruction and actions are inconsistent\u201d:\n\nThe Room-to-Room dataset comes with the ground-truth starting location, goal location, and the instruction associated with it. The ground-truth trajectory is computed as the shortest path from the navigational graph from starting location to the goal, and the quality of given instructions is verified by humans which achieved 86% success rate on the test set. From our own observation, the ordering of actions is consistent with the ordering of instructions. If this was not the case, our current language grounding mechanism may still applicable since it can represent an arbitrary weighting over the sentence. Similarly, since progress monitoring is a learned function over this it could still learn to estimate progress. However, note that our assessment depends on how different the orders of instruction and actions are. If the difference is small, our agent is very likely to be able to recover from incorrect instruction. We believe this can be an interesting direction for future work. "}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "H1x9gg_70m", "original": null, "number": 14, "cdate": 1542844402366, "ddate": null, "tcdate": 1542844402366, "tmdate": 1542844402366, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "S1g2c3eVn7", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Results for sigmoid, ablation study table updated, SPL metric included. ", "comment": "Hi,\n\nWe would like to thank the reviewer for their thoughtful and constructive feedback.\n\n1. Regarding \u201cpresentation and naming\u201d:\n\nWe thank the reviewer for pointing out the potential issue regarding the naming of the paper. We agree with the reviewer and we have been trying our best to find a better name. For now, one of the options that we come up with is \u201cSelf-Monitoring Navigation Agent via Auxiliary Progress Estimation\u201d. We hope this new title is more clear and suitable for the work. If the reviewers have other suggestions, please kindly let us know. \n\n2. Regarding \u201cthe definition of distance\u201d:\n\nThe distance is defined in units of length the same as the simulator. We also agree that using the number of steps is also a reasonable approach, and it would be interesting to explore in the future. We have clarified the definition in the revision. \n\n3. Regarding \u201dusing sigmoid as opposed to tanh\u201d:\n\nWe have found that using sigmoid performs similarly to using the tanh function. The results on different metrics are shown below:\n-----------------------------------------------------------------------------------------------------------------------------------\n                         \t   Val-seen\t                    \t   Val-unseen\n                                   NE\u2193    SR\u2191    OSR\u2191    SPL\u2191     NE\u2193    SR\u2191    OSR\u2191    SPL\u2191\n-----------------------------------------------------------------------------------------------------------------------------------\nOurs (Tanh)\t\t   3.72    0.63    0.75     0.56     5.98    0.44    0.58      0.30\nOurs (Sigmoid)        3.72    0.64    0.72     0.59\t   5.92    0.44    0.56      0.33\n-----------------------------------------------------------------------------------------------------------------------------------\n\nThe main incentive for using tanh is to allow the agent to wander around when it\u2019s distance to the goal is larger than the original starting distance. As a result, this allows the progress monitor to output values lower than 0 indicating that the agent has lost the track of textual grounding. \n\nWe simply normalize the output of progress monitor between 0 to 1 before combining the score for beam search. We have revised the paper to clarify this accordingly. \n\n4. Regarding \u201cinstructions with various lengths\u201d:\n\nWe use zero-padding to handle instructions with various lengths. We have also explored ideas like using interpolation to upsample the attention weights of short instructions to a fixed length of 80, but it produces a similar performance as without interpolation.\nGenerally, we have observed that the validation samples with longer instructions are more likely to fail, but this may due to the fact that the required total number of steps of these samples is larger. Hence, the agent is prone to errors since it needs to predict actions correctly for more steps.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "Bkg3NyOmAm", "original": null, "number": 13, "cdate": 1542844212323, "ddate": null, "tcdate": 1542844212323, "tmdate": 1542844212323, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "S1gQk2P70X", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "(Cont\u2019d)", "comment": "\nIf the goal is given then appearance features can inform progress, both immediately near the goal but also contextually (e.g. rooms that tend to be near the goal or tend to contain the object in the goal). In a scenario where the instruction only describes the goal rather than the path to the goal, the agent will require a map or positions to estimate the progress using the proposed progress monitor. It would be interesting to see when the progress monitor combines with the semantic maps proposed recently in [1], [2], or [3], where the positions of the semantic map are also associated with the progress prediction condition on the given instruction. The agent can be constrained to select directions that have the closest image representation with the expected image feature representation extracted from semantic map representation and use the associated progress estimate as an additional indicator for action selection.\n\n[1] Gordon, Daniel, et al. \"IQA: Visual question answering in interactive environments.\" CVPR. 2018.\n[2] Savinov, Nikolay, Alexey Dosovitskiy, and Vladlen Koltun. \"Semi-parametric topological memory for navigation.\" ICLR (2018).\n[3] Walter, Matthew R., et al. \"A framework for learning semantic maps from grounded natural language descriptions.\" The International Journal of Robotics Research 33.9 (2014): 1167-1190."}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "rkgCnbU-6Q", "original": null, "number": 12, "cdate": 1541657013683, "ddate": null, "tcdate": 1541657013683, "tmdate": 1541657013683, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1lxWit7nX", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "1. It seems that the performance with \"Textual only\" is 1% beyond the Co-Grounding model. So why not use the \"textual only\" in the further experiments?\n\n2. Could you give more details on the method that you submitted to the test server for the \"Test-Unseen\" result? Is it \"Co-Grouding\" or \"Co-Grounding + Progress Monitor\"?\nIf the progress monitor is included, is it still non-beam search result? Because the progress monitor is only used in beam-search according to your paper.\n\nPlease let me know if I have any misunderstanding. ", "title": "Result is not clear"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "SJeQkmIR3m", "original": null, "number": 3, "cdate": 1541460699196, "ddate": null, "tcdate": 1541460699196, "tmdate": 1541533790605, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Review", "content": {"title": "Interesting Approach to Route Instruction Following with Thorough Evaluation", "review": "The paper considers the problem of following natural language route instructions in an unknown environment given only images. Integral to the proposed (\"self-aware\") approach is its ability to reason over which aspects of the instruction have been completed, which are to be followed next, which direction to go in next, as well as the agents current progress. This involves two primary components of the architecture. The first is a visual-textual module that grounds to the completed instruction, the next instruction, and the next direction based upon the visual input. The second is a \"progress monitor\" that takes the grounded instruction as input and captures the agent's progress towards completing the instruction.\n\n\nSTRENGTHS\n\n+ The paper describes an interesting approach to reasoning over which aspects of a given instruction have been correctly followed and which aspect to act on next. This takes the form of a visual-textual co-grounding model that identifies the instruction previously completed, the instruction corresponding to the next action, and the subsequent direction in which to move. The inclusion of a \"progress monitor\" allows the method to reason over whether the navigational progress matches the instruction.\n\n+ The paper provides a thorough evaluation on a challenging benchmark language understanding dataset. This evaluation includes detailed comparisons to state-of-the-art baselines together with ablation studies to understand the contribution of the different components of the architecture.\n\n+ The paper is well written and provides a thorough description of the framework with sufficient details to support replication of the results.\n\n\nWEAKNESSES\n\n- The paper would benefit from a more compelling argument for the importance of reasoning over which aspects of the instruction have been completed vs. which to act on next.\n\n- The paper emphasizes the use of images, the visual grounding reasons over visual features.\n\n- The paper incorrectly states that existing methods for language understanding require an explicit representation of the target. Several existing methods do not have this requirement. For example, Matuszek et al., 2012 parse free-form language into a formal logic representation for a downstream controller that interprets these instructions in unknown environments. Meanwhile, Duvallet et al., 2014 and Hemachandra et al., 2015 exploit language (together with vision and LIDAR) to learn a distribution over the unknown environment that guides grounding. Meanwhile, Mei et al., 2016 reason only over natural language text and parsed images, without knowledge of the environment or an explicit representation of the goal.\n\nC. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox, \u201cLearning to parse natural language commands to a robot control system,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2012.\n\nS. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, \u201cLearning models for following natural language directions in unknown environments,\u201d in Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), 2015\n\nF. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz, \u201cInferring maps and behaviors\nfrom natural language instructions,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2014.\n\n- While it's not a neural approach, the work of Arkin et al., 2017 which reasons over the entire instruction history when deciding on actions (through a statistical symbol grounding formulation)\u2060\n\nJ. Arkin, M. Walter, A. Boteanu, M. Napoli, H. Biggie, H. Kress-Gazit, and T. Howard. \"Contextual Awareness: Understanding Monologic Natural Language Instructions for Autonomous Robots,\" In Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2017\n\n- The paper misses the large body of literature on grounded language acquisition for robotics.\n\nQUESTIONS\n\n* What is the effect of using positional encoding for textual grounding as opposed to standard alignment methods such as those used by Mei et al., 2016?\n\n* Perhaps I missed it, but what happens if instructions are specified in such a way that their ordering is not consistent with the correct action ordering (e.g., with corrections interjected)?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Review", "cdate": 1542234406947, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335776560, "tmdate": 1552335776560, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJee1Q403m", "original": null, "number": 11, "cdate": 1541452504166, "ddate": null, "tcdate": 1541452504166, "tmdate": 1541452504166, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "Skgq_VrU37", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "Hi, \n\nThanks a lot for your responses. My reproduction of co ground model (without progress monitor) achieved comparable performance to reported. However, training the progress monitor seems pretty hard. Mind answering a few more questions? \n- Did you include data augmentation in the reported 46% for co-ground + progress monitor?\n- Any chance you could verify EQ5? \n- It seems from EQ6 that you used a training loss equal to the sum of loss on any trajectory. Did you consider using per-step loss? Did you take the average loss across batch? \n- The scale of co-ground loss and progress-monitor loss are roughly 10:1. Is this expected for training? ", "title": "Training Progress Monitor"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "SygyyCBKh7", "original": null, "number": 10, "cdate": 1541131734798, "ddate": null, "tcdate": 1541131734798, "tmdate": 1541131795640, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "HyeQF5h_2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Only progress monitor use beam search during inference", "comment": "Hi, \n\nWe are sorry if there is any confusion regarding table 2 in the paper. As stated in the Sec. 2.3 of \u201cProgress Monitor\u201d in the paper, during inference we use beam search with the progress monitor. Therefore, the 1st row in our proposed method with only co-grounding does not use beam search, which outperformed baseline with panoramic action space by 15% on validation-unseen SR. We hope that the ablation study table above clarifies this. \n\nIn the comment below, we have shown that, with beam size 5, our proposed method already achieved the state-of-the-art performance. By further increasing beam size, the performance gradually increases until it saturates.\n\nWith panoramic action space, many of the beams are actually empty when using a very large beam size due to limited navigable directions per viewpoints. Thus, increasing the beam size to a larger number will not necessarily help if the competing between beams already provides good selections (we achieved this by leveraging the progress monitor). Nonetheless, we provide the result with a beam size 40 as per requested. \n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\tCo-Grounding\t  Progress    Data\t\t\t        Beam \t  \t\tValidation-Seen \t    Validation-Unseen\n\t\t    #  Visual\tTextual     Monitor     Augmentation\tSearch (size) \tNE\u2193    SR\u2191    OSR\u2191      NE\u2193     SR\u2191     OSR\u2191\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nOurs\t\t   \u2714  \t     \u2714\t\t\u2714 \t\t  \u2714 \t  \t\t        40\t\t  \t\t3.13    0.70    0.77 \t    4.51    0.58     0.68\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "HyeQF5h_2Q", "original": null, "number": 9, "cdate": 1541094011313, "ddate": null, "tcdate": 1541094011313, "tmdate": 1541094011313, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1lxWit7nX", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "The 5th row reported above clearly corresponds to 2nd row of Table 2 in your paper: \n1  3.65 0.65 0.75 6.07 0.42 0.57\n2  3.23 0.70 0.78 5.04 0.57 0.70\n3  3.04 0.71 0.78 4.62 0.58 0.68\n\nSo all the reported numbers in Table 2 use beam search. However, 1st row in Table 2 matches entirely with 3rd row reported above (has no beam search),  which is contradicting and confusing. \n\nIn the table below, you claimed the proposed approach is better than Speaker-Follower with or without beam search. However, the following comparison was also misleading, as you used beam size 15. Why can't you adopt the same beam size and compare? On the other hand, the 61% SR is 4% boost compared to no data-augmentation counterpart in Table 1 of paper, while on validation-unseen, the gap is merely 1%, any idea why?\n\n\t\t\t\t\t\t\t\t\twith beam search\n--------------------------------------------------------------------------------------------------------------------------------------\nSpeaker-Follower\t\t   \t         1257.38    4.87     0.53    0.96   \t0.01 \nOurs\t\t\t   \t\t\t 373.09      4.48     0.61    0.97   \t0.02 ", "title": "Contradictions of statistics reported above"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "Skgq_VrU37", "original": null, "number": 9, "cdate": 1540932722111, "ddate": null, "tcdate": 1540932722111, "tmdate": 1540932731493, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "HygR_tZW2m", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Zero-padding for various lengths of instructions", "comment": "Hi,\n\nThanks for the comment. \n\nZero-padding is exactly how we handled it for various lengths of instructions. We have also explored ideas like using interpolation to upsample the attention weights of short instructions to a fixed length of 80, but it produces a similar performance as without interpolation."}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "HygR_tZW2m", "original": null, "number": 7, "cdate": 1540589942445, "ddate": null, "tcdate": 1540589942445, "tmdate": 1540926417570, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "HJgT5ts0im", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "Thanks so much for the prompt reply.\n\nOne more question regarding Eq6: how do you handle the text attention weights, alpha? The supplementary material suggests that W_pm is one linear layer of shape 592 x 1. The text attention weights, however, is produced from various lengths of instructions and can have variable lengths. It seems a bit odd to simply pad zeros at the end of it to extend it to the fixed length 80. How did you handle it?", "title": "What about the variable length text attention weights?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "BJelNhWS2X", "original": null, "number": 8, "cdate": 1540852776117, "ddate": null, "tcdate": 1540852776117, "tmdate": 1540852898534, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "B1l-VpoQ37", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Difference in beam size and their performance", "comment": "Hi, \n\nThanks for the opportunity to further clarify our usage of a smaller beam size (15 as opposed to 40).\n\nWe do not claim that using a smaller number of beams is one of the major contributions. It was a nice side effect that resulted from our progress monitor, where we can evaluate the partial and unfinished candidate routes during beam search. As a result, we are able to maintain a lower number of beams but still achieve state-of-the-art success rate. \n\nBelow are the results with different beam size for reference. \n\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\tCo-Grounding\t  Progress    Data\t\t\t        Beam \t  \t\tValidation-Seen \t                        Validation-Unseen\n\t\t    #  Visual\tTextual     Monitor     Augmentation\tSearch (size) \tlength\u2193   NE\u2193    SR\u2191    OSR\u2191 \tlength\u2193     NE\u2193     SR\u2191     OSR\u2191\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t    1         \u2714  \t     \u2714\t\t\u2714 \t\t  \u2714 \t  \t\t        5\t\t  \t\t159.19\t3.03    0.71    0.79 \t        168.13\t   4.77     0.55     0.68\nOurs\t    2         \u2714  \t     \u2714\t\t\u2714 \t\t  \u2714 \t  \t\t        10\t\t  \t\t271.51\t3.11    0.71    0.78 \t        277.13\t   4.64     0.57     0.68\n\t\t    3         \u2714  \t     \u2714\t\t\u2714 \t\t  \u2714 \t  \t\t        15\t\t  \t\t355.13\t3.04    0.71    0.78 \t        360.46\t   4.62     0.58     0.68\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "B1l-VpoQ37", "original": null, "number": 8, "cdate": 1540762920730, "ddate": null, "tcdate": 1540762920730, "tmdate": 1540762920730, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1lxWit7nX", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "thanks for the ablation study.\n\nI notice that in your paper the beam size is 15, while the beam size used in the speaker-follower model is 40. Playing with the beam size can influence the performance and the trajectory lengths (shorter lengths as you said). But it might not be appropriate to claim this as the contribution of your work. ", "title": "Beam size"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "r1lxWit7nX", "original": null, "number": 6, "cdate": 1540754167595, "ddate": null, "tcdate": 1540754167595, "tmdate": 1540754187394, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "Hye-BY21nX", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Additional results for ablation study", "comment": "Hi, \n\nThank you for the suggestions on ablation study. Below are the results as requested. We replaced the soft attention on visual or textual inputs with a simple mean pooling, e.g., only visual grounding means we simply use mean-pooling on textual input, and vice versa. \n\n\n------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t   Co-Grounding\t  Progress    Beam \t  Validataion-Seen \t        Validation-Unseen\n\t\t    #     Visual\tTextual     Monitor     Search \t  NE\u2193        SR\u2191        OSR\u2191 \tNE\u2193\t         SR\u2191        OSR\u2191\n------------------------------------------------------------------------------------------------------------------------------------------------\nBaseline \t\t\t\t\t\t\t\t\t\t  4.36       0.54 \t0.68\t        7.22         0.27        0.39\n------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t    1         \u2714 \t\t\t\t\t\t\t\t  3.94 \t0.62 \t0.73 \t6.34 \t0.40 \t0.53\n\t\t    2    \t             \u2714\t\t\t\t\t\t  3.60 \t0.65 \t0.75 \t6.27 \t0.43 \t0.54\nOurs\t    3         \u2714 \t     \u2714\t\t\t\t\t\t  3.65 \t0.65 \t0.75 \t6.07 \t0.42 \t0.57\n\t\t    4         \u2714 \t     \u2714\t\t\u2714 \t\t\t\t  3.56 \t0.65 \t0.75\t        5.89 \t0.46 \t0.60\n\t\t    5         \u2714  \t     \u2714\t\t\u2714 \t\t  \u2714 \t\t  3.23 \t0.70 \t0.78 \t5.04 \t0.57 \t0.70\n------------------------------------------------------------------------------------------------------------------------------------------------"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "SklYDcv12X", "original": null, "number": 5, "cdate": 1540483681407, "ddate": null, "tcdate": 1540483681407, "tmdate": 1540579878465, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "rJxkZKSCoQ", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "We use the model with panoramic action space as baseline", "comment": "Hi,\n\nThank you for your interest in our work and kind words!\n\nOur proposed method built upon the established work, and we use the panoramic action space proposed in the Speaker-Follower as the baseline (hence making the comparison to our improvements fair). We focus on highlighting the novel contributions/ideas that improve on this baseline. Removing the panoramic action space from the proposed method requires non-trivial changes to be made, including visual grounding module, action selection module, and finally the progress monitor itself. We thus encourage readers to refer to the Speaker-Follower paper for performance improvement regarding panoramic action space.  \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "HJgT5ts0im", "original": null, "number": 4, "cdate": 1540434325056, "ddate": null, "tcdate": 1540434325056, "tmdate": 1540579847339, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "Hkl2ncv0jQ", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Thank you for bringing this to our attention", "comment": "Hi,\n\nThank you for your interest in our paper and trying to reproduce it!\n\nYou are correct. The cross entropy loss in Eq 6 should be a Mean Squared Error loss (MSELoss). The equation should thus be changed to \\sum_{t=1}^{T} (y^{pm}_{t} - p^{pm}_{t})^2.  The original cross entropy loss was a variant we experimented with predicting whether the agent is making progress or not (binary prediction). The current version with tanh() at the output of the progress monitor and trained with MSE loss gave us better performance. \n\nWe will correct this error in the revision. "}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "Hye-BY21nX", "original": null, "number": 6, "cdate": 1540503865470, "ddate": null, "tcdate": 1540503865470, "tmdate": 1540503865470, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "Skejk716om", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "I think the ablation with or without beam search is very valuable, please add it to paper or at least supplementary? \nPlus, ablation with (only visual grounding) (only textual grounding) (only cogrounding without progress monitor) would be very illuminating as well. ", "title": "Consider adding ablations to paper / supplementary?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "Hkl2ncv0jQ", "original": null, "number": 5, "cdate": 1540418228502, "ddate": null, "tcdate": 1540418228502, "tmdate": 1540418228502, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "Hi interesting paper! I am trying to reproduce the result and is having problem with Eq 6. \nSpecifically, the progress monitor module seems to output p_t^pm which seems to be a 1D value between -1~1 after tanh(). The target y_t^pm is also a 1D number that is between -inf ~ 1. \nIn this case how do you use CrossEntropy Loss as suggested in the paper? \nEq 6 suggested that the loss should incorporate - y_t^pm * log (p_t^pm). Well I am confused if this is still \"cross entropy loss\". What's more log() does not like negative values? ", "title": "Confusions about Eq 6"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "rJxkZKSCoQ", "original": null, "number": 4, "cdate": 1540409590890, "ddate": null, "tcdate": 1540409590890, "tmdate": 1540409590890, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "Hi, thanks for the good performance!\n\nI am wondering how much the panoramic action space helps in your model. Can you report the performance without the panoramic action space? Thanks!\n\nBest regards", "title": "Panoramic Action Space "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "Skejk716om", "original": null, "number": 3, "cdate": 1540317923433, "ddate": null, "tcdate": 1540317923433, "tmdate": 1540318188367, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "HJx7k51tsX", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "content": {"title": "Results without beam search", "comment": "Hi,\n\nThank you for raising an important discussion about metrics. \n\nThe VLN task was recently introduced less than 1 year ago with a more than 60% success rate gap between the best-known model and human performance. Each existing work has step by step helped us to advance and reduce this gap to 33%. \n\nOur proposed work, when combined with beam search, is able to further close the gap to 25% measured with success rate. We focused on this metric since that was the metric used by recent state of art. Yet, there is obviously still room for improvement. Ideally, the common goal from the research community is to develop an agent that achieves a high success rate with low trajectory length. We argue that given the complexity of the VLN task which requires the agent to simultaneously achieve visual grounding, textual reasoning, temporal memorization/reasoning, and intelligently select actions to navigate, the need to relax the task along multiple directions in order to make progress is important and essential. There is no current best model for both metrics (SR and SPL), and beam search typically differentiates the two regimes.\n\nEven from the robotics perspective, these are two important objectives that one might want to trade off (length/time and success rate) given that there is no one solution that is pareto-optimal, and beam search can be seen as an exploration mechanism which is not uncommon in robotics. Beam search, which thoroughly explores the environment, eases the burden for the agent in intelligently selecting actions given the progress made towards the goal so that the agent can solely focus on identifying the implicit target represented by navigational instruction. However, the best performing model with beam search is still more than 20% lower than the human performance. Further, note that our state of art success rate has only about 29% of the trajectory length of the Speaker-Follower model which also uses beam search.\n\nNonetheless, we agree that the newly introduced SPL metric is also important, though it emphasizes a different aspect of the navigation task. For future comparison, we thus submitted our proposed model without beam search to the test server. Our result achieves state of art SPL results compared to existing approaches (note that we exclude submissions after the ICLR deadline) and is shown in the table below (the leaderboard only allows one result to be shown from a team). For each metric, with or without beam search, our proposed method outperforms existing approaches by a large margin.\n\n--------------------------------------------------------------------------------------------------------------------------------------\n\t  \t      \t\t\t\t\t\t               Test-Unseen\n   \t\t\t\t\t\t\tlength\u2193    NE\u2193     SR\u2191     OSR\u2191  \tSPL\u2191 \n--------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t\t\t\t\t\twithout beam search\n--------------------------------------------------------------------------------------------------------------------------------------\nSeq2Seq Baseline\t\t   \t8.13           7.85     0.20    0.27       0.18\nLook Before You Leap\t   \t        9.15\t          7.53     0.25    0.32       0.23    \nOurs\t\t  \t \t\t\t18.04         5.67     0.48    0.59       0.35\n--------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t\t\t\t\t\twith beam search\n--------------------------------------------------------------------------------------------------------------------------------------\nSpeaker-Follower\t\t   \t         1257.38    4.87     0.53    0.96   \t0.01 \nOurs\t\t\t   \t\t\t 373.09      4.48     0.61    0.97   \t0.02 \n--------------------------------------------------------------------------------------------------------------------------------------"}, "signatures": ["ICLR.cc/2019/Conference/Paper669/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616224, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1GAsjC5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper669/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper669/Authors|ICLR.cc/2019/Conference/Paper669/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616224}}}, {"id": "H1eXo56ji7", "original": null, "number": 3, "cdate": 1540246171435, "ddate": null, "tcdate": 1540246171435, "tmdate": 1540246171435, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "HJx7k51tsX", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "ICLR reviewer guidelines state that \"no paper will be considered prior work if it appeared on arxiv, or another online venue, less than 30 days prior to the ICLR deadline.\" The paper defining the SPL metric appeared on arXiv on 18 July. However, as an organizer of the VLN challenge and a co-author of the arXiv paper mentioned above, I would like to state for the benefit of reviewers that the SPL metric was not added to the public VLN leaderboard until September 8th (19 days before the ICLR deadline). In fairness to authors with work in progress, reviewers may wish to exclude this metric from the definition of prior work for ICLR 2019 since it was not implemented on the leaderboard 30 days prior to the deadline. Existing work on the dataset has been primarily evaluated in terms of 'Success Rate', as reported in this submission. ", "title": "A note from the challenge organizers on the SPL metric"}, "signatures": ["~Peter_Anderson1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["~Peter_Anderson1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}, {"id": "HJx7k51tsX", "original": null, "number": 2, "cdate": 1540057563481, "ddate": null, "tcdate": 1540057563481, "tmdate": 1540057563481, "tddate": null, "forum": "r1GAsjC5Fm", "replyto": "r1GAsjC5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "content": {"comment": "This paper only reports the absolute Success Rate as the evaluation metric and hides the trajectory lengths. It is well known that the Success Rate can be generally improved by exhaustedly exploring the environment before committing to a decision. However, beam search is not appropriate for robotics, because longer trajectories have more costs (battery, wear, delays for the user, etc). \n\nTherefore, Success rate weighted by normalized inverse Path Length (SPL) trades-off Success Rate against Trajectory Length. SPL is defined in the paper On Evaluation of Embodied Navigation Agents (https://arxiv.org/abs/1807.06757) and introduced as one of the evaluation metrics for the VLN task.\n\nI am not sure why the authors didn't include the trajectory lengths in the paper. But from the VLN challenge leaderboard, the SPL score of the authors' submission is only 0.02 (out of 1.00), which is severely worse than the Seq2Seq baseline (0.18). The trajectory length is 373.09 meters. It seems like the authors are gaming the Success Rate with exhaustive search. Hence, I do not think it is proper to claim that the method has achieved new SOTA performance. \n", "title": "Concerns on the evaluation metric and the so-called SOTA performance "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper669/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent.", "keywords": ["visual grounding", "textual grounding", "instruction-following", "navigation agent"], "authorids": ["cyma@gatech.edu", "jiasenlu@gatech.edu", "zxwu@cs.umd.edu", "alregib@gatech.edu", "zkira@gatech.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "authors": ["Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong"], "TL;DR": "We propose a self-monitoring agent for the Vision-and-Language Navigation task.", "pdf": "/pdf/762c20bbac6e90eaa124a149c5ce57d1480fbba2.pdf", "paperhash": "ma|selfmonitoring_navigation_agent_via_auxiliary_progress_estimation", "_bibtex": "@misc{\nma2019selfmonitoring,\ntitle={Self-Monitoring Navigation Agent via Auxiliary Progress Estimation},\nauthor={Chih-Yao Ma and Jiasen Lu and Zuxuan Wu and Ghassan AlRegib and Zsolt Kira and Richard Socher and Caiming Xiong},\nyear={2019},\nurl={https://openreview.net/forum?id=r1GAsjC5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper669/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311781344, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "r1GAsjC5Fm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper669/Authors", "ICLR.cc/2019/Conference/Paper669/Reviewers", "ICLR.cc/2019/Conference/Paper669/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311781344}}}], "count": 33}