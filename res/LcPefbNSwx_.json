{"notes": [{"id": "LcPefbNSwx_", "original": "KAsLudsbZCt", "number": 524, "cdate": 1601308065130, "ddate": null, "tcdate": 1601308065130, "tmdate": 1614985726748, "tddate": null, "forum": "LcPefbNSwx_", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Factor Normalization for Deep Neural Network Models", "authorids": ["qihaobo_gsm@pku.edu.cn", "~Jing_Zhou3", "hansheng@pku.edu.cn"], "authors": ["Haobo Qi", "Jing Zhou", "Hansheng Wang"], "keywords": ["factor normalization", "ultrahigh dimensional features", "adaptive learning rate", "factor decomposition"], "abstract": "Deep neural network (DNN) models often involve features of high dimensions. In most cases, the high-dimensional features can be decomposed into two parts. The first part is a low-dimensional factor. The second part is the residual feature, with much-reduced variability and inter-feature correlation. This leads to a number of interesting theoretical findings for deep neural network training. Accordingly, we are inspired to develop a new factor normalization method for better performance. The proposed method leads to a new deep learning model with two important features. First, it allows factor related feature extraction. Second, it allows adaptive learning rates for factors and residuals, respectively. This leads to fast convergence speed on both training and validation datsets. A number of empirical experiments are presented to demonstrate its superior performance. The code is available at https://github.com/HazardNeo4869/FactorNormalization", "one-sentence_summary": "We develop a new factor normalization method for fast deep neural network training.", "pdf": "/pdf/99766523b3c41824c8f16835265557af4e4a47c2.pdf", "supplementary_material": "/attachment/dcdde06437a430fa26dc10408b2ddda84820657c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|factor_normalization_for_deep_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r9IWJJXiWk", "_bibtex": "@misc{\nqi2021factor,\ntitle={Factor Normalization for Deep Neural Network Models},\nauthor={Haobo Qi and Jing Zhou and Hansheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=LcPefbNSwx_}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "81bIZBmUfj8", "original": null, "number": 1, "cdate": 1610040413510, "ddate": null, "tcdate": 1610040413510, "tmdate": 1610474011233, "tddate": null, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "invitation": "ICLR.cc/2021/Conference/Paper524/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors proposed to pre-process the original input features into a low dimensional term and its corresponding residual term via SVD. The paper empirically demonstrated the neural networks trained on such factorized exhibit faster convergence in training. Several issues of clarity were addressed during the rebuttal period by the authors. \n\nHowever, the reviewers still felt that there were some remaining fundamental issues with the paper, \n\n1)  The motivation is not echoed in the experiments, namely most of the experiments on CIFAR and CatDog dataset using a low dimensional factorization of d=1 which is trivial and often part of the whitening preprocessing. \n\n2) The proposed factorization via SVD will be difficult to scale up to high dimensional features, large training sets and higher d >> 1.  \n\n3) The empirical experiments show a marginal improvement in the training speed, especially in the image recognition tasks, yet there seems an early plateau in test performance when compared to the baselines.\n\n4) The theoretical analysis in Section 2 studied linear models. Yet, the rest of the paper focuses on non-linear neural networks. It is difficult to see the connection between the analysis and the rest of the paper. \n\nThus, I recommend rejection of the paper at this time as the current version of the paper needs further development, and non-trivial modifications, to be broadly applicable."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factor Normalization for Deep Neural Network Models", "authorids": ["qihaobo_gsm@pku.edu.cn", "~Jing_Zhou3", "hansheng@pku.edu.cn"], "authors": ["Haobo Qi", "Jing Zhou", "Hansheng Wang"], "keywords": ["factor normalization", "ultrahigh dimensional features", "adaptive learning rate", "factor decomposition"], "abstract": "Deep neural network (DNN) models often involve features of high dimensions. In most cases, the high-dimensional features can be decomposed into two parts. The first part is a low-dimensional factor. The second part is the residual feature, with much-reduced variability and inter-feature correlation. This leads to a number of interesting theoretical findings for deep neural network training. Accordingly, we are inspired to develop a new factor normalization method for better performance. The proposed method leads to a new deep learning model with two important features. First, it allows factor related feature extraction. Second, it allows adaptive learning rates for factors and residuals, respectively. This leads to fast convergence speed on both training and validation datsets. A number of empirical experiments are presented to demonstrate its superior performance. The code is available at https://github.com/HazardNeo4869/FactorNormalization", "one-sentence_summary": "We develop a new factor normalization method for fast deep neural network training.", "pdf": "/pdf/99766523b3c41824c8f16835265557af4e4a47c2.pdf", "supplementary_material": "/attachment/dcdde06437a430fa26dc10408b2ddda84820657c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|factor_normalization_for_deep_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r9IWJJXiWk", "_bibtex": "@misc{\nqi2021factor,\ntitle={Factor Normalization for Deep Neural Network Models},\nauthor={Haobo Qi and Jing Zhou and Hansheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=LcPefbNSwx_}\n}"}, "tags": [], "invitation": {"reply": {"forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040413496, "tmdate": 1610474011216, "id": "ICLR.cc/2021/Conference/Paper524/-/Decision"}}}, {"id": "5_kAUE_L0ak", "original": null, "number": 1, "cdate": 1603558416387, "ddate": null, "tcdate": 1603558416387, "tmdate": 1605024668683, "tddate": null, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "invitation": "ICLR.cc/2021/Conference/Paper524/-/Official_Review", "content": {"title": "Interesting simple idea, but requires more scholarship. ", "review": "Review of \"Factor normalization for DNNs\". \n\nThe paper makes an observation that datasets used for training many deep neural nets exhibit a strong factor structure, i.e. have a small number of dominant principal components explaining most of the variance.  If we were to remove the dominant factors, the residuals would have much weaker correlation structure and allow faster DNN convergence with SGD.   The paper proposes to separate the dominant factors and the residuals,  train the original DNN on residuals with a much faster SGD learning rate,  and then recombine with a shallow small NN learned on the dominant factors trained using its own (slower) learning rate. \n\nFirst the positive aspects:  this is an interesting idea, and I haven't seen it commonly used in practice for DNNs.  While mathematical analysis is conducted for linear models with GD and is fairly straightforward, it nicely illustrates the main issues, and the hope is that the linear intuition continues to apply to deep NNs.  Experimental results suggest that indeed convergence rate can be improved on several datasets.   In terms of criticisms,  there is very limited scholarship of related ideas that have been used both for linear models and for DNNs, in particular (a) various factor-based models that already exist,  (b) preconditioning of linear systems,  (c) neural nets trained on some other sort of residuals -- e.g. laplacian pyramid DNNs .  Another criticism -- is that there is no discussion of how to do large-scale PCA / factor analysis for high-dimensional image data arising in say modern DNN image classification pipelines, and its computational cost,  as simple numpy.linalg.svd won't work.  The paper also claims that strong factor structure (with a small number of dominant components) is prevalent in modern ultra-large scale DNN applications -- I would like to see some references / supporting evidence beyond just computer vision.  \n\nOverall in my opinion the paper needs to consider the context of related works, and has a few other correctable issues, but I would certainly encourage the authors to continue to improve it. \n\nAdditional details: \n\n1)  Prior work -- that should be cited / contrasted with your approach:   \n(a) Since a substantial part of the paper analyzes linear models,  it's important to mention that factor structure has been long exploited in various ML / stats works.  For example factor models, and principal component regression (PCR) attempt to focus the modeling power on the principal components. In other applications (e.g. in financial modeling) one can assume that factors components are less predictable and focus the modeling effort on the residuals. Recent work by Alex Smola et al,  \"Deep factors for forecasting\" has looked at the deep-NN instantiation of this idea.  Basic autoencoders or bottlenecks used in DNN architectures also attempt to capture the low-dimensional structure.  These are all different from what you're doing, but hinge on the same basic concept -- so providing a discussion of your work in context of related work would be important. \nb)  There is a long history of using preconditioners for gradient-based and other iterative solvers of linear systems.  In particular there are some low-rank preconditioners for accelerating convergence of linear systems:  Nicholas Higham, et. al, \"A new preconditioner that exploits low-rank approximations to factorization error\".  There is also work on applying preconditioners specifically for SGD, e.g. see works by Michael Mahoney. \nc)  For some domains a low-pass filter, or some other low-resolution model can serve to replace PCA or factor models. For example Laplacian pyramids have been widely used in image processing to separate the dominant modes from details.  There are existing DNN approaches based on laplacian pyramids: \ne.g. \"Deep laplacian pyramid networks for fast an accurate super-resolution\". \n\n2)  You mention that \"none of these optimization methods has considered the covariance structure\".. related to the Hessian.  There is a substantial effort to develop second-order methods for stochastic gradient descent, in particular at ICML 2020 there was a workshop on this topic.  \"Beyond first order methods in ML\".   http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=102387&copyownerid=156008\n\n3) In section 2.1. -- it's worth mentioning explicitly that you're specifically analyzing linear regression under gradient descent.  as you look at Loss(y, theta' X).   It's also worth explicitly mentioning that the condition number of the Hessian plays a crucial role in convergence rate of GD.   You talk about the top eigenvalue, where the bottom one in your example is fixed at 1,  but it's worth mentioning the condition number. \n\n4) Sec. 3.1. How do you conduct 'standard principal component analysis' for ultrahigh dimensional features.  This is a computationally tricky problem, \"standard\" methods won't work.   How do you decide on the dimension -- i..e. number of factors to keep? \n\n5) Is the \"time consumption\"  including the time to do PCA? \n\n6) Can you give some references claiming strong factor structure in several DNN applications in ultra-high dimensions?  I do not contest that this is the case -- but it would be useful to have supporting evidence.  What number/fraction of factors is typically required in these applications to capture a nontrivial fraction of variance?\n\n7) Sec. 2.1. Using lower-case kappa for a matrix is strange,  I initially assumed it's a scalar.  Maybe use another capital letter. \n\n8) While the paper is mostly pretty readable, there are various small issues with english language (from stylistic to grammar) use e.g. \"In fact ample amounts of empirical evidence\" --> \"ample empirical evidence\", e.t.c. There are typos in references, e.g.  Zeiler,   \"Computer ence, 2012\". What is that? \n ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper524/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper524/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factor Normalization for Deep Neural Network Models", "authorids": ["qihaobo_gsm@pku.edu.cn", "~Jing_Zhou3", "hansheng@pku.edu.cn"], "authors": ["Haobo Qi", "Jing Zhou", "Hansheng Wang"], "keywords": ["factor normalization", "ultrahigh dimensional features", "adaptive learning rate", "factor decomposition"], "abstract": "Deep neural network (DNN) models often involve features of high dimensions. In most cases, the high-dimensional features can be decomposed into two parts. The first part is a low-dimensional factor. The second part is the residual feature, with much-reduced variability and inter-feature correlation. This leads to a number of interesting theoretical findings for deep neural network training. Accordingly, we are inspired to develop a new factor normalization method for better performance. The proposed method leads to a new deep learning model with two important features. First, it allows factor related feature extraction. Second, it allows adaptive learning rates for factors and residuals, respectively. This leads to fast convergence speed on both training and validation datsets. A number of empirical experiments are presented to demonstrate its superior performance. The code is available at https://github.com/HazardNeo4869/FactorNormalization", "one-sentence_summary": "We develop a new factor normalization method for fast deep neural network training.", "pdf": "/pdf/99766523b3c41824c8f16835265557af4e4a47c2.pdf", "supplementary_material": "/attachment/dcdde06437a430fa26dc10408b2ddda84820657c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|factor_normalization_for_deep_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r9IWJJXiWk", "_bibtex": "@misc{\nqi2021factor,\ntitle={Factor Normalization for Deep Neural Network Models},\nauthor={Haobo Qi and Jing Zhou and Hansheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=LcPefbNSwx_}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper524/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141184, "tmdate": 1606915776378, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper524/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper524/-/Official_Review"}}}, {"id": "A_OdbWQgcUu", "original": null, "number": 2, "cdate": 1603877161146, "ddate": null, "tcdate": 1603877161146, "tmdate": 1605024668621, "tddate": null, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "invitation": "ICLR.cc/2021/Conference/Paper524/-/Official_Review", "content": {"title": "Interesting phenomena but not easy to understand", "review": "Summary:\nIn this paper, a learning method that accelerates the training of DNN is proposed. Given an input X, the proposed method decomposes X as X = BZ + E where BZ is a low-rank approximation of X and E is the residual term. E is used as an input of DNN and Z is used as an additional feature of the input of the last layer. Experiments using MNIST and CIFAR10 show that the proposed method accelerates the speed to reduce the training loss. \n\n\nDetailed comments:\nThe observed phenomena --- the proposed method accelerates the learning speed of SGD --- is quite interesting. I haven't noticed existing studies reporting such findings. However, I feel \"why\" parts are not clearly explained in this paper. For example, I have the following questions.\n- The analysis in Section 2 is based on a shallow network model. How can we apply this analysis to a deep network model?\n- What kind of intuition is behind the architecture design (Fig 1)? Why the residual term should place as the input of the first layer and the main term as the input of the last layer, rather than e.g. some intermediate layer?\n\nAlso, I feel the experiments are not convincing enough. The main motivation of this paper is that \"the ultrahigh dimensional features can be decomposed into two parts\". However, the dimensions of MNIST and CIFAR10 are not quite ultrahigh (28^2=784 and 32^2=1024).  Experiments with more high-dimensional data such as ImageNet would be necessary to convince the research concept. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper524/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper524/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factor Normalization for Deep Neural Network Models", "authorids": ["qihaobo_gsm@pku.edu.cn", "~Jing_Zhou3", "hansheng@pku.edu.cn"], "authors": ["Haobo Qi", "Jing Zhou", "Hansheng Wang"], "keywords": ["factor normalization", "ultrahigh dimensional features", "adaptive learning rate", "factor decomposition"], "abstract": "Deep neural network (DNN) models often involve features of high dimensions. In most cases, the high-dimensional features can be decomposed into two parts. The first part is a low-dimensional factor. The second part is the residual feature, with much-reduced variability and inter-feature correlation. This leads to a number of interesting theoretical findings for deep neural network training. Accordingly, we are inspired to develop a new factor normalization method for better performance. The proposed method leads to a new deep learning model with two important features. First, it allows factor related feature extraction. Second, it allows adaptive learning rates for factors and residuals, respectively. This leads to fast convergence speed on both training and validation datsets. A number of empirical experiments are presented to demonstrate its superior performance. The code is available at https://github.com/HazardNeo4869/FactorNormalization", "one-sentence_summary": "We develop a new factor normalization method for fast deep neural network training.", "pdf": "/pdf/99766523b3c41824c8f16835265557af4e4a47c2.pdf", "supplementary_material": "/attachment/dcdde06437a430fa26dc10408b2ddda84820657c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|factor_normalization_for_deep_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r9IWJJXiWk", "_bibtex": "@misc{\nqi2021factor,\ntitle={Factor Normalization for Deep Neural Network Models},\nauthor={Haobo Qi and Jing Zhou and Hansheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=LcPefbNSwx_}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper524/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141184, "tmdate": 1606915776378, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper524/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper524/-/Official_Review"}}}, {"id": "CYJQSkshC2", "original": null, "number": 3, "cdate": 1603896238221, "ddate": null, "tcdate": 1603896238221, "tmdate": 1605024668554, "tddate": null, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "invitation": "ICLR.cc/2021/Conference/Paper524/-/Official_Review", "content": {"title": "experiments not very convincing", "review": "The paper describes a training scheme based on decomposing input features into two parts which have different training dynamics: a low rank \"factor feature\" computed using PCA on the raw features, and a high rank \"residual\".  The former is processed  by a very shallow network, while the latter passes through the full network, and parameters for each are updated with different learning rates.  Experiments show that the proposed algorithm can speed up wall-time to  a certain accuracy on MNIST and CIFAR10 classification, across several neural network architectures and optimizers.\n\nPros:\n- Simple and straightforward proposal, easy to understand, and seems to lead to improved accuracy/reduced loss early in training.\n- Experiments cover several datasets and architectures spanning very simple to very deep, which is nice to see.\n\nConcerns:\n\n- Very \"mathy\" presentation in Sec. 2 is very dense and difficult to follow, and only seems to motivate the method in the special cases of a very shallow linear regression model, where the gradient is easily related to eigenvalues of the input features $X$ (and assuming a \"strong factor structure\", which has been shown to be reasonable for natural images.)  It's not clear why these results would necessarily generalize to very deep and highly nonlinear networks such as AlexNet.  Perhaps this is the motivation for only processing \"factor features\" with an extremely shallow (albeit still nonlinear) network? If so, this should be more clearly explained in the text.\n\n- The experiments are not very convincing. \n - The baselines trained with vanilla SGD or other optimized (Adam, etc.) are not clearly explained.  Do they use the same feature factorization as SGD+FN or do they consist of only the raw features passed into the full network (i.e. the left side of Figure 1)?  If the latter, the comparisons are not totally fair as the input features and networks differ.\n - Models are not trained until convergence, but only for a fixed amount of time, or until a fixed (but not very high) accuracy is reached.  How does the proposed scheme affect final accuracy/loss at convergence?  (E.g., I'd expect simple logistic regression on MNIST to achieve over 95% accuracy)   Is it strictly a training speedup, while converging to the same performance as baselines?  Or is it only suitable when operating within a limit training time budget?\n  - This issue is illustrated in the training curves in Fig 3 (MLP on MNIST), where the validation accuracy for the proposed SGD+FN appears to be plateauing faster than the baseline SGD curves.\n - Furthermore, the speedup over NAG or Adam on CIFAR10 using  ResNet50 isn't very large. \n - All training comparisons are measured only as a function of wall-time.  But the training hardware is not explained.\n - Missing obvious baselines of training separate models on either the factor features or residual alone.  At least for MNIST I'd expect decent performance to be obtainable from the factor features alone.  Maybe the residual is mostly irrelevant to the task?\n\nOverall I feel that the paper is not ready for publication at this time since the experimental validation is incomplete and does not fully explore the benefits and potential downsides of the proposed method.\n\nOther comments:\n\n- Sec 3.3: \"Adaptive learning\".  If I'm understanding correctly, the learning rates are defined based on the eigenvalues of $X^T X$, but are then kept fixed.  So, unlike e.g. AdaGrad or Adam,  the algorithm is not really adaptive in the sense that the learning rate is kept constant throughout training.\n\n- Sec 3.3: \"a standard SGD algorithm cannot be used to train a DNN model\".   This seems like an unnecessarily strong statement given how commonly SGD is used for DNN training.\n\n- The linked code does not appear to include resnet experiments\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper524/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper524/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factor Normalization for Deep Neural Network Models", "authorids": ["qihaobo_gsm@pku.edu.cn", "~Jing_Zhou3", "hansheng@pku.edu.cn"], "authors": ["Haobo Qi", "Jing Zhou", "Hansheng Wang"], "keywords": ["factor normalization", "ultrahigh dimensional features", "adaptive learning rate", "factor decomposition"], "abstract": "Deep neural network (DNN) models often involve features of high dimensions. In most cases, the high-dimensional features can be decomposed into two parts. The first part is a low-dimensional factor. The second part is the residual feature, with much-reduced variability and inter-feature correlation. This leads to a number of interesting theoretical findings for deep neural network training. Accordingly, we are inspired to develop a new factor normalization method for better performance. The proposed method leads to a new deep learning model with two important features. First, it allows factor related feature extraction. Second, it allows adaptive learning rates for factors and residuals, respectively. This leads to fast convergence speed on both training and validation datsets. A number of empirical experiments are presented to demonstrate its superior performance. The code is available at https://github.com/HazardNeo4869/FactorNormalization", "one-sentence_summary": "We develop a new factor normalization method for fast deep neural network training.", "pdf": "/pdf/99766523b3c41824c8f16835265557af4e4a47c2.pdf", "supplementary_material": "/attachment/dcdde06437a430fa26dc10408b2ddda84820657c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|factor_normalization_for_deep_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r9IWJJXiWk", "_bibtex": "@misc{\nqi2021factor,\ntitle={Factor Normalization for Deep Neural Network Models},\nauthor={Haobo Qi and Jing Zhou and Hansheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=LcPefbNSwx_}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper524/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141184, "tmdate": 1606915776378, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper524/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper524/-/Official_Review"}}}, {"id": "-mCRwD6Kdem", "original": null, "number": 4, "cdate": 1603951001791, "ddate": null, "tcdate": 1603951001791, "tmdate": 1605024668481, "tddate": null, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "invitation": "ICLR.cc/2021/Conference/Paper524/-/Official_Review", "content": {"title": "The paper proposed to project input to factor (low-dimensional) and residual features (high-dimensional) to improve neural network training.", "review": "**Strong points**\n\nThe paper provides a very detailed theoretical analysis of motivation.\n\n**Weak points**\n\nAnalysis in section 2 is based on linear regression, but the proposed method is based on deep models.\n\nThe proposed method is more similar to feature extraction instead of a training method.\n\nMany models in the chart are not fully converged. It is important to compare convergence speed but also the final accuracy. I think they are not converged, because the training and testing curve is perfectly smoothing without any fluctuations.\n\nComparison in chart 3 needs improvement. Since the two models have different input signals and model structure, they inherently need different hyper-parameters (not only learning rate) for best performance. Only comparing them with the same learning rate may not adequate to prove the significance of the proposed method.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper524/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper524/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factor Normalization for Deep Neural Network Models", "authorids": ["qihaobo_gsm@pku.edu.cn", "~Jing_Zhou3", "hansheng@pku.edu.cn"], "authors": ["Haobo Qi", "Jing Zhou", "Hansheng Wang"], "keywords": ["factor normalization", "ultrahigh dimensional features", "adaptive learning rate", "factor decomposition"], "abstract": "Deep neural network (DNN) models often involve features of high dimensions. In most cases, the high-dimensional features can be decomposed into two parts. The first part is a low-dimensional factor. The second part is the residual feature, with much-reduced variability and inter-feature correlation. This leads to a number of interesting theoretical findings for deep neural network training. Accordingly, we are inspired to develop a new factor normalization method for better performance. The proposed method leads to a new deep learning model with two important features. First, it allows factor related feature extraction. Second, it allows adaptive learning rates for factors and residuals, respectively. This leads to fast convergence speed on both training and validation datsets. A number of empirical experiments are presented to demonstrate its superior performance. The code is available at https://github.com/HazardNeo4869/FactorNormalization", "one-sentence_summary": "We develop a new factor normalization method for fast deep neural network training.", "pdf": "/pdf/99766523b3c41824c8f16835265557af4e4a47c2.pdf", "supplementary_material": "/attachment/dcdde06437a430fa26dc10408b2ddda84820657c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "qi|factor_normalization_for_deep_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=r9IWJJXiWk", "_bibtex": "@misc{\nqi2021factor,\ntitle={Factor Normalization for Deep Neural Network Models},\nauthor={Haobo Qi and Jing Zhou and Hansheng Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=LcPefbNSwx_}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LcPefbNSwx_", "replyto": "LcPefbNSwx_", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper524/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141184, "tmdate": 1606915776378, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper524/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper524/-/Official_Review"}}}], "count": 6}