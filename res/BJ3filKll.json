{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488122965027, "tcdate": 1478195987740, "number": 75, "id": "BJ3filKll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJ3filKll", "signatures": ["~Ronen_Basri1"], "readers": ["everyone"], "content": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396341023, "tcdate": 1486396341023, "number": 1, "id": "H1p2iMLdl", "invitation": "ICLR.cc/2017/conference/-/paper75/acceptance", "forum": "BJ3filKll", "replyto": "BJ3filKll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396341512, "id": "ICLR.cc/2017/conference/-/paper75/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJ3filKll", "replyto": "BJ3filKll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396341512}}}, {"tddate": null, "tmdate": 1484260913425, "tcdate": 1484260913425, "number": 4, "id": "rktNLYSUg", "invitation": "ICLR.cc/2017/conference/-/paper75/public/comment", "forum": "BJ3filKll", "replyto": "SyHDxy8Vl", "signatures": ["~Ronen_Basri1"], "readers": ["everyone"], "writers": ["~Ronen_Basri1"], "content": {"title": "Reply to AnonReviewer2", "comment": "We thank the reviewer for these additional comments.\n \n'It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks'\nWe agree. Our construction can be incorporated in deep networks in various ways, both in order to represent complex manifolds by using higher layers to combine several monotonic chains (as we have demonstrated for the swiss roll) or in producing hierarchical representations of the data (such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space). We refer the reviewer to Appendix D in the revised version of our paper, which now includes further discussion of such deep architectures. \n \n'to what extent the proposed picture describes the totality of functions that are representable by the networks'\nOur approach suggests an efficient way to represent low-dimensional manifolds with neural nets. This can serve as part of representations of functions defined on manifolds. Our work does not suggest specific methods to represent such functions, although, as noted, Shaham et al. (2015) suggest one way to approach this problem.\n\nWe appreciate the minor comments of the reviewer and have incorporated them into our revision.\nRegarding the minor comment 'On page 5, mention how the orthogonal projection on S_k is realized in the network' please note that the orthogonal projection is not realized in the network. It is used at this point merely to derive a bound on the error.\u200b"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739509, "id": "ICLR.cc/2017/conference/-/paper75/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ3filKll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper75/reviewers", "ICLR.cc/2017/conference/paper75/areachairs"], "cdate": 1485287739509}}}, {"tddate": null, "tmdate": 1484260653501, "tcdate": 1484260653501, "number": 3, "id": "BySNBFrUl", "invitation": "ICLR.cc/2017/conference/-/paper75/public/comment", "forum": "BJ3filKll", "replyto": "BJoiKE_Nl", "signatures": ["~Ronen_Basri1"], "readers": ["everyone"], "writers": ["~Ronen_Basri1"], "content": {"title": "Reply to AnonReviewer3", "comment": "We thank the reviewer for these constructive comments.\n\n'The experiments are all with a regression loss and a shallow network'\nIndeed monotonic chains can efficiently be handled with a shallow network. Our construction, however, can be incorporated in deep networks in various ways, both in order to represent complex manifolds by using higher layers to combine several monotonic chains (as we have demonstrated for the swiss roll) or in producing hierarchical representations of the data (such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space). We refer the reviewer to Appendix D in the revision, which now includes further discussion of such deep architectures. \n \n'It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression'\nClassification loss is generally more permissive than a regression loss. Consequently, if we apply a classification loss to an architecture that can perform embedding in the lower levels and classification in the higher levels we will generally obtain a distorted embedding, in which each linear segment can deform (e.g. in the directions of class separation) -- such embeddings are often sufficient to achieve accurate classification. We now demonstrate this and provide further discussion in Appendix D of our revised manuscript.  \n \n'The theory sections could do with being more clearly written'\nWe appreciate the reviewers suggestions for improving the presentation of the theory, and will incorporate these in a revised version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739509, "id": "ICLR.cc/2017/conference/-/paper75/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ3filKll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper75/reviewers", "ICLR.cc/2017/conference/paper75/areachairs"], "cdate": 1485287739509}}}, {"tddate": null, "tmdate": 1484260328428, "tcdate": 1484260328428, "number": 2, "id": "HkegVFHLe", "invitation": "ICLR.cc/2017/conference/-/paper75/public/comment", "forum": "BJ3filKll", "replyto": "ryGb3w-rg", "signatures": ["~Ronen_Basri1"], "readers": ["everyone"], "writers": ["~Ronen_Basri1"], "content": {"title": "Reply to AnonReviewer4", "comment": "We thank the reviewer for these constructive comments.\n \n(1) 'work on learning data representations from sets of local tangent planes'\nThank you for the references, which we have included in a revision of the paper. In particular, Zhang and Zha's analysis indicates that in the limit when the input points are sampled densely and lie near a piecewise linear manifold their embedding algorithm projects points that are off the manifold orthogonally onto the manifold. Our analysis, in contrast, indicates that a neural network with an efficient architecture will generally not project these points orthogonally. We note however that less efficient networks with significantly more units can also learn to project such points orthogonally onto the manifold. Our main objective in this paper is to show that networks can be significantly more efficient, although this comes at the price of increased error when the data is not fit exactly by the piecewise linear approximation.\n \n'how these old techniques compare to the deep network trained to produce the embedding of Figure 6'\nUnfortunately, it is not viable to experimentally compare the results of traditional manifold learning approaches and our constructions.  In our experiments, which serve to illustrate the types of constructions that can be learned by a deep network, we use supervision so that the network learns to map input points to a ground truth embedding.  Traditional manifold learning is unsupervised, and so must solve a more difficult problem.  In response to the reviewers comments we have applied LLE to the data used to produce Figure 6.  However, while a neural network can learn a good embedding with supervision, learning a construction similar to the one we derive theoretically, LLE is not able to produce a sufficiently good embedding using the somewhat limited data in our experiment, as it does not make use of supervision.  It learns an embedding that maps input faces to a straight line based on their azimuth, ignoring elevation.  However, we are grateful for the reviewer\u2019s suggestion, and feel that it will be interesting in future work to more comprehensively explore how the manifold embedding with deep networks is related to these now classical approaches.\n \n(2) 'data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space'\nSimilar to existing manifold learning techniques, our approach also assumes there are sufficiently many data points to allow identification of the manifold structure. We note that when the data is sparsely spread in a high dimensional space manifold embedding is probably not a preferred strategy. \n \n'network architectures that are not pure ReLU networks'\nCommon activation functions like softplus, sigmoid and tanh can be approximated accurately with piecewise linear functions that include just a few pieces. With such approximations our analysis can be readily shown to apply also to these activation functions.\n \n'most modern networks use a variant of batch normalization'\nOur analysis would not change significantly with batch normalization. Ioffe and Szegedy (2015) show that a network with batch normalization can represent any function that could be represented by a network without batch normalization, since added parameters allow a linear transformation to be learned that recovers the original network.\n \n(3) 'The error bound presented in Section 4 appears vacuous for any practical setting'\nOur error analysis represents a worst case analysis, illustrating what the error would be when the projection directions for every segment chosen in training align with the direction of the largest singular vector of the projection operator. Our experiments indicate that the average case is significantly more favorable. We hope to further analyze this case in future work. \n \n'only refer to fully supervised siamese network approaches'\nWe thank the reviewer for these references and have included them in our revision. We note that our construction can be used both in supervised and unsupervised settings, but for our experiments we only show results in supervised ones.\n \n'What loss do the authors use in their experiments?'\nIn our experiments we use the square loss || y - \\hat y ||^2 where for an input point x (in ambient space) y denotes the embedding coordinates produced by the network and \\hat y denotes the ground truth embedding coordinates. We also tested our method in a siamese setting where for two inputs xi xj we used the square loss (||yi - yj|| - ||\\hat yi - \\hat yj||)^2. These experiments yielded similar results.\n \nWe also somewhat disagree with the reviewer that our results are not surprising. It may seem natural that a neural network can embed a manifold, but we find it very surprising that this can be done so efficiently.  In contrast to these results, as we discuss in our paper, Shaham et al. (2015) have recently published a related construction that is much less efficient. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739509, "id": "ICLR.cc/2017/conference/-/paper75/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ3filKll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper75/reviewers", "ICLR.cc/2017/conference/paper75/areachairs"], "cdate": 1485287739509}}}, {"tddate": null, "tmdate": 1482943481901, "tcdate": 1482943481901, "number": 3, "id": "ryGb3w-rg", "invitation": "ICLR.cc/2017/conference/-/paper75/official/review", "forum": "BJ3filKll", "replyto": "BJ3filKll", "signatures": ["ICLR.cc/2017/conference/paper75/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper75/AnonReviewer4"], "content": {"title": "Novel analysis, but potentially limited impact", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \"monotonic chains of linear segments\", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.\n\nWhile the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \"compatible\". In particular, I have three main concerns with respect to the results presented in this paper:\n\n(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?\n\n(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.\n\n(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the \"bound for this case is very loose\". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.\n\nI would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.\n\n\nMinor comments: \n\n- In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009).\n- What loss do the authors use in their experiments? Using \"the difference between the ground truth distance ... and the distance computed by the network\" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482943482530, "id": "ICLR.cc/2017/conference/-/paper75/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper75/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper75/AnonReviewer2", "ICLR.cc/2017/conference/paper75/AnonReviewer3", "ICLR.cc/2017/conference/paper75/AnonReviewer4"], "reply": {"forum": "BJ3filKll", "replyto": "BJ3filKll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482943482530}}}, {"tddate": null, "tmdate": 1482340770787, "tcdate": 1482340770787, "number": 2, "id": "BJoiKE_Nl", "invitation": "ICLR.cc/2017/conference/-/paper75/official/review", "forum": "BJ3filKll", "replyto": "BJ3filKll", "signatures": ["ICLR.cc/2017/conference/paper75/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper75/AnonReviewer3"], "content": {"title": "A very interesting direction, but clearer presentation and additional experiments would be more convincing.", "rating": "5: Marginally below acceptance threshold", "review": "Summary:\nIn this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. \nThey define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.\n\nThey also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. \n\nExperiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.\n\nComments:\n\nThe direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction.\n\nHowever, the current version of the paper could use some more work:\n\nThe experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.\n\nIt also seems important to confirm that embedding works well when *classification* loss is used, instead of regression\n\nThe theory sections could do with being more clearly written -- I\u2019m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that \u201caccurately and efficiently\u201d preserves a monotonic chain, etc.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482943482530, "id": "ICLR.cc/2017/conference/-/paper75/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper75/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper75/AnonReviewer2", "ICLR.cc/2017/conference/paper75/AnonReviewer3", "ICLR.cc/2017/conference/paper75/AnonReviewer4"], "reply": {"forum": "BJ3filKll", "replyto": "BJ3filKll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482943482530}}}, {"tddate": null, "tmdate": 1482186844731, "tcdate": 1482186844731, "number": 1, "id": "SyHDxy8Vl", "invitation": "ICLR.cc/2017/conference/-/paper75/official/review", "forum": "BJ3filKll", "replyto": "BJ3filKll", "signatures": ["ICLR.cc/2017/conference/paper75/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper75/AnonReviewer2"], "content": {"title": "review of ``EFFICIENT REPRESENTATION OF LOW-DIMENSIONAL MANIFOLDS USING DEEP NETWORKS''", "rating": "7: Good paper, accept", "review": "SUMMARY \nThis paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. \n\nPROS \nInteresting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. \n\nCONS \nThe paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). \n\nCOMMENTS \nIt would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. \nAlso, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. \n\nMINOR COMMENTS \n- Figure 1 could be referenced first in the text.  \n- ``Color coded'' where the color codes what? \n- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. \n- On page 5, mention how the orthogonal projection on S_k is realized in the network. \n- On page 6 ``divided into segments'' here `segments' is maybe not the best word. \n- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean? \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482943482530, "id": "ICLR.cc/2017/conference/-/paper75/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper75/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper75/AnonReviewer2", "ICLR.cc/2017/conference/paper75/AnonReviewer3", "ICLR.cc/2017/conference/paper75/AnonReviewer4"], "reply": {"forum": "BJ3filKll", "replyto": "BJ3filKll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482943482530}}}, {"tddate": null, "tmdate": 1481148179316, "tcdate": 1480980244287, "number": 1, "id": "S13MPuXmg", "invitation": "ICLR.cc/2017/conference/-/paper75/public/comment", "forum": "BJ3filKll", "replyto": "HJfrkIk7e", "signatures": ["~David_Jacobs1"], "readers": ["everyone"], "writers": ["~David_Jacobs1"], "content": {"title": "Reply to questions", "comment": "Thank you for your helpful questions.  In response:\n\uf06e\tWe should say that the chains can be flattened by an isometry.\n\uf06e\tYes, we mean pieces of affine spaces.  We think the best way to clarify this is to refer to \u201csegments\u201d instead of \u201csubspaces.\u201d\n\uf06e\tThanks for pointing this out.  We meant to say: {\\cal C} \\cap H_{k+1}\\subseteq {\\cal C} \\cap H_{k}\n\uf06e\tWe think that the obvious points of comparison would be to networks with more or fewer units.  We can say that with a much less efficient construction, in which an independent basis is created for each segment separately, the relative error can be reduced to 0.  Such a construction (see Shaham et al.) would require more than m units for each additional segment, while our construction requires only one unit per additional segment.  A second possible comparison is to a network that represents the manifold in a single basis, but this would produce a very large relative error.\nWe will revise our paper to take account of these points.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287739509, "id": "ICLR.cc/2017/conference/-/paper75/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJ3filKll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper75/reviewers", "ICLR.cc/2017/conference/paper75/areachairs"], "cdate": 1485287739509}}}, {"tddate": null, "tmdate": 1480707898200, "tcdate": 1480707898196, "number": 1, "id": "HJfrkIk7e", "invitation": "ICLR.cc/2017/conference/-/paper75/pre-review/question", "forum": "BJ3filKll", "replyto": "BJ3filKll", "signatures": ["ICLR.cc/2017/conference/paper75/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper75/AnonReviewer2"], "content": {"title": "terminology", "question": "In page 3 one reads ``assume that these chains can be flattened...''. Flattened by what kind of operation? \nIn the definition from Page 3, should affine spaces not rather be pieces of affine spaces? \nIn the same definition, the assumption H_{k+1}\\subseteq H_{k} would imply that the hyperplanes are parallel. Are you sure this is what you have in mind? \nAt the end of Section 4 one reads about a mean relative error. Is there also a baseline? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space.  We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data.  Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space.  Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.", "pdf": "/pdf/bd97974927edd491da29b50692917677f1ccd41a.pdf", "TL;DR": "We show constructively that deep networks can learn to represent manifold data efficiently", "paperhash": "basri|efficient_representation_of_lowdimensional_manifolds_using_deep_networks", "keywords": ["Theory", "Deep learning"], "conflicts": ["weizmann.ac.il", "cs.umd.edu", "ethz.ch", "ens-cachan.fr"], "authors": ["Ronen Basri", "David W. Jacobs"], "authorids": ["ronen.basri@weizmann.ac.il", "djacobs@cs.umd.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959477675, "id": "ICLR.cc/2017/conference/-/paper75/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper75/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper75/AnonReviewer2"], "reply": {"forum": "BJ3filKll", "replyto": "BJ3filKll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959477675}}}], "count": 10}