{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396480223, "tcdate": 1486396480223, "number": 1, "id": "rkuBnzLOg", "invitation": "ICLR.cc/2017/conference/-/paper284/acceptance", "forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper addresses the importance task of learning generative models of multiple modalities. There are two concerns about the paper: limited novelty, which will not have sufficient impact; ineffectiveness of evaluation. The paper extends VAEs in an interesting way, but this extension on its own does provide sufficient new insight understanding. And the log-likelihood evaluations and data sets are not enough to be convincing. As a result, the paper is not yet ready for acceptance at the conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396480740, "id": "ICLR.cc/2017/conference/-/paper284/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396480740}}}, {"tddate": null, "tmdate": 1482168947711, "tcdate": 1482168947711, "number": 3, "id": "rJ3O99HEe", "invitation": "ICLR.cc/2017/conference/-/paper284/official/review", "forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "signatures": ["ICLR.cc/2017/conference/paper284/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper284/AnonReviewer2"], "content": {"title": "The JMVAE is rather straightforward extension of VAE.", "rating": "5: Marginally below acceptance threshold", "review": "The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.\n\nThe proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636634, "id": "ICLR.cc/2017/conference/-/paper284/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper284/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper284/AnonReviewer1", "ICLR.cc/2017/conference/paper284/AnonReviewer3", "ICLR.cc/2017/conference/paper284/AnonReviewer2"], "reply": {"forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636634}}}, {"tddate": null, "tmdate": 1481939111568, "tcdate": 1481939021412, "number": 2, "id": "HJrLdzMEl", "invitation": "ICLR.cc/2017/conference/-/paper284/official/review", "forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "signatures": ["ICLR.cc/2017/conference/paper284/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper284/AnonReviewer3"], "content": {"title": "Some interesting ideas, but has major problems", "rating": "3: Clear rejection", "review": "The proposed method of modeling multimodal datasets is a VAE with an inference network for every combination of missing and present modalities. The method is evaluated on modeling MNIST and CelebA datasets.\n\nMNIST is hardly a multimodal dataset. The authors propose to use the labels as a separate modality that gets modeled with a variational autoencoder. The reviewer finds this choice perplexing.\nEven then the modalities are never actually missing, so the applicability of the suggested method is questionable.\nIn addition the differences in log-likelihoods between different models are tiny, and likely to be due to noise.\n\nThe other experiment reports log-likelihood of models that were not trained to maximize log-likelihood. It is not clear what conclusions can be drawn from such comparison.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636634, "id": "ICLR.cc/2017/conference/-/paper284/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper284/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper284/AnonReviewer1", "ICLR.cc/2017/conference/paper284/AnonReviewer3", "ICLR.cc/2017/conference/paper284/AnonReviewer2"], "reply": {"forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636634}}}, {"tddate": null, "tmdate": 1481937841643, "tcdate": 1481908493169, "number": 1, "id": "H1Sfbi-Nl", "invitation": "ICLR.cc/2017/conference/-/paper284/official/review", "forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "signatures": ["ICLR.cc/2017/conference/paper284/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper284/AnonReviewer1"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512636634, "id": "ICLR.cc/2017/conference/-/paper284/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper284/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper284/AnonReviewer1", "ICLR.cc/2017/conference/paper284/AnonReviewer3", "ICLR.cc/2017/conference/paper284/AnonReviewer2"], "reply": {"forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512636634}}}, {"tddate": null, "tmdate": 1481767695614, "tcdate": 1481665838694, "number": 4, "id": "rkP46kA7x", "invitation": "ICLR.cc/2017/conference/-/paper284/public/comment", "forum": "Hk8rlUqge", "replyto": "BJiH4Ey7x", "signatures": ["~Masahiro_Suzuki1"], "readers": ["everyone"], "writers": ["~Masahiro_Suzuki1"], "content": {"title": "Question on Equation (5)", "comment": "Thank you for informative comments and sorry for my late reply.\n\nAs you say,  p(z | w) and p(z | x) are not strictly included in the proposed generative model since logp (x | w) + logp (w | x) and JMVAE do not have the same generative model.\n\nHowever, what we would like to claim here is that if the probability distributions are parameterized with neural networks, both maximization of logp (x | w) + logp (w | x) and maximization of JMVAE-kl have the same meaning as optimizing the same network as well, because both parameterized p(z|w;\u03b8) and q(z|w; \u03b8) can be expressed in the same network. \n\nAnyway, the way of writing the current proof is not very correct, so we will fix it as soon as possible.\n\nThanks,\n\nMasahiro"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641042, "id": "ICLR.cc/2017/conference/-/paper284/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk8rlUqge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper284/reviewers", "ICLR.cc/2017/conference/paper284/areachairs"], "cdate": 1485287641042}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481719347822, "tcdate": 1478283325857, "number": 284, "id": "Hk8rlUqge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hk8rlUqge", "signatures": ["~Masahiro_Suzuki1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": ["BkL7bONFe"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480700994792, "tcdate": 1480700994786, "number": 2, "id": "BJiH4Ey7x", "invitation": "ICLR.cc/2017/conference/-/paper284/pre-review/question", "forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "signatures": ["ICLR.cc/2017/conference/paper284/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper284/AnonReviewer2"], "content": {"title": "Question on Equation (5)", "question": "I am not entirely sure whether the proof in Appendix A is correct. Especially, in the first equality in Equation (5), authors replace p(z|w) into q(z|w) (and similarly for p(z|x) and q(z|x)), but this doesn't seem correct since p(z|w) and p(z|x) are posteriors, not proposal distribution under a proposed generative model p(x|z)p(w|z)p(z)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959362078, "id": "ICLR.cc/2017/conference/-/paper284/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper284/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper284/AnonReviewer1", "ICLR.cc/2017/conference/paper284/AnonReviewer2"], "reply": {"forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959362078}}}, {"tddate": null, "tmdate": 1480576468566, "tcdate": 1480576468540, "number": 3, "id": "HkhATBpGx", "invitation": "ICLR.cc/2017/conference/-/paper284/public/comment", "forum": "Hk8rlUqge", "replyto": "HkYlp79zl", "signatures": ["~Masahiro_Suzuki1"], "readers": ["everyone"], "writers": ["~Masahiro_Suzuki1"], "content": {"title": "Masking", "comment": "Thank you very much for your valuable comments.\n\nAs you say, in settings such as collaborative filtering, it seems that it is common to train sparse data by randomly adding filters to the input at training time. However, the setting we tackle in our paper differs from these settings in the following points.\n\nFirst, an encoder of JMVAE is composed of a concatenation of multiple networks of \"different\" architecture corresponding to each modality (i.e, dense networks for attributes and labels, convolutional networks for images). Second, our objective is to exchange multiple modalities bi-directionally, so all inputs of a certain modality's network are \"completely\" missing at test time. In neural recommender systems, I think that all input layers are at the same level and which part of the input is missing depends on each sample.\n\nI have tried to add a binary mask to the input at training time or to put noise in place of zero value at test time, but we found that these attempts are not very effective when the completely missing input has large dimensions such as images.\n\nThus, we think that JMVAE-kl is effective when the encoder is composed of a concatenation of multiple networks and the input of the network corresponding to a certain modality becomes completely zero.\n\nThanks,\n\nMasahiro"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641042, "id": "ICLR.cc/2017/conference/-/paper284/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk8rlUqge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper284/reviewers", "ICLR.cc/2017/conference/paper284/areachairs"], "cdate": 1485287641042}}}, {"tddate": null, "tmdate": 1480371440721, "tcdate": 1480371440717, "number": 1, "id": "HkYlp79zl", "invitation": "ICLR.cc/2017/conference/-/paper284/pre-review/question", "forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "signatures": ["ICLR.cc/2017/conference/paper284/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper284/AnonReviewer1"], "content": {"title": "Masking", "question": "Hello, \n\nModeling multimodal data is indeed an interesting problem. However, I fail to understand what motivated you to pick a JMVAE-kl model over, for example, training a VAE with random masking. You could use random binary mask b_x and b_w and then use an encoder q(z|b_x * x, b_w * w, b_x, b_w). This kind of modeling using binary masks as input is common practice in neural recommender systems and seems more principled. Did you try that approach ? How does it compare to the current path you are taking ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959362078, "id": "ICLR.cc/2017/conference/-/paper284/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper284/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper284/AnonReviewer1", "ICLR.cc/2017/conference/paper284/AnonReviewer2"], "reply": {"forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959362078}}}, {"tddate": null, "tmdate": 1479725270720, "tcdate": 1479725270714, "number": 2, "id": "Bk11Z8xGl", "invitation": "ICLR.cc/2017/conference/-/paper284/public/comment", "forum": "Hk8rlUqge", "replyto": "SJQ0eh2-e", "signatures": ["~Masahiro_Suzuki1"], "readers": ["everyone"], "writers": ["~Masahiro_Suzuki1"], "content": {"title": "Related work", "comment": "Hi Diane.\n \nThank you very much for your feedback.\n \nWe didn't cite this Cadena's paper in our paper, but this is an application study with MAE which we cited in our paper.  The main differences between theirs and our method, JMVAE, are as follows:\n \n\u30fbJMVAE is based on VAEs, deep generative models, while MAE used in Cadena's paper is based on AEs,  deep discriminative models.  For this reason, JMVAE can obtain joint representation which captures manifold as well as is suitable for reconstruction.  Therefore, we can generate modalities from corresponding other modalities even if they are varied (such as Fig. 5).\n\n\u30fbBoth of Cadena's and our papers have same goal of trying to generate missing modalities at test time, but we found that they can't be generated properly when each modality has different dimensions (e.g. images and texts) and architectures (e.g. dense and convolutional), which are shown in our results. So we proposed JMVAE-kl and our results showed that it works well even if we use both convolutional and dense networks. In Cadena's paper, they only used dense and relatively shallow networks for each modality and considered that each modality had same dimensions and architectures.\n\nThanks,\n\nMasahiro"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641042, "id": "ICLR.cc/2017/conference/-/paper284/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk8rlUqge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper284/reviewers", "ICLR.cc/2017/conference/paper284/areachairs"], "cdate": 1485287641042}}}, {"tddate": null, "tmdate": 1479487691532, "tcdate": 1479487691526, "number": 1, "id": "SJQ0eh2-e", "invitation": "ICLR.cc/2017/conference/-/paper284/public/comment", "forum": "Hk8rlUqge", "replyto": "Hk8rlUqge", "signatures": ["~Diane_Nicole_Bouchacourt1"], "readers": ["everyone"], "writers": ["~Diane_Nicole_Bouchacourt1"], "content": {"title": "Related work", "comment": "Have you considered the following related work : \"Multi-modal Auto-Encoders as Joint Estimators for\nRobotics Scene Understanding\" Cadena et al. ? Could you explain how your method differs from theirs ?\n\nThanks,\n\nDiane"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Joint Multimodal Learning with Deep Generative Models", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "pdf": "/pdf/cb01f96bb430eeac976c1e4ea3a12095c97881a8.pdf", "paperhash": "suzuki|joint_multimodal_learning_with_deep_generative_models", "conflicts": ["u-tokyo.ac.jp"], "keywords": [], "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287641042, "id": "ICLR.cc/2017/conference/-/paper284/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk8rlUqge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper284/reviewers", "ICLR.cc/2017/conference/paper284/areachairs"], "cdate": 1485287641042}}}], "count": 11}