{"notes": [{"id": "SylKikSYDH", "original": "SklETRAuwS", "number": 1922, "cdate": 1569439648811, "ddate": null, "tcdate": 1569439648811, "tmdate": 1583912054388, "tddate": null, "forum": "SylKikSYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "HuSNXEo6H", "original": null, "number": 6, "cdate": 1580164347314, "ddate": null, "tcdate": 1580164347314, "tmdate": 1580164347314, "tddate": null, "forum": "SylKikSYDH", "replyto": "vPvdG8Lvb2", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment", "content": {"title": "Principled As Well As Practical SOTA Benchmarking", "comment": "I agree that the time is long-since past for a prize based on an expanded corpus, such as the one you are (soon?) going to publish from Project Gutenberg. This should have been done by someone with deep-pockets, i.e. Google with the advent of the one billion word benchmark, because it came years after Hutter's enwik8 prize.\n\nBut I would strongly suggest that any move forward toward such a benchmark specify the minimum _practical_ common algorithmic resources (Universal Turing Machine instruction set) upon which to run a decompression program that produces the benchmark corpus. \n\nBy \"_practical_\" I admit that nowadays it may only be _practical_ to assume an \"instruction set\" consisting of the entire Tensorflow API -- particularly for a DeepMind-financed benchmark prize.\n\nAs for seeing what language model generalizes best, there are two quite distinct levels to this question:\n\n1) Empirical testing of the MDL principle in SOTA claims.\n2) Application of the MDL principle in SOTA claims.\n\nPhilosophically, the MDL principle is already assumed in virtually all science and engineering due to \"the unreasonable effectiveness of mathematics in the natural sciences.\"  So, on that basis, #1 is similarly assumed by those, such as Hutter, who finance #2, as would a DeepMind prize based on size of self-extracting archive.\n\n#1  is for who don't place their \"faith\" in such philosophical arguments, and is where tests, such as yours based on division of test and training sets, can do more than just measure \"what generalizes best\":  They can, through model compression/model ablation/knowledge distillation, see if MDL* holds as a meta-empirical truth.\n\nSee \"Extreme Language Model Compression with Optimal Subwords and Shared Projections\"\n\nhttps://arxiv.org/abs/1909.11687\n\n*By \"MDL\" I am here assuming UTM algorithmic capacity in the description's language."}, "signatures": ["~James_A_Bowery1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~James_A_Bowery1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187859, "tmdate": 1576860580304, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment"}}}, {"id": "vPvdG8Lvb2", "original": null, "number": 14, "cdate": 1580139418941, "ddate": null, "tcdate": 1580139418941, "tmdate": 1580139418941, "tddate": null, "forum": "SylKikSYDH", "replyto": "t2u50Z4XfZ", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"title": "RE", "comment": "I think I agree with a lot of your comment. Just to be clear, although we use enwik8 as a dataset for language modelling, we have no stake in the Hutter Prize. This model, along with pretty much all neural network language models trained on this dataset, are too large to be competitive with the algorithms devised by Rhatushnyak. If the prize had been devised using 10GB of wikipedia then it would be a different story. There are lots of tricks to cut the final parameter count (e.g. make some of the linears low-rank, prune the weights, distill the large model to a smaller model etc.) if one wants to benchmark models at a fixed parameter budget. Our opinion is that it's a worthwhile pursuit to see what language model generalizes best irrespective of parameter size. Simply scaling the transformerxl to a larger no. parameters via larger width or a larger number of layers did not improve generalization. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "t2u50Z4XfZ", "original": null, "number": 5, "cdate": 1580021662717, "ddate": null, "tcdate": 1580021662717, "tmdate": 1580021717181, "tddate": null, "forum": "SylKikSYDH", "replyto": "HyxIZ7ZaYr", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment", "content": {"title": "Commensurability of Table 4 Items", "comment": "The judging* criterion for the Hutter Prize is size of a self-extracting archive of the enwik8 corpus, to standardized on the algorithmic resources available to the archive.  This is essential for commensurability under the principle of minimum description length (MDL) approximation of Kolmogorov Complexity.  Dividing the corpus into training and testing sets is neither necessary nor desirable under this metric.\n\nControlling for the same \"model setup\" is a big step in the right direction -- as it increases the commensurability with TransformerXL -- particularly as compared to the other items in Table 4.  While model ablation can produce even more commensurable measures, it would be helpful for SOTA comparisons to be more rigorous in defining the algorithmic resources assumed in their measurements.\n\nA consequence of improved rigor would be to expose just how important \"small\" improvements, such as .99 to .97 can  be, as indeed they are.\n\n*I'm on the Hutter Prize judging committee."}, "signatures": ["~James_A_Bowery1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~James_A_Bowery1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187859, "tmdate": 1576860580304, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment"}}}, {"id": "X1S0P5GNA", "original": null, "number": 1, "cdate": 1576798735982, "ddate": null, "tcdate": 1576798735982, "tmdate": 1576800900388, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposes a \"compressive transformer\", an extension of the transformer, that keeps a compressed long term memory in addition to the fixed sized memory.  Both memories can be queried using attention weights.  Unlike TransfomerXL that discards the oldest memories, the authors propose to \"compress\" those memories.  The main contribution of this work is that that it introduces a model that can handle extremely long sequences. The authors also introduces a new language modeling dataset based on text from Project Gutenberg that has much longer sequences of words than existing datasets.  They provide comprehensive experiments comparing against different compression strategies and compares against previous methods, showing that this method is able to result in lower word-level perplexity. In addition, the authors also present evaluations on speech, and image sequences for RL.\n\nInitially the paper received weak positive responses from the reviewers. The reviewers pointed out some clarity issues with details of the method and figures and some questions about design decisions. After rebuttal, all of the reviewers expressed that they were very satisfied with the authors responses and increased their scores (for a final of 2 accepts and 1 weak accept).\n\nThe authors have provided a thorough and well-written paper, with comprehensive and convincing experiments. In addition, the ability to model long-range sequences and dependencies is an important problem and the AC agrees that this paper makes a solid contribution in tackling that problem.  Thus, acceptance is recommended.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718274, "tmdate": 1576800268732, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Decision"}}}, {"id": "HkxPYrrhYS", "original": null, "number": 1, "cdate": 1571734911102, "ddate": null, "tcdate": 1571734911102, "tmdate": 1574656149824, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposes a way to compress past hidden states for modeling long sequences. Attention is used to query the compressed representation. The authors introduce several methods for compression such as convolution, pooling etc. The outcome is a versatile model that enables long-range sequence modeling, achieving strong results on not only language model tasks but also RL and speech. For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG-19, a new benchmark based on Project Gutenberg narratives. \n\nThe idea is a simple and straightforward one. The choices of compression functions are intuitive and natural. The probably more interesting part of this paper is the training schemes designed to train the memory compression network. \n\nResults are very strong and there is a pretty diverse set of experiments. That said,  it seems like a huge amount of resources were spent on this work alone. It also seems like these models are not trivial to train (or get them to work). It would be interesting to find out how much resources were spent (in terms of preliminary experiments) to getting these models to start working decently. There are also no reports of parameter counts, which might make the experiments unfair. \n\nAchieving SOTA is one thing, which could be attributed to large resource pools and maybe larger parameter sizes of models.\n\nOverall, I am voting for a weak accept. While this paper is more incremental and novelty may be slightly lacking, I think the breadth of experiments and competitive results warrants an acceptance. \n\nSeveral issues and questions for the authors:\n\n1) Why are the results on PG-19 not reported in a Table format? Why are there no results of the base Transformer on PG-19? I think this is really necessary and should be reported.\n2) The authors mention that this memory compression architecture enables long sequence modeling. However, is there an intended way of use for long-text that is not necessarily framed as a LM problem? For instance, results on NarrativeQA benchmark would be nice. \n\nUPDATE: I have read the author response and other reviewer's comments. I am happy with the efforts made by the authors and I am raising my score to 8 (accept). \n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576090576928, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Reviewers"], "noninvitees": [], "tcdate": 1570237730348, "tmdate": 1576090576941, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Review"}}}, {"id": "rkloDrohKB", "original": null, "number": 2, "cdate": 1571759459054, "ddate": null, "tcdate": 1571759459054, "tmdate": 1574441031635, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "## Updated review\n\nI have read the rebuttal. First I'd like to thank the authors for the detailled rebuttal. \nThe latest version of the paper adressed all my concerns, hence I change my rating to Accept.\n\n## Original review\n\nThis paper presents a new variation of the Transformer model, named Compressive Transformer. The key novelty of this model is to preserve long range memory in a compressed form, instead of discarding them as previous models have done. This improves the long-range dependencies modelling capabilities of the approach. The model is evaluated on two common language modelling benchmarks and yields state of the art results in both of them. The paper also introduces a new benchmark for long-range dependencies modelling composed of thousands of books. The paper finally presents an analysis of the compressed memory and provide some insights, including the fact that the attention model uses the compressed memory. The model is also evaluated on two other tasks: speech generation and reinforcement learning on videos.\n\nI think this paper should be accepted, mainly because:\n- The proposed model is novel as far as I can tell. \n- The presented approach is significant, as modelling long-range dependencies is an important milestone in sequence modelling.\n- The new benchmark is a good addition.\n- The comparison with the relevant literature is thorough and well done.\n- The experiments are convincing and demonstrate the viability of the approach, although some aspects can be improved (see below).\n\nDetailed comments:\n- About the character-level language modelling on Enwik8, the improvement is very small, it seems that the task doesn't benefit from have long-range memory, could it be because character-level modelling is less dependent on the long-range past? can the authors comment on that? It would also been interesting to evaluate the gain of the memory, for instance by varying the size of the compressed memory from 0 to 1152. \n- The WikiText-103 evaluation is interesting, specially Table 6, which shows the advantages of the model. However when comparing with the literature, it's not clear if the performance gain is due to the compressed memory or to the network capacity. A study with different lengths of the compressed memory (starting at 0) would bring some insights about that.\n- In Section 5.6.2: can the authors justify why the attention weights were split in only 6 bins? creating a trended curve on only 6 points could be problematic, and I don't see why more bins couldn't be used.\n- The speech analysis section (5.7) is not very insightful. It shows that the proposed model is on par with WaveNet on unconstrained speech generation, which is not very useful and feels a bit half-finished. I think that the authors should either commit to this study by constraining the model with linguistic features like in (Oord et al. 2018) and evaluate it in a TTS framework with subjective evaluation or discard this section entirely. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576090576928, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Reviewers"], "noninvitees": [], "tcdate": 1570237730348, "tmdate": 1576090576941, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Review"}}}, {"id": "Hylu-DRpYH", "original": null, "number": 3, "cdate": 1571837695714, "ddate": null, "tcdate": 1571837695714, "tmdate": 1574201044635, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper investigates a so-called \"compressive transformer\" approach. The idea is to compress distant past memories into a coarse-grained representation while keeping a fine-grained representation for close past memories.  A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning. Particularly, the authors propose a new benchmark PG-19 for long-term sequence modeling.  \n\nOverall, I found the work interesting and experiments are thorough and strong.   It is always great to see a new benchmark released to the community.  That being said, I have concerns regarding the paper.  The authors put huge amount of effort into the experiments but only describe the proposed technique in a very rough and abstract way, lacking necessary technical details to formulate the technique. What is the mathematical formulation of the problem?  How exactly the compression is carried out on various network architectures is not clear after reading the paper.  Also, I guess many readers including me do not have a perfect understanding of Fig. 1 although it shows something intuitively. (What is the difference between different colors? What is the difference between sequence, memory, and compressed memory?  What do the arrows mean? There is no explanation whatsoever either in the figure or in the caption).  This is the major concern I have regarding the paper.  Despite of the strong experimental presentation, lacking the technical details has significantly hurt the quality of the paper.  \n\nP.S.  Thanks for the rebuttal.  I have lifted my score. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576090576928, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Reviewers"], "noninvitees": [], "tcdate": 1570237730348, "tmdate": 1576090576941, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Review"}}}, {"id": "ryl_UkcYoH", "original": null, "number": 8, "cdate": 1573654352283, "ddate": null, "tcdate": 1573654352283, "tmdate": 1573654352283, "tddate": null, "forum": "SylKikSYDH", "replyto": "BkgQZoLdoB", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"title": "^", "comment": "Fixed some typos and further clarified algorithm box in paper update. Please feel free to scan over the revised text and express any other points of concern!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1922/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "BkgQZoLdoB", "original": null, "number": 7, "cdate": 1573575418698, "ddate": null, "tcdate": 1573575418698, "tmdate": 1573575418698, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"title": "Updated paper", "comment": "Thanks for the comprehensive reviews, they have certainly improved the quality of the paper.\n\nList of changes:\n\n[credit to reviewer 1]\n- Updated figure 1 and caption with more details.\n- Re-written model section: added formal notation, added algorithm box for full model, and for attention-reconstruction loss.\n- Added subsection on temporal receptive field.\n\n[credit to reviewer 2]\n- Attention bins are more granular, include uncertainty over attention per bucket. Remember, the self-attention is causally masked (mentioned in the text) thus the increase in attention to earlier sequence. Crucially, there is an increase in attention from the oldest memories, to the newest compressed memories (which are older).\n- Added memory size ablations (Table 8 & 9).\n\n[credit to reviewer 3]\n- Added PG-19 results table with Compressive Transformer and TransformerXL (improved both models from original result, using deeper networks).\n\nWe appreciate the reviewers have a limited time to read paper revisions, however we feel almost all points have been substantially addressed and thus we would strongly welcome feedback."}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "B1e2NFwSjH", "original": null, "number": 6, "cdate": 1573382452461, "ddate": null, "tcdate": 1573382452461, "tmdate": 1573382452461, "tddate": null, "forum": "SylKikSYDH", "replyto": "HkxPYrrhYS", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"title": "Re resources, training difficulty, other text applications, PG-19 results", "comment": "Thank you for your thorough review!\n\nRe. \u201cIt would be interesting to find out how much resources were spent (in terms of preliminary experiments) to getting these models to start working decently.\u201d \n\nThe majority of experiments were spent reproducing the sota (at the time) TransformerXL; that is, getting the model and training setup working well. We plan to open-source the TransformerXL baseline alongside the Compressive Transformer in TensorFlow (The TXL is now open-sourced in a few locations also). We considered 7 model/loss compressive transformer variants, displayed in Table 4, and ran 16 experiments in total on enwik8. These experiments swept over compression rates (typically 1-4) and then we experimented with different model setups. We then ran 6 compressive transformer experiments on WikiText-103. \n\nRe \u201cIt also seems like these models are not trivial to train (or get them to work)\u201d\nWe trained these models with the same parameters as the transformerxl and we basically found (as shown in Table 4) that pretty much all compression approaches worked ok. Even mean-pooling activations performed reasonably (exceeded baseline performance and matched the current sota). However the learnable conv1d performed the best. The optimization schedule of decreasing optimization updates (S5.6.3) allowed us to achieve better results but this wasn\u2019t necessary to train the models. So we would challenge the conclusion that this model is difficult to train. \n\nRe.  is there an intended way of use for long-text that is not necessarily framed as a LM problem? \u2026 Such as NarrativeQA\n\nWe think any sequential prediction problem with long-range dependencies is a good fit for this model. Ideally a streaming task where you need to maintain an online representation of the past that is quickly updated. So perhaps reading comprehension tasks where you read a book but periodically answer questions about it, a little like Children\u2019s Book Test but with longer contexts. For summarization, such as NarrativeQA, only one set of predictions needs to be made at the end of the book and it appears that the best solutions are (currently) maintaining the book statically in a simple embedded space and repeatedly attending to it, possibly copying sections of text. It would be interesting to see the results from simple autoregressive models for summarization nonetheless.\n\nRe. Why are the results on PG-19 not reported in a Table format? \n\nVery good point. We have remedied this, it is now in a table. We also have new results with larger models that serve as a better initial baselines\n\n36 layer TransformerXL (3,000 mem) \t\t\t\t\t                36.25\n36 layer Compressive Transformer (1,500mem + 1,500 CM)\t\t33.6\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "ByltkFPHsB", "original": null, "number": 5, "cdate": 1573382368809, "ddate": null, "tcdate": 1573382368809, "tmdate": 1573382368809, "tddate": null, "forum": "SylKikSYDH", "replyto": "rkloDrohKB", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"title": "Re model ablations, enwik8, attention weights & speech modelling", "comment": "Thank you for your kind review! \n\nRegarding memory size: here\u2019s an ablation with performance versus compressed memory size for both enwik8 and wikitext-103! Both models improve significantly as a function of compressed memory size from small values. There is an optimal value, if we make the compressed memory much larger than the training regime then performance eventually deteriorates as the model\u2019s attention drifts out-of-distribution (e.g. 4096+ for Enwik8). We have added this table to the paper also.\n\nEnwik8\nCompressed Memory Size\t         512\t       1024\t         2048\t3072\t4096\nBPC\t\t\t\t                         1.01\t0.99\t         0.98\t0.97\t        1.00\t\t\t\n(Model has a chunk size of 768 and memory size of 768)\n\nWikiText-103\nCompressed Memory Size\t  256\t512\t         1024\t1536\t2048\nPerplexity\t\t\t         18.2\t17.9\t         17.6\t17.1\t        17.7\n(Model has a chunk size of 256 and memory of size 512)\n\nNote that CM=0 is literally the TransformerXL which we have included results for in the paper (incl. our implementation). For the published TransformerXL\u2019s 18.3 perplexity, it was using an attention window of 1600 but we improve on this result with an attention window of only 768 (512 + 256).\n\nRe Enwik8: We agree the improvement on Enwik8 may seem quite small but this is partially due to the metric. BPC has a very small range. If we look at the word-level perplexity of these models, the 0.99bpc transformerxl has a word-level perplexity of 170 whereas the 0.97bpc sota compressive transformer has a word-level perplexity of 153. So a gain of 17 perplexity. This calculation comes from ppl_word = 2^(7.48 * bpc) as 7.48 is the average word-length in enwik8\u2019s test set. Enwik8 actually has a longer range of dependency over wikitext-103 because of the more granular sequence data; they both represent wikipedia pages but processing the article at the character-level stresses the model\u2019s range of attention.\n\nRe speech: It would be preferable to perform a full human quality survey. The observation we wanted to convey was that one can get a transformer-like model to model high-frequency speech unconditionally and the compressive model helped in obtaining learning dynamics that are comparable with wavenet (in comparison to the TransformerXL which performs worse). \n\nHowever we do not wish to claim that this implies we have a better text-to-speech model; this would require substantially more work, conditioning on linguistic features, and expert human raters. Instead of focusing on text-to-speech, we look at raw speech modelling which has many downstream applications beyond text-to-speech (e.g. speaker identification) and stresses long-range dependency. We have made this more clear in the text (update soon-to-be-posted), and will consider removing the results entirely if other reviewers feel the experiment is misleading.\n\nRe. why six attention bins? We just chose a multiple of 3 (so the buckets have boundaries at the compressed_memory, memory, sequence boundaries) that is not too large such that there\u2019s not too much noise. However we have re-run this analysis with 18 buckets and are including the updated figure in our (soo to be posted) updated paper. This is a better visualization of the data and captures the trend more carefully (we also remove the trend curve and switch to violin plots to better display the variability of each bucket). However the conclusion remains the same - that there is an increase in attention weight over the compressed memories versus the older regular memories"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "ByebjUDHjB", "original": null, "number": 4, "cdate": 1573381784951, "ddate": null, "tcdate": 1573381784951, "tmdate": 1573381784951, "tddate": null, "forum": "SylKikSYDH", "replyto": "Hylu-DRpYH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"title": "Re technical detal", "comment": "We completely agree that the model could be described more explicitly. *We are updating the paper with more mathematical details and an algorithm box to make things more explicit*. We originally wrote this paper to convey the key components of the model for those familiar with TransformerXLs, with the idea that all of the fine details are better represented in the code --- however we realize this was not the best strategy. We will still open-source the code so people can use the model and be certain of every detail, but we are completely re-writing the model section with the inclusion of an algorithm box. As pseudo-code here, the compression mechanism is really just passing memories that would otherwise be forgotten through a conv1d compression network:\n\ncompression_rate <- 3\nold_memory  <- memory[:-seq_size]  # the memories to be forgotten\ncompression_fn <- conv_1d(kernel_size=compression_rate, stride=compression_rate)\nnew_cm <- compression_fn(old_memory )  # new compressed memories\n\nThen for attention, before in the TransformerXL one would compute\nattention(seq, [memory, seq])\nwhereas here we compute\nattention(seq, [compressed_memory, memory, seq])\n\nBefore in the TransformerXL one would update memory by concatenating the sequence and truncating the oldest memories (to keep the memory fixed-size):\nmemory <- concat_and_truncate(memory, sequence)\n\nwhere 'concat_and_truncate' refers to:\ndef concat_and_truncate(old_state, new_state):\n    new_state_size <- new_state.shape[1]  # time dimension\n    return concat([old_state, new_state])[new_state_size:]  \n\nNow we update both the memory and compressed_memory:\nmemory <- concat_and_truncate(memory, sequence)\ncompressed_memory <- concat_and_truncate(compressed_memory, new_cm)\n\nIn Figure 1 we kept the sequence and memory the same colour, as these hidden activations represent information for a single time-step in the transformer. We use an arrow to indicate that we map a set of memories to a smaller set of compressed memories. We chose a different colour for the compressed memories (and made the ticks more frequent) to indicate that these represent information over multiple time-steps. We are updating the figure and caption with more details such that this is clearer.\n\nIf there is anything else that is unclear, feel free to give us feedback!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "rJgAnjgk5S", "original": null, "number": 3, "cdate": 1571912630033, "ddate": null, "tcdate": 1571912630033, "tmdate": 1571912630033, "tddate": null, "forum": "SylKikSYDH", "replyto": "B1eU8key9B", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"title": "re. Predicting the past", "comment": "This is a good point, one room for improvement is further analysis of whether the model's temporal range is indeed increased. The greater relative improvement in prediction of rare words (vs frequent words) hints that the performance improvement is due to longer-range reasoning, but it would be nice to make this more explicit. We could fine-tune a trained model on the task of predicting the past at varying intervals to see how it compares to the TXL. If we get time to perform this analysis before the discussion period is over, we will include these results. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "B1eU8key9B", "original": null, "number": 4, "cdate": 1571909453876, "ddate": null, "tcdate": 1571909453876, "tmdate": 1571909453876, "tddate": null, "forum": "SylKikSYDH", "replyto": "rkxqwxZatB", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment", "content": {"title": "Some thoughts", "comment": "Thank you very much for your response. It may be hard to compensate for the computation overhead due to self-attention at this moment, but I'm very excited for further development in this topic. I was also trying to make the context of Transformer unlimited as in this study (nevertheless not succeeded). \n\nOne thing I think is worth considering is, by generalize the idea of [1], to let the model to occasionally predict a randomly sampled segment of (either near or distant) past sequence (\"distant\" seq. comes from outside of the current TBPTT segment). Successful memory architecture should be able to recall the past easily, so this may become an alternative to measure how far your architecture can track back. Also, this may improve the retention of memory. \n\n[1] Learning Longer-term Dependencies in RNNs with Auxiliary Losses https://arxiv.org/abs/1803.00144"}, "signatures": ["~Aran_Komatsuzaki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Aran_Komatsuzaki1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187859, "tmdate": 1576860580304, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment"}}}, {"id": "HyxIZ7ZaYr", "original": null, "number": 2, "cdate": 1571783421977, "ddate": null, "tcdate": 1571783421977, "tmdate": 1571783421977, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylGq1FVdH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"comment": "We wanted to use the exact model setup from the TransformerXL, which we used as our baseline. So for WikiText-103 this was 18 layers with a hidden size of 1024 (16 heads), 4096 mlp hidden size, using the same adaptive input representations scheme to embed words. For Enwik8 we used the same 24 layer model, 1024 embedding and hidden size, 8 heads, 3072 mlp hidden size.\n\nIn terms of the number of parameters optimizing the loss, this is exactly the same as the TransformerXL 277M for Enwik8 and 257M for WikiText-103.\n\nFor the compression network, which was only optimized with respect to the auxiliary compression loss, this consumed 0 params for max/mean pooling, and most-used. For 1D conv it consumed 1M x compression_rate x #layers params, and for the dilated convolution it consumed more. We will update the paper with much more explicit model details since this is clearly a room for improvement.", "title": "re. model sizes"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "rkxqwxZatB", "original": null, "number": 1, "cdate": 1571782754234, "ddate": null, "tcdate": 1571782754234, "tmdate": 1571782754234, "tddate": null, "forum": "SylKikSYDH", "replyto": "BkeY9OZfdH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment", "content": {"comment": "Thanks so much for your comments and innovative line of thinking. This is something we have considered but had not come around to trying!\n\nI don't think it's infeasible, however there are several ways of using attention for compression, some of them are not desirable.\n\nE.g. if one has n memories to compress to n/c compressed memories. One could instantiate n/c learnable parameters, each performs attention over the memories to compress. This would certainly result in n/c compressed memories, where attention was used to perform the compression. This scheme could be effective, but it makes the scheme dependent on the memory size.\n\nAnother idea was to use the conv1D, or even pooling, to reduce the number of memories (from n -> n/c) and then do self-attention over this set to absorb information across. We did not try this but it seems reasonable. Conversely we could perform self-attention over the memories and then compress, we think this would be powerful but too expensive as it would effectively double the compute of the whole model.\n\nIf there's anything obvious we are missing, feel free to comment!", "title": "attention for compression"}, "signatures": ["ICLR.cc/2020/Conference/Paper1922/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1922/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1922/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1922/Authors|ICLR.cc/2020/Conference/Paper1922/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148929, "tmdate": 1576860546951, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Official_Comment"}}}, {"id": "SylGq1FVdH", "original": null, "number": 3, "cdate": 1570176905948, "ddate": null, "tcdate": 1570176905948, "tmdate": 1570176905948, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment", "content": {"comment": "What are the model sizes (the total number of parameters, hidden sizes, the number of heads, etc.) used in the language modeling experiments? I can't find it in the paper.", "title": "model sizes"}, "signatures": ["~Sainbayar_Sukhbaatar1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Sainbayar_Sukhbaatar1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187859, "tmdate": 1576860580304, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment"}}}, {"id": "BkeY9OZfdH", "original": null, "number": 2, "cdate": 1570015377508, "ddate": null, "tcdate": 1570015377508, "tmdate": 1570015377508, "tddate": null, "forum": "SylKikSYDH", "replyto": "Skxx2RqbOr", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment", "content": {"comment": "Seems possible but would be more costly (square vs linear). Would indeed be interesting to see if attention on data compressed with attention is better than attention on data compressed by convolutions is more effective. \n\n100 TPUv3 cores is a nice bunch of compute.", "title": "Re:"}, "signatures": ["~Artus_KG1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Artus_KG1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187859, "tmdate": 1576860580304, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment"}}}, {"id": "Skxx2RqbOr", "original": null, "number": 1, "cdate": 1569988263970, "ddate": null, "tcdate": 1569988263970, "tmdate": 1569988263970, "tddate": null, "forum": "SylKikSYDH", "replyto": "SylKikSYDH", "invitation": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment", "content": {"comment": "I believe you could've instead used (self)-attention to produce compressed memory. Or is this not viable for some reason?", "title": "(Self-)attention for compression"}, "signatures": ["~Aran_Komatsuzaki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Aran_Komatsuzaki1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jwrae@google.com", "apotapenko@google.com", "sidmj@google.com", "chillier@google.com", "countzero@google.com"], "title": "Compressive Transformers for Long-Range Sequence Modelling", "authors": ["Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chloe Hillier", "Timothy P. Lillicrap"], "pdf": "/pdf/17ec86b2097b2959e8da394a7633d9c6bef1aca1.pdf", "TL;DR": "Long-range transformer using a compressive memory, achieves sota in wikitext-103 and enwik8 LM benchmarks, release a new book-level LM benchmark PG-19.", "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "keywords": ["memory", "language modeling", "transformer", "compression"], "paperhash": "rae|compressive_transformers_for_longrange_sequence_modelling", "_bibtex": "@inproceedings{\nRae2020Compressive,\ntitle={Compressive Transformers for Long-Range Sequence Modelling},\nauthor={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylKikSYDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2c4f5ecc625a4d130818f86a3da7e20ea1a761d7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylKikSYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187859, "tmdate": 1576860580304, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1922/Authors", "ICLR.cc/2020/Conference/Paper1922/Reviewers", "ICLR.cc/2020/Conference/Paper1922/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1922/-/Public_Comment"}}}], "count": 20}