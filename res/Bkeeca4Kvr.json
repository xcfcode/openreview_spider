{"notes": [{"id": "Bkeeca4Kvr", "original": "Sye9bPTPwS", "number": 693, "cdate": 1569439111765, "ddate": null, "tcdate": 1569439111765, "tmdate": 1583912026746, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Rrine5bsTk", "original": null, "number": 1, "cdate": 1576798703457, "ddate": null, "tcdate": 1576798703457, "tmdate": 1576800932578, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The authors propose a method for few-shot learning for graph classification. The majority of reviewers agree on the novelty of the proposed method and that the problem is interesting. The authors have addressed all major concerns.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723271, "tmdate": 1576800274720, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper693/-/Decision"}}}, {"id": "SyeJ_2r0tH", "original": null, "number": 1, "cdate": 1571867751102, "ddate": null, "tcdate": 1571867751102, "tmdate": 1574719547023, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper introduces few-shot graph classification problem and proposes super-class based graph neural network (GNN) to solve it. Experiments on two datasets demonstrate that the proposed model outperforms a number of baseline methods. Some ablation study and analysis are also provided. Followings are my detail review. \n\nIt is interesting for the authors to introduce few-shot graph classification problem which is meaningful. If I understood correctly, the authors use graph spectral distance to find prototype graph of each class, then employ prototype graph clustering to obtain super-classes, which are further fed to GNN as the joint optimization of super-class and regular class prediction. To me, the novelty is incremental. \n\nThe authors use two new datasets for experiments due to the requirement of numerous class labels. I concerned about performances of different GNN baseline methods in Letter data due to small graph size (with 4.6 nodes in average). The context neighbor information is important for multi-layer GNN. Thus I could not fully judge the effectiveness of proposed model in this data. \n\nIn Table 3, I found performance of proposed model with one super-class is still better than different GNN. I did get the point from this result. Why there is no performance decrease as all have the same super-class label? What is the model performance when removing super-class augmentation? I would like to see more discussion or experiment about this. \n\nUpdate: I am satisfied with author's response and raised my score. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper693/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576388262916, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper693/Reviewers"], "noninvitees": [], "tcdate": 1570237748453, "tmdate": 1576388262937, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Review"}}}, {"id": "B1x0N0pmsr", "original": null, "number": 7, "cdate": 1573277238161, "ddate": null, "tcdate": 1573277238161, "tmdate": 1573280912990, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "rylOp2aXsH", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment", "content": {"title": "Response to Reviewer 3, Part 2", "comment": "R3Q3: Why there is no performance decrease as all have the same superclass label?\nR3A3: While all the graphs possess the same superclass label (SC), there is still a supergraph (k-NN graph) constructed on ALL the graph feature vectors belonging to the single SC, which captures inter-class information flow. The only difference is that with more superclasses, there are separate k-NN graphs for each superclass and there is no unnecessary information flow between classes that don\u2019t belong to the same superclass, whereas in the 1-SC situation, we end up relaxing this restriction and have just a single k-NN graph across all class labels grouped under the single SC. This supergraph is then fed to $C^{GAT}$ which still learns and manages to further increase class separation that was present in the feature vectors extracted by GIN. Therefore, even with a single SC our $C^{GAT}$ manages to outperform other baseline GNNs.\n\nWe further generated t-SNE plots and a silhouette coefficient table (refer to Appendix section 7) to study the class-separation effect of \u201cjust a GIN\u201d, versus our method which uses a \u201cGIN followed by a GAT with 1 SC\u201d and other best performing number of superclasses for each dataset. We observe that our method with 1 SC outperforms both GIN and WL-kernel. \nSILHOUETTE COEFFICIENT TABLE\n+-------------------------+-------------------+-----------------+-----------------------+--------------------------+--------+\n|    Method              |     Reddit-12K        |    ENZYMES       |    Letter-High     |    TRIANGLES    |\n+-------------------------+-------------------+------------------+-------------------+------------------+-------------------+\n|                                 | 1-SC     | 2-SC       |  1-SC   |  2-SC    |  1-SC   |   3-SC   |   1-SC   |   3-SC   |\n| GIN                         | -0.0652 | -0.0652 | 0.0432 | 0.0432 | 0.2316 | 0.2316 | 0.1256 | 0.1256 |\n| WL Kernel              | -0.0626 | -0.0626 | 0.0366 | 0.0366 | 0.2490 | 0.2490 | 0.0186 | 0.0186 |\n| OurMethod-GAT  | -0.0593 | -0.0559 | 0.1172 | 0.0989 | 0.3519 | 0.3787 | 0.3975 | 0.4508 | \n+-------------------------+-------------------+------------------+-------------------+------------------+-------------------+\n\nR3Q4: What is the model performance when removing super-class augmentation?\nR3A4: We refer you to the ablative studies conducted by us in Table 3 of the main paper which shows the comparison between \u201cNo super class\u201d and \u201cwith super class\u201d. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper693/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/Reviewers", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeeca4Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper693/Authors|ICLR.cc/2020/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167654, "tmdate": 1576860554073, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment"}}}, {"id": "rylOp2aXsH", "original": null, "number": 6, "cdate": 1573276864101, "ddate": null, "tcdate": 1573276864101, "tmdate": 1573280903025, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "SyeJ_2r0tH", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment", "content": {"title": "Response to Reviewer 3, Part 1", "comment": " We thank the reviewer for his comments and observations. Following are the answers to each question/comment you have raised.\n\nR3Q1: \u201cIt is interesting for the authors to introduce few-shot graph classification problem which is meaningful. If I understood correctly, the authors use graph spectral distance to find prototype graph of each class, then employ prototype graph clustering to obtain super-classes, which are further fed to GNN as the joint optimization of super-class and regular class prediction. To me, the novelty is incremental.\u201d\nR3A1: Yes, your summary can serve as a high-level coarse overview of our work. Regarding the novelty being incremental, we respectfully disagree due to the following reasons. \nThe task of few-shot learning on graphs to begin with is novel (as is also pointed out by you and all the other reviewers) and extremely challenging as there are no previous works in this few-shot setting on graphs. The introduction of using the $L_p$-Wasserstein distance between the spectrum of graphs in order to build a supergraph and also cluster class-labels into superclasses is also novel. We are not aware of any graph NNs that have introduced a graph spectral Wasserstein distance between graphs for classification.\n\nWe believe our superclass construction per batch embodies the \u201cfull context embedding\u201d feature of matching networks [1], which essentially capture a wider context of the support set per batch and not just a single graph and prototype networks [2] by using precomputed Wasserstein prototypes to decide the superclasses that allow information flow between various classes too. Our architecture that jointly learns the superclass and regular class using two-phase (training and fine-tuning phase) training is also a novel construction. Finally, we have also shown our method works well in the semi-supervised and adaptive learning setting (Appendix A.5 and A.6). \n[1] Vinyals et~al. \u201cMatching Networks for One Shot Learning\u201c, NIPS 2016\n[2] Snell et~al. \u201cPrototypical Networks for Few-shot Learning\u201d, NIPS 2017\n\nR3Q2: Low average number of nodes per graph in the datasets used.\nR3A2: Thanks for pointing this out. We found two widely-used graph datasets in graph classification literature with larger average number of nodes, namely Enzymes and Reddit-12K.\nWe conducted all our experiments (including sensitivity and ablation studies) on both these datasets and the results have been added to the revised paper. Enzymes has 33 nodes per graph and Reddit-12K has 391 nodes per graph on average. \nPlease refer to the table 2 for new results. We find that our results show a marked improvement on the new datasets as well. \n+-------------------------+-----------------------------------------------------------+-----------------------------------------------------------+\n|    Method               |                            Reddit-12K                             |                              Enzymes                               |\n+-------------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+\n|                                 |       5SHOT     |      10SHOT    |      20SHOT    |       5SHOT     |      10SHOT    |      20SHOT    |\n| WL                          | 40.26 +- 5.17 | 42.57 +- 3.69 | 44.41 +- 3.43 | 55.78 +- 4.72 | 58.47 +- 3.84 | 60.10 +- 3.18 |\n| Graphlet                | 33.76 +- 6.94 | 37.59 +- 4.60 | 41.11 +- 3.71 | 53.17 +- 5.92 | 55.30 +- 3.78 | 56.90 +- 3.79 |\n| AWE                        | 30.24 +- 2.34 | 33.44 +- 2.04 | 36.13 +- 1.89 | 43.75 +- 1.85 | 45.58 +- 2.11 | 49.98 +- 1.54 |\n| Graph2Vec            | 27.85 +- 4.21 | 29.97 +- 3.17 | 32.75 +- 2.02 | 55.88 +- 4.86 | 58.22 +- 4.30 | 62.28 +- 4.14 |\n| Diffpool                 | 35.24 +- 5.69 | 37.43 +- 3.94 | 39.11 +- 3.52 | 45.64 +- 4.56 | 49.64 +- 4.23 | 54.27 +- 3.94 |\n| CapsGNN              | 36.58 +- 4.28 | 39.16 +- 3.73 | 41.27 +- 3.12 | 52.67 +- 5.51 | 55.31 +- 4.23 | 59.34 +- 4.02 |\n| GIN                        | 40.36 +- 4.69 | 43.70 +- 3.98 | 46.28 +- 3.49 | 55.73 +- 5.80 | 58.83 +- 5.32 | 61.12 +- 4.64 |\n| GIN-k-NN              | 41.31 +- 2.84 | 43.58 +- 2.80 | 45.12 +- 2.19 | 57.24 +- 7.06 | 59.34 +- 5.24 | 60.49 +- 3.48 |\n| OurMethod-GCN | 40.77 +- 4.32 | 44.28 +- 3.86 | 48.67 +- 4.22 | 54.34 +- 5.64 | 58.16 +- 4.39 | 60.86 +- 3.74 |\n| OurMethod-GIN  | 41.59 +- 4.12 | 45.67 +- 3.68 | 50.34 +- 2.71 | 55.42 +- 5.74 | 60.64 +- 3.84 | 62.81 +- 3.56 |\n+-------------------------+-------------------+------------------+-------------------+------------------+-------------------+------------------+\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeeca4Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper693/Authors|ICLR.cc/2020/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167654, "tmdate": 1576860554073, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment"}}}, {"id": "S1x3ALTQjS", "original": null, "number": 3, "cdate": 1573275347962, "ddate": null, "tcdate": 1573275347962, "tmdate": 1573280890988, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "ryxUOGLHcH", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for his comments and observations. Following are the answers to each question you have raised.\nR1Q1(a): \u201cThe classification of base class into super classes seems questionable to me. In the meta-learning language, the author attempts to learn a good representation of graphs based on different graph classification tasks generated by a task distribution. In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).\u201d \nR1A1(a): Our model has two major components, $C^{sup}$ (for super-class prediction) and $C^{GAT}$ (for graph label prediction). During the training phase of our classification, $C^{sup}$, which is a MLP layer, learns the super-class labels of the samples based on GIN\u2019s extracted feature vectors (which represent base class labeled graphs). While, $C^{GAT}$ takes as input the \u201cgraph of graphs\u201d (supergraph) which models the latent inter-class as well as intra-class information and is constructed in every training batch, along with base-class labels, to learn the associated class distribution. \nThen, during the fine-tuning phase on graphs with novel class labels, the feature extractor\u2019s (GIN) parameters are fixed and $C^{sup}$ is used to infer the super-class label of the novel class labeled graphs. Then, the parameters learned by $C^{GAT}$ get updated and further \u201cfine-tuned\u201d for better performance on the novel samples.\nIn addition to our brief overview, you could also find a very neatly detailed summarization of our method in reviewer 2\u2019s comments (paragraphs 1-4). \n\nThe meta-learning framework, where batches are sampled as \u201cepisodes\u201d with N-way K-shot setting, does not perform as well in our few-shot setting on graphs for the following reasons:\n1) We have very limited total number of training classes (in order of 10s), when compared to the image domain (order of 100s and 1000s). This limitation hampers learning across tasks and generalization to new unseen tasks.  \n2) In each of our batches, we randomly sample a fixed-size of training samples belonging to the set of N labels chosen. Therefore, when building our supergraph, we end up with k-NN graphs of \u201cvariable size\u201d per super-class, compared to fixed size (K nodes) k-NN graphs that we would have got using episodic learning. We suspect this further allows our GAT to learn and generalize better to unseen graphs. \nFurthermore, in [1], the authors use a similar strategy in their \u201cbaseline++\u201d method and produce good results. Their findings are also in sync with our empirical finding.  \n[1] Chen et~al. \u201cA closer look at Few-Shot Classification\u201d, ICLR 2019\n\nR1Q1(b): Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.\u201d\nR1A1(b): In every batch of graphs during both training and fine-tuning phase, each graph is associated with its corresponding graph label. In case of training, its a base-class and in the case of fine-tuning its a novel class. In the case of $C^{GAT}$, the graph is accompanied by a regular class label and in case of $C^{sup}$, the graph is accompanied by a superclass label.\n\nR1Q2: \u201cThough seemingly very important to the architecture, the purpose of constructing the super-graph $g^{sup}$ in the training of $C^{CAT}$ seems to be unclear to me.\u201d\nR1A2: What makes few-shot learning particularly difficult compared to common machine learning settings is the dearth of training examples, which results in a bad empirical risk approximation for the expected risk and therefore gives rise to an empirical risk minimizer that is sub-optimal. Reducing the required sample complexity can result in a better empirical risk minimizer. Therefore, given a very large space of hypotheses H, our goal is to further restrict and constrain H using some prior knowledge because a reduced H has reduced sample complexity and thus requires fewer training samples to be trained. We provide this \u201cprior knowledge\u201d in the form of a \u201cgraph of graphs\u201d, namely our super-graph $g^{sup}$, which captures both the latent inter-class and intra-class relationships between classes. Observe that in $g^{sup}$, we build a k-NN graph PER super-class, restricting any flow of information between super-classes, thus further restricting H. We force our model to jointly learn both the superclass and graph class labels. This way similar classes (grouped under a superclass) together contribute to learning a general prior representing the superclasses and each superclass also provides \u201cguidance\u201d to better train with the few samples assigned to that superclass. \t\nThe introduction of this prior knowledge in the form of a supergraph in $C^{GAT}$ during training also helps generalize better to the novel samples that are presented to our model in the fine-tuning stage.\n\nAdditionally, we would also like to draw attention to the supergraph usage summary provided by reviewer 2 in paragraph 3 of their comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeeca4Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper693/Authors|ICLR.cc/2020/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167654, "tmdate": 1576860554073, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment"}}}, {"id": "HyxyssTQsr", "original": null, "number": 5, "cdate": 1573276567296, "ddate": null, "tcdate": 1573276567296, "tmdate": 1573280877172, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "BylX4567or", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment", "content": {"title": "Response to Reviewer 2, Part 2", "comment": "R2Q5: Discussion of few-shot in graphs vs few-shot in images.\nR2A5: Few shot learning (FSL) has gained wide-spread traction in the image domain in recent years. However the success of FSL in images is not easily translated to the graph domain for the following reasons:\n      (a) Images are typically represented in Euclidean space and thus can easily be manipulated and \n            handled using well-known metrics like cosine similarity, $l_p$ norms etc. However, graphs originate \n            from non-Euclidean domains and exhibit much more complex relationships and interdependency \n            between objects. Furthermore, the notion of a distance between graphs is also not \n            straightforward and requires construction of graph kernels or the use of standard metrics on \n            graph embeddings [1]. Additionally, such graph kernels don\u2019t capture higher order relations very \n            well.\n      (b) In the FSL setting on images, the number of training samples from various classes is also \n            abundantly  more than what is available for graph datasets. The image domain allows training \n            generative models  to learn the task distribution and can further be used to generate samples for \n            \u201cdata augmentation\u201d, which act as very good priors. In contrast, graph generative models are still \n            in their infancy and work in very restricted settings [2] . Furthermore, methods like cropping and \n            rotation to improve the models can't be used for graphs given the permutation invariant nature \n            of graphs. Additionally, removal of any component from the graph can adversely affect its \n            structural properties, such as in biological datasets.\n      (c) The image domain has very well-known regularization methods (e.g. Tikhonov, Lasso)  that help \n            generalize much better to novel datasets. Although, they don\u2019t bring any extra supervised \n            information and hence cannot fully address the problem of FSL in the image domain. To the best \n            of our knowledge, this is still an open research problem even in the image domain. On the other \n            hand,  in the graph domain, our work would be a  first step towards graph classification in an FSL \n            setting, which would then hopefully pave the path for better FSL graph regularizers.\n     (d) Transfer learning [3] has led to substantial improvements on various image related tasks due to \n            the  high degree of transferability of feature extractors. Thus, downstream tasks like few-shot \n            learning can  be  performed well with high quality feature extractor models, such as Resnet \n            variants trained on Imagenet. Transfer learning, or for that matter even good feature extractors, \n            remains a daunting challenge in the graph domain. For graphs, there neither exists a dataset \n            which can serve as a pivot for high quality feature learning, nor does there exist a Graph NN which can capture the higher order relations between various categories of graphs, thus making this a highly challenging problem.\n[1]  Kriege et~al. \u201cA survey on graph kernels\u201d, Arxiv 2019\n[2]  Simonovsky et~al, \u201cGraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders\u201d, Arxiv 2018\n[3]  Tan et~al. \u201cA Survey on Deep Transfer Learning\u201d, Arxiv 2018\n\nR2Q6: The authors do not even describe in the paper what the graph classes in these datasets actually are or represent, which would be good to know. \nR2A6: Experiments on two new datasets, Enzymes and Reddit-12K have been added to the paper. These two datasets are popular and have been extensively used in graph classification literature prior to this work. We now describe each dataset.\n    (a) Reddit-12K - dataset contains 11929 graphs where each graph corresponds to a thread in which \n          each node represents a user and each edge represents that one user has responded to a comment \n          from some other user. There are 11 different types of discussion forums corresponding to each of the \n          11 classes.\n   (b) ENZYMES - is a dataset of protein tertiary structures consisting of 600 enzymes from the BRENDA \n         enzyme database. The dataset contains 6 different graph categories corresponding to each different \n         top-level EC enzyme.\n   (c) TRIANGLES- dataset contains 10 different classes where the classes are numbered from 1 to 10 \n         corresponding to the number of triangles/3-cliques in each graph of the dataset. \n   (d) Letter-High - dataset contains graphs which represent distorted letter drawings from the english \n         alphabet - (A, E, F, H, I, K, L, M, N, T, V, W, X, Y, Z). Each graph is a prototype manual construction of the \n         alphabets.\n\nR2Q7:\nMinor errors:\n- there appears to be bracket imbalance in eq.6\n- \"Lloyd's\" is misspelled a few times\nR2A7: Thanks for pointing out these errors. We have corrected them in the revised draft.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeeca4Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper693/Authors|ICLR.cc/2020/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167654, "tmdate": 1576860554073, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment"}}}, {"id": "BylX4567or", "original": null, "number": 4, "cdate": 1573276203315, "ddate": null, "tcdate": 1573276203315, "tmdate": 1573280867797, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "S1g3KSyaqr", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment", "content": {"title": "Response to Reviewer 2, Part 1", "comment": "We thank the reviewer for his comments and observations. We are in total agreement with the detailed overview of our method provided by you. Following are the answers to each question/comment you have raised.\n\nR2Q1: \u201cThis process assumes that the novel test graph classes belong to the same set of super classes as the training graph classes, a point that is, unfortunately, not discussed.\u201d\nR2A1: Thanks for pointing out the omission of this crucial discussion. Yes, we assume that the novel test classes belong to the same set of superclasses from the training graphs. The reason being that the novel class labeled samples are so much fewer than the base class labeled samples that the resulting supergraph ends up being extremely sparse and deviates a lot from the shape of the supergraph from the base classes; therefore it severely hinders $C^{GAT}$\u2019s ability to effectively aggregate information from the embeddings of the novel class labeled graphs. Instead, we pass the novel graph samples through our trained $C^{sup}$ and infer its super-class label and this works very effectively for us, as is evidenced by our empirical results. We have accordingly updated our draft and this explanation can be found in the new Discussion subsection of Section 4.2 of our revised draft.\n\nR2Q2: I got lost somewhat in the Classifier description in Section 4, while Section 3 defines many things...\nR2A2: We have updated the description in Section 4 by adding in the new discussion section concerned above.\n\nR2Q3: It is also not immediately obvious that fine-tuning takes place on the set $G_N$ and testing on the set $G_U$.\nR2A3: We have described this in the \u201cproblem definition\u201d subsection in Section 3 (Preliminaries). Additionally, we added the sentence \u201cFinally the evaluation is performed on the samples from the unseen test set $G_U$.\u201d at the end of our Classification subsection of 4.2.\n\nR2Q4: Some additional figures that depict the super-graph construction and clustering would be useful.\nR2A4: Thanks. We have added an illustration (Fig 2) that depicts the working of our Wasserstein super-class k-means clustering algorithm.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeeca4Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper693/Authors|ICLR.cc/2020/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167654, "tmdate": 1576860554073, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment"}}}, {"id": "B1l8tM0QoB", "original": null, "number": 8, "cdate": 1573278334223, "ddate": null, "tcdate": 1573278334223, "tmdate": 1573280857697, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment", "content": {"title": "Response to All Reviewers", "comment": "Dear Reviewers,\nWe would like to thank all of you for your questions, comments, and suggestions. \nAlong with our rebuttal, we have also updated the draft with new experiments on\ntwo additional datasets. Below is a list of the changes we have made to the \nrevised draft along with their rebuttal answer IDs for easy cross-referencing.\n1) (R2A5) Comparison of FSL in graph versus image domain:Added after Related Work.\n2) (R2A4) Figure 2 providing description of our Wasserstein super-class k-means clustering algorithm added in main paper.\n3) (R2A1) Discussion of Super Class Construction of Novel classes: Last paragraph of section 4.2\n4) (R2A3) \"Finally the evaluation is performed on the samples from the unseen test set $G_U$\" added at the end of the \n     Classifier subsection in Section 4.2\n5) (R2A6) Section 5.2 Updated with ENZYMES and REDDIT-12K datasets along with experimental results in Table 2.\n6) (R3A2) Sections 5.3, 5.4 Updated along with their tables for ENZYMES and REDDIT-12K datasets.\n7)  Section A.1 updated with descriptions of the datasets.\n8) (R3A2) Sections A.4, A.5 and A.6. Updated their tables for silhouette scores, semi-supervised and active learning settings \n      on the new datasets.\n9) (R3A3) Section A.7 added with new t-SNE plots and tables for silhouette coefficients. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper693/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkeeca4Kvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper693/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper693/Authors|ICLR.cc/2020/Conference/Paper693/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167654, "tmdate": 1576860554073, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper693/Authors", "ICLR.cc/2020/Conference/Paper693/Reviewers", "ICLR.cc/2020/Conference/Paper693/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Comment"}}}, {"id": "ryxUOGLHcH", "original": null, "number": 2, "cdate": 1572328046370, "ddate": null, "tcdate": 1572328046370, "tmdate": 1572972563943, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a few-shot graph classification algorithm based on graph neural networks. The learning is based on a large set of base class labeled graphs and a small set of novel class labeled graphs. The goal is to learn a classification algorithm over the novel class based on the sample from the base class and novel class. The learning process constitutes of the following steps. First, the base class is classified into K super classes based on the spectral embedding of the graph (onto distributions over the corresponding graph spectrum) and the k-means algorithm with the Wasserstein metric. Second, for each super class, the classification is done through a feature extractor and a classifier. In the training of the feature extractor and classifier, the author introduces a super-graph with each node representing a super class. Finally, in the fine-tuning stage, the feature extractor is fixed, and the classifier is trained based on the novel class.\n\nThis work seems to be the first attempt to adopt the few-shot learning in graph classification tasks. The architecture is novel, and the classification of graph based on spectral embedding together with the Wasserstein metric is novel to me.\n\nI vote for rejecting this submission for the following concerns. \n\n1. The classification of base class into super classes seems questionable to me. In the meta-learning language, the author attempts to learn a good representation of graphs based on different graph classification tasks generated by a task distribution. In terms of graph classification, the task distribution is supported on the joint distributions (G, Y). Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.\n \n2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.  I would appreciate it if the author could provide more explanation on the introduction of the super-graph in training.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper693/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576388262916, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper693/Reviewers"], "noninvitees": [], "tcdate": 1570237748453, "tmdate": 1576388262937, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Review"}}}, {"id": "S1g3KSyaqr", "original": null, "number": 3, "cdate": 1572824452186, "ddate": null, "tcdate": 1572824452186, "tmdate": 1572972563902, "tddate": null, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "invitation": "ICLR.cc/2020/Conference/Paper693/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces few-shot learning for graph classification. The authors propose a pre-training->fine-tuning approach to handle graph classes unseen at training time (and in only a few shots at test time).\n\nAt a high level, and to my understanding, their method a priori generates the ingredients for a graph of graphs, a \"super-graph\", in two steps: first, it discovers a prototype graph for each graph class in the dataset, then it clusters the prototype graphs into a set of super-classes by k-NN. Both of these operations rely on the spectral distance between graphs, defined in this work using the spectrum of a graph's normalized Laplacian matrix and the pth Wasserstein distance between probability measures. The intuition is that the super-graph built from these ingredients helps model latent relations between graph classes; these relations can be used at test time to improve classification of unseen graph classes.\n\nDuring (pre-)training, the model builds super-graphs on each batch of data. It uses super-graph information in two ways: an auxiliary classification head (an MLP called C^sup) is trained to map graph embeddings to their corresponding super-class labels, and the super-graph itself, whose nodes are embeddings for individual graphs in the batch, passes through a graph attention network (GAT) that outputs a base class for each graph -- this is the classification head C^GAT. The graph embeddings themselves come from a feature extractor F_\u03b8 implemented as a graph isomorphism network (GIN).\n\nDuring the fine-tuning stage the model adapts to and classifies graphs from classes unseen during training. Here the parameters of the feature extractor GIN and the C^sup MLP are frozen. C^sup outputs a set of super-class labels that are used to construct a super-graph, which in turn feeds into C^GAT, which in turn yields labels for the test graphs. C^GAT (but not the GIN or C^sup) fine-tunes on a small number q of labelled examples of each novel test class. The full model is evaluated on unlabelled examples from the novel test classes. This process assumes that the novel test graph classes belong to the same set of super classes as the training graph classes, a point that is, unfortunately, not discussed.\n\nThere's a lot to digest in this paper, on both the technical and architectural sides. There are graphs of graphs (super-graphs) and different GNN variants operating on both, with the output of one graph network, the feature extracting GIN, feeding into another, the GAT classifier. Understanding all of these pieces and how they fit together is challenging for the reader: I got lost somewhat in the Classifier description in Section 4, while Section 3 defines many things and gives some math that might be extraneous. It is also not immediately obvious that fine-tuning takes place on the set G_N and testing on the set G_U. Overall, though, the paper became clear to me with time and I found the overall presentation to be good. Some additional figures that depict the super-graph construction and clustering would be useful.\n\nThe construction and use of the super-graph structure to model relations between graph classes is interesting and novel to me, though it relies on well-established techniques (Wasserstein barycenters, Lloyd's algorithm for k-NN). The architecture itself, which combines GINs and GATs, is also novel to me; a downside is that it is highly complex.\n\nExperiments were undertaken on two datasets and seem fairly thorough, with variance established on a high number of seeds (high in the deep learning literature). They demonstrate that the proposed method makes significant improvements over baselines. The baselines are somewhat limited because, as the authors state, \"there do not exist any standard state-of-the-art methods for few-shot graph classification\". However, I do not think the authors should be penalized for trying something new. On the other hand, given the novelty of the task, it would be nice to see an investigation/discussion of how few-shot graph learning differs from few-shot image learning (where there has been much more work).\n\nI found the ablation and sensitivity studies illuminating, and I was pleased to see that the authors do support their claim that the super-graphs improve class separation over the feature extractor embeddings -- the GIN-k-NN baseline results provide evidence of this.\n\nOne place where I lack confidence in the results: I am not very familiar with the datasets used (TRIANGLES and Letter-High) nor how standard they are in the graph-learning literature. The authors do not even describe in the paper what the graph classes in these datasets actually are or represent, which would be good to know.\n\nOverall, I think the paper is worth seeing and discussing at the conference, although it could be improved in various ways.\n\nMinor errors:\n- there appears to be bracket imbalance in eq.6\n- \"Lloyd's\" is misspelled a few times\n\nReviewer's note: I have significant experience in few-shot learning but not in graph neural networks."}, "signatures": ["ICLR.cc/2020/Conference/Paper693/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper693/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES", "authors": ["Jatin Chauhan", "Deepak Nathani", "Manohar Kaul"], "authorids": ["chauhanjatin100@gmail.com", "deepakn1019@gmail.com", "mkaul@iith.ac.in"], "keywords": ["Few shot graph classification", "graph spectral measures", "super-classes"], "abstract": "We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph\u2019s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to our proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. We conduct exhaustive empirical evaluations of our proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and our naive baseline GNNs. Additionally, we also extend and study the behavior of our method to semi-supervised and active learning scenarios.", "pdf": "/pdf/bee36665ecd241cf376155802366db833fada9cf.pdf", "code": "https://github.com/chauhanjatin10/GraphsFewShot", "paperhash": "chauhan|fewshot_learning_on_graphs_via_superclasses_based_on_graph_spectral_measures", "_bibtex": "@inproceedings{\nChauhan2020FEW-SHOT,\ntitle={FEW-SHOT LEARNING ON GRAPHS VIA SUPER-CLASSES BASED ON GRAPH SPECTRAL MEASURES},\nauthor={Jatin Chauhan and Deepak Nathani and Manohar Kaul},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkeeca4Kvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/cdacd80a1926df2e3b89646537fc8b1cbd711b94.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkeeca4Kvr", "replyto": "Bkeeca4Kvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper693/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576388262916, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper693/Reviewers"], "noninvitees": [], "tcdate": 1570237748453, "tmdate": 1576388262937, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper693/-/Official_Review"}}}], "count": 11}