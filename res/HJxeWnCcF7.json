{"notes": [{"id": "HJxeWnCcF7", "original": "SJlrSiN9KQ", "number": 1139, "cdate": 1538087928159, "ddate": null, "tcdate": 1538087928159, "tmdate": 1550904135881, "tddate": null, "forum": "HJxeWnCcF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1l3VC9rl4", "original": null, "number": 1, "cdate": 1545084467634, "ddate": null, "tcdate": 1545084467634, "tmdate": 1545354480442, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Meta_Review", "content": {"metareview": "This paper proposes a novel framework for tractably learning non-eucliean embeddings that are product spaces formed by hyperbolic, spherical, and Euclidean components, providing a heterogenous mix of curvature properties.  On several datasets, these product space embeddings outperform single Euclidean or hyperbolic spaces. The reviewers unanimously recommend acceptance.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Novel framework for learning non-euclidean embeddings"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1139/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352951035, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352951035}}}, {"id": "ryxIIdud37", "original": null, "number": 2, "cdate": 1541077070013, "ddate": null, "tcdate": 1541077070013, "tmdate": 1543340081545, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Review", "content": {"title": "The problem studied in the paper is interesting. However, there are various mathematical and theoretical problems with the paper, some of which are mentioned below. In addition, the claims and novelty of the paper fall short in the provided methods and results.", "review": "\nPage 2: What are p_i, i=1,2,...,n, their set T and \\mathcal{P}?\n\nWhat is | | used to compute distortion between a and b?\n\nPlease fix the definition of the Riemannian manifold, such that M is not just any manifold, but should be a smooth manifold or a particular differentiable manifold. Please update your definition more precisely, by checking page 328 in J.M. Lee, Introduction to Smooth Manifolds, 2012, or Page 38 in do Cormo, Riemannian Geometry, 1992.\n\nPlease define \\mathcal{P} in equation (1).\n\nDefine K used in the definition of the hyperboloid more precisely.\n\nPlease provide proofs of these statements for product of manifolds with nonnegative and nonpositive curvatures: \u201cIn particular, the squared distance in the product decomposes via (1). In other words, dP is simply the l2 norm of the component distances dMi.\u201d\n\nPlease explain what you mean by \u201cwithout the need for optimization\u201d in \u201cThese distances provide simple and interpretable embedding spaces using P, enabling us to introduce combinatorial constructions that allow for embeddings without the need for optimization.\u201d In addition, how can you compute geodesic etc. if you use l1 distance for the embedded space?\n\nBy equation (2), the paper focuses on embedding graphs, which is indeed the main goal of the paper. Therefore, first, the novelty and claims of the paper should be revised for graph embedding. Second, three particular spaces are considered in this work, which are the sphere, hyperbolic manifold, and Euclidean space. Therefore, you cannot simply state your novelty for a general class of product spaces. Thus, the title, novelty, claims and other parts of the paper should be revised and updated according to the particular input and output spaces of embeddings considered in the paper. \n\nPlease explain how you compute the metric tensor  g_P and apply the Riemannian correction (multiply by the inverse of the metric tensor g_P) to determine the Riemannian gradient in the Algorithm 1, more precisely. \n\nStep (9) of the Algorithm 1 is either wrong, or you compute v_i without projecting the Riemannian gradient. Please check your theoretical/experimental results and code according to this step.\n\nWhat is h_i used in the Algorithm 1? Can we suppose that it is the ith component of h?\n\nIn step (6) and step (8), do you project individual components of the Riemannian gradient to the product manifold? Since their dimensions are different, how do you perform these projections, since definitions of the projections given on Page 5 cannot be applied? Please check your theoretical/experimental results and code accordingly.\n\nPlease define exp_{x^(t)_i}(vi) and Exp(U) more precisely. I suppose that they denote exponential maps.\n\nHow do you initialize x^(0) randomly?\n\nThe notation is pretty confusing and ambiguous. First, does x belong to an embedded Riemannian manifold P or a point on the graph, which will be embedded? According to equation (2), they are on the graph and they will be embedded. According to Algorithm 1, x^0 belongs to P, which is a Riemannian manifold as defined before. So, if x^(0) belongs to P, then L is already defined from P to R (in input of the Algorithm 1). Thereby, gradient \\nabla L(x) is already a Riemannian gradient, not the Euclidean gradient, while you claim that \\nabla L(x) is the Euclidean gradient in the text.\n\nOverall, Algorithm 1 just performs a projection of Riemannian or Euclidean gradient  \\nabla L(x) onto a point v_i for each ith individual manifold. Then, each v_i is projected back to a point on an individual component of the product manifold by an exponential map. \n\nWhat do you mean by \u201csectional curvature, which is a function of a point p and two directions x; y from p\u201d? Are x and y not points on a manifold?\n \nYou define \\xi_G(m;b,c) for curvature estimation for a graph G. However, the goal was to map G to a Riemannian manifold. Then, do you also consider that G is itself a Riemannian manifold, or a submanifold?\n\nWhat is P in the statement \u201cthe components of\nthe points in P\u201d in Lemma 2?\n\nWhat is \\epsilon in Lemma 2?\n\nHow do you optimize positive w_i, i=1,2,...,n?\n\nWhat is the \u201cgradient descent\u201d refered to in Lemma 2?\n\nPlease provide computational complexity and running time of the methods.\n\nPlease define \\mathbb{I}_r.\n\nAt the third line of the first equation of the proof of Lemma 1, there is no x_2. Is this equation correct?\n\nIf at least of two of x1, y1, x2 and y2 are linearly dependents, then how does the result of Lemma 1 change?\n\nStatements and results given in Lemma 1 are confusing. According to the result, e.g. for K=1, curvature of product manifold of sphere S and Euclidean space E is 1, and that of E and hyperbolic  H is 0. Then, could you please explain this result for the product of S, E and H, that is, explain the statement \u201cThe last case (one negative, one positive space) follows along the same lines.\u201d? If the curvature of the product manifold is non-negative, then does it mean that the curvature of H is ignored in the computations?\n\nWhat is \\gamma more precisely? Is it a distribution or density function? If it is, then what does (\\gamma+1)/2 denote?\n\nThe statements related to use of Algorithm 1 and SGD to optimize equation (2) are confusing. Please explain how you employed them together in detail.\n\nCould you please clarify estimation of K_1 and K_2, if they are unknown. More precisely, the following statements are not clear;\n\n- \u201cFurthermore, without knowing K1, K2 a priori, an estimate for these curvatures can be found by matching the distribution of sectional curvature from Algorithm 2 to the empirical curvature computed from Algorithm 3. In particular, Algorithm 2 can be used to generate distributions, and K1, K2 can then be found by matching moments.\u201d Please explain how in more detail? What is matching moments?\n\n- \u201cwe find the distribution via sampling (Algorithm 3) in the calculations for Table 3, before being fed into Algorithm 2 to estimate Ki\u201d How do you estimation K_1 and K_2 using Algorithm 3?\n\n- Please define, \u201crandom (V)\u201d, \u201crandom neighbor m\u201d and \u201c\\delta_K/s\u201d used in Algorithm 3 more precisely.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Review", "cdate": 1542234297239, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335881424, "tmdate": 1552335881424, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkevXvEQ0Q", "original": null, "number": 12, "cdate": 1542829855364, "ddate": null, "tcdate": 1542829855364, "tmdate": 1542829855364, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "SkxRH9mg07", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We appreciate the reviewer's thoughtful reading of our response and reconsidering the score. After considering the reviewer\u2019s points, we have changed the title of the submission to \"Learning Mixed-Curvature Representations in Products of Model Spaces\" in order to more accurately reflect that we perform embeddings specifically into products of hyperbolic, Euclidean, and spherical spaces (the traditional Riemannian \u201cmodel spaces\u201d of constant curvature [1]). We have clarified this throughout the paper (e.g. statement of Lemma 2). In the revised draft, we also make it more explicit that we follow notation from standard references and recent work in this area such as [2].\n\nBeyond feedback on details, we welcome further comments on the overall merits of our approach and its contributions to representation learning. We believe our contributions are now more accurately reflected in the title and claims, and look forward to the reviewer\u2019s evaluation of their intrinsic value.\n\n[1] John Lee. Riemannian manifolds: an introduction to curvature\n[2] Nickel and Kiela. Poincar\u00e9 embeddings for learning hierarchical representations.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "H1exAN8qaQ", "original": null, "number": 9, "cdate": 1542247623863, "ddate": null, "tcdate": 1542247623863, "tmdate": 1542247623863, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "ryxIIdud37", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "Addressing your concerns", "comment": "We welcome the reviewer's detailed questions and suggestions on the technical presentation of our paper, and we appreciate the opportunity to improve it. To the best of our understanding, many of the reviewer's questions are addressed in the submitted draft, or pertain to standard notation and arguments. Nevertheless, we respond to the reviewer\u2019s comments in detail below, clarifying ideas or pointing out specific lines where questions are answered.\n\nWe sincerely hope that our response clarifies any potential notational confusions, and we look forward to further engaging in a substantial discussion on the overall merits of our work.\n\nAll pages and lines referenced refer to the original submission.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "B1la9NI96Q", "original": null, "number": 8, "cdate": 1542247572729, "ddate": null, "tcdate": 1542247572729, "tmdate": 1542247572729, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "ryxIIdud37", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "Line-by-line response (1)", "comment": "- Page 2: What are p_i, i=1,2,...,n, their set T and \\mathcal{P}?\n\nThis refers to an arbitrary set T containing points p_1,...,p_n on a manifold P, for which we wish to define a mean.\n\n\n- What is | | used to compute distortion between a and b?\n\nAbsolute value\n\n\n- Please fix the definition of the Riemannian manifold, such that M is not just any manifold, but should be a smooth manifold or a particular differentiable manifold. Please update your definition more precisely, by checking page 328 in J.M. Lee, Introduction to Smooth Manifolds, 2012, or Page 38 in do Cormo, Riemannian Geometry, 1992.\n\nYes, it is a smooth manifold, as specified in the first line of the \u201cProduct Manifolds\u201d paragraph.\n\n\n- Please define \\mathcal{P} in equation (1).\n\n\\mathcal{P} is a product manifold.\n\n\n- Define K used in the definition of the hyperboloid more precisely.\n\nK is an arbitrary constant that indexes the curvature. This is described in the first paragraph of section \u201cLearning the curvature\u201d.\n\n\n- Please provide proofs of these statements for product of manifolds with nonnegative and nonpositive curvatures: \u201cIn particular, the squared distance in the product decomposes via (1). In other words, dP is simply the l2 norm of the component distances dMi.\u201d\n\nThe given statement is a standard fact about products of Riemannian manifolds: some classical references are [Levy] and [Ficken], although the result is stated directly in, e.g, [TS, pg. 81, eq. (4.19)].  Here is a sketch of the proof: first, the Levi-Civita connection on the manifold decomposes along the product components [DoCarmo Ex. 6.1]. This implies that the acceleration is 0 iff it is 0 in each component; in other words, geodesics in the product manifold decompose into geodesics in each of the factors. The distance function\u2019s decomposition follows from the additivity of the Riemannian metric, i.e. |\\dot{\\gamma}(t)| = \\sqrt{\\dot{\\gamma_1}(t)^2 + \\dot{\\gamma_2}(t)^2}.\n\n\n- Please explain what you mean by \u201cwithout the need for optimization\u201d in\u2026 In addition, how can you compute geodesic etc. if you use l1 distance for the embedded space?\n\n* We are referring to embedding algorithms that do not require optimizing a loss function via, for example, gradient descent. This concept is detailed in Appendix C.3. For example, the second paragraph on page 19 shows how to embed a cycle by explicitly writing down the coordinates of the points, with no optimization. Similarly, for hyperbolic space, the combinatorial construction previously studied in [Sarkar, SDGR] embeds trees in hyperbolic space without optimization.\n* Additionally, it is explicitly mentioned in the first line of the corresponding paragraph that the alternative distances proposed are meant to \u201cignore the Riemannian structure\u201d, because many common applications of embeddings such as link prediction do not actually require Riemannian manifold structure, or related notions such as geodesics. Conversely, the motivation for the application in Section 4.2 is to show a task where manifold structure and geodesics are actually required, where the (Riemannian) product is effective.\n\n\n- By equation (2), the paper focuses on embedding graphs, which is indeed the main goal of the paper. Therefore, first, the novelty and claims of the paper should be revised for graph embedding. Second, three particular spaces are considered in this work, which are the sphere, hyperbolic manifold, and Euclidean space. Therefore, you cannot simply state your novelty for a general class of product spaces. Thus, the title, novelty, claims and other parts of the paper should be revised and updated according to the particular input and output spaces of embeddings considered in the paper.\n\nOur embedding technique is not limited to graphs, and indeed we perform word embeddings into product manifolds as described in Section 4.2. Graphs, however, are used as a standard metric for non-Euclidean embeddings [NK1, SDGR, NK2], and so we evaluate our approach on a variety of graphs in Section 4.1. The language of graphs is also convenient for stating some of our results, but not necessary, as described in Footnote 1.\n\nThe three particular spaces are the standard spaces of constant curvature, which has been considered in previous work. Our claimed novelty is in combining these using the Riemannian product construction to perform efficient embeddings into mixed-curvature spaces, as stated in the abstract (3rd sentence), introduction (3rd paragraph), and many other places throughout.\n\n\n- Please explain how you compute the metric tensor  g_P and apply the Riemannian correction (multiply by the inverse of the metric tensor g_P) to determine the Riemannian gradient in the Algorithm 1, more precisely. \n\nThis is standard, as in [NK1,NK2, WL]. The only place it is necessary for us is for the hyperbolic components in Step (9)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "S1lIwN896m", "original": null, "number": 7, "cdate": 1542247518397, "ddate": null, "tcdate": 1542247518397, "tmdate": 1542247518397, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "ryxIIdud37", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "Line-by-line response (2)", "comment": "- Step (9) of the Algorithm 1 is either wrong, or you compute v_i without projecting the Riemannian gradient. Please check your theoretical/experimental results and code according to this step.\n\nThere is a typo; the RHS should have v_i instead of h_i.\n\n\n- What is h_i used in the Algorithm 1? Can we suppose that it is the ith component of h?\n\nh_i refers to the coordinates corresponding to the i-th component or factor.\n\n\n- In step (6) and step (8), do you project individual components of the Riemannian gradient to the product manifold? Since their dimensions are different, how do you perform these projections, since definitions of the projections given on Page 5 cannot be applied? Please check your theoretical/experimental results and code accordingly.\n\nEach projection is within its component; the text mentions each component is handled independently. A subscript i has been added to the RHS of steps (6),(8).\n\n\n- Please define exp_{x^(t)_i}(vi) and Exp(U) more precisely. I suppose that they denote exponential maps.\n\nExp denotes the exponential map as defined in Section 2. The image Exp(U) refers to the standard notation f(S) := {f(s) : s \\in S} where S is a set.\n\n\n- How do you initialize x^(0) randomly?\n\nThe initialization scheme depends on the application. An example of a standard initialization selects each coordinate of x^(0) either uniform or Gaussian with std on the order of 1e-2 to 1e-3 [NK1, LW], which is what we also use in our empirical evaluation. We have clarified this in Appendix D.\n\n- The notation is pretty confusing and ambiguous. First, does x belong to an embedded Riemannian manifold P or a point on the graph, which will be embedded? According to equation (2), they are on the graph and they will be embedded. According to Algorithm 1, x^0 belongs to P, which is a Riemannian manifold as defined before. So, if x^(0) belongs to P, then L is already defined from P to R (in input of the Algorithm 1). Thereby, gradient \\nabla L(x) is already a Riemannian gradient, not the Euclidean gradient, while you claim that \\nabla L(x) is the Euclidean gradient in the text.\n\nx is the manifold point to be optimized. The notation \\nabla L(x) is defined to be the Euclidean gradient at the bottom of page 4 of the initial submission. Note that this is the gradient of the embedding into ambient space; this is standard as in [NK2, WL].\n\n\n- Overall, Algorithm 1 just performs a projection of Riemannian or Euclidean gradient  \\nabla L(x) onto a point v_i for each ith individual manifold. Then, each v_i is projected back to a point on an individual component of the product manifold by an exponential map.\n\nThat is correct.\n\n- What do you mean by \u201csectional curvature, which is a function of a point p and two directions x; y from p\u201d? Are x and y not points on a manifold?\n\nAs mentioned earlier in the section, sectional curvature is a function of a point p and two directions (i.e. tangent vectors) u,v. However, tangent vectors can be identified with points on the manifold via geodesics (i.e. through Exp). The way our discrete curvature estimation is described in this section is analogous to other discrete curvature analogs [B]. For example, the Ricci curvature is defined for a point p and a tangent vector u, and the coarse Ricci curvature is defined for a node p and neighbor x [Ollivier2].\n\n\n- You define \\xi_G(m;b,c) for curvature estimation for a graph G. However, the goal was to map G to a Riemannian manifold. Then, do you also consider that G is itself a Riemannian manifold, or a submanifold?\n\nG is a graph and does not have manifold structure. The goal of \\xi is to provide a discrete analog of curvature which satisfies similar properties to curvature and facilitates choosing an appropriate Riemannian manifold to embed G into. There are other similar notions of discrete curvature on graphs, for example the Forman-Ricci [WSJ] and Ollivier-Ricci [Ollivier1] curvatures.\n\n\n- What is P in the statement \u201cthe components of the points in P\u201d in Lemma 2?\n\nIt is the product manifold. We have changed it to \\mathcal{P}.\n\n\n- What is \\epsilon in Lemma 2?\n\n\\epsilon refers to a desired tolerance within which to compute the solution, in this case the mean. This is also explicitly mentioned in the last line of the second to last paragraph of Section 1. This is standard notation for gradient descent-based rates.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "SygOB4LcTX", "original": null, "number": 6, "cdate": 1542247488514, "ddate": null, "tcdate": 1542247488514, "tmdate": 1542247488514, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "ryxIIdud37", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "Line-by-line response (3)", "comment": "- How do you optimize positive w_i, i=1,2,...,n?\n\nBy convention, the weights w_i are constants independent of the optimization. For example, to compute the standard Euclidean mean one would take w_i = 1/n for all i.\n\n\n- What is the \u201cgradient descent\u201d refered to in Lemma 2?\n\nThe usual Riemannian gradient descent, since it is a manifold.\n\n\n- Please provide computational complexity and running time of the methods.\n\nThe complexity of the Karcher mean algorithm is O(nr log epsilon^(-1)), as described on Page 2, PP 3, line 4. The convergence rate of RSGD is standard [ZS]: it converges to a stationary point with rate O(c/t), where c is a constant and t is the number of iterations. Algorithms 2 and 3 find good estimates of the corresponding distributions in a small number (~10^4) of samples; each sample requires constant time for both algorithms.\n\n\n- Please define \\mathbb{I}_r.\n\nThis is standard notation for the r x r identity matrix, but we have explicitly defined it now.\n\n\n- At the third line of the first equation of the proof of Lemma 1, there is no x_2. Is this equation correct?\n\nThe second R_1(x_1, y_1)x_1 should be R_2(x_2, y_2)x_2, which follows from directly applying equation (5) to the previous line.\n\n\n- If at least of two of x1, y1, x2 and y2 are linearly dependents, then how does the result of Lemma 1 change?\n\nThe result does not change.\n\n\n- Statements and results given in Lemma 1 are confusing. According to the result, e.g. for K=1, curvature of product manifold of sphere S and Euclidean space E is 1, and that of E and hyperbolic  H is 0. Then, could you please explain this result for the product of S, E and H, that is, explain the statement \u201cThe last case (one negative, one positive space) follows along the same lines.\u201d? If the curvature of the product manifold is non-negative, then does it mean that the curvature of H is ignored in the computations?\n\nIn the case of a product of E and H, the sectional curvature ranges in [-1,0]. The line \u201cand similarly for K_1, K_2 non-positive\u201d implies that in the non-positive case we have K(u,v) \\in [min(K_1, K_2), 0], since everything is negated.\n\n\n- What is \\gamma more precisely? Is it a distribution or density function? If it is, then what does (\\gamma+1)/2 denote?\n\n\\gamma is a random variable which is distributed as the dot product of two uniformly random unit vectors, as defined on the bottom of page 16. Hence (\\gamma+1)/2 is a well-defined random variable.\n\n\n- The statements related to use of Algorithm 1 and SGD to optimize equation (2) are confusing. Please explain how you employed them together in detail.\n\nEquation (2) is a loss function from \\mathcal{P}^n to \\mathbb{R} where the embeddings x_i are variables, and can thus be optimized using RSGD (Algorithm 1) on each point simultaneously. This is the same approach taken in previous works [NK1, SDGR, NK2] for the case of single space embeddings.\n\n\n- On estimation of K_1, K_2 and matching moments\n\nAlgorithm 2 and 3 both produce distributions. Moment matching (or the method of moments) is a standard term referring to parameter estimation via equating the moments of distributions. More details have been added to the revised draft.\n\n\n- Please define, \u201crandom (V)\u201d, \u201crandom neighbor m\u201d and \u201c\\delta_K/s\u201d used in Algorithm 3 more precisely.\n\nWe have clarified that the random sampling is uniform. \\delta_K refers to the delta function.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "ryljZN8caX", "original": null, "number": 5, "cdate": 1542247427488, "ddate": null, "tcdate": 1542247427488, "tmdate": 1542247427488, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "ryxIIdud37", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "References", "comment": "[B] Bauer et al. Modern Approaches to Discrete Curvature. Lecture Notes in Mathematics\n[Ficken] Ficken, \u201cThe Riemannian and Affine Differential Geometry of Product-Spaces\u201d, Annals of Math., 1939.\n[LW] Leimeister and Wilson. Skip-gram word embeddings in hyperbolic space.\n[Levy] Levy, \"Symmetric Tensors of The Second Order Whose Covariant Derivatives Vanish\", Annals of Math., 1926.\n[NK1] Nickel and Kiela. Poincar\u00e9 embeddings for learning hierarchical representations.\n[NK2] Nickel and Kiela. Learning continuous hierarchies in the Lorentz model of hyperbolic geometry.\n[Ollivier1] Ollivier. Ricci curvature of Markov chains on metric spaces.\n[Ollivier2] Ollivier. A visual introduction to Riemannian curvatures and some discrete generalizations.\n[SDGR] Sala, De Sa, Gu, R\u00e9. Representation tradeoffs for hyperbolic embeddings.\n[Sarkar] Sarkar. Low distortion Delaunay embedding of trees in hyperbolic plane.\n[TS] Turaga and Srivastava, Riemannian Computing in Computer Vision, Springer 2016.\n[WSJ] Weber, Saucan, and Jost. Characterizing complex networks with Forman-Ricci curvature and associated geometric flows.\n[WL] Wilson and Leimeister. Gradient descent in hyperbolic space.\n[ZS] Zhang and Sra. First-order methods for geodesically convex optimization.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "r1gDfZ8ca7", "original": null, "number": 2, "cdate": 1542246670529, "ddate": null, "tcdate": 1542246670529, "tmdate": 1542246670529, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "r1x-tAv3jm", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We appreciate the reviewer\u2019s thoughtful feedback on our work.\n\n- On the definition of K\n\nK is a constant that parametrizes the curvature of the model spaces (hyperbolic, Euclidean, and spherical); for any constant K, there is a corresponding space with curvature K. In our notation, \\mathbb{E}^d has curvature 0, \\mathbb{S}^d_K has curvature K, and \\mathbb{H}^d_K has curvature -K.\n\n\n- On the use of the signature estimation\n\nTable 2 does not use Algorithms 2 and 3, instead using Algorithm 1 with a variety of signatures to show the interaction between signature and dataset. For every experiment, the curvatures are initialized to -1,0, or 1 for H, E, and S components resp., and learned using the method described in Section 3.1; this is what is reported in the Best model. These details have been clarified in Appendix D.\n\nAs the reviewer has correctly observed, Algorithm 1 can be initialized with the estimated signature from Algorithms 2 and 3, which saves on hyperparameter searching and computation time. Table 3 shows that this method would indeed choose the best signature among the two-component options.\n\n\n- On comparison vs ISOMAP\n\nWe thank the reviewer for pointing out ISOMAP, a non-linear dimensionality reduction algorithm. We ran an experiment to compare against our proposed techniques. We first embedded the graphs from 4.1 into a higher (100) dimensional Euclidean space, than used ISOMAP to reduce the dimension to 10 in order to compare the average distortion against the product manifolds from Section 4.1. We saw a d_avg for PhD's/Facebook/Power Graph/Cities 0.4085 / 2.2295 / 0.4863 / 0.3711. We hypothesize that while ISOMAP can be good for dimensionality reduction for an already-good Euclidean embedding (with many dimensions), it does not perform as well as our technique for situations when the higher-dimensional Euclidean embeddings themselves have non-zero distortion---nor can it capture the mixed-curvature manifolds our approach offers.\n\n\n- On the link between the different contributions \n\nThe operational flow is the following. We start with the data to be embedded. We \n\n(i) seek an appropriate space to embed it in (in order to get a high-quality representation). To find what this embedding space should be, we estimate the signature (Section 3.2). More concretely, we use Algorithm 3 to estimate the distribution of discrete curvature of the data and Algorithm 2 to find a matching product manifold. This yields the \"signature\", i.e., a the number of factors and each factor's type and dimension for our product manifold.\n\nWe have now selected an embedding space, and we \n\n(ii) perform the embedding. This is done via Algorithm 1(RSGD) in Section 3.1.\n\nNow we have an embedding. There are many further tasks to be done with these representations. Perhaps the most fundamental is to take the mean of the representations for a subset of the data. Since our embeddings are into a product manifold, this requires a slightly more sophisticated approach; we\n\n(iii) compute this mean via the Karcher mean detailed in Section 3.3.\n\n\n- On the complexity of learning the product space and the limited data sample regime\n\nThis is an excellent point. We point out that (1) Optimization in the sphere and hyperboloid has the same complexity up to a constant as in Euclidean space, so that the complexity of our product manifold proposal is roughly the same as using SGD to produce typical embeddings, as we simply use R-SGD on the factor spaces. (2) The heuristic for choosing a space is very cheap (i.e., Algorithms 2 and 3) compared to the main embedding procedure, and is better suited for simple products anyways, avoiding the sample complexity issue of a large search space. Indeed, we do not seek to embed into higher dimensional spaces: our approach shows good results with few dimensions in a product space.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "Byxc9eI9am", "original": null, "number": 1, "cdate": 1542246546051, "ddate": null, "tcdate": 1542246546051, "tmdate": 1542246546051, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "Hklpflm6h7", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "We appreciate the reviewer\u2019s positive comments about our work."}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616380, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJxeWnCcF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1139/Authors|ICLR.cc/2019/Conference/Paper1139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers", "ICLR.cc/2019/Conference/Paper1139/Authors", "ICLR.cc/2019/Conference/Paper1139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616380}}}, {"id": "Hklpflm6h7", "original": null, "number": 3, "cdate": 1541382164703, "ddate": null, "tcdate": 1541382164703, "tmdate": 1541533389197, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Review", "content": {"title": "Solid Paper", "review": "This paper proposes a new method to embed a graph onto a product of spherical/Euclidean/hyperbolic manifolds. The key is to use sectional curvature estimations to determine proper signature, i.e., all component manifolds, and then optimize over these manifolds. The results are validated on various synthetic and real graphs. The proposed idea is new, nontrivial, and is well supported by experimental evidence.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Review", "cdate": 1542234297239, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335881424, "tmdate": 1552335881424, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1x-tAv3jm", "original": null, "number": 1, "cdate": 1540288120660, "ddate": null, "tcdate": 1540288120660, "tmdate": 1541533388733, "tddate": null, "forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "invitation": "ICLR.cc/2019/Conference/-/Paper1139/Official_Review", "content": {"title": "Interesting ideas to explore towards understanding the geometry of data sets", "review": "The paper proposes a dimensionality reduction method that embeds data into a product manifold of spherical, Euclidean, and hyperbolic manifolds. The proposed algorithm is based on matching the geodesic distances on the product manifold to graph distances. I find the proposed method quite interesting and think that it might be promising in data analysis problems. Here are a few issues that would be good to clarify:\n\n- Could you please formally define K in page 3?\n\n- I find the estimation of the signature very interesting. However, I am confused about how the curvature calculation process is (or can be) integrated into the embedding method proposed in Algorithm 1. How exactly does the sectional curvature estimation find use in the current results? Is the \u201cBest model\u201d reported in Table 2 determined via the sectional curvature estimation method? If yes, it would be good to see also the Davg and mAP figures of the best model in Table 2 for comparison.\n\n- I think it would also be good to compare the results in Table 2 to some standard dimensionality reduction algorithms like ISOMAP, for instance in terms of Davg. Does the proposed approach bring advantage over such algorithms that try to match the distances in the learnt domain with the geodesic distances in the original graph?\n\n- As a general comment, my feeling about this paper is that the link between the different contributions does not stand out so clearly. In particular, how are the embedding algorithm in Section 3.1, the signature estimation algorithm in Section 3.2, and the Karcher mean discussed in Section 3.3 related? Can all these ideas find use in an overall representation learning framework? \n\n- In the experimental results in page 7, it is argued that the product space does not perform worse than the optimal single constant curvature spaces. The figures in the experimental results seem to support this. However, I am wondering whether the complexity of learning the product space should also play a role in deciding in what kind of space the data should be embedded in. In particular, in a setting with limited availability of data samples, I guess the sample error might get too high if one tries to learn a very high dimensional product space.  \n\n\nTypos: \n\nPage 3: Note the \u201canalogy\u201d to Euclidean products\nPage 7 and Table 1: I guess \u201cring of cycles\u201d should have been \u201cring of trees\u201d instead\nPage 13: Ganea et al formulates \u201cbasic basic\u201d machine learning tools \u2026", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1139/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Mixed-Curvature Representations in Product Spaces", "abstract": "The quality of the representations achieved by embeddings is determined by how well the geometry of the embedding space matches the structure of the data.\nEuclidean space has been the workhorse for embeddings; recently hyperbolic and spherical spaces have gained popularity due to their ability to better embed new types of structured data---such as hierarchical data---but most data is not structured so uniformly.\nWe address this problem by proposing learning embeddings in a product manifold combining multiple copies of these model spaces (spherical, hyperbolic, Euclidean), providing a space of heterogeneous curvature suitable for a wide variety of structures.\nWe introduce a heuristic to estimate the sectional curvature of graph data and directly determine an appropriate signature---the number of component spaces and their dimensions---of the product manifold.\nEmpirically, we jointly learn the curvature and the embedding in the product space via Riemannian optimization.\nWe discuss how to define and compute intrinsic quantities such as means---a challenging notion for product manifolds---and provably learnable optimization functions.\nOn a range of datasets and reconstruction tasks, our product space embeddings outperform single Euclidean or hyperbolic spaces used in previous works, reducing distortion by 32.55% on a Facebook social network dataset. We learn word embeddings and find that a product of hyperbolic spaces in 50 dimensions consistently improves on baseline Euclidean and hyperbolic embeddings, by 2.6\npoints in Spearman rank correlation on similarity tasks\nand 3.4 points on analogy accuracy.\n", "keywords": ["embeddings", "non-Euclidean geometry", "manifolds", "geometry of data"], "authorids": ["albertgu@stanford.edu", "fredsala@stanford.edu", "bgunel@stanford.edu", "chrismre@cs.stanford.edu"], "authors": ["Albert Gu", "Frederic Sala", "Beliz Gunel", "Christopher R\u00e9"], "TL;DR": "Product manifold embedding spaces with heterogenous curvature yield improved representations compared to traditional embedding spaces for a variety of structures.", "pdf": "/pdf/db3deb7cab1bde95904a4a1e330cd3d9a0fc56b6.pdf", "paperhash": "gu|learning_mixedcurvature_representations_in_product_spaces", "_bibtex": "@inproceedings{\ngu2018learning,\ntitle={Learning Mixed-Curvature Representations in Product Spaces},\nauthor={Albert Gu and Frederic Sala and Beliz Gunel and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HJxeWnCcF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1139/Official_Review", "cdate": 1542234297239, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJxeWnCcF7", "replyto": "HJxeWnCcF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1139/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335881424, "tmdate": 1552335881424, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1139/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}