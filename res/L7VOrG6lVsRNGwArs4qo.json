{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458067795506, "tcdate": 1458067795506, "id": "lx9AMy7JXH2OVPy8CvYo", "invitation": "ICLR.cc/2016/workshop/-/paper/155/review/10", "forum": "L7VOrG6lVsRNGwArs4qo", "replyto": "L7VOrG6lVsRNGwArs4qo", "signatures": ["ICLR.cc/2016/workshop/paper/155/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/155/reviewer/10"], "content": {"title": "Nice to see an empirical negative result", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper is interesting.\n\nBayesian optimization (BO) is used to support the claim that there are no shallow models that are as good as the best deep models for CIFAR-10.\nRationale being if BO couldn't find hyperparameters and a learning algorithm to train such a good shallow model, then there is no such shallow model to be found. This evidence is strongest when you search a large set of shallow models and training algorithms, the BO search appears to have converged, and BO search has found models at least as good as the best ones known to be in the search space.\n\nRe: searching a large-enough set of shallow models & algorithms, it would help to show that the best models were not discovered near the boundary of the search space. If they were, the search space should perhaps be bigger to find still-better models.\n\nRe: convergence of BO\nA plot of error over time of the BO searches would increase the strength of the case. Such a curve is not proof that a huge space has been searched but if the best models were all found near the end of the search, then it would undermine the conclusion.\n\nRe: finding the best known models at each depth\nThis appears to be true but a resume of recent high-scores' citations would be appropriate.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?", "abstract": "Yes, apparently they do.\n\nPrevious research by Ba and Caruana (2014)\ndemonstrated that shallow feed-forward nets\nsometimes can learn the complex functions pre-\nviously learned by deep nets while using a simi-\nlar number of parameters as the deep models they\nmimic. In this paper we investigate if shallow\nmodels can learn to mimic the functions learned\nby deep convolutional models. We experiment\nwith shallow models and models with a vary-\ning number of convolutional layers, all trained to\nmimic a state-of-the-art ensemble of CIFAR-10\nmodels. We demonstrate that we are unable to\ntrain shallow models to be of comparable accu-\nracy to deep convolutional models. Although the\nstudent models do not have to be as deep as the\nteacher models they mimic, the student models\napparently need multiple convolutional layers to\nlearn functions of comparable accuracy.\n", "pdf": "/pdf/L7VOrG6lVsRNGwArs4qo.pdf", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_or_even_convolutional", "conflicts": ["uci.edu", "uni-heidelberg.de", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan Shengjie Wang", "Rich Caruana", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "rcaruana@microsoft.com", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580028708, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580028708, "id": "ICLR.cc/2016/workshop/-/paper/155/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "L7VOrG6lVsRNGwArs4qo", "replyto": "L7VOrG6lVsRNGwArs4qo", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/155/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458060613271, "tcdate": 1458060613271, "id": "E8VDZoYlkh31v0m2iDQW", "invitation": "ICLR.cc/2016/workshop/-/paper/155/review/11", "forum": "L7VOrG6lVsRNGwArs4qo", "replyto": "L7VOrG6lVsRNGwArs4qo", "signatures": ["ICLR.cc/2016/workshop/paper/155/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/155/reviewer/11"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The paper confirms the importance of being deep and convolutional empirically by showing that the shallow models cannot achieve as high accuracy as deep and convolutional counterparts. The experiments has been held extensively and the conclusion made in the paper sounds quite convincing even though it is based only on empirical results.\n\nOverall, the claim of the paper is not surprising, but many details in the paper such as architecture selection or the effectiveness of distillation would be good to be presented. Nevertheless, it would be great if authors can provide more analysis why and when the shallow network fails to be as good as deep network than simply presenting the numbers. \n\nIt'll be good to provide training loss for student model and compare with the teacher model to show less overfitting.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?", "abstract": "Yes, apparently they do.\n\nPrevious research by Ba and Caruana (2014)\ndemonstrated that shallow feed-forward nets\nsometimes can learn the complex functions pre-\nviously learned by deep nets while using a simi-\nlar number of parameters as the deep models they\nmimic. In this paper we investigate if shallow\nmodels can learn to mimic the functions learned\nby deep convolutional models. We experiment\nwith shallow models and models with a vary-\ning number of convolutional layers, all trained to\nmimic a state-of-the-art ensemble of CIFAR-10\nmodels. We demonstrate that we are unable to\ntrain shallow models to be of comparable accu-\nracy to deep convolutional models. Although the\nstudent models do not have to be as deep as the\nteacher models they mimic, the student models\napparently need multiple convolutional layers to\nlearn functions of comparable accuracy.\n", "pdf": "/pdf/L7VOrG6lVsRNGwArs4qo.pdf", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_or_even_convolutional", "conflicts": ["uci.edu", "uni-heidelberg.de", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan Shengjie Wang", "Rich Caruana", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "rcaruana@microsoft.com", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580028428, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580028428, "id": "ICLR.cc/2016/workshop/-/paper/155/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "L7VOrG6lVsRNGwArs4qo", "replyto": "L7VOrG6lVsRNGwArs4qo", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/155/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457132999507, "tcdate": 1457132999507, "id": "lx9rypE1wF2OVPy8CvyW", "invitation": "ICLR.cc/2016/workshop/-/paper/155/review/12", "forum": "L7VOrG6lVsRNGwArs4qo", "replyto": "L7VOrG6lVsRNGwArs4qo", "signatures": ["ICLR.cc/2016/workshop/paper/155/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/155/reviewer/12"], "content": {"title": ".", "rating": "10: Top 5% of accepted papers, seminal paper", "review": "This paper extends the work of Ba and Caruana, Do deep nets really need to be deep? by asking the same question about deep _convolutional_ nets, and reaches the opposite conclusion in this new context.\n\nI don't think the conclusion (that deep convents work better than not-deep-convnets on images) is going to surprise anyone; but, the architecture search is quite extensive, so at least this paper provides some circumstantial evidence to support the commonly held intuition.\n\nI do wish the authors had been similarly meticulous when writing the paper as they were when running experiments.  There are a lot of moving parts involved here and I would have really appreciated if some effort was made to synthesize the results in a comprehensible way, rather than simply dumping the all the details into paragraphs of latex and expecting the reader to untangle them.\n\nFor example, understanding what is \"Teacher CNN 1\" on page 3 requires digging into Section 4.7 in the appendix, finding the paragraph talking about 129 CIFAR models trained that talks about the performance of the first and _fifth_ best model, to finally discover that this is the top three performing models from the \"Super Teacher\" ensemble (which then requires a bit more digging to verify that this is the same thing as the \"Ensemble of 16 CNNs\" from the table on page 3).\n\nI am recommending accepting this paper because the experimentation is quite thorough and I think the ICLR workshop is the right venue to to present something like this, but I strongly encourage the authors to spend some effort making tables and diagrams and organizing the presentation of their hyperparamter search in a way that is comprehensible.  I appreciate the level of detail, especially in a paper supporting a negative result with experiments, but the presentation needs serious work.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?", "abstract": "Yes, apparently they do.\n\nPrevious research by Ba and Caruana (2014)\ndemonstrated that shallow feed-forward nets\nsometimes can learn the complex functions pre-\nviously learned by deep nets while using a simi-\nlar number of parameters as the deep models they\nmimic. In this paper we investigate if shallow\nmodels can learn to mimic the functions learned\nby deep convolutional models. We experiment\nwith shallow models and models with a vary-\ning number of convolutional layers, all trained to\nmimic a state-of-the-art ensemble of CIFAR-10\nmodels. We demonstrate that we are unable to\ntrain shallow models to be of comparable accu-\nracy to deep convolutional models. Although the\nstudent models do not have to be as deep as the\nteacher models they mimic, the student models\napparently need multiple convolutional layers to\nlearn functions of comparable accuracy.\n", "pdf": "/pdf/L7VOrG6lVsRNGwArs4qo.pdf", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_or_even_convolutional", "conflicts": ["uci.edu", "uni-heidelberg.de", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan Shengjie Wang", "Rich Caruana", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "rcaruana@microsoft.com", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580027984, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580027984, "id": "ICLR.cc/2016/workshop/-/paper/155/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "L7VOrG6lVsRNGwArs4qo", "replyto": "L7VOrG6lVsRNGwArs4qo", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/155/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455829401118, "tcdate": 1455829401118, "id": "L7VOrG6lVsRNGwArs4qo", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "L7VOrG6lVsRNGwArs4qo", "signatures": ["~Rich_Caruana1"], "readers": ["everyone"], "writers": ["~Rich_Caruana1"], "content": {"CMT_id": "", "title": "Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?", "abstract": "Yes, apparently they do.\n\nPrevious research by Ba and Caruana (2014)\ndemonstrated that shallow feed-forward nets\nsometimes can learn the complex functions pre-\nviously learned by deep nets while using a simi-\nlar number of parameters as the deep models they\nmimic. In this paper we investigate if shallow\nmodels can learn to mimic the functions learned\nby deep convolutional models. We experiment\nwith shallow models and models with a vary-\ning number of convolutional layers, all trained to\nmimic a state-of-the-art ensemble of CIFAR-10\nmodels. We demonstrate that we are unable to\ntrain shallow models to be of comparable accu-\nracy to deep convolutional models. Although the\nstudent models do not have to be as deep as the\nteacher models they mimic, the student models\napparently need multiple convolutional layers to\nlearn functions of comparable accuracy.\n", "pdf": "/pdf/L7VOrG6lVsRNGwArs4qo.pdf", "paperhash": "urban|do_deep_convolutional_nets_really_need_to_be_deep_or_even_convolutional", "conflicts": ["uci.edu", "uni-heidelberg.de", "polymtl.ca", "cs.ualberta.ca", "cs.washington.edu", "microsoft.com"], "authors": ["Gregor Urban", "Krzysztof J. Geras", "Samira Ebrahimi Kahou", "Ozlem Aslan Shengjie Wang", "Rich Caruana", "Abdelrahman Mohamed", "Matthai Philipose", "Matt Richardson"], "authorids": ["gurban@uci.edu", "k.j.geras@sms.ed.ac.uk", "samira.ebrahimi-kahou@polymtl.ca", "ozlem@cs.ualberta.ca", "wangsj@cs.washington.edu", "rcaruana@microsoft.com", "asamir@microsoft.com", "matthaip@microsoft.com", "mattri@microsoft.com"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}