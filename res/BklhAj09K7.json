{"notes": [{"id": "BklhAj09K7", "original": "BJeLJ6KUFm", "number": 931, "cdate": 1538087891880, "ddate": null, "tcdate": 1538087891880, "tmdate": 1547506299278, "tddate": null, "forum": "BklhAj09K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkxtDDta1V", "original": null, "number": 1, "cdate": 1544554337305, "ddate": null, "tcdate": 1544554337305, "tmdate": 1545354499532, "tddate": null, "forum": "BklhAj09K7", "replyto": "BklhAj09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Meta_Review", "content": {"metareview": "This paper proposes a new solution for tackling domain adaptation across disjoint label spaces. Two of the reviewers agree that the main technical approach is interesting and novel. The final reviewer asked for clarification of the problem setting which the authors have provided in their rebuttal. We encourage the authors to include this in the final version. However, there is also a consensus that more experimental evaluation would improve the manuscript and complete experimental details are needed for reliable reproduction.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "An interesting approach for joint domain adaptation and transfer learning"}, "signatures": ["ICLR.cc/2019/Conference/Paper931/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper931/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353030254, "tddate": null, "super": null, "final": null, "reply": {"forum": "BklhAj09K7", "replyto": "BklhAj09K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper931/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper931/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper931/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353030254}}}, {"id": "SJei8ZU81E", "original": null, "number": 4, "cdate": 1544081747313, "ddate": null, "tcdate": 1544081747313, "tmdate": 1544081747313, "tddate": null, "forum": "BklhAj09K7", "replyto": "BklPeLVUyN", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "content": {"title": "response", "comment": "Hi Hui-Po, \n\nThanks for your comment.\n\nAs you mentioned, the conventional domain adaptation problems assume the same \"task\" between the source and the target domains and this allows to transfer discriminative knowledge (e.g., classifier) learned from the source domain to the target domain. On the other hand, not all domains with significant domain shift in the input data space share the same output label spaces, such as cross-ethnicity face recognition or other applications in [1].\n\nIn this work, we resolve such limitation of conventional domain adaptation methods and provide a framework that is also applicable when label spaces of two domains are disjoint by converting disjoint identification tasks into a shared verification task. Note that, as we clarified in our response to R3, the conversion of identification to verification allows the problem definition fits perfectly into that of domain adaptation as the source and target domains now have the shared verification task. That being said, the knowledge we are transferring from source to the target domain is verification, i.e., binary classification for pair of data being the same class or not. This is also evident from our theoretical analysis presented in Section 3 and Appendix A where we prove that the verification error defined on the pair of data from the target domain can be bounded by the verification error on the source pair and the domain discrepancy.\n\nHope this clarifies your concern on \"what kind of knowledge is being transferred\" between two domains. Please let us know if further clarification is required.\n\n[1] Luo et al., Label efficient learning of transferable representations across domains and tasks, NIPS 2017"}, "signatures": ["ICLR.cc/2019/Conference/Paper931/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625022, "tddate": null, "super": null, "final": null, "reply": {"forum": "BklhAj09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper931/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper931/Authors|ICLR.cc/2019/Conference/Paper931/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625022}}}, {"id": "BklPeLVUyN", "original": null, "number": 1, "cdate": 1544074735043, "ddate": null, "tcdate": 1544074735043, "tmdate": 1544074735043, "tddate": null, "forum": "BklhAj09K7", "replyto": "BklhAj09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Public_Comment", "content": {"comment": "Hi authors,\n\nI appreciate you provide thorough and various extension of existing loss functions. However, I would like to know further what's the main problem you want to solve in this work. It seems not to be clear to me.\n\nLet me make a guess and maybe explain the main idea in other words. The proposed method is trying to leverage the \"semantic\" knowledge in the source domain and perform \"clustering\" on those target samples with unseen labels (because labels are disjoint).\n\nAssuming I am correct above, I would like to ask the following questions:\n\nIn conventional domain adaptation problem, we usually assume that both domains share some common knowledge so that you can utilize the knowledge (labels and corresponding discriminative power) from the source domain to solve similar problems in the target domain. In your work, however, both input and label spaces are \"disjoint\". I am curious what kind of knowledge you would like to transfer to the target domain and how you can make sure that the knowledge can be applied to those target samples with unseen labels. If these problems are not clarified, as mentioned by reviewer 3, I would say the major improvement all comes from MCEM, which performs clustering algorithm on target samples, instead of the proposed method.\n\nIf I made any mistake above, please correct me directly.\nThank you for your patient reading.\n\nbest,\n\n", "title": "What's the main task you want to address"}, "signatures": ["~Hui-Po_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hui-Po_Wang1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311718596, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BklhAj09K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311718596}}}, {"id": "HJx8mmk7Am", "original": null, "number": 3, "cdate": 1542808350159, "ddate": null, "tcdate": 1542808350159, "tmdate": 1542808350159, "tddate": null, "forum": "BklhAj09K7", "replyto": "HylVtcW53Q", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for their valuable comments.\n\n(In response to 3) We argue that many distance metric adaptation or transfer learning algorithms in deep learning are based on distribution matching. For example, [3,4] uses discriminator-based adversarial loss and [5] uses kernel-based MMD loss to reduce the domain discrepancy. Regardless of the discriminator or the kernel, these methods will push two domains closeby and thus have the same limitation as DANN. The proposed FTN resolves this issue by learning \u201cdomain-equivariant\u201d representation and we provide empirical evidence (e.g. Table 2 or Figure 2(b-c)) using DANN as the most representative baseline. While one may try adding more components, such as deep supervision (e.g., applying MMD loss at multiple feature layers) as in [5], we believe that our contribution is orthogonal and complementary to those additional components. \n\n(In response to 3) We note that the MCEM is one of our novel contributions, which is only made available through our view on converting the classification task into verification. We agree that it plays a critical role to obtain a highly discriminative representation. For example, [6] considers a similar setting of domain adaptation with disjoint label spaces but they require labeled examples and complete definition of the label space of the target domain to apply classification-based adversarial adaptation learning and entropy regularization. Nonetheless, we provide the within-domain (Table 1) and cross-domain (Table 2) identification accuracy of DANN+MCEM below. We will include this result in the revision:\n\nDANN (for within-domain identification, CAU / AA / EA / ALL; for cross-domain, CAU / AA / EA):\nwithin-domain identification: 89.5 / 75.3 / 78.0 / 78.9\ncross-domain identification: 89.6 / 83.9 / 86.5\n\nDANN+MCEM: \nwithin-domain identification: 90.0 / 80.1 / 81.4 / 81.9\ncross-domain identification: 89.4 / 87.1 / 89.1\n\nFTN+MCEM:\nwithin-domain identification: 90.3 / 80.7 / 82.3 / 83.4\ncross-domain identification: 94.0 / 93.1 / 92.8\n\nSimilarly to the FTN, we observe improvement using MCEM with DANN, as compared to the DANN only model. Comparing between adaptation models with MCEM, we still observe better performance when combined with FTN. Especially, the contrast in performance becomes significant in cross-domain identification task, which confirms the unique capability of FTN in learning to transfer discriminative knowledge by alignment while separating representations across domains.\n\n\n(In response to 1) Our problem setting is adaptation from labeled source to unlabeled target with disjoint label spaces. Following the nomenclature of [1], it contains flavors from both domain adaptation (DA) and transfer learning (TL). The difference in input distribution between source and target domains and the lack of labels in the target domain are similar to that of DA or transductive TL [1], while the difference in label distribution and task definitions between two domains is akin to inductive TL [1,2]. In our work, we formalize this problem in domain adaptation framework using verification as a common task. This is a key contribution that allows theoretical analysis on the generalization bound as presented in Section 3 and Appendix A, while also allowing important novel applications like cross-ethnicity face recognition.\n\n\n(In response to 2) We acknowledged in the second paragraph of Section 2 some existing works on domain adaptation that use the verification loss for problems such as face recognition and person re-identification, while highlighting our novel contribution. We will include more discussion and references [5] related to this.\n\n\n[1] Pan and Yang, A survey on Transfer Learning, 2010\n[2] Daume, https://nlpers.blogspot.com/2007/11/domain-adaptation-vs-transfer-learning.html\n[3] Ganin et al., Domain Adversarial Training of Neural Networks, JMLR 2016\n[4] Sohn et al., Unsupervised domain adaptation for face recognition in unlabeled videos, ICCV 2017\n[5] Hu et al., Deep Transfer Metric Learning, CVPR 2015\n[6] Luo et al., Label efficient learning of transferable representations across domains and tasks, NIPS 2017\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper931/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625022, "tddate": null, "super": null, "final": null, "reply": {"forum": "BklhAj09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper931/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper931/Authors|ICLR.cc/2019/Conference/Paper931/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625022}}}, {"id": "Sy86MymAm", "original": null, "number": 2, "cdate": 1542808253516, "ddate": null, "tcdate": 1542808253516, "tmdate": 1542808253516, "tddate": null, "forum": "BklhAj09K7", "replyto": "ByemonjVnm", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank the reviewer for their valuable comments.\n\nWe understand the concern in Table 3 that the performance improvement is not as significant as in Table 1. As mentioned in footnote 5, we observe that the ethnicity bias not only exists in the training dataset, but also in public benchmark datasets, such as LFW or IJB-A. While we observe the benefit of FTN over source only model in all evaluation metrics or over DANN in low FAR regime, thus requiring more within as well as cross-domain discriminativeness, we believe that these datasets may not be the best to evaluate the fairness of face recognition algorithms. This indeed is our motivation to collect an ethnicity-balanced test dataset for fair evaluation. We will make the dataset publicly available to the community upon publication."}, "signatures": ["ICLR.cc/2019/Conference/Paper931/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625022, "tddate": null, "super": null, "final": null, "reply": {"forum": "BklhAj09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper931/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper931/Authors|ICLR.cc/2019/Conference/Paper931/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625022}}}, {"id": "rylttGymA7", "original": null, "number": 1, "cdate": 1542808193361, "ddate": null, "tcdate": 1542808193361, "tmdate": 1542808193361, "tddate": null, "forum": "BklhAj09K7", "replyto": "BJecfeN52Q", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "content": {"title": "clarification on feature reconstruction loss", "comment": "We thank the reviewer for their valuable comments.\n\n1. We clarify that the reference network is pretrained on the labeled source data and fixed over the training of DANN/FTN. In other words, the gradient in Equation(6) is only backpropagated through f, but not through f_{ref}.\n\nWe note that the training procedure of reference network resembles the training of teacher network in distillation framework [1], in the sense that both teacher network and our reference network are \u201cpretrained and fixed\u201d during the training of student or DANN/FTN, respectively.\n\n[1] Hinton et al., Distilling the knowledge in a neural network, NIPS 2014 DL Workshop\n\n2. We will add a reference (section 3 and appendix) as suggested.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper931/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625022, "tddate": null, "super": null, "final": null, "reply": {"forum": "BklhAj09K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper931/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper931/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper931/Authors|ICLR.cc/2019/Conference/Paper931/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper931/Reviewers", "ICLR.cc/2019/Conference/Paper931/Authors", "ICLR.cc/2019/Conference/Paper931/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625022}}}, {"id": "BJecfeN52Q", "original": null, "number": 3, "cdate": 1541189649588, "ddate": null, "tcdate": 1541189649588, "tmdate": 1541533567651, "tddate": null, "forum": "BklhAj09K7", "replyto": "BklhAj09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Official_Review", "content": {"title": "A good paper addressing domain adaptation for disjoint labels.", "review": "The authors studied an interesting problem of unsupervised domain adaptation when the source and the target domains have disjoin labels spaces. The paper proposed a novel feature transfer network, that optimizes domain adversarial loss and domain separation loss.\n\nStrengths:\n\n1) The proposed approach on Feature Transfer Network was novel and interesting.\n2) The paper was very well written with a good analysis of various choices.\n3) Extensive empirical analysis on multi-class settings with a traditional MNIST dataset and a real-world face recognition dataset. \n\n\nWeakness:\n1) Practical considerations addressing feature reconstruction loss needs more explanation.\n\nComments:\n\nThe technical contribution of the paper was sound and novel. The paper considered existing work and in a good way generalizes and extends into disjoint label spaces. It was easy to read and follow, most parts of the paper including the Appendix make it a good contribution. However, the reviewer has the following suggestions\" \n\n1. Under the practical considerations for preventing the mode collapse via feature reconstruction, how is the reference network trained? In the Equation(6) for feature reconstruction, the f_ref term maps the source and target domain examples to new feature space. What do you mean by references network trained on the label data? Please clarify.\n\n2. Under the practical considerations for replacing the verification loss, it is said that \"Our theoretical analysis suggests to use a verification\nthe loss that compares the similarity between a pair of images\" - Can you please cite the references to make it easier for the reader to follow.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper931/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Official_Review", "cdate": 1542234344138, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BklhAj09K7", "replyto": "BklhAj09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper931/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335835817, "tmdate": 1552335835817, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper931/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HylVtcW53Q", "original": null, "number": 2, "cdate": 1541180028259, "ddate": null, "tcdate": 1541180028259, "tmdate": 1541533567426, "tddate": null, "forum": "BklhAj09K7", "replyto": "BklhAj09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Official_Review", "content": {"title": "The motivation is clear but the experiments are not sufficient.", "review": "In this work, authors consider transfer learning problem when labels for the target domain is not available. Unlike the conventional transfer learning, they introduce a new loss that separates examples from different domains. Besides, they apply the multi-class entropy minimization to optimize the performance in the target domain. Here are my concerns.\n1.\tThe concept is not clear. For domain adaptation, we usually assume domains share the same label space. When labels are different, it can be a transfer learning problem.\n2.\tOptimizing the verification loss is conventional for distance metric learning based transfer learning and authors should discuss more in the related work.\n3.\tThe empirical study is not sufficient. There lacks the method of transfer learning with distance metric learning. Moreover, the major improvement seems from the MCEM rather than the proposed network. How about DANN+MCEM?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper931/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Official_Review", "cdate": 1542234344138, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BklhAj09K7", "replyto": "BklhAj09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper931/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335835817, "tmdate": 1552335835817, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper931/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByemonjVnm", "original": null, "number": 1, "cdate": 1540828315403, "ddate": null, "tcdate": 1540828315403, "tmdate": 1541533567005, "tddate": null, "forum": "BklhAj09K7", "replyto": "BklhAj09K7", "invitation": "ICLR.cc/2019/Conference/-/Paper931/Official_Review", "content": {"title": "Interesting paper addressing a difficult problem. Good formalization and reasonable evaluation ", "review": "I like the idea of the paper and I believe it addressing a very relevant problem. While the authors provide a good formalization of the problem and convincing demonstration of the generalization bound, the evaluation could have been better by including some more challenging experiments to really prove the point of the paper. It is surely good to present the toy example with the MNIST dataset but the ethnicity domain is less difficult than what the authors claim. This is also pretty evident from the results presented (e.g., in Table 3). The proposed approach provides maybe slightly better results than the state of the art but the results do not seem to be statistically significant. This is probable also due to the fact that the problem itself is made simpler by the cropped faces, no background, etc. I would have preferred to see an application domain where the improvement would be more substantial. Nevertheless, I think the theoretical presentation is good and I believe the manuscript has very good potential. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper931/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Domain Adaptation for Distance Metric Learning", "abstract": "Unsupervised domain adaptation is a promising avenue to enhance the performance of deep neural networks on a target domain, using labels only from a source domain. However, the two predominant methods, domain discrepancy reduction learning and semi-supervised learning, are not readily applicable when source and target domains do not share a common label space. This paper addresses the above scenario by learning a representation space that retains discriminative power on both the (labeled) source and (unlabeled) target domains while keeping representations for the two domains well-separated. Inspired by a theoretical analysis, we first reformulate the disjoint classification task, where the source and target domains correspond to non-overlapping class labels, to a verification one. To handle both within and cross domain verifications, we propose a Feature Transfer Network (FTN) to separate the target feature space from the original source space while aligned with a transformed source space. Moreover, we present a non-parametric multi-class entropy minimization loss to further boost the discriminative power of FTNs on the target domain. In experiments, we first illustrate how FTN works in a controlled setting of adapting from MNIST-M to MNIST with disjoint digit classes between the two domains and then demonstrate the effectiveness of FTNs through state-of-the-art performances on a cross-ethnicity face recognition problem.\n", "keywords": ["domain adaptation", "distance metric learning", "face recognition"], "authorids": ["kihyuk.sohn@gmail.com", "wendyshang1208@gmail.com", "xiangyu@nec-labs.com", "manu@nec-labs.com"], "authors": ["Kihyuk Sohn", "Wenling Shang", "Xiang Yu", "Manmohan Chandraker"], "TL;DR": "A new theory of unsupervised domain adaptation for distance metric learning and its application to face recognition across diverse ethnicity variations.", "pdf": "/pdf/e74ca52b8d3c2088099523253d1f999a64ae2e93.pdf", "paperhash": "sohn|unsupervised_domain_adaptation_for_distance_metric_learning", "_bibtex": "@inproceedings{\nsohn2018unsupervised,\ntitle={Unsupervised Domain Adaptation for Distance Metric Learning},\nauthor={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BklhAj09K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper931/Official_Review", "cdate": 1542234344138, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BklhAj09K7", "replyto": "BklhAj09K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper931/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335835817, "tmdate": 1552335835817, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper931/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}