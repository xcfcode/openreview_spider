{"notes": [{"id": "S1grRoR9tQ", "original": "Bkgzvl_tKX", "number": 894, "cdate": 1538087885466, "ddate": null, "tcdate": 1538087885466, "tmdate": 1545355410551, "tddate": null, "forum": "S1grRoR9tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation", "abstract": "We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic \u201cdropout\u201d and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on the convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks. ", "keywords": ["generalized stochastic approximation", "stochastic gradient Markov chain Monte Carlo", "adaptive algorithm", "EM algorithm", "convolutional neural networks", "Bayesian inference", "sparse prior", "spike and slab prior", "local trap"], "authorids": ["deng106@purdue.edu", "zhang923@purdue.edu", "fmliang@purdue.edu", "guanglin@purdue.edu"], "authors": ["Wei Deng", "Xiao Zhang", "Faming Liang", "Guang Lin"], "TL;DR": "a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables", "pdf": "/pdf/a833d91ac20ca1e9f766344271cd57a51fd1d187.pdf", "paperhash": "deng|bayesian_deep_learning_via_stochastic_gradient_mcmc_with_a_stochastic_approximation_adaptation", "_bibtex": "@misc{\ndeng2019bayesian,\ntitle={Bayesian Deep Learning via Stochastic Gradient {MCMC} with a Stochastic Approximation Adaptation},\nauthor={Wei Deng and Xiao Zhang and Faming Liang and Guang Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1grRoR9tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1xyINe1gV", "original": null, "number": 1, "cdate": 1544647751439, "ddate": null, "tcdate": 1544647751439, "tmdate": 1545354503676, "tddate": null, "forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper894/Meta_Review", "content": {"metareview": "This paper proposes a Bayesian alternative to dropout for deep networks by extending the EM-based variable selection method with SG-MCMC for sampling weights and stochastic approximation for tuning hyper-parameters. The method is well presented with a clear motivation. The combination of SMVS, SG-MCMC, and SA as a mixed optimization-sampling approach is technically sound.\n\nThe main concern raised by the readers is the limited originality. SG-MCMC has been studied extensively for Bayesian deep networks and applying the spike-and-slab prior as an alternative to dropout is a straightforward idea. The main contribution of the paper appears to be extending EMVS to deep net with commonly used sampling techniques for Bayesian networks.\n\nAnother concern is the lack of experimental justification for the advantage of the proposed method. While the authors promise to include more experiment results in the camera-ready version, it requires a considerable amount of effort and the decision unfortunately has to be made based on the current revision.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Good Bayesian approach to deep networks with spike-and-slab prior but with limited originality and lack of experiment support"}, "signatures": ["ICLR.cc/2019/Conference/Paper894/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper894/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation", "abstract": "We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic \u201cdropout\u201d and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on the convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks. ", "keywords": ["generalized stochastic approximation", "stochastic gradient Markov chain Monte Carlo", "adaptive algorithm", "EM algorithm", "convolutional neural networks", "Bayesian inference", "sparse prior", "spike and slab prior", "local trap"], "authorids": ["deng106@purdue.edu", "zhang923@purdue.edu", "fmliang@purdue.edu", "guanglin@purdue.edu"], "authors": ["Wei Deng", "Xiao Zhang", "Faming Liang", "Guang Lin"], "TL;DR": "a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables", "pdf": "/pdf/a833d91ac20ca1e9f766344271cd57a51fd1d187.pdf", "paperhash": "deng|bayesian_deep_learning_via_stochastic_gradient_mcmc_with_a_stochastic_approximation_adaptation", "_bibtex": "@misc{\ndeng2019bayesian,\ntitle={Bayesian Deep Learning via Stochastic Gradient {MCMC} with a Stochastic Approximation Adaptation},\nauthor={Wei Deng and Xiao Zhang and Faming Liang and Guang Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1grRoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper894/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353044728, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper894/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper894/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper894/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353044728}}}, {"id": "Sygu7OqSxE", "original": null, "number": 6, "cdate": 1545082911749, "ddate": null, "tcdate": 1545082911749, "tmdate": 1545082911749, "tddate": null, "forum": "S1grRoR9tQ", "replyto": "SJlutyw5am", "invitation": "ICLR.cc/2019/Conference/-/Paper894/Official_Comment", "content": {"title": "Reply", "comment": "I\u2019m not fully convinced that replacing MAP by SG-MCMC in EMVS can solve the local trap problem. It is known that SG-MCMC could be locally trapped and there are methods to alleviate this problem such as adjusting the temperature you mentioned. But the paper did not use any of these techniques. I think only using SG-MCMC itself may not be able to solve the local trap problem. Besides, the empirical results could not demonstrate that the improvement over EMSV is because SG-MCMC-SA solves the local trap. The regression experiment only has one mode, so there is no local trap problem, why is SG-MCMC-SA better than EMSV here? I wonder if ESM will perform similarly to SG-MCMC-SA on this task. It would be better to compare to ESM in all the experiments since it is one of the most related methods.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper894/AnonReviewer3"], "readers": ["ICLR.cc/2019/Conference/Paper894/Reviewers", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper894/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper894/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation", "abstract": "We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic \u201cdropout\u201d and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on the convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks. ", "keywords": ["generalized stochastic approximation", "stochastic gradient Markov chain Monte Carlo", "adaptive algorithm", "EM algorithm", "convolutional neural networks", "Bayesian inference", "sparse prior", "spike and slab prior", "local trap"], "authorids": ["deng106@purdue.edu", "zhang923@purdue.edu", "fmliang@purdue.edu", "guanglin@purdue.edu"], "authors": ["Wei Deng", "Xiao Zhang", "Faming Liang", "Guang Lin"], "TL;DR": "a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables", "pdf": "/pdf/a833d91ac20ca1e9f766344271cd57a51fd1d187.pdf", "paperhash": "deng|bayesian_deep_learning_via_stochastic_gradient_mcmc_with_a_stochastic_approximation_adaptation", "_bibtex": "@misc{\ndeng2019bayesian,\ntitle={Bayesian Deep Learning via Stochastic Gradient {MCMC} with a Stochastic Approximation Adaptation},\nauthor={Wei Deng and Xiao Zhang and Faming Liang and Guang Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1grRoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper894/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619670, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1grRoR9tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper894/Authors", "ICLR.cc/2019/Conference/Paper894/Reviewers", "ICLR.cc/2019/Conference/Paper894/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper894/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper894/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper894/Authors|ICLR.cc/2019/Conference/Paper894/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper894/Reviewers", "ICLR.cc/2019/Conference/Paper894/Authors", "ICLR.cc/2019/Conference/Paper894/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619670}}}, {"id": "BJeOIPB9nQ", "original": null, "number": 3, "cdate": 1541195600292, "ddate": null, "tcdate": 1541195600292, "tmdate": 1541533601202, "tddate": null, "forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper894/Official_Review", "content": {"title": "Unclear benefits of SG-MCMC with SA and the experiments are not sufficiently convincing", "review": "The authors describe a new method of posterior sampling with latent variables based on SG-MCMC and stochastic approximation (SA). The new method uses a spike and slab prior on the weights of the deep neural networks to encourage sparsity. Experiments on toy regressions, classification and adversarial attacks demonstrate the superiority over SG-MCMC and EMSV.\n\nCompared to the previous work EMSV (ESM), the novelty of SG-MCMC-SA is replacing the MAP in EMSV by SG-MCMC with stochastic approximation to alleviate the local trap problem in DNNs. However, I did not see why SG-MCMC with SA can achieve this goal. It is known that SG-MCMC methods tend to get trapped in a local optimal [1]. How did SA solve this problem? Besides, it is unclear to me where Eq. 17 uses stochastic approximation. The authors need to explain more about stochastic approximation for the readers who are not familiar with this method. \n\nEmpirical results on a synthetic example, MNIST and FMNIST show that SG-MCMC-SA outperforms the previous methods. However, the improvements of the proposed method are marginal. MNIST and FMNIST are small and easy datasets and it is very hard to tell the effectiveness of SG-MCMC-SA. It would be more convincing to show the empirical results on other datasets, e.g. CIFAR, using some larger architectures. The comparison would be more significant in that case. \n\n[1]. Zhang, Yizhe, et al. \"Stochastic Gradient Monomial Gamma Sampler.\" arXiv preprint arXiv:1706.01498 (2017).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper894/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation", "abstract": "We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic \u201cdropout\u201d and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on the convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks. ", "keywords": ["generalized stochastic approximation", "stochastic gradient Markov chain Monte Carlo", "adaptive algorithm", "EM algorithm", "convolutional neural networks", "Bayesian inference", "sparse prior", "spike and slab prior", "local trap"], "authorids": ["deng106@purdue.edu", "zhang923@purdue.edu", "fmliang@purdue.edu", "guanglin@purdue.edu"], "authors": ["Wei Deng", "Xiao Zhang", "Faming Liang", "Guang Lin"], "TL;DR": "a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables", "pdf": "/pdf/a833d91ac20ca1e9f766344271cd57a51fd1d187.pdf", "paperhash": "deng|bayesian_deep_learning_via_stochastic_gradient_mcmc_with_a_stochastic_approximation_adaptation", "_bibtex": "@misc{\ndeng2019bayesian,\ntitle={Bayesian Deep Learning via Stochastic Gradient {MCMC} with a Stochastic Approximation Adaptation},\nauthor={Wei Deng and Xiao Zhang and Faming Liang and Guang Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1grRoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper894/Official_Review", "cdate": 1542234352640, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper894/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335827336, "tmdate": 1552335827336, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper894/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByguuVqt3X", "original": null, "number": 2, "cdate": 1541149808126, "ddate": null, "tcdate": 1541149808126, "tmdate": 1541533600994, "tddate": null, "forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper894/Official_Review", "content": {"title": "Interesting work, but in my view not substantial novelty and significance", "review": "TITLE\nBayesian deep learning via stochastic gradient mcmc with a stochastic approximation adaptation\n\nREVIEW SUMMARY\nFairly well written paper on SG-MCMC type inference in neural networks with slab and spike priors. In my view, the originality and significance is limited.\n\nPAPER SUMMARY\nThe paper develops a method for sampling/optimization of a Bayesian neural network with slab and spike priors on the weights.\n\nQUALITY\nI belive the contribution is technically sound (but I have not checked all equations or the proof of Theorem 1). The empirical evaluation is not unreasonable, but also not strongly convincing.\n\nCLARITY\nThe paper is fairly well written, but grammar and use of English could be slightly improved (not so important).    \n\nORIGINALITY\nThe paper builds on existing work on EM-type algorithms for slab and spike models and SG-MCMC for Bayesian inference in neural networks. The novelty of the contribution is limited: The main contribution is the combination of the two methods and some theoretical results. I am not able to judge if there is significant originality in the theoretical results (Theorem 1 + Corr 1+2) but if I am not mistaken it is more or less an application of a known result to this particular setting?\n\nSIGNIFICANCE\nWhile I think the proposed algorithm is reasonable and most likely useful in practice, I am not sure the contribution is substantial enough to gain large interest in the community.  \n\nFURTHER COMMENTS\nFigure 2 (d+e) are in my view not so useful for assessing the training/test performance, but I am not even completely sure what the figures shows, as there are no axis labels. I would prefer some results on the loss, perhaps averaged over multiple data sets.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper894/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation", "abstract": "We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic \u201cdropout\u201d and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on the convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks. ", "keywords": ["generalized stochastic approximation", "stochastic gradient Markov chain Monte Carlo", "adaptive algorithm", "EM algorithm", "convolutional neural networks", "Bayesian inference", "sparse prior", "spike and slab prior", "local trap"], "authorids": ["deng106@purdue.edu", "zhang923@purdue.edu", "fmliang@purdue.edu", "guanglin@purdue.edu"], "authors": ["Wei Deng", "Xiao Zhang", "Faming Liang", "Guang Lin"], "TL;DR": "a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables", "pdf": "/pdf/a833d91ac20ca1e9f766344271cd57a51fd1d187.pdf", "paperhash": "deng|bayesian_deep_learning_via_stochastic_gradient_mcmc_with_a_stochastic_approximation_adaptation", "_bibtex": "@misc{\ndeng2019bayesian,\ntitle={Bayesian Deep Learning via Stochastic Gradient {MCMC} with a Stochastic Approximation Adaptation},\nauthor={Wei Deng and Xiao Zhang and Faming Liang and Guang Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1grRoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper894/Official_Review", "cdate": 1542234352640, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper894/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335827336, "tmdate": 1552335827336, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper894/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyxiMQrDh7", "original": null, "number": 1, "cdate": 1540997906886, "ddate": null, "tcdate": 1540997906886, "tmdate": 1541533600777, "tddate": null, "forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper894/Official_Review", "content": {"title": "The proposed SGLD-SA algorithm with its convergence properties is interesting", "review": "* The proposed SGLD-SA algorithm, together with its convergence properties, is very interesting. The introduction of step size $w^{k}$ is very similar to the \"convex combination rule\" in (Zhang & Brand 2017) to guarantee convergence.\n  \n* It seems that this paper only introduced Bayesian inference in the output layers. It would be more interesting to have a complete Bayesian model for the full network including the inner and activation layers.\n\n* This paper imposed spike-and-slab prior on the weight vector which can yield sparse connectivity. Similar ideas have been explored to compress the model size of deep networks (Lobacheva, Chirkova and Vetrov 2017; Louizos, Ullrich and Welling 2017 ). It would make this paper stronger to compare the sparsification and compression properties with the above work.\n\n* In equation (11) there is a summation from $\\beta_{p+1}$ to $\\beta_{p+u}$. I wonder where this term comes from, as I thought $\\beta$ is a vector of dimension $p$.\n\nReference:\nZhang, Ziming, and Matthew Brand. \"Convergent block coordinate descent for training tikhonov regularized deep neural networks.\" Advances in Neural Information Processing Systems. 2017.\n\nLobacheva, Ekaterina, Nadezhda Chirkova, and Dmitry Vetrov. \"Bayesian Sparsification of Recurrent Neural Networks.\" arXiv preprint arXiv:1708.00077 (2017).\n\nLouizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" Advances in Neural Information Processing Systems. 2017.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper894/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation", "abstract": "We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic \u201cdropout\u201d and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on the convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks. ", "keywords": ["generalized stochastic approximation", "stochastic gradient Markov chain Monte Carlo", "adaptive algorithm", "EM algorithm", "convolutional neural networks", "Bayesian inference", "sparse prior", "spike and slab prior", "local trap"], "authorids": ["deng106@purdue.edu", "zhang923@purdue.edu", "fmliang@purdue.edu", "guanglin@purdue.edu"], "authors": ["Wei Deng", "Xiao Zhang", "Faming Liang", "Guang Lin"], "TL;DR": "a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables", "pdf": "/pdf/a833d91ac20ca1e9f766344271cd57a51fd1d187.pdf", "paperhash": "deng|bayesian_deep_learning_via_stochastic_gradient_mcmc_with_a_stochastic_approximation_adaptation", "_bibtex": "@misc{\ndeng2019bayesian,\ntitle={Bayesian Deep Learning via Stochastic Gradient {MCMC} with a Stochastic Approximation Adaptation},\nauthor={Wei Deng and Xiao Zhang and Faming Liang and Guang Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=S1grRoR9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper894/Official_Review", "cdate": 1542234352640, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1grRoR9tQ", "replyto": "S1grRoR9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper894/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335827336, "tmdate": 1552335827336, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper894/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}