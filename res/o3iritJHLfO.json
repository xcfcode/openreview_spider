{"notes": [{"id": "o3iritJHLfO", "original": "VXoQtyu2jJx", "number": 3537, "cdate": 1601308392551, "ddate": null, "tcdate": 1601308392551, "tmdate": 1615993570394, "tddate": null, "forum": "o3iritJHLfO", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8yuOuoixLg", "original": null, "number": 1, "cdate": 1610040411934, "ddate": null, "tcdate": 1610040411934, "tmdate": 1610474009420, "tddate": null, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Non autoregressive modelling for text to speech (TTS) is an important and challenging problem. This paper proposes a deep VAE approach and show promising results. Both the reviewers and the authors have engaged in a constructive discussion on the merits and claims of the paper. This paper will not be the final VAE contribution to TTS but represents a significant enough contribution to the field to warrant publication. It is highly recommended that the authors take into account the reviewers' comments."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040411921, "tmdate": 1610474009403, "id": "ICLR.cc/2021/Conference/Paper3537/-/Decision"}}}, {"id": "K-8KCHnVmK", "original": null, "number": 4, "cdate": 1604045809775, "ddate": null, "tcdate": 1604045809775, "tmdate": 1607127889783, "tddate": null, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review", "content": {"title": "Novel, fast architecture with many insightful ideas - accept", "review": "Post rebuttal and discussion\n========================\nSeveral reviewers have pointed out that the paper needs more comparisons/ablations with existing models (e.g. Paranet/Fastnet). To this end, I think we at least need a comparison with Paranet, which is a 'comparable' non-autoregressive CNN based VAE based model with a few other components such as attention distillation. \n\nThere are components in the paper that could do with more ablation studies \n- argmax with straight through estimator\n- some guidelines on BVAE blocks and tuning\n\nIn light of these points, together with the fact that we don't have any theoretical novelties in this paper, I reduce my score to 6. Even so, I feel that the paper would be a valuable contribution because \na) A generative model (GAN/VAE/VQVAE/Flow based models/score matching based models) might add extra benefit in the synthesis problem, as compared with a supervised model without a similar generative component such as Tacotron. The NVAE has been shown to significantly outperform the regular VAE in image generation tasks. It stands to reason that it would do well in speech generation also. \nb) Speed, robustness and ease of implementation (although this remains to be demonstrated).\n \nInitial Review\n===========\n\nThis paper proposes a non-autoregressive (non AR) way to perform text to speech synthesis. It uses a VAE based setup - adapted from the recent image paper NVAE to build two stacks of hierarchical VAE blocks  (in priors), one going bottom up and the other, top down. The key claims are that it results in improved speed, and reduced model footprint from using a non AR architecture, with excellent quality comparable to the best autoregressive/recurrent methods in Tacotron2 [2] and non AR glow-TTS[3].\n\nThe work contains many interesting ideas for TTS, and I am very interested in seeing how this work pans out in practical speech synthesis applications.\n\nKey ideas:\n1. The bidirectional stack, which they call BVAE is adapted from the recent NVAE work which has produced stellar image generations. The model uses 1D convolutions under the hood, in contrast with the fashionable, but slow autoregressive flows or recurrent models. If one can get such a model to work, it could be advantageous in effecting savings in computational time and model size. \n\nDuring training, at the top of the bottom-up stack, text features are inflated to the size of the mel spectrogram features, and reconstructed with the top down BVAE stack. For inference, text is inflated to an expanded text matching audio mels, and then sent down the top-down stack to give a mel sample.\n\n2. Attention modeling: An important consideration here is to align text and mel, commonly done with an attention mechanism. In this work, the attention alignment shows up as a duration model, which is rather interesting, and seemingly gives additional flexibility. After aligning text and mel (using dot product), the alignment can be reinterpreted as a duration model by comparing phoneme and mel frame alignments. Furthermore, they use a discrete match with argmax rather than a sum over all attention alignments as is generally done. This also necessitates the use of the straight-through estimator while backpropagating since the durations are rounded entities. This type of modeling seems also to be used in the Glow-TTS  work but with alignments determined through dynamic programming.\n\nI found the result that the model is not very sensitive to alignment mismatches to be quite remarkable.\n\n3. Fittings for robustness during inference: They use several instructive ideas - jittering text, adding positional embeddings, diagonal penalty (since alignment is mostly diagonal) and KLD annealing. \n\n4. Analyses - ablations to see which of the VAE blocks affect the result by varying temperature (from Glow [3]).\n\nMy thoughts:\nGenerally, the paper made for fascinating reading. Having worked with Tacotron, I have always felt that adding a VAE to that (RNN based) setup would improve its generative capabilities by giving it additional regularization qualities, among other things. That we can see the model perform better when we add jitter and can also respond to the duration specified seems to corroborate that in a loose way (figure 10). \n\n- Could the authors clarify how the duration modeling results in 'monotonic' alignments? As far as I can see, the argmax guarantees a unique match, but is monotonicity necessary?\n\nFrom section 5.3.2:\n\"Since the text is forced to be used monotonically in the duration-based generation, it makes the model more robust to the attentionerrors while making fewer pronouncing mistakes.\"\n\n- A comparison with an equivalent soft attention implementation might be insightful. \n\n- Multi Speaker TTS: I am wondering how this model would perform in a multispeaker dataset, say libritts. One aspect that the paper does not touch in detail is in its capabilities as a generative model. It would be interesting, for instance, to see if this model can in any way separate speaker style from content with a multispeaker model.\n\nOverall, I think this paper would be a good addition to the body of speech synthesis work, and recommend that it is accepted.\n\n\n[1] NVAE: https://arxiv.org/pdf/2007.03898.pdf\n[2]: Tacotron2: https://arxiv.org/pdf/1712.05884.pdf\n[3] Glow-TTS: https://arxiv.org/pdf/2005.11129.pdf\n[4]: Glow: https://arxiv.org/pdf/1807.03039.pdf\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074207, "tmdate": 1606915775929, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3537/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review"}}}, {"id": "5QX06tv9adT", "original": null, "number": 3, "cdate": 1603942455863, "ddate": null, "tcdate": 1603942455863, "tmdate": 1606260289198, "tddate": null, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review", "content": {"title": "Potentially valuable contribution to parallel TTS, with some concerns. ", "review": "Summary:\nThis paper presents BVAE-TTS, which applies hierarchical VAEs (using an approach motivated by NVAE and Ladder VAEs) to the problem of parallel TTS.  The main components of the system are a dot product-based attention mechanism that is used during training to produce phoneme duration targets for the parallel duration predictor (that is used during synthesis) and the hierarchical VAE that converts duration-replicated phoneme features into mel spectrogram frames (which are converted to waveform samples using a pre-trained WaveGlow vocoder).  The system is compared to Glow-TTS (a similar parallel system that uses flows instead of VAEs) and Tacotron 2 (a non-parallel autoregressive system) in terms of MOS naturalness, synthesis speed, and parameter efficiency. \n\n\nReasons for score:\nOverall, I think the system presented in this paper could be a valuable contribution to the field of end-to-end TTS; however, from a machine learning perspective, the contributions are incremental and quite specific to TTS.  In addition, I have some slight concerns about the clarity of the presentation that made it harder to understand the (fairly simple) approach and its motivation than I\u2019d expect from an ICLR paper.  Finally, the quality of the speech produced by the system is only evaluated on a single dataset and uses only 50 synthesized examples in the subjective ratings.  For these reasons, I feel this paper would be a better fit for a speech conference or journal after addressing the evaluation and presentation issues, but I would still support acceptance if other reviewers push for it and my concerns are addressed. \n\n\nHigh-level Comments:\n* The speed, parameter efficiency, and MOS results are quite promising.  However, when considering the Glow-TTS paper (which this seems like a direct followup to), the system improvements seem quite incremental (replace flows with HVAEs and replace the monotonic alignment search with soft attention plus argmax).  \n* Incremental system improvements are great if they result in significant improvements that are demonstrated through rigorous experiments, however, compared to Glow-TTS, the experiments are not nearly as comprehensive and convincing. Listening to a few of the audio examples provided in the supplemental materials, I don\u2019t get the sense that the audio quality is significantly better than that of Glow-TTS as is suggested by the MOS numbers (BVAE-TTS sounds a bit muffled to my ears relative to Glow-TTS). \n* Since this system uses the same deterministic duration prediction paradigm as Glow-TTS (and other parallel TTS systems), it suffers from the same duration averaging effects and inability to sample from the full distribution of prosodic realizations.  \n* The motivation would be made clearer if you were more specific early on about the potential advantage of VAE's relative to flows however you want to describe it (parameter efficiency, more flexible layer architectures, more powerful transformations per layer, etc.).  \n* I'd recommend providing similar motivation for using dot-product soft attention plus straight-through argmax instead of Glow-TTS's alignment search or other competing approaches.  Is it because it's a superior approach or just because it's different from existing approaches? \n\nDetailed Comments:\n* Section 2:  I don\u2019t believe Tacotron is actually the *first* end-to-end TTS system.  Maybe it was the first to gain widespread attention, but I know that char2wav (if you count that as e2e TTS) preceded it chronologically in terms of first arxiv submission date.\n* Section 2: The Related Work section is fairly redundant with information that is already presented in the introduction.  It might be worth combining the two sections.  This should free up space for additional experiments, explanations, or analysis. \n* Section 4.1: The first paragraph here was quite confusing upon a first reading.  I had to read the second sentence (\u201cVia the attention network\u2026\u201d) many times to understand what was being described.\n* Section 5.2: I\u2019m curious how you arrived at a sample temperature of 0.333.  Was this empirically tuned for BVAE-TTS or in response to Glow-TTS\u2019s findings? \n* Section 5.2, \u201cInference Time\u201d: It seems important to include details about the hardware platform used to gather the speed results. \n* There are minor English style and grammar issues throughout the paper that make the paper slightly more difficult to read.  Please have the paper proofread to improve readability. \n\nUpdate (Nov 24, 2020):\nAfter reading through the author responses and the updated version of the paper, I feel like a sufficient number of my concerns have been addressed to increase my score to 6.  Specifically, the motivation has been made clearer, the related work section is no longer redundant with the intro, and the authors gave an adequate explanation about the necessity of their attention-based alignment method.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074207, "tmdate": 1606915775929, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3537/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review"}}}, {"id": "qQJH4OGmi-9", "original": null, "number": 9, "cdate": 1605675016868, "ddate": null, "tcdate": 1605675016868, "tmdate": 1606208842414, "tddate": null, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "The revised paper is uploaded. (Last edited Nov 23 21:05 AoE)", "comment": "We have uploaded a revised paper to incorporate the reviewers' comments, concerns, and suggestions.  \nThank all the reviewers for their constructive comments and extensive analysis that are really helpful to make our paper more complete.  \n  \nSpecifically, the updated version includes:  \n* We have modified the configuration of the paper focusing on clarifying the motivation and advantages of BVAE-TTS.  \n* We have done much more extensive proofreading to improve its readability and we have tried to help readers understand our approach by adding more explanations, including the pseudo-codes for training and inference of BVAE-TTS in the appendix section.  \n* Supplementary material has been updated including the audio samples for MOS-OOD.  \n* Minor typos and inconsistent reference format have been fixed in the revised version. (Last edited Nov 23 21:05 AoE)  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "wc7GszqUOZg", "original": null, "number": 11, "cdate": 1606190811579, "ddate": null, "tcdate": 1606190811579, "tmdate": 1606190811579, "tddate": null, "forum": "o3iritJHLfO", "replyto": "8BZS_mAPEie", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "Thank you for your careful reply", "comment": "Thanks for your reply, and below are our answers to your questions.  \n  \nQ1. You mean when you use simple VAE, the TTS model also fails to learn?  \nA1. Yes. In that case, the training of the TTS model also failed.  \n  \nQ2. To show the robustness of your model, you should compare with more robust TTS\u3000models such as FastSpeech, or Tacotron 2 with a location-relative attention mechanism. So this is a wrong statement \"our BVAE-TTS outperforms Glow-TTS\"  \nA2-1. Thank you for your careful reply and we all agree with your opinion. We are also very sorry that we cannot conduct the experiments on the non-AR models such as FastSpeech, because there is no officially released source codes or weights.  \nA2-2. As you mentioned, although BVAE-TTS has lost to Tacotron 2 in terms of speech quality on the in-domain dataset, we think our BVAE-TTS outperforms Glow-TTS, which is the state-of-the-art non-AR TTS model, in terms of both quality and speech as shown in the experiments.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "62YyDhKdlPe", "original": null, "number": 8, "cdate": 1605514139112, "ddate": null, "tcdate": 1605514139112, "tmdate": 1605523826987, "tddate": null, "forum": "o3iritJHLfO", "replyto": "5QX06tv9adT", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "Responses to AnonReviewer4 (Part 2/2)", "comment": "Thank you so much for the detailed comments and suggestions. They are really helpful to improve the quality of our paper, especially to make our paper clearer and more convincing.\nBelow are the itemized responses regarding each comment. We hope our answers can help our paper sound more convincing to you.\nBecause of the maximum 5000 character limit, we write the answers in two parts.  \n  \nQ7. I'd recommend providing similar motivation for using dot-product soft attention plus straight-through argmax instead of Glow-TTS's alignment search or other competing approaches. Is it because it's a superior approach or just because it's different from existing approaches?  \nA7. Our attention mechanism with ST-argmax is a different approach rather than the improved one of Monotonic Alignment Search (MAS) of Glow-TTS. In terms of the alignment search algorithm, MAS is developed specifically for Glow-TTS. This is because it is trained to maximize the likelihood by directly obtaining the conditional prior distribution of latent representation \u2018z\u2019. Since the decoder of BVAE-TTS does not consist of normalizing flows, MAS can not be used in BVAE-TTS. Although it might be possible to use other monotonic alignment search algorithms such as [3], it needs additional dynamic programming computation after the dot-product, and it goes beyond the scope of this study.\n\nQ8. I don\u2019t believe Tacotron is actually the first end-to-end TTS system.  \nA8. We missed the paper. We will remove the word \u2018first\u2019 and cite the paper too. Thank you for sharing this work.\n\nQ9. The Related Work section is fairly redundant with information that is already presented in the introduction.  \nA9. We are thinking of clarifying the advantages of BVAE-TTS over the flow-based TTS models and other previous TTS models in the introduction and related work sections. We will consider your suggestion and reflect it in the revised version.\n\nQ10. The first paragraph of Sec 4.1 is quite confusing upon a first reading. I had to read the second sentence (\u201cVia the attention network\u2026\u201d) many times to understand what was being described.  \nA10. Thank you for pointing this out. We will edit the part to help the readers understand more clearly, especially focusing on the second sentence.\n\nQ11. I\u2019m curious how you arrived at a sample temperature of 0.333. Was this empirically tuned for BVAE-TTS or in response to Glow-TTS\u2019s findings?  \nA11. We chose the temperature 0.333 after listening to the samples generated with different temperatures, 0, 0.333, 0.6, 1.0. As the qualities are not that sensitive to the temperatures, we unified the temperature to 0.333 following the Glow-TTS. (It showed the best performance on LJSpeech in Glow-TTS.)\n\nQ12.\u201cInference Time\u201d: It seems important to include details about the hardware platform used to gather the speed results.  \nA12. We described our hardware setting in Section 5.1, but not mentioned how we use the hardware setting to measure the inference time, i.g. Are the inference times measured on CPU or GPU?. We will add the description in the revised version.\n\nQ13. There are minor English style and grammar issues throughout the paper that make the paper slightly more difficult to read. Please have the paper proofread to improve readability.  \nA13. We will make much more effort to improve the readability of our paper by having much more extensive proofreading.\n\n[1]: Kim, Jaehyeon, et al. \"Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search.\" arXiv preprint arXiv:2005.11129 (2020).  \n[2]: Miao, Chenfeng, et al. \"Flow-TTS: A Non-Autoregressive Network for Text to Speech Based on Flow.\" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.  \n[3]: He, M., Deng, Y., He, L. (2019) Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS. Proc. Interspeech 2019, 1293-1297, DOI: 10.21437/Interspeech.2019-1972."}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "sbYDbQv8UIV", "original": null, "number": 5, "cdate": 1605513791791, "ddate": null, "tcdate": 1605513791791, "tmdate": 1605523481051, "tddate": null, "forum": "o3iritJHLfO", "replyto": "4bygvQ1cRVB", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2 (Part 1/2)", "comment": "We thank the reviewer for the extensive comments, which were very constructive and helpful for building a better paper.  \nBelow are our answers to your questions. Because of the maximum 5000 character limit, we write the answers in two parts.  \n  \nQ1. What is the difference between your model and FastSpeech 2?  \nA1. As you mentioned, FastSpeech 2 said it succeeded in removing the teacher-student distillation. However, to achieve this, it requires additional duration labels and other acoustic features such as pitch and energy information obtained from external tools. On the contrary, since our model only utilizes a text-mel-spectrogram pair, it does not depend on the external tools, and so the training is simpler than the FastSpeech2. We think that the differences will be helpful to clarify the advantages of our model, so we will add this in the modified version with a citation of the FastSpeech2 paper. Thank you for your fruitful question.  \n \nQ2. ParaNet and FastSpeech1, 2 are very related to this paper. But why only compare with Glow-TTS?  \nA2. We agree that ParaNet and FastSpeech 1,2 can be good baselines for the experiment, but the official source codes for the models are not provided. Therefore, to fairly compare the performance of BVAE-TTS to other TTS models, we choose two models, Tacotron 2 and Glow-TTS, where each represents an AR and a Non-AR TTS model. A pre-trained Glow-TTS model is provided by the author. Although Tacotron 2 from NVIDIA is not provided by the official author, it is widely used and is recognized in the field of speech synthesis as being correctly implemented.  \n \nQ3. The paper has an ablation study section, but it is missing a couple very simple baseline: 1) remove VAE, purely predict mel-features based on duration and phoneme embeddings; 2) use a simple VAE instead of hierarchical one.  \nA3. When we removed VAE, the TTS model failed to learn mel-spectrogram generation, and when we used simple VAE, the result was the same. Therefore, we worry that the results would be not that informative to be compared with BVAE-TTS. However, if the conclusion of this discussion is that we need to do the further ablation studies, we will report the results in the modified version.\n\nQ4. Tacotron 2 shows better speech quality for in-domain dataset and worse for out-of-domain dataset. However, there are no audio samples generated using out-of-domain texts in supplementary material. Could you also provide out-of-domain audio samples?  \nA4. Yes. We will update the supplementary files including the audio samples generated using OOD text data.  \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "QuoKFXYNY5C", "original": null, "number": 7, "cdate": 1605514058833, "ddate": null, "tcdate": 1605514058833, "tmdate": 1605523258216, "tddate": null, "forum": "o3iritJHLfO", "replyto": "5QX06tv9adT", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "Responses to AnonReviewer4 (Part 1/2) ", "comment": "Thank you so much for the detailed comments and suggestions. They are really helpful to improve the quality of our paper, especially to make our paper clearer and more convincing.  \nBelow are the itemized responses regarding each comment. We hope our answers can help our paper sound more convincing to you.  \nBecause of the maximum 5000 character limit, we write the answers in two parts.  \n  \nQ1. I have some slight concerns about the clarity of the presentation that makes it harder to understand the approach and its motivation.  \nA1. To help the readers understand our motivation and the approach better, we are planning to revise our paper by focusing on making it clearer and making our motivations and advantages more prominent. Furthermore, we will also add a pseudo-code explanation following the R3\u2019s comment in the modified manuscript to improve its understandability.\n\nQ2. The quality of the speech produced by the system is only evaluated on a single dataset and uses only 50 synthesized examples in the subjective ratings.  \nA2. We totally understand why you think in that way. Here is our answer. We used the LJSpeech dataset because it is easy to access and many TTS papers also had used only the LJSpeech dataset as a single speaker dataset. In addition, although BVAE-TTS was trained only on the LJSpeech dataset, we evaluated the model on the other 50 out-of-domain sentences to see its generalization ability. When it comes to the number of sentences used for the test, we followed previous TTS papers [1, 2] measuring MOS for about fifty or less sentences.\n\nQ3. When considering the Glow-TTS paper (which this seems like a direct follow-up to), the system improvements seem quite incremental  \nA3. We can relate to your concern, however, we think BVAE-TTS is a new direction of non-AR TTS, rather than direct follow-up research of Glow-TTS. In this context, we think our model has so much potential, and we hope that it leads to many improved VAE-based TTS models.\n\nQ4. Listening to a few of the audio examples provided in the supplemental materials, I don\u2019t get the sense that the audio quality is significantly better than that of Glow-TTS as is suggested by the MOS numbers (BVAE-TTS sounds a bit muffled to my ears relative to Glow-TTS).  \nA4. Thank you for carefully listening to the audio samples and sharing your impression. When we asked people in my laboratory to listen to the audio samples, many people said they didn\u2019t feel that BVAE-TTS sounds muffled. On the contrary, in terms of naturalness, they said BVAE-TTS is even better than Glow-TTS, e.g. LJ0023-0016, LJ046-0191.\nAlso, we think that the muffled sound stands out when the audio samples of BVAE-TTS and Glow-TTS are compared side-by-side. However, since we measured the MOS for the different TTS models independently, we guess BVAE-TTS obtained better MOS than Glow-TTS in terms of naturalness.\n\nQ5. It suffers from the duration averaging effects and inability to sample from the full distribution of prosodic realizations.  \nA5. As you point out, we tried to consider the durations also as other latent variables, but it was hard to successfully combine it with an attention mechanism. However, we think it is a very plausible approach and we will study it in future work.\n\nQ6. The motivation would be made clearer if you were more specific early on about the potential advantage of VAE's relative to flows however you want to describe it (parameter efficiency, more flexible layer architectures, more powerful transformations per layer, etc.).  \nA6. Thank you for your suggestion. We will revise the introduction and related work sections to clarify the advantages and potential of BVAE-TTS. In the sections, we will add more descriptions about the advantages of BVAE-TTS over the flow-based models that you mentioned.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "-p6HsCEf1mv", "original": null, "number": 6, "cdate": 1605513966801, "ddate": null, "tcdate": 1605513966801, "tmdate": 1605523156347, "tddate": null, "forum": "o3iritJHLfO", "replyto": "4bygvQ1cRVB", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "Responses to AnonReviewer2 (Part 2/2) ", "comment": "We thank the reviewer for the extensive comments, which were very constructive and helpful for building a better paper.  \nBelow are our answers to your questions. Because of the maximum 5000 character limit, we write the answers in two parts.  \n  \nQ5. Is the non-autoregressive text-to-mel-spectrogram model necessary? For neural based TTS systems, most of time is in vocoder.  \nA5. Yes, your point is correct in terms of inference time. However, we think the fact that the inference time does not increase linearly as a text gets longer is still important. Furthermore, non-autoregressive generation shows its strength more in the out-of-domain data, i.e. very long input text, or the text patterns not existing in the training dataset. This is because the AR models suffer from accumulated prediction error. Thank you for your question and we will clarify it.\n\nQ6. Even if we assume the speed for text-to-mel-spectrogram is important, I don't think measuring speed with batch size = 1 is important, because non-autoregressive models can not be used for streaming. A proper comparison is measure FLOPS and throughput.  \nA6. We think comparing the inference time of TTS models is more practical to evaluate the models. This is because lower FLOPS for generating a speech does not guarantee shorter inference time. As far as we know, most previous studies on non-autoregressive TTS models also reported their inference time instead of FLOPS or throughput, including ParaNet and FastSpeech 1,2. [1,2,3,4]\n\nQ7. The paper claims their model is more compact, but there is no comparison for a smaller Tacotron2 model or other non-autoregressive model.  \nA7. Our initial motivation is to build a new VAE-based non-autoregressive TTS model instead of developing a compact TTS model. Therefore, when we compare the number of parameters, we mainly compare BVAE-TTS and Glow-TTS in terms of both MOS and the number of parameters. This is because both models are non-AR TTS models without a teacher model.\n\n[1]: Ren, Yi, et al. \"Fastspeech: Fast, robust and controllable text to speech.\" Advances in Neural Information Processing Systems. 2019.  \n[2]: Kainan Peng, Wei Ping, Zhao Song, and Kexin Zhao. Non-autoregressive neural text-to-speech. In Proceedings of the 37th International Conference on Machine Learning, pp. 10192\u201310204. PMLR, 2020.  \n[3]: Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-tts: A generative flow for text-to-speech via monotonic alignment search. arXiv preprint arXiv:2005.11129, 2020.  \n[4]: Ren, Yi, et al. \"FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech.\" arXiv preprint arXiv:2006.04558 (2020).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "kd1_9L5TkJB", "original": null, "number": 4, "cdate": 1605513410896, "ddate": null, "tcdate": 1605513410896, "tmdate": 1605513410896, "tddate": null, "forum": "o3iritJHLfO", "replyto": "pkC866oRd-8", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "Responses to AnonReviewer3", "comment": "We thank the reviewer for the great feedback. The feedback is very constructive and helpful for building a better paper.  \nBelow are our answers to your questions.  \n  \nQ1. It is quite difficult to understand how the BVAE-TTS works. For example, what are the exact layer inputs and outputs, and how the parameters of the normal distributions are used?  \nA1. Thank you for pointing this out, and we also agree that the architecture of BVAE-TTS is quite complicated. In BVAE-TTS, the mean and covariance values are predicted with a 1-D Conv layer (+ softplus). The delta values make the difference between prior and posterior caused by the data observation, and they are not the values accumulated along the layers. Following your suggestion, we will add the pseudo-code explanation of the network in the modified manuscript. Thank you for your suggestion.\n  \nQ2.  Why is the output of the attention layer not provided to the encoder?  \nA2. If the encoder you mentioned is the bottom-up path of BVAE-TTS, the attention is conducted only at the top of the bottom-up path, instead of between every BVAE block. The output is then inputted directly to the top-down path. We think you have some misunderstandings about this part. However, we think it could be an interesting research to use attention mechanisms between the BVAE blocks, so that the encoder effectively extracts acoustic features that are disentangled to the textual contents. Thank you for your interesting suggestions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "fCQkS_lGRN-", "original": null, "number": 3, "cdate": 1605512904428, "ddate": null, "tcdate": 1605512904428, "tmdate": 1605513214515, "tddate": null, "forum": "o3iritJHLfO", "replyto": "K-8KCHnVmK", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment", "content": {"title": "Responses to AnonReviewer1", "comment": "We thank you for your interest in our research and we also hope it becomes a good starting point for VAE-based TTS research.  \nBelow are our answers to your questions.\n\n Q1. How does the duration modeling result in 'monotonic\u2019 alignment?  \n A1. \u2018Monotonic alignment\u2019 means phoneme representations are used in an orderly manner. In other words, there is no case where a phoneme representation that appears later in a sentence is used earlier in the decoder. In this context, if we inflate the phoneme representations based on their durations, the above situation never happens. Thank you for your fruitful question and we will clarify this in the revised version.\n \nQ2. A comparison with an equivalent soft attention implementation might be insightful.  \nA2. It is the situation where we train BVAE-TTS without using ST-argmax technique. (Sec 5.3.2.) When we remove the constraint of a one-to-one mapping between phonemes and mel-spectrogram frames, our model fails to learn the alignment. Thank you for pointing this out and we will clarify this in the revised version.\n \nQ3. I am wondering how this model would perform in a multi-speaker dataset. One aspect that the paper does not touch in detail is in its capabilities as a generative model. It would be interesting, for instance, to see if this model can in any way separate speaker style from content with a multi-speaker model.  \nA3. Thank you for your suggestion, however, our initial aim was to develop a novel TTS model based on VAE architecture, and so we focus on succeeding in generating speech with competitive quality. However, we are actually planning to extend our model to the multi-speaker scenario in future work. For example, we expect that, by letting the model extract a global latent vector from a mel-spectrogram, the model can change the global style of the speech (e.g. speaker identity) by controlling the global latent vector.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "o3iritJHLfO", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3537/Authors|ICLR.cc/2021/Conference/Paper3537/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836577, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Comment"}}}, {"id": "pkC866oRd-8", "original": null, "number": 1, "cdate": 1602738500470, "ddate": null, "tcdate": 1602738500470, "tmdate": 1605023983847, "tddate": null, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review", "content": {"title": "Great results and thorough evaluation with a well-motivated model, but presentation could be better", "review": "Summary: Neural models that autoregressively generate mel spectrograms from text (or phonemes), such as Tacotron, have been used to generate high quality synthetic speech. However, they suffer from slow inference speed due to their autoregressive nature. To alleviate this, non-autoregressive models have been proposed, such as FastSpeech and Glow-TTS. The proposed model, BVAE-TTS, is yet another non-autoregressive speech synthesis model (outputting spectrograms), with two key advantages over the aforementioned models: (a) no autoregressive teacher model is required, as in FastSpeech, which simplifies training, and (b) fewer parameters are needed than in Glow-TTS, since there is no bijectivity constraint (allowing a more expressive architecture to be used). Models are compared with inference speed and MOS, and BVAE-TTS compares favorably on both both metrics when compared to Glow-TTS.\n\nPros:\n\n1. The evaluation of the model is done well, in a clear way. LJSpeech is used, a dataset which is commonly used and easily accessible. MOS and inference speech are provided, and error bars are provided for MOS values. BVAE-TTS is compared to Glow-TTS and Tacotron 2 (one other non-autoregressive model, and one well-known AR baseline), and hyperparameters are provided. A single vocoder (pretrained WaveGlow) is used on all models, isolating the effect of the spectrogram prediction model used.\n\n2. Section 4.3, pertaining to using attention distributions to learn a duration predictor, is interesting and novel. Using positional encodings is standard and using a loss guide is unsurprising. However, while jitter and straight-through estimators are not uncommon, all of these things together make a compelling and novel approach to using attention to infer discretized durations and compensate for that train-test mismatch well. I believe that a similar technique could be used in other models as well.\n\n3. The model is an application of similar ideas from image synthesis, which is interesting, in that it demonstrates that some of those techniques work equally well for spectrogram synthesis. This sort of cross-modal result points to the strength of the method being used, which is a valuable data point for the research community.\n\nCons:\n\n1. The biggest weakness of this paper, in my view, is that deciphering the model itself is quite difficult. Although the model bears resemblance to NVAE (for which code is released), understanding the fine details is tricky, and the paper does little to aid in that effort. \n\nIn particular, understanding the exact layer inputs and outputs and parameters of the normal distributions being used is difficult, and I believe the paper would benefit significantly from a pseudocode explanation of the network. For example, I did not understand why the generative model produced both $\\mu_l$ and $\\Delta \\mu_l$, and whether $\\mu_l$ was predicted with a dense layer or was the accumulation of the prior BVAE stacks' $\\Delta \\mu_l$ values (and similar for $\\Sigma$). \n\nI also wonder why the output of the attention layer is not provided to the encoder; perhaps there is a fundamental reason for this which I am missing, or perhaps this is simply an architecture choice.\n\nA very clear explanation of the method itself, perhaps as psuedocode for where the means and variances come from and which features they interact with and what it sampled when, would in my view make this among the top papers.\n\nRecommendation:  Accept. The paper is well written and results are strong, although I would prefer if the method itself were explained more clearly.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074207, "tmdate": 1606915775929, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3537/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review"}}}, {"id": "4bygvQ1cRVB", "original": null, "number": 2, "cdate": 1603876229254, "ddate": null, "tcdate": 1603876229254, "tmdate": 1605023983772, "tddate": null, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "invitation": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review", "content": {"title": "An interesting paper with some nonsolid claims", "review": "This paper combined fastspeech with a hierarchical VAE (or ladder VAE? in their paper it called bidirectional VAE) to achieve parallel and high quality text-to-mel syntheisis. \n\nThe paper claims these contributions: (1) Introducing an online fashion for duration prediction, instead of distillation in FastSpeech and ParaNet. So the model is more e2e. (2) Introducing an BVAE, which extract features hierarchically to better capture prosody (overcome one-to-many) problem. During inference, can use the prior directly. This is directly than previous VAE application in TTS, which is only use to capture residual information. (3) it's faster and with same quality as autoregressive Tacotron and with better quality than other published non-autoregressive model.\n\nThe key strength of this paper is the architecture is new. I think using a hierarchical VAE here is reasonable. \n\nMy concerns mostly from the conclusion and experiments.\n(1) The paper claims compare to previous non-autoregressive model, they are more e2e, since both FastSpeech (also use duration predictor) and ParaNet (without VAE) rely on distillation. However, there is another paper called FastSpeech 2 (https://arxiv.org/abs/2006.04558, published on June 8th), the model also claim \" 1) removing the teacher-student distillation to simplify the training pipeline\".  Can the author explain the difference? Also i think need to cite that paper because it published in June and very related.\n(2) As mentioned in (1), ParaNet and FastSpeech1/2 are very related to this paper. But why only compare with waveglow?\n(3) The paper has an ablation study section, but it missing couple very simple baseline. 1) remove VAE, purely predict mel-features based on duration and phoneme embeddings. 2) using a simple VAE instead of hierachical one. How it affect the performance.\n(4) One key claim of this paper is that it is as good as Tacotron 2. However, for the in-domain test, the 0.2 behind. By listening the audio samples provided by the author, it indeed significantly worse. The out of domain looks better, I suspect the reason is Tacotron 2 has some attention failures due to it not robust as duration based model. A proper baseline here, is a FastSpeech model. Could you also provide OOD samples? It's really hard to believe such prosody gap can be filled by switch domain.\n(5) Back to the original motivation, why we need non-autoregressive model for TTS? For neural based TTS system, most of time is in vocoder. Even we assume the speed for mel-to-spec is important, I don't think measure speed with batch size = 1 is important, because non-autoregressive model can not be streaming. A proper comparison is measure FLOPS and throughput. This might make more sense for offline TTS. This is a minor concern, as long as the quality are good enough.\n(6) The paper claims their model is more compact, but there is no comparison for a smaller Taco2 model or other non-autoregressive model. \n\nIn summary, based on my understanding, this paper proposed a new non-autoregressive based text-to-mel model with quality regression but possible better robustness. My opinion is that it's a borderline for ICLR, since the importance of the proposed VAE was not well justified, and the quality was not as good as autoregressive model. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3537/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3537/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech", "authorids": ["~Yoonhyung_Lee2", "~Joongbo_Shin1", "~Kyomin_Jung1"], "authors": ["Yoonhyung Lee", "Joongbo Shin", "Kyomin Jung"], "keywords": ["text-to-speech", "speech synthesis", "non-autoregressive", "VAE"], "abstract": "Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.", "one-sentence_summary": "In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|bidirectional_variational_inference_for_nonautoregressive_texttospeech", "supplementary_material": "/attachment/121cd3bdc0f77c9e3b2fcceb31ce666690f27c7b.zip", "pdf": "/pdf/db03d769745da96b32be80606e358d64a7641d2b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021bidirectional,\ntitle={Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech},\nauthor={Yoonhyung Lee and Joongbo Shin and Kyomin Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=o3iritJHLfO}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "o3iritJHLfO", "replyto": "o3iritJHLfO", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3537/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074207, "tmdate": 1606915775929, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3537/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3537/-/Official_Review"}}}], "count": 14}