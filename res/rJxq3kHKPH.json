{"notes": [{"id": "rJxq3kHKPH", "original": "BJxcHMkKPB", "number": 1962, "cdate": 1569439666011, "ddate": null, "tcdate": 1569439666011, "tmdate": 1577168218580, "tddate": null, "forum": "rJxq3kHKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "myLM_Km5oA", "original": null, "number": 1, "cdate": 1576798737008, "ddate": null, "tcdate": 1576798737008, "tmdate": 1576800899349, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Decision", "content": {"decision": "Reject", "comment": "This paper focuses on mitigating the effect of label noise. They provide a new class of loss functions along with a new stopping criteria for this problem. The authors claim that these new losses improves the test accuracy in the presence of label corruption and helps avoid memorization. The reviewers raised concerns about (1) lack of proper comparison with many baselines (2) subpar literature review and (3) state that parts of the paper is vague. The authors partially addressed these concerns and have significantly updated the paper including comparison with some of the baselines. However, the reviewers were not fully satisfied with the new updates. I mostly agree with the reviewers. I think the paper has potential but requires a bit more work to be ready for publication and can not recommend acceptance at this time. I have to say that the authors really put a lot of effort in their response and significantly improved their submission during the discussion period. I recommend the authors follow the reviewers' suggestions to further improve the paper (e.g. comparing with other baselines) for future submissions", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719126, "tmdate": 1576800269720, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Decision"}}}, {"id": "SyxBYBr6Fr", "original": null, "number": 3, "cdate": 1571800445102, "ddate": null, "tcdate": 1571800445102, "tmdate": 1574494656725, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "\nUpdate after rebuttal:\n\nThe good:\nThe rebuttal and updated paper address many of my concerns. Most importantly, the updated paper demonstrates the three-stage phenomenon on Open Images and adds experiments on IMDB showing that the Gambler's loss with AES helps a lot. The LAES iteration introduced in the updated paper alleviates my concern about performance drop compared to the CT baseline at certain corruptions on CIFAR-10.\n\nThe bad:\n- From Figure 12, it looks like the three-stage phenomenon doesn't hold on IMDB. Does AES provide additional benefit beyond the Gambler's loss on IMDB? This needs to be clarified with the way Figure 12 turned out.\n- There is a serious missing citation [1] that should be included as a baseline. The proposed method in [1] is at least superficially similar to the Gambler's loss and also makes use of the fact that it is easier to fit clean labels than noisy labels. My apologies for not noticing this earlier.\n\nOverall:\nI would still suggest acceptance, because the three-stage phenomenon is an interesting find that the authors make good use of. In light of the missing citation, though, I cannot raise my score.\n\n\n[1]: Zhilu Zhang, Mert R. Sabuncu. \"Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\". NeurIPS 2018.\n\n-----------------------------------------------------------------------\nSummary:\nThis paper proposes a method to alleviate label noise. It opens with the observation of three distinct stages when training in the presence of label noise. Importantly, there is a \u2018gap\u2019 stage during which the network has not begun memorizing noisy labels and early stopping is ideal. The authors then observe that the Gambler\u2019s loss (Ziyin et al., 2019) elongates the gap stage and propose an analytic early stopping (AES) criterion for identifying when to stop training.\n\nThe analysis of the AES criterion, e.g. in Figure 5, and the observation of a phase transition when tuning the o hyperparameter are quite interesting, and the latter observation is of practical value when using the AES criterion.\n\nThe AES criterion seems to be well-motivated, and the empirical evaluation of the Gambler\u2019s loss with and without early stopping is good. The results are strong on MNIST but somewhat weak on CIFAR-10. Specifically, the improvements on CIFAR-10 only appear for large corruption rates (0.7+), and performance is lower than the baselines for other corruption rates. This is a worrying problem, because it calls into question the value of the method on larger problems. However, seeing as this is a distinct approach from the baselines and that it demonstrates some promise, I recommend borderline accept. The authors could raise my score by demonstrating more consistent gains on another larger-than-MNIST CV dataset or an NLP/speech dataset. Other points of concern that I have are listed below.\n\nMajor points:\nAt the top of page 3, the authors say that the idealized gap assumption \u201cholds well for simple datasets such as MNIST and on datasets with very high corruption rate, where our method achieves best results, and less so on more complicated datasets such as CIFAR10\u201d. The idealized gap assumption is behind the AES criterion, but Figure 5 suggests that the AES criterion works well on CIFAR-10, so what do the authors mean when they say the assumption doesn\u2019t work as well on CIFAR-10? Is this just referring to the results?\n\nSaying traditional label noise correction methods are \u201cof no use when one is not aware of the existence of label noise\u201d seems unfair. The FC method and others do not require foreknowledge of the corruption rate and do not harm performance in the absence of label noise, so they can also be said to automatically correct label noise.\n\n\u201cFC, however, requires knowing the whole transition matrix, and is outperformed significantly by our method.\u201d\nThis is not quite true, because Patrini et al. propose an estimate of the transition matrix as part of the Forward correction. Did you use the estimated or true transition matrix for the FC method? It would be good to clarify this in the paper.\n\nMinor points:\nThere are a few grammatical errors and typos in the paper:\n\n\u201cor explicit regularization, this is also what is suggested by Abiodun et al. (2018)\u201d (run-on sentence)\\\n\n\u201cCIFAR10\u201d should be \u201cCIFAR-10\u201d", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575671365020, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Reviewers"], "noninvitees": [], "tcdate": 1570237729765, "tmdate": 1575671365033, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Review"}}}, {"id": "rJepBpantB", "original": null, "number": 2, "cdate": 1571769669163, "ddate": null, "tcdate": 1571769669163, "tmdate": 1574361237365, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Updated review: Thanks for your comments. I feel the latest version of the paper is better than the previous version. \nHowever, as stated by other reviewers as well, the claims of the paper are quite ambiguous. Another example from the author response is the point about how Chapter 6 of the Elements of Infomation Theory is related to Gambler's loss. This is not clear to me. I would not object to accepting the paper but I find it difficult to recommend accept for this paper. Perhaps the authors can be more clear in their claims. \n\n----------------------------------------------------------------------------------------------------------------------------------------------------------\n\nSummary: The paper focusses on the problem of noisy labels in supervised learning with deep neural networks. The paper, in turn, proposes an early stopping criterion for handling label noise. The early stopping criterion is dependent on a new loss function that is defined as the log of true label + weight on a reservation option? The paper shows that when the labels are corrupted then the propose early stopping criterion does better than early stopping criteria obtained via the validation set. \n\n\\The first section of the paper establishes that when label noise is present in the dataset, then there are three stages to training a deep neural network. \nThe learning stage where the highest accuracy on the test set is achieved. \nThe gap stage where test set accuracy goes down. \nMemorization stage corresponds to when a deep neural network memorizes corrupt labels and test accuracy goes completely down. \nI cannot understand figure 1(a). The y-label says accuracy but it seems that the plot is about loss. What dataset was this and what architecture of DNN was used? The plot shows that the DNN achieved a 100% accuracy in 5 epochs. Is this result meaningful? Before establishing a hypothesis based on this should the hypothesis not be tested on multiple datasets.\nThe paper says that these stages are persistent across multiple architectures and datasets and as proof the paper says \u2018we verified that\u2019. Why can\u2019t the reader see the experiments? By across datasets does the paper mean MNIST and CIFAR? By across architecture does the paper mean the two architectures mentioned in the appendix one each for MNIST and CIFAR respectively? \n\nThe paper makes the assumption that label noise is symmetrically corrupted. Why and where does such an assumption hold? What happens to the proposed method if that is not true. \n\nAssumption 2: During the gap stage the model has learned nothing about the corrupt data points. \nHow is that even possible? \n\nEquation 1: So the loss function proposed is log(f(x)_y + (1/o) f(x)_m+1) .  What is y here? The true label? Why is y called a point mass? Is this different from the cross-entropy loss + log loss on m+1 ?\n\nI do not understand equations 2 to 5. \n\nFor figure 3 again what datasets were used?\n\n\u201c Making random bet will help with making money and a skilled gambler will not make such bets\u201d \nWhy does making random bet help with making money? If random is good how can a skilled gambler exist in such a game? What is this skill?\n\nk denotes the sum of probability of predicting anything that is not y or m+1 (it does not denote prediction). \n\nIn the experiments section what was the symbol for the rate of corruption changed from epsilon to r. Are they different? \n \nWhat is nll? \n\nIt seems that gamblers loss best shines when the corruption rate is as high as 80% . That is 80 percent of the data is corrupted. Does this mean that if I trained with only 20% of the non-corrupt data I would still get a 99% accuracy on MNIST (even without gamblers loss)? A comparison of this sort would have been useful.  \nOne astonishing result the paper presents is that with gambler\u2019s loss even with 80% corrupt labels a 94% test accuracy is possible on MNIST dataset. I think this is significant, this raises the question that is it required to label all the data points ina dataset to achieve high accuracy or is it possible to achieve just as much with only 20% of the labels?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575671365020, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Reviewers"], "noninvitees": [], "tcdate": 1570237729765, "tmdate": 1575671365033, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Review"}}}, {"id": "Skl4y3ytoB", "original": null, "number": 7, "cdate": 1573612507741, "ddate": null, "tcdate": 1573612507741, "tmdate": 1573612507741, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "HJleZsO2KB", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment", "content": {"title": "Experiment Updates", "comment": "Hi! We just updated the experiment sections as well. The updated experiments were suggested by the other reviewers and we think helped with making the paper more solid and demonstrated the effectiveness of the proposed method. Please let me summarize the update to the experiments as follows:\n\n(1) we updated Table 1 in section 4.2 to study our method on the IMDB dataset, which is a standard NLP task for sentiment analysis. The model used is a standard LSTM. We have updated the experiment section of our paper to include experiment on the IMDB dataset, see table 1 (r=0.1-0.3) and also see appendix section M (r=0.02-0.08), where the gambler\u2019s loss is shown to outperform the baseline and the AES criterion is shown to outperform validation early stopping method significantly. \n\n(2) section J: we include two more demonstrations of the three stage phenomenon. One is on IMDB, using LSTM and GloVe word embedding, and the other is on openimage which is a noisy dataset whose noise rate is hard to estimate), for this example, we also show how the proposed criterion might help to estimate the noise level in the dataset\n\n(3) section K: we include some experiment to demonstrate that the proposed method might also be robust to asymmetric noise; the improvement over the baseline is consistent and significant; however, given the time and computational resource we had, the experiments are small scale; moreover, we actually plan to deal with the asymmetric label noise in a future work \n\n(4) section L: we compare our method to the \u201cupper bound\u201d, i.e. training without the corrupted data (no early stopping). We notice that for some range (r=0.2-0.5), analytical early stopping perform at least as good as the upper bound, we hypothesize that this is because our method also effective prevents overfiting by stopping early\n\n(5) Besides, we removed the part that combine our method with CT (co-gambling), because, as argued in the original version, the two methods do not seem very compatible. In its place, we added a simple schedule to improve our method consistently, called LAES, which starts the training by five epochs of warmup (o=m) and then switch to smaller o.\n\n(6) we also updated the CIFAR10 experiments in table 2 to include a comparison with simple training the nll loss, and we thinks this further shows the effectiveness of our method. While the proposed method does not achieve SOTA results in the range (r=0.2-0.6), it still achieves significant improvement over simply training on the nll loss (by 10-30% absolute accuracy, see the updated table 2), and we think this deserves some merit. In comparison with methods such as CT, we note that CT outperforms the proposed method only marginally by about 2-5% accuracy in the rage (r=0.2-0.6) while requiring twice as many parameters and training time, and so we think another merit of the proposed method is its simplicity.\n\n\nWhile the theory part bases on assumptions that seem quite strong, we think the above experiments further verified these assumptions, and the fact that the proposed method is shown to be effective on these datasets and tasks further suggest the correctness and wide applicability of the theory.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1962/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1962/Authors|ICLR.cc/2020/Conference/Paper1962/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148335, "tmdate": 1576860559776, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment"}}}, {"id": "rkxyVdJFjH", "original": null, "number": 6, "cdate": 1573611559350, "ddate": null, "tcdate": 1573611559350, "tmdate": 1573611559350, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "rJepBpantB", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment", "content": {"title": "Experiment Updates", "comment": "Hi! We have updated our experiment section to answer your questions! Indeed, we think that adding in these experiments make the current paper more solid. The update to experiments include:\n\n(1) section J: we include two more demonstrations of the three stage phenomenon. One is on IMDB, using LSTM and GloVe word embedding, and the other is on openimage which is a noisy dataset whose noise rate is hard to estimate), for this example, we also show how the proposed criterion might help to estimate the noise level in the dataset\n\n(2) section K: we include some experiment to demonstrate that the proposed method might also be robust to asymmetric noise; the improvement over the baseline is consistent and significant; however, given the time and computational resource we had, the experiments are small scale; moreover, we actually plan to deal with the asymmetric label noise in a future work \n\n(3) section L: we compare our method to the \u201cupper bound\u201d, i.e. training without the corrupted data (no early stopping). We notice that for some range (r=0.2-0.5), analytical early stopping perform at least as good as the upper bound, we hypothesize that this is because our method also effective prevents overfiting by stopping early\n\n(4) moreover, we also updated Table 1 in section 4.2 to study our method on the IMDB dataset, which is a standard NLP task for sentiment analysis. The model used is a standard LSTM. We have updated the experiment section of our paper to include experiment on the IMDB dataset, see table 1 (r=0.1-0.3) and also see appendix section M (r=0.02-0.08), where the gambler\u2019s loss is shown to outperform the baseline and the AES criterion is shown to outperform validation early stopping method significantly. \n\n(5) Besides, we removed the part that combine our method with CT (co-gambling), because, as argued in the original version, the two methods do not seem very compatible. In its place, we added a simple schedule to improve our method consistently, called LAES, which starts the training by five epochs of warmup (o=m) and then switch to smaller o.\n\nLet us also provide an answer the the following question:\nA. 'It seems that gamblers loss best shines when the corruption rate is as high as 80% . That is 80 percent of the data is corrupted. Does this mean that if I trained with only 20% of the non-corrupt data I would still get a 99% accuracy on MNIST (even without gamblers loss)? A comparison of this sort would have been useful.  '\n\n- we indeed think that the gambler's loss can be very useful when the strength of noise present is very strong. Since the current methods can hardly deal with extremely strong label noise (>0.7) especially because the noise rate becomes very hard to estimate at this stage. However, we also argue that the value for the proposed method should not be underestimated when the noise rate is small. For example, on CIFAR10, while the proposed method does not achieve SOTA results in the range (r=0.2-0.6), it still achieves significant improvement over simply training on the nll loss (by 10-30% absolute accuracy, see the updated table 2), and we think this deserves some merit. In comparison with methods such as CT, we note that CT outperforms the proposed method only marginally by about 2-5% accuracy in the rage (r=0.2-0.6) while requiring twice as many parameters and training time, and so we think another merit of the proposed method is its simplicity.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1962/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1962/Authors|ICLR.cc/2020/Conference/Paper1962/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148335, "tmdate": 1576860559776, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment"}}}, {"id": "B1eD-EkKjr", "original": null, "number": 5, "cdate": 1573610494661, "ddate": null, "tcdate": 1573610494661, "tmdate": 1573610494661, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "SyxBYBr6Fr", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment", "content": {"title": "Reply and experiment updates", "comment": "Hi! Thank you so much for your reply! I think your advice really helped us improving the paper. We have updated the experiment section of our paper to include experiment on the IMDB dataset, see table 1 (r=0.1-0.3) and also see appendix section M (r=0.02-0.08), where the gambler\u2019s loss is shown to outperform the baseline and the AES criterion is shown to outperform validation early stopping method significantly. Besides, we removed the part that combine our method with CT (co-gambling), because, as argued in the original version, the two methods do not seem very compatible. In its place, we added a simple schedule to improve our method consistently, called LAES, which starts the training by five epochs of warmup (o=m) and then switch to smaller o.\n\nBesides this part, other updates to the experiments include: (1) section J: we include two more demonstrations of the three stage phenomenon (on IMDB and on openimage); (2) section K: we include some experiment to demonstrate that the proposed method might also be robust to asymmetric noise; (3) section L: on MNIST, we compare our method to the \u201cupper bound\u201d, i.e. training without the corrupted data. If you are interested , we also included two sections to elaborate on our theory part (Section A and section B). \n\nNow please let me address your specific questions.\n\nA1. \u2018Specifically, the improvements on CIFAR-10 only appear for large corruption rates (0.7+), and performance is lower than the baselines for other corruption rates. This is a worrying problem, because it calls into question the value of the method on larger problems.\u2019\n- While the proposed method does not achieve SOTA results in the range (r=0.2-0.6), it still achieves significant improvement over simply training on the nll loss (by 10-30% absolute accuracy), and we think this deserves some merit. In comparison with methods such as CT, we note that CT outperforms the proposed method only by 2-5% accuracy in the rage (r=0.2-0.6) while requiring twice as many parameters and training time, and so we think another merit of the proposed method is its simplicity.\n\nA2. \u2018At the top of page 3, the authors say that the idealized gap assumption \u201cholds well for simple datasets such as MNIST and on datasets with very high corruption rate, where our method achieves best results, and less so on more complicated datasets such as CIFAR10\u201d. The idealized gap assumption is behind the AES criterion, but Figure 5 suggests that the AES criterion works well on CIFAR-10, so what do the authors mean when they say the assumption doesn\u2019t work as well on CIFAR-10? Is this just referring to the results?\u2019 \n- Sorry for this confusion! The same problem was also pointed to by reviewer 3. We removed this ambiguous sentence and added an example for this. It relates to the fact that on complicated dataset such as CIFAR10, the training accuracy on the clean dataset might be actually significant below 100%. While for MNIST, the training accuracy on the clean part is very close to 100%.\n\nA3. \u2018Saying traditional label noise correction methods are \u201cof no use when one is not aware of the existence of label noise\u201d seems unfair. The FC method and others do not require foreknowledge of the corruption rate and do not harm performance in the absence of label noise, so they can also be said to automatically correct label noise.\u2019\n- Yes, it is true that other methods, such as FC, when used without label noise, can be seen as a simple label smoothing method and should not harm learning. What we really meant was the cases in which the noise rate might be small (say, r=0.02-0.08) and might be hard to estimate, and in these case, other methods are unlikely to provide improvement when r is unknown. However, using the gambler\u2019s loss with some benign value of o is observed to actually improve the performance of the model (compared to training with nll loss). For example see section M (where o is set to 1.9 without any tuning).\n\nA4. \u2018\u201cFC, however, requires knowing the whole transition matrix, and is outperformed significantly by our method.\u201dThis is not quite true, because Patrini et al. propose an estimate of the transition matrix as part of the Forward correction. Did you use the estimated or true transition matrix for the FC method? It would be good to clarify this in the paper.\u2019\n- Yes, we used the estimation method proposed in the original paper, as we described in the related works section. When the transition matrix is exactly known, the Forward correction method can actually be quite strong (or even the best method).\n\nA5. We corrected the minor points you mentioned.\n\nAgain, thank you very much for the comments, and please let us know if you have any other questions!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1962/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1962/Authors|ICLR.cc/2020/Conference/Paper1962/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148335, "tmdate": 1576860559776, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment"}}}, {"id": "ByluCM5MsB", "original": null, "number": 4, "cdate": 1573196496252, "ddate": null, "tcdate": 1573196496252, "tmdate": 1573196496252, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "BklPOzqGor", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment", "content": {"title": "Reply (part 2)", "comment": "A7. 'Equation 1: So the loss function proposed is log(f(x)_y + (1/o) f(x)_m+1) .  What is y here? The true label? Why is y called a point mass? Is this different from the cross-entropy loss + log loss on m+1 ?'\n- Sorry for making this confusing! We have rewritten section 2 to make this as clear as we can. y is indeed the true label. Saying that y is a point mass simply means that it is a 0-1 loss (having value 1 for the correct label, and having 0 for the incorrect labels). The loss function function indeed takes the form log(f(x)_y + (1/o) f(x)_m+1), this has well studied information-theoretic properties (see chapter 6 of [1] for a detailed discussion on its intuitive meaning and mathematical properties). The function you mentioned might work in practice? But it is hard to give interpretation on such loss, and its theoretical properties are, to our knowledge, yet unknown.\n\nA8. 'For figure 3 again what datasets were used?'\n-As we mentioned in the paper, the exact detail for figure 3 is given in the appendix section I, and as is described there, it is done in MNIST with corruption rate 0.5. Many more experiments are also shown there.\n\nA9. 'I do not understand equations 2 to 5. '\n- Reviewer 3 also found this part a little confusing, and we agree that our original presentation of this part can indeed be greatly improved. We have reorganized and rewrote section 2 and added some intuitions of the mechanism at working (see the short paragraph above the current equation 5), in the hope to make this clearer. We also added appendix section A and appendix section B to clalrify further details. Please let us know if you still have any question about this part.\n\nA10. '\u201c Making random bet will help with making money and a skilled gambler will not make such bets\u201dWhy does making random bet help with making money? If random is good how can a skilled gambler exist in such a game? What is this skill?'\n- Sorry! This is a typo! It should be \u201c Making random bet will NOT help with making money and a skilled gambler will not make such bets\" \n\nA11. 'k denotes the sum of probability of predicting anything that is not y or m+1 (it does not denote prediction). '\n- Hmmm, what we really meant was that 'k denotes the sum of PREDICTED probability of predicting anything that is not y or m+1'. We have updated section 2.1 to clarify this. Please also see appendix section A for more detailed information about the gambler's loss.\n\nA12. 'In the experiments section what was the symbol for the rate of corruption changed from epsilon to r. Are they different? '\n- epsilon refers to (1-r), meaning the non-corrupt rate, so they are two different symbols.\n\nA13. 'What is nll? '\n- nll is shorthand for negative log loss, which is also called cross entropy loss\n\nPlease let us know if you find any other parts confusing! We will do our best to revise the manuscript and clarify the points. \n\n[1] Thomas M. Cover. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing).\n[2] https://arxiv.org/abs/1905.11604\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1962/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1962/Authors|ICLR.cc/2020/Conference/Paper1962/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148335, "tmdate": 1576860559776, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment"}}}, {"id": "BklPOzqGor", "original": null, "number": 3, "cdate": 1573196399354, "ddate": null, "tcdate": 1573196399354, "tmdate": 1573196399354, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "rJepBpantB", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment", "content": {"title": "Reply (part 1)", "comment": "Hi! Thank you so much for the comment! I have read your comment carefully and I think, fortunately, many questions are due to misunderstanding, and we have updated the manuscript to make many points more clear (this includes a rewriting of section 2, and addition of section A and section B in the appendix). Before adding in experiments or rewriting sections, let me answer the questions that we can already provide answer to. We will let you know once we update the experiment section.\n\nA1. 'I cannot understand figure 1(a). The y-label says accuracy but it seems that the plot is about loss.'\n-For figure 1a, please note that figure 1a has two y-axes; on the left it says accuracy, and on the right it says loss, also, please note that we consistently we used this 2-axes style in the paper (e.g. Fig.3, Fig.5, Fig.6).\n\nA2. 'What dataset was this and what architecture of DNN was used?'\n-As is described in the title of Figure 1. This plot is for MNIST with corruption rate 0.5. Since the dataset is MNIST, the architecture is the one described in Table 3 in the appendix.\n\nA3. 'The plot shows that the DNN achieved a 100% accuracy in 5 epochs. Is this result meaningful?'\n-Hmmm, it is a little hard to answer this question without a clear definition of 'meaningful'. The dataset is MNIST, and usually the accuracy reaches >95% using a DNN without label noise.\n\nA4. 'Before establishing a hypothesis based on this should the hypothesis not be tested on multiple datasets.The paper says that these stages are persistent across multiple architectures and datasets and as proof the paper says \u2018we verified that\u2019. Why can\u2019t the reader see the experiments? By across datasets does the paper mean MNIST and CIFAR? By across architecture does the paper mean the two architectures mentioned in the appendix one each for MNIST and CIFAR respectively? '\n-In addition to the two CNN architectures we described in the paper, two experiments are done using ResNet18 (please see Figure 5.c and 5.d, and search for the word ResNet). In fact, this 3-stage phenomenon is quite easily observable for many datasets when decent level of label noise is present. Since the paper is already 16 pages long, we did not include more experiments in the initial version, but as you suggested, we will also include a few plots from other dataset and architectures soon (we will let you know once we update the experiments). \n-- 'we verified that .' was actually a type, we have removed this sentence. \n\nA5. 'The paper makes the assumption that label noise is symmetrically corrupted. Why and where does such an assumption hold? What happens to the proposed method if that is not true. '\n- Since the criterion does not apply to assymetrically corrupted data, to deal with asymmetrically corrupted data, some other criterion is needed. However, simply using gambler's loss in this case should also improve the final performance. We can also update some toy experiment on this if time allows by the time the rebuttal period ends.\n\nA6. 'Assumption 2: During the gap stage the model has learned nothing about the corrupt data points. How is that even possible? '\n- Empirically, the situation is that, on average, the model makes random prediction on the corrupted dataset. So there might be some corrupt points that the model actual learned, but the number of such points should be small compared to the ones the model has not learned. Moreover, this assumption can be verified by Figure 1, where the training loss on the corrupt part does not start to decrease significantly until 20th epoch, where learning of the clean data is very low. Theoretically speaking, this is because gradient descent tends to learn function of increasing complexity, and the corrupt labels constitute a very high complexity function, and so is very hard to learn compared with clean points [2]. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1962/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1962/Authors|ICLR.cc/2020/Conference/Paper1962/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148335, "tmdate": 1576860559776, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment"}}}, {"id": "HkghJo9-jS", "original": null, "number": 2, "cdate": 1573133028408, "ddate": null, "tcdate": 1573133028408, "tmdate": 1573133028408, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "HJleZsO2KB", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment", "content": {"title": "Reply", "comment": "Hi! Thank you so much for your comment! We agree that the original writing needs a lot improvement for better clarity, and we have made updates according to your comments. We reorganized and rewrote large parts in section 2, 2.1 and 2.2 to make our theory part clearer. We also corrected sentences that we feel inappropriate.\n\nNow please let me address your questions specifically.\nA1. '3 The assumptions \\hat{p}+\\hat{k}+\\hat{l}=1 is very strong to me. The events should be dependent. This makes all the theoretical analyses pseudo and not convincing at all. The authors may spend more effort to make the part clear, reasonable, and convincing.'\n- We added in section A to explain this. We think it is a misunderstanding. In particular, see equation 12 and the discussion around it. \\hat{p}+\\hat{k}+\\hat{l}=1 is not an assumption and holds by construction. Please also see the current beginning part to section 2 (discussion around equation 1).\n\nA2. '(1 Why the derivative of Eq. (1) is Eq. (2)? The notation of f, f_\\theta, and f_w has been abused. The notation is confusing without explanation. It seems Eq. (2) is not correct and the following is not convincing.'\n- Sorry for our sloppy notation in the original version! We have updated this section to make it clearer. Please check the current section 2.2 and let us know if the clarity is improved. We will work hard to make this as clear as possible. Please also see the newly added appendix section B if you are interested in further discussion regarding the robustness of gambler's loss.\n\nA3. (2 Why a small gradient will slower the model to fit the data? This is not clear and maybe not true.\n- Yes, this is indeed an intuitive assumption, and might not hold for some special cases, since zero gradient means no learning has occurred. In fact, please note that the goal of (revised) section 2.2 is to explain, at a high level, a very surprising phenomenon, i.e. training with gambler's loss improves final accuracy when label noise is present. The actual mechanism might be very complicated and task-dependent, and a precise theoretical study of this phenomenon is extremely difficult given our current knowledge about deep learning theory. We added more intuition to this section, and we think that a clear theoretical understanding of this phenomenon is way beyond the scope of this work.\n\nA4. 'Many claims are very ambiguous. For example, \"the key point is that it always holds on some degree\", on which degree and why always holds?'\n- Sorry for this confusion! We removed this ambiguous sentence and added an example for this. It relates to the fact that on complicated dataset such as CIFAR10, the training accuracy on the clean dataset might be actually significant below 100%.\n\nA5.  'In some cases, it even leads to a better memorization phenomenon\", in which cases and why lead to better memorization phenomenon?'\n- There is no such sentence in our paper. In fact, we think this is a misreading of the two neighboring sentences in section 2:\n\"As a result, this widens the test accuracy plateau by slowing down the memorization phenomenon. In some cases, it even leads to a better convergence speed and better peak performance (see Figure 4). \"\n\nA6. 'Some claims are even wrong. For example, \"Traditional methods in label noise often involves introducing a surrogate loss function that is specialized for the corrupted dataset at hand and is of no use when one is not aware of the existence of label noise\" This is just for some specific methods, not for the most of them.'\n- Hmmm, thanks for pointing this out. Actually, what we really meant was that 'one common approach amongst others is to introduce a surrogate loss function', since the context was to introduce another loss function approach to the field. We have corrected this sentence.\n\nA7. We have also fixed the typo you mentioned.\n\nAgain, thank you so much for the review, and please check the newly revised section 2 and section A B in the appendix for your questions. Please let us know if you have more questions regarding section 2, and we will do our best to make it as clear as possible. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1962/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1962/Authors|ICLR.cc/2020/Conference/Paper1962/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148335, "tmdate": 1576860559776, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment"}}}, {"id": "ryxCkugxjr", "original": null, "number": 1, "cdate": 1573025765803, "ddate": null, "tcdate": 1573025765803, "tmdate": 1573025765803, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "BkxvUxMA5H", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment", "content": {"title": "Reply", "comment": "Hi Yilun! Thanks for your comment and for noticing our work! The paper you pointed us to is indeed interesting, and using determinant of the estimated joint matrix is very novel and seems to work very well (I personally learned a lot from reading your paper). We were not aware of this paper before and we think it is good for us to relate to this work in our paper. However, we decided that we will refrain from making a comparison with this method for the following two reasons: (1) let C denote the number of classes, then computing the determinant of the joint matrix is of O(C^3) complexity (e.g., for the numpy implementation), for tasks such as CIFAR100 or ImageNet, it does not look like the method will scale up easily, this is then followed by a matrix inversion, which is again of Omega(C^2.3) complexity; (2) the loss function also seems to have non-trivial batchsize dependence in order to make the estimated joint matrix full rank, for Imagenet, for example, this method needs at least 1000 batchsize, and to ensure that the matrix is full rank with high probability, the batchsize seems to need to be another order of magnitude larger. In short, we really want to compare with methods with similar computational complexity, and we will relate to this work in the paper. Still, we are very excited to hear about it."}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1962/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1962/Authors|ICLR.cc/2020/Conference/Paper1962/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148335, "tmdate": 1576860559776, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Comment"}}}, {"id": "HJleZsO2KB", "original": null, "number": 1, "cdate": 1571748599971, "ddate": null, "tcdate": 1571748599971, "tmdate": 1572972401182, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a new loss function for dealing with label noise, claiming that the loss function is helpful in preventing overfitting caused by noisy labels. Some experiments show the effectiveness.\n\nThe theory part is unclear to me. (1 Why the derivative of Eq. (1) is Eq. (2)? The notation of f, f_\\theta, and f_w has been abused. The notation is confusing without explanation. It seems Eq. (2) is not correct and the following is not convincing. (2 Why a small gradient will slower the model to fit the data? This is not clear and maybe not true. (3 The assumptions \\hat{p}+\\hat{k}+\\hat{l}=1 is very strong to me. The events should be dependent. This makes all the theoretical analyses pseudo and not convincing at all. The authors may spend more effort to make the part clear, reasonable, and convincing.\n\nMany claims are very ambiguous. For example, \"the key point is that it always holds on some degree\", on which degree and why always holds? \"In some cases, it even leads to a better memorization phenomenon\", in which cases and why lead to better memorization phenomenon?\n\nSome claims are even wrong. For example, \"Traditional methods in label noise often involves introducing a surrogate loss function that is specialized for the corrupted dataset at hand and is of no use when one is not aware of the existence of label noise\" This is just for some specific methods, not for the most of them.\n\nTypo: \"We verify that . Therefore,\"\n\nOverall, I cannot understand why the proposed loss function works and cannot recommend acceptance for the current version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1962/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1962/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1962/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575671365020, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1962/Reviewers"], "noninvitees": [], "tcdate": 1570237729765, "tmdate": 1575671365033, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Official_Review"}}}, {"id": "BkxvUxMA5H", "original": null, "number": 1, "cdate": 1572900942871, "ddate": null, "tcdate": 1572900942871, "tmdate": 1572901008040, "tddate": null, "forum": "rJxq3kHKPH", "replyto": "rJxq3kHKPH", "invitation": "ICLR.cc/2020/Conference/Paper1962/-/Public_Comment", "content": {"title": "Related work/baseline missing", "comment": "Hi,\n\n[1] proposes the first loss function that is provably not sensitive to noise patterns and noise amount. Also, [1] does not require to know the noise patterns or noise amount beforehand.  I wonder how the noise-robust function[1] performs in your setting. In experiments of [1], the noise pattern has both symmetry and asymmetry patterns, as well as diagonal-dominant and non-diagonal-dominant patterns. \n\nThank you!\n\n\n[1] Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_DMI: A novel information-theoretic loss function for training deep nets robust to label noise.  NeurIPS 2019"}, "signatures": ["~Yilun_Xu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yilun_Xu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zliu@cat.phys.s.u-tokyo.ac.jp", "wangru1994305@gmail.com", "pliang@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "morency@cs.cmu.edu", "ueda@phys.s.u-tokyo.ac.jp"], "title": "A Simple Approach to the Noisy Label Problem Through the Gambler's Loss", "authors": ["Liu Ziyin", "Ru Wang", "Paul Pu Liang", "Ruslan Salakhutdinov", "Louis-Philippe Morency", "Masahito Ueda"], "pdf": "/pdf/e06213bdd4666aefeb0530dff72547f8a6e10690.pdf", "TL;DR": "We propose to a simple loss function and an analytical early stopping criterion to deal with the label noise problem.", "abstract": "Learning in the presence of label noise is a challenging yet important task. It is crucial to design models that are robust to noisy labels. In this paper, we discover that a new class of loss functions called the gambler's loss provides strong robustness to label noise across various levels of corruption. Training with this modified loss function reduces memorization of data points with noisy labels and is a simple yet effective method to improve robustness and generalization. Moreover, using this loss function allows us to derive an analytical early stopping criterion that accurately estimates when memorization of noisy labels begins to occur. Our overall approach achieves strong results and outperforming existing baselines.", "code": "https://github.com/codesubmiter/b3124134", "keywords": ["noisy labels", "robust learning", "early stopping", "generalization"], "paperhash": "ziyin|a_simple_approach_to_the_noisy_label_problem_through_the_gamblers_loss", "original_pdf": "/attachment/962c3fcb47f0966dda49a7a531d9de46ac59833d.pdf", "_bibtex": "@misc{\nziyin2020a,\ntitle={A Simple Approach to the Noisy Label Problem Through the Gambler's Loss},\nauthor={Liu Ziyin and Ru Wang and Paul Pu Liang and Ruslan Salakhutdinov and Louis-Philippe Morency and Masahito Ueda},\nyear={2020},\nurl={https://openreview.net/forum?id=rJxq3kHKPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJxq3kHKPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187085, "tmdate": 1576860592801, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1962/Authors", "ICLR.cc/2020/Conference/Paper1962/Reviewers", "ICLR.cc/2020/Conference/Paper1962/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1962/-/Public_Comment"}}}], "count": 13}