{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1394470920000, "tcdate": 1394470920000, "number": 7, "id": "w0XswsNFad7Qu", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["anonymous reviewer f4a8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I apologize for the delay in my reply.\r\nVerdict:  weak accept."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394470920000, "tcdate": 1394470920000, "number": 9, "id": "SPk0N0RlUTrqv", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["anonymous reviewer f4a8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I apologize for the delay in my reply.\r\nVerdict:  weak accept."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394470860000, "tcdate": 1394470860000, "number": 8, "id": "1toZvrIP-Xvme", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["anonymous reviewer f4a8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I apologize for the delay in my reply.\r\nVerdict:  weak accept."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362379980000, "tcdate": 1362379980000, "number": 1, "id": "ZVb9LYU20iZhX", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["anonymous reviewer f4a8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "review": "Regularization methods are critical for the successful applications of\r\nneural networks.  This work introduces a new dropout-inspired\r\nregularization method named stochastic pooling.  The method is simple,\r\napplicable applicable to convolutional neural networks with positive\r\nnonlinearites, and achieves good performance on several tasks.\r\n\r\nA potentially severe issue is that the results are no longer state of\r\nthe art, as maxout networks get better results.  But this does not\r\nstrongly suggest that stochastic pooling is inferior to maxout, since\r\nthe methods are different and can therefore be combined, and, more\r\nimportantly, maxout networks may have used a more thorough\r\narchitecture and hyperparameter search, which would explain their\r\nbetter performance.\r\n\r\nThe main problem with the paper is that the experiments are lacking in\r\nthat there is no proper comparison to dropout.  While the results on\r\nCIFAR-10 are compared to the original dropout paper and result in an\r\nimprovement, the paper does not report results for the remainder of\r\nthe datasets with dropout and with the same architecture (if the\r\narchitecture is not the same in all experiments, then performance\r\ndifferences could be caused by architecture differences).  It is thus\r\npossible that dropout would achieve nearly identical performance on\r\nthese tasks if given the same architecture on MNIST, CIFAR-100, and\r\nSVHN.  What's more, when properly tweaked, dropout outperforms the\r\nresults reported here on CIFAR-10 as reported in Snoek et al. [A] (sub\r\n15% test error); and it is conceivable that Bayesian-optimized\r\nstochastic pooling would achieve worse results.\r\n \r\nIn addition to dropout, it is also interesting to compare to dropout\r\nthat occurs before max-pooling.  This kind of dropout bears more\r\nresemblance to stochastic pooling, and may achieve results that  are\r\nsimilar (or better -- it cannot be ruled out).\r\n\r\nFinally, a minor point.  The paper emphasizes the fact that stochastic\r\npooling averages 4^N models while dropout averages 2^N models, where N\r\nis the number of units.  While true, this is not relevant, since both\r\nquantities are vast, and the performance differences between the two \r\nmethods will stem from other sources. \r\n\r\nTo conclude, the paper presented an interesting and elegant technique\r\nfor preventing overfitting that may become widely used.  However, this\r\npaper does not convincingly demonstrate its superiority over dropout. \r\n\r\nReferences \r\n----------\r\n[A] Snoek, J. and Larochelle, H. and Adams, R.P., Practical Bayesian\r\nOptimization of Machine Learning Algorithms, NIPS 2012"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362369360000, "tcdate": 1362369360000, "number": 3, "id": "obPcCcSvhKovH", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["Marc'Aurelio Ranzato"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Another minor comment related to the visualization method: since there is no iterative 'inference' step typical of deconv. nets (the features are already given by a direct forward pass) then this method is perhaps more similar to this old paper of mine:\r\nM. Ranzato, F.J. Huang, Y. Boureau, Y. LeCun, 'Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition'. Proc. of Computer Vision and Pattern Recognition Conference (CVPR 2007), Minneapolis, 2007.\r\nhttp://www.cs.toronto.edu/~ranzato/publications/ranzato-cvpr07.pdf\r\nThe only difference being the new pooling instead of max-pooling, the use of ReLU instead of tanh and the tying of the weights (filters optimized for feature extraction but used also for reconstruction).\r\n\r\nOverall, I think that even this visualization method constitutes a nice contribution of this paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362349140000, "tcdate": 1362349140000, "number": 6, "id": "BBmMrdZA5UBaz", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["Marc'Aurelio Ranzato"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I really like this paper because:\r\n- it is simple yet very effective and\r\n- the empirical validation not only demonstrates the method but it also helps understanding where the gain comes from (tab. 5 was very useful to understand the regularization effect brought by the sampling noise).\r\n\r\nI also found intriguing the visualization method: using deconv. nets to reverse a trained conv. net; that's clever! Maybe that can become a killer app for deconv nets. Videos are also very nice.\r\nHowever, I was wondering how did you invert the normalization layer?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362101820000, "tcdate": 1362101820000, "number": 2, "id": "lWJdCuzGuRlGF", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["anonymous reviewer cd07"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "review": "The authors introduce a stochastic pooling method in the context of\r\nconvolutional neural networks, which replaces the traditionally used\r\naverage or max pooling operators. In the stochastic pooling a\r\nmultinomial distribution is created from input activations and used to\r\nselect the index of the activation to pass to the next layer of the\r\nnetwork. On first read, this method resembled that of 'probabilistic max\r\npooling' by Lee et. al in 'Convolutional Deep Belief Networks for\r\nScalable Unsupervised Learning of Hierarchical Representations',\r\nhowever the context and execution are different.\r\n\r\nDuring testing, the authors employ a separate pooling function that is a\r\nweighted sum of the input activations and their corresponding\r\nprobabilities that would be used for index selection during training.\r\nThis pooling operator is speculated to work as a form of\r\nregularization through model averaging. The authors substantiate this claim with results averaging multiple samples at each pool of the stochastic architectures and visualizations of images obtained from\r\nreconstructions using deconvolutional networks.\r\n\r\nMoreover, test set accuracies for this method are given for four\r\nrelevant datasets where it appears stochastic pooling CNNs are able to\r\nachieve the best known performance on three. A good amount of detail\r\nhas been provided allowing the reader to reproduce the results.\r\n\r\nAs the sampling scheme proposed may be combined with other regularization techniques, it will be exciting to see how multiple forms of regularization can contribute or degrade test accuracies.\r\n\r\nSome minor comments follow:\r\n\r\n- Mini-batch size for training is not mentioned.\r\n\r\n- Fig. 2 could be clearer on first read, e.g. if boxes were drawn\r\naround (a,b,c), (e,f), and (g,h) to indicate they are operations on\r\nthe same dataset.\r\n\r\n- In Section 4.2 it is noted that stochastic pooling avoids\r\nover-fitting unlike averaging and max pooling, however in Fig. 3 it\r\ncertainly appears that the average and max techniques are not severely\r\nover-fitting as in the typical network training case (with noticeable\r\ndegradation in test set performance). However, the network does train\r\nto near zero error on the training set. It may be more accurate to state\r\nthat stochastic pooling promotes better generalization yet additional training epochs may make the over-fitting argument clearer.\r\n\r\n- Fig. 3 also suggests that additional training may improve the final\r\nreported test set error in the case of stochastic pooling. The\r\nreference to state-of-the-art performance on CIFAR-10 is no longer\r\ncurrent.\r\n\r\n- Section 4.8, sp 'proabilities'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362085800000, "tcdate": 1362085800000, "number": 5, "id": "OOBjrzG_LdOEf", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I'm excited about this paper because it introduces another trick for cheap model averaging like dropout. It will be interesting to see if this kind of fast model averaging turns into a whole subfield.\r\n\r\nI recently got some very good results ( http://arxiv.org/abs/1302.4389 ) by using a model that works well with the kinds of approximations to model averaging that dropout makes. Presumably there are models that get the same kind of synergy with stochastic pooling. I think this is a very promising prospect, since stochastic pooling works so well even with just vanilla rectifier networks as the base model."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361845800000, "tcdate": 1361845800000, "number": 4, "id": "WilRXfhv6jXxa", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "l_PClqDdLb5Bp", "replyto": "l_PClqDdLb5Bp", "signatures": ["anonymous reviewer 2b4c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "review": "This paper introduces a new regularization technique based on inexpensive approximations to model averaging, similar to dropout. As with dropout, the training procedure involves stochasticity but the trained model uses a cheap approximation to the average over all possible models to make a prediction.\r\n\r\nThe paper includes empirical evidence that the model averaging effect is happening, and uses the method to improve on the state of the art for three datasets.\r\n\r\nThe method is simple and in principle, computationally inexpensive.\r\n\r\nTwo criticisms of this paper:\r\n-The result on CIFAR-10 was not in fact state of the art at the time of submission; it was just slightly worse than Snoek et al's result using Bayesian hyperparameter optimization.\r\n-I think it's worth mentioning that while this method is computationally inexpensive in principle, it is not necessarily easy to get a fast implementation in practice. i.e., people wishing to use this method must implement their own GPU kernel to do stochastic pooling, rather than using off-the-shelf implementations of convolution and basic tensor operations like indexing.\r\n\r\nOtherwise, I think this is an excellent paper. My colleagues and I have made a slow implementation of the method and used it to reproduce the authors' MNIST results. The method works as advertised and is easy to use."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358408700000, "tcdate": 1358408700000, "number": 14, "id": "l_PClqDdLb5Bp", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "l_PClqDdLb5Bp", "signatures": ["zeiler@cs.nyu.edu"], "readers": ["everyone"], "content": {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\r\n    Networks", "decision": "conferenceOral-iclr2013-conference", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "pdf": "https://arxiv.org/abs/1301.3557", "paperhash": "zeiler|stochastic_pooling_for_regularization_of_deep_convolutional_neural_networks", "keywords": [], "conflicts": [], "authors": ["Matthew Zeiler", "Rob Fergus"], "authorids": ["zeiler@cs.nyu.edu", "robfergus@gmail.com"]}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 10}