{"notes": [{"id": "BkN5UoAqF7", "original": "B1xamrg5tQ", "number": 202, "cdate": 1538087762509, "ddate": null, "tcdate": 1538087762509, "tmdate": 1550871013375, "tddate": null, "forum": "BkN5UoAqF7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkepFB9geN", "original": null, "number": 1, "cdate": 1544754565147, "ddate": null, "tcdate": 1544754565147, "tmdate": 1545354517941, "tddate": null, "forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper202/Meta_Review", "content": {"metareview": "The paper proposes a simple method for improving the sample efficiency of GAIL, essentially a way of turning inverse reinforcement learning into classification. As reviewers noted, the method is based on a simple idea with potentially broad applicability.\n\nConcerns were raised about the multiple components of the system and what each contributed, and missing pointers to the literature. A baseline wherein GAIL is initialized with behaviour cloning, although only suggested but not tried in previous works. The authors did, however, attempt this setting and found it to hurt, not help, performance. I find this surprising and would urge the authors to validate that this isn't merely an uninteresting artifact of the setup, however I commend the authors for trying it and don't believe that a surprising result in this regard is a barrier to publication.\n\nAs several reviewers did not provide feedback on revisions addressing their concerns, this Area Chair was left to determine to a large degree whether or not reviewer concerns were in fact addressed.  I thank AnonReviewer4 for revisiting their review towards the end of the period, and concur with them that many of the concerns raised by reviewers have indeed been adequately dealt with.  ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "A simple but potentially impactful idea and a manuscript that has greatly improved since submission."}, "signatures": ["ICLR.cc/2019/Conference/Paper202/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper202/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper202/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353300750, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper202/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper202/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper202/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353300750}}}, {"id": "rye5EKn1pm", "original": null, "number": 3, "cdate": 1541552433812, "ddate": null, "tcdate": 1541552433812, "tmdate": 1544664768535, "tddate": null, "forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "content": {"title": "Nice idea, nice results, but paper needs work.", "review": "This paper proposed an imitation learning algorithm that achieves competitive results with GAIL, while requiring significantly fewer interactions with the environment.\n\nI like the method proposed in this paper. It seems similar to ideas in this concurrent submission: https://openreview.net/forum?id=B1excoAqKQ\n\nHowever, the paper is a bit difficult to read. The proposed method is made up of several changes compared to the baselines (e.g. using Q-learning without IRL instead of IRL, using off-policy learning, using conditioning to obtain a stochastic policy) but motivation for each component is presented late within the paper. The terminology used to describe these components is a bit confusing. Also some math is presented without intuitive descriptions.\n\nI\u2019d like to see more ablations performed: there are three main changes compared to GAIL, but an ablation is only performed for the stochastic policy. It would be interesting to tease out what is more important, off-policy learning, or bypassing IRL.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "cdate": 1542234515871, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper202/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671269, "tmdate": 1552335671269, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper202/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1l5nIEJxN", "original": null, "number": 14, "cdate": 1544664754207, "ddate": null, "tcdate": 1544664754207, "tmdate": 1544664754207, "tddate": null, "forum": "BkN5UoAqF7", "replyto": "Bkekr2VHA7", "invitation": "ICLR.cc/2019/Conference/-/Paper202/Official_Comment", "content": {"title": "Reviewer Response", "comment": "Thank you for the updated manuscript. The updates address my major concerns, and similar concerns from other reviewers.\n\nI believe the presentation is now relatively clear and uses more reasonable terminology. Also the additional  ablation experiments suggests that each component of the approach adds to the overall performance.\n\nI believe the manuscript now clearly presents simple but impact modifications to a popular imitation learning approach (GAIL) that should be useful for the community.\n\nSince this work overlaps substantially with several concurrent submissions to ICLR this cycle, it may be appropriate to mention them as concurrent work in section 4 (Related Work).\n\nI will update my rating to 7."}, "signatures": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper202/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper202/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615069, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkN5UoAqF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper202/Authors", "ICLR.cc/2019/Conference/Paper202/Reviewers", "ICLR.cc/2019/Conference/Paper202/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper202/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper202/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper202/Authors|ICLR.cc/2019/Conference/Paper202/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper202/Reviewers", "ICLR.cc/2019/Conference/Paper202/Authors", "ICLR.cc/2019/Conference/Paper202/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615069}}}, {"id": "rJxX8KR1R7", "original": null, "number": 2, "cdate": 1542609226719, "ddate": null, "tcdate": 1542609226719, "tmdate": 1542609226719, "tddate": null, "forum": "BkN5UoAqF7", "replyto": "SyeklVC1Cm", "invitation": "ICLR.cc/2019/Conference/-/Paper202/Official_Comment", "content": {"title": "response", "comment": "I think there is at least one paper Kang et al. \"Policy optimization with demonstrations.\" ICML'18, which conducts extensive experiments as extension to GAIL. The experiment setup is also simple: use both the reward function in the demonstration set, as well as the Discriminator reward function for training GAIL.  \n\nMoreover, it is not difficult at all to initialize GAIL with BC. Although combining both of them is not shown in any of the literatures according to your response, I am not sure why we should ignore some simple baselines while designing complicated methods. You claim you don't agree to add this baseline, then please detail about the experiments you did to show \" it is a myth that the BC initialization improves the learning speed of GAIL\".\n\nAlso, could you please point me to the exact location of your modification? "}, "signatures": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper202/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper202/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615069, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkN5UoAqF7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper202/Authors", "ICLR.cc/2019/Conference/Paper202/Reviewers", "ICLR.cc/2019/Conference/Paper202/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper202/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper202/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper202/Authors|ICLR.cc/2019/Conference/Paper202/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper202/Reviewers", "ICLR.cc/2019/Conference/Paper202/Authors", "ICLR.cc/2019/Conference/Paper202/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615069}}}, {"id": "B1lQQqme6X", "original": null, "number": 4, "cdate": 1541581338986, "ddate": null, "tcdate": 1541581338986, "tmdate": 1541581338986, "tddate": null, "forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "content": {"title": "Good results, but needs ablations to clearly identify contributions and improved presentation", "review": "Summary/contributions:\n\nThe primary aim of this paper is to improve the sample efficiency of GAIL (Ho et al. 2016). The claimed contributions can be summarized by consisting of 1) replacing TRPO (which was used in the original paper) with a off-policy RL with a modified reward, 2) using a policy parameterizing where the noise is used as an input rather than at the output. While conceptually simple, this paper contributes a method that shows improved sample efficiency on a series of benchmark mujoco tasks, which has practical implications for real world environments. \n\nPros:\n- a simple idea with good empirical results that would be of interest to the community\n\nCons:\n- (extremely) unclear presentation which hinders the message of the paper.\n- the novelty of the approach is somewhat limited\n\nJustification for score:\nI gave my rating based upon the following considerations. The approach in this paper makes sense from a practical perspective and presents strong results. However, the experiments in the paper do not clearly identify which components of their method lead to their improved performance (i.e., an ablation on their stated contributions). The writing is also extremely poor. The paper makes use of non-standard notation (in relation to the prior work which it builds on) and unusual terminology. Overall however, I am on the fence about this paper, since I recognize the good results presented in this paper, in addition to the timely nature of the idea (there are at least two concurrent submissions that I am aware of that are similar).\n\nOther:\n- I would appreciate if the related work discussed prior off-policy methods that use demonstrations (e.g Hester et al. 2017)\n- The paper has a large number of ungrammatical sentences and unidiomatic expressions. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "cdate": 1542234515871, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper202/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671269, "tmdate": 1552335671269, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper202/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgMjKRznX", "original": null, "number": 1, "cdate": 1540708762009, "ddate": null, "tcdate": 1540708762009, "tmdate": 1541534199230, "tddate": null, "forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "content": {"title": "Efficient imitation learning ", "review": "This paper proposes a new method to imitate expert efficiently. The paper first proposes a way to compute reward function from expert demonstration and uses the log probability to represent this reward function.  Then they find a form of bellman equation that can optimize the reward stably. After the 'Q learning without IRL', an off-policy RL off-pac is applied. So this paper achieves comparable results to GAIL but uses much less data amount. \n\nclarity:\nThis paper is clearly written.\n\noriginality:\nThis paper is original.\n\npros:\nComparable performance with GAIL.\nBetter performance than Behavioral Cloning\nNew way of using demonstrations\n\ncons:\nAlthough both the method and the experiments look promising, there is a very simple yet competitive baseline missing. This baseline is also mentioned in the original GAIL paper: you initialize GAIL with BC, and then train GAIL. That's the baseline for a set of fair comparison.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "cdate": 1542234515871, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper202/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671269, "tmdate": 1552335671269, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper202/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Sklwhthhnm", "original": null, "number": 2, "cdate": 1541355951364, "ddate": null, "tcdate": 1541355951364, "tmdate": 1541534199025, "tddate": null, "forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "invitation": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "content": {"title": "Hard to read, probably overfits ?", "review": "The paper proposes a method for imitation learning via inverse reinforcement learning based on a specific modeling of the reward. It is modeled as the log probability of a state action pair to belong to the expert policy. It models this distribution as a Bernoulli one and thus it reduces the IRL problem to a classification task. The global method also uses an off-policy algorithm to learn the value function of the current agent policy to improve sample efficiency. The method is tested on a set of continuous control tasks such as walker, hopper or humanoid. \n\nI think the paper has several flaws. First, I found the paper not very well written and organized. It is hard to read. It uses some terminology in a way that is different from the rest of the litt\u00e9rature (such as Q-learning as learning the Q-function of the expert policy instead of using the  optimal Bellman operator (even if the expert is supposed to be optimal)). I also think that the related work section is missing a lot of important refs because it really focuses on recent papers while imitation learning has a long history. \n\nYet, my main concern is that the proposed method seems to reduce to a classification problem to me and is likely to suffer from the same issues than the supervised learning method (AKA behavior cloning). It probably overfits a lot and there is nothing in the experiments that shows how robust is the method to perturbations. In a discrete world, this method would ideally place a reward of 1 in every state visited by the expert and 0 elsewhere which is very likely to overfit and result in unstable behaviors in the presence of noise etc. I would like to see experiments showing robustness. \n\nThe experiments are also a bit strange since the learning is stopped early for the proposed method. Is it because the learning is unstable ?\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper202/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sample Efficient Imitation Learning for Continuous Control", "abstract": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior given expert demonstrations. Recently, generative adversarial imitation learning (GAIL) has shown significant progress on IL for complex continuous tasks. However, GAIL and its extensions require a large number of environment interactions during training. In real-world environments, the more an IL method requires the learner to interact with the environment for better imitation, the more training time it requires, and the more damage it causes to the environments and the learner itself. We believe that IL algorithms could be more applicable to real-world problems if the number of interactions could be reduced. \nIn this paper, we propose a model-free IL algorithm for continuous control. Our algorithm is made up mainly three changes to the existing adversarial imitation learning (AIL) methods \u2013 (a) adopting off-policy actor-critic (Off-PAC) algorithm to optimize the learner policy, (b) estimating the state-action value using off-policy samples without learning reward functions, and (c) representing the stochastic policy function so that its outputs are bounded. Experimental results show that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "keywords": ["Imitation Learning", "Continuous Control", "Reinforcement Learning", "Inverse Reinforcement Learning", "Conditional Generative Adversarial Network"], "authorids": ["fumihiro.fs.sasaki@jp.ricoh.com"], "authors": ["Fumihiro Sasaki", "Tetsuya Yohira", "Atsuo Kawaguchi"], "TL;DR": "In this paper, we proposed a model-free, off-policy IL algorithm for continuous control. Experimental results showed that our algorithm achieves competitive results with GAIL while significantly reducing the environment interactions.", "pdf": "/pdf/5714931bba7474798f468a5fe126845abc67560a.pdf", "paperhash": "sasaki|sample_efficient_imitation_learning_for_continuous_control", "_bibtex": "@inproceedings{\nsasaki2018sample,\ntitle={Sample Efficient Imitation Learning for Continuous Control},\nauthor={Fumihiro Sasaki and Tetsuya Yohira and Atsuo Kawaguchi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkN5UoAqF7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper202/Official_Review", "cdate": 1542234515871, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkN5UoAqF7", "replyto": "BkN5UoAqF7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper202/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335671269, "tmdate": 1552335671269, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper202/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}