{"notes": [{"id": "BJlXUsR5KQ", "original": "r1ep1T7FYm", "number": 161, "cdate": 1538087755120, "ddate": null, "tcdate": 1538087755120, "tmdate": 1545355399281, "tddate": null, "forum": "BJlXUsR5KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks", "abstract": "The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g. the recti\ufb01er and the LSTM units) plays a crucial role in the complexity of learning. Based on this remark, this paper discusses an optimal selection of the neuron non-linearity in a functional framework that is inspired from classic regularization arguments. A representation theorem is given which indicates that the best activation function is a kernel expansion in the training set, that can be effectively approximated over an opportune set of points modeling 1-D clusters. The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.", "keywords": ["Activation functions", "Kernel methods", "Recurrent networks"], "authorids": ["g.marra@unifi.it", "dario.zanca@unifi.it", "alessandro.betti@unifi.it", "marco.gori@unisi.it"], "authors": ["Giuseppe Marra", "Dario Zanca", "Alessandro Betti", "Marco Gori"], "pdf": "/pdf/a5813434d3fa840a35057f8dec6dcf1a4c7f1e40.pdf", "paperhash": "marra|learning_neuron_nonlinearities_with_kernelbased_deep_neural_networks", "_bibtex": "@misc{\nmarra2019learning,\ntitle={Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks},\nauthor={Giuseppe Marra and Dario Zanca and Alessandro Betti and Marco Gori},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlXUsR5KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Byxo5QBll4", "original": null, "number": 1, "cdate": 1544733587073, "ddate": null, "tcdate": 1544733587073, "tmdate": 1545354513018, "tddate": null, "forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper161/Meta_Review", "content": {"metareview": "This paper proposes to automatically learn the form of the non-linearities of neural networks in deep neural networks, which the reviewers noted to be an interesting albeit significantly studied direction.   Overall, this paper falls just below the bar, with no reviewer really willing to champion for acceptance.  Reviewer 3 found the paper to be marginally above the acceptance threshold and found the insights provided in the paper (in Section 2) to be a neat and strong contribution.  Reviewers 1-2, however, found the paper marginally below the bar and seemed confused by the presentation of the paper.  They seemed to believe in the motivation and idea, but they found the paper hard to follow and not particularly clearly written.  It would seem that the paper could significantly benefit from careful editing and restructuring to disambiguate contributions from motivation and existing literature.  Also, the authors should provide clear justification for their design choices and modeling assumptions. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "An interesting proposal to automatically learn activation non-linearities but novelty is unclear and the paper is hard to follow."}, "signatures": ["ICLR.cc/2019/Conference/Paper161/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper161/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks", "abstract": "The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g. the recti\ufb01er and the LSTM units) plays a crucial role in the complexity of learning. Based on this remark, this paper discusses an optimal selection of the neuron non-linearity in a functional framework that is inspired from classic regularization arguments. A representation theorem is given which indicates that the best activation function is a kernel expansion in the training set, that can be effectively approximated over an opportune set of points modeling 1-D clusters. The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.", "keywords": ["Activation functions", "Kernel methods", "Recurrent networks"], "authorids": ["g.marra@unifi.it", "dario.zanca@unifi.it", "alessandro.betti@unifi.it", "marco.gori@unisi.it"], "authors": ["Giuseppe Marra", "Dario Zanca", "Alessandro Betti", "Marco Gori"], "pdf": "/pdf/a5813434d3fa840a35057f8dec6dcf1a4c7f1e40.pdf", "paperhash": "marra|learning_neuron_nonlinearities_with_kernelbased_deep_neural_networks", "_bibtex": "@misc{\nmarra2019learning,\ntitle={Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks},\nauthor={Giuseppe Marra and Dario Zanca and Alessandro Betti and Marco Gori},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlXUsR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper161/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353315139, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper161/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper161/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper161/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353315139}}}, {"id": "S1xA6on62m", "original": null, "number": 3, "cdate": 1541422022388, "ddate": null, "tcdate": 1541422022388, "tmdate": 1541534232538, "tddate": null, "forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper161/Official_Review", "content": {"title": "LEARNING NEURON NON-LINEARITIES WITH KERNEL-BASED DEEP NEURAL NETWORKS", "review": "The paper investigates the problem of designing the activation functions of neural networks with focus on recurrent architectures. The authors frame such problem as learning the activation functions in the space of square integrable functions by adding a regularization term penalizing the differential properties of candidate functions. In particular, the authors observe that this strategy is related to some well-established approaches to select activation functions such as ReLUs. \n\n\nThe paper has some typos and some passages are hard to read/interpret. The write-up needs to be improved significantly.  \n\nWhile some of the observations reported by the authors are interesting it is in general hard to evaluate the contributions of the paper. In particular the discussion of Sec. 2 is very informal, although ti describes the key technical observations used in the paper to devise the model (Sec. 3) that is then evaluated in the experiments (Sec. 4). In particular, it is unclear whether the authors are describing some known results - in which case they should add references - or original contributions - in which case they should report their results with more mathematical rigour. Indeed, in the abstract, the authors state that a representation theorem is given, but in the text they provide only an informal discussion of such result. \n\nOverall, it is hard to agree with the authors' conclusion that \"the KBRN architecture exhibits an ideal computational structure to deal with classic problems of capturing long-term dependencies\": the theoretical discussion does not provide sufficient evidence in this sense.\n\nSome minor points: \n\nConfusing notation: why were the alpha^k replaced with the \\chi^k between Sec. 2 and Sec. 3?\n\nUnclear motivation for some design choices. For instance 1) the justification given by the authors to neglect the linear terms from both g(x) and k(x) in Sec. 3 is unclear. 2) why was the \\ell_1 norm used as penalty for the regularizer R(\\chi) in Sec.3? One could argue that \\ell_1 is used to encourage sparse solutions, but the authors should explain why sparsity is desirable in this setting. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper161/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks", "abstract": "The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g. the recti\ufb01er and the LSTM units) plays a crucial role in the complexity of learning. Based on this remark, this paper discusses an optimal selection of the neuron non-linearity in a functional framework that is inspired from classic regularization arguments. A representation theorem is given which indicates that the best activation function is a kernel expansion in the training set, that can be effectively approximated over an opportune set of points modeling 1-D clusters. The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.", "keywords": ["Activation functions", "Kernel methods", "Recurrent networks"], "authorids": ["g.marra@unifi.it", "dario.zanca@unifi.it", "alessandro.betti@unifi.it", "marco.gori@unisi.it"], "authors": ["Giuseppe Marra", "Dario Zanca", "Alessandro Betti", "Marco Gori"], "pdf": "/pdf/a5813434d3fa840a35057f8dec6dcf1a4c7f1e40.pdf", "paperhash": "marra|learning_neuron_nonlinearities_with_kernelbased_deep_neural_networks", "_bibtex": "@misc{\nmarra2019learning,\ntitle={Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks},\nauthor={Giuseppe Marra and Dario Zanca and Alessandro Betti and Marco Gori},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlXUsR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper161/Official_Review", "cdate": 1542234524813, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper161/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335662224, "tmdate": 1552335662224, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper161/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJl3u3O5nX", "original": null, "number": 2, "cdate": 1541209204425, "ddate": null, "tcdate": 1541209204425, "tmdate": 1541534232329, "tddate": null, "forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper161/Official_Review", "content": {"title": "Unclear presentation and weak results.", "review": "Summary: the authors propose a method for learning activation functions in neural networks using kernels. Each activation function is modeled as a weighted set of kernels, where (as I understand it) the weights are learned simultaneously with the linear weights in the network. The authors apply their method to learn the XOR function and to a simple sequence memory task.\n\nMy main concern with the paper is that the presentation is very hard to follow. The motivation, background, and contributions of this paper are all mixed in the abstract and introduction, making it hard to understand what is the current state of learned activations, what this paper introduces, and how the work in this paper relates to prior work. Instead, I found the presentation in Scardapane et al much clearer (that paper has a clear separation of background, related work, and their contributions). By contrast, this paper has one large paragraph in the introduction that muddles together multiple threads of thought, making it hard to digest. Before the reader has had time to digest the main ideas, the paper launches into a highly technical description of the method, without a clear high level explanation of what the main technique is. A lot of the mathematical notation introduced in Sec2 is not clearly motivated.\n\nMy other main concern is with the results. The paper solves two simple tasks with their method, and it is unclear what their kernel methods really buy them. For the XOR problem, it is such a simple task that it is hard to judge the method (it seems like complex activation functions are not required to solve the task, and it is not clear what including them gets you). For the sequence memory task, it seems unfair to compare their results to recurrent networks with a *single* hidden unit. If you have networks with 2 or 4 or 8 hidden units, do they solve the task? These experiments do not shed much light on the advantages of using kernel activations in recurrent networks.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper161/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks", "abstract": "The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g. the recti\ufb01er and the LSTM units) plays a crucial role in the complexity of learning. Based on this remark, this paper discusses an optimal selection of the neuron non-linearity in a functional framework that is inspired from classic regularization arguments. A representation theorem is given which indicates that the best activation function is a kernel expansion in the training set, that can be effectively approximated over an opportune set of points modeling 1-D clusters. The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.", "keywords": ["Activation functions", "Kernel methods", "Recurrent networks"], "authorids": ["g.marra@unifi.it", "dario.zanca@unifi.it", "alessandro.betti@unifi.it", "marco.gori@unisi.it"], "authors": ["Giuseppe Marra", "Dario Zanca", "Alessandro Betti", "Marco Gori"], "pdf": "/pdf/a5813434d3fa840a35057f8dec6dcf1a4c7f1e40.pdf", "paperhash": "marra|learning_neuron_nonlinearities_with_kernelbased_deep_neural_networks", "_bibtex": "@misc{\nmarra2019learning,\ntitle={Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks},\nauthor={Giuseppe Marra and Dario Zanca and Alessandro Betti and Marco Gori},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlXUsR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper161/Official_Review", "cdate": 1542234524813, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper161/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335662224, "tmdate": 1552335662224, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper161/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkgv-Z-I2X", "original": null, "number": 1, "cdate": 1540915454764, "ddate": null, "tcdate": 1540915454764, "tmdate": 1541534232115, "tddate": null, "forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper161/Official_Review", "content": {"title": "Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks", "review": "The scope of the paper is interesting: to additionally learn the nonlinear activation function of the neuron.\n\nThe insights provided in section 2 with eqs (2)-(5) are interesting and naturally build on the previous work of Poggio & Girosi (1990) and Smola (1998). I found this a nice new insight and the strongest part of the paper. It is e.g. revealing to see to which P and L the rectifier nonlinearity is corresponding.\n\nOn the other hand I also have a number of suggestions for further improvement:\n\n- Section 1: related to the overall function to be learned, the authors state \"this general problem has been already solved\". I think this statement is not completely correct, because depending on the choice of the stabilizer one obtains different optimal representations (e.g. Gaussian RBF or thin plate splines) as explained in Poggio & Girosi (1990). The theory does not tell what the best stabilizer is.\n\nAdditional relevant work that would be good to mention at this point, in the area of kernel methods, is e.g. learning the kernel.\n\n- It seems that no other existing work on deep kernel machines has been mentioned in the paper, while in the conclusions the authors state \"In this paper we have introduced Kernel-based deep neural networks\". \n\n- Related to the training set T_N the notation e^kappa is not explained. It is not clear how this is related to eq (1).\n\n- It would be good to comment on the difference between (3)(4) and Poggio & Girosi (1990).\n\n- unnumbered eq after (5): are there multiple solutions to the problem (non-convex)? \n\n- The explanation of the recurrent network at the end of section 2 is too limited. Moreover, LSTM is not just a neuron nonlinearity, but a recurrent network with a particular structure. To which P and L would LSTM correspond?\n\n- Fig.2: some of the nonlinearities look quite complicated and some of them are oscillatory (is this desirable? it reminds us of overfitting). Often one is interested in activation functions with a \"simple shape\" like sigmoid, tanh, relu. A more complicated nonlinearity may reduce the interpretability of the model.\n\n- The examples given are rather conceptual (though nice) examples of the proposed method. However, no comparisons with other methods have been made yet in terms of generalization performance, e.g. on a few standard classification benchmark data sets, in comparison with other deep or shallow models.\n\nA possible drawback of the proposed method might be (or maybe not) that additional unknown parameters need to be learned, which could possibly lead to worse generalization. It might be good to further investigate this.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper161/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks", "abstract": "The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g. the recti\ufb01er and the LSTM units) plays a crucial role in the complexity of learning. Based on this remark, this paper discusses an optimal selection of the neuron non-linearity in a functional framework that is inspired from classic regularization arguments. A representation theorem is given which indicates that the best activation function is a kernel expansion in the training set, that can be effectively approximated over an opportune set of points modeling 1-D clusters. The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.", "keywords": ["Activation functions", "Kernel methods", "Recurrent networks"], "authorids": ["g.marra@unifi.it", "dario.zanca@unifi.it", "alessandro.betti@unifi.it", "marco.gori@unisi.it"], "authors": ["Giuseppe Marra", "Dario Zanca", "Alessandro Betti", "Marco Gori"], "pdf": "/pdf/a5813434d3fa840a35057f8dec6dcf1a4c7f1e40.pdf", "paperhash": "marra|learning_neuron_nonlinearities_with_kernelbased_deep_neural_networks", "_bibtex": "@misc{\nmarra2019learning,\ntitle={Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks},\nauthor={Giuseppe Marra and Dario Zanca and Alessandro Betti and Marco Gori},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlXUsR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper161/Official_Review", "cdate": 1542234524813, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlXUsR5KQ", "replyto": "BJlXUsR5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper161/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335662224, "tmdate": 1552335662224, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper161/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}