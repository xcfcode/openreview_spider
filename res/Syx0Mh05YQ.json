{"notes": [{"id": "Syx0Mh05YQ", "original": "S1e47365Y7", "number": 1317, "cdate": 1538087958462, "ddate": null, "tcdate": 1538087958462, "tmdate": 1550886112971, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SyeloZtfgE", "original": null, "number": 1, "cdate": 1544880535639, "ddate": null, "tcdate": 1544880535639, "tmdate": 1545354533729, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Meta_Review", "content": {"metareview": "The authors have presented a simple yet elegant model to learn grid-like responses to encode spatial position, relying only on relative Euclidean distances to train the model, and achieving a good path integration accuracy. The model is simpler than recent related work and uses a structure of 'disentangled blocks' to achieve multi-scale grids rather than requiring dropout or injected noise. The paper is clearly written and it is intriguing to get down to the fundamentals of the grid code. On the negative side, the section on planning does not hold up as well and makes unverifiable claims, and one reviewer suggests that this section be replaced altogether by additional analysis of the grid model. Another reviewer points out that the authors have missed an opportunity to give a theoretical perspective on their model. Although there are aspects of the work which could be improved, the AC and all reviewers are in favor of acceptance of this paper.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1317/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352882481, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352882481}}}, {"id": "rklaoMVFCm", "original": null, "number": 12, "cdate": 1543221925129, "ddate": null, "tcdate": 1543221925129, "tmdate": 1543221925129, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "S1ls-yVtnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Thank you", "comment": "The response is thorough and my concerns are addressed, I have updated the score accordingly."}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "S1ls-yVtnm", "original": null, "number": 2, "cdate": 1541123843508, "ddate": null, "tcdate": 1541123843508, "tmdate": 1543221589328, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Review", "content": {"title": "Elegant but simplistic model for grid cells; unnecessary extension to path planning", "review": "Updated score from 6 to 7 after the authors addressed my comments below.\n\nPrevious review:\n\nThis paper builds upon the recent work on computational models of grid cells that rely on trainable (parametric) models such as recurrent neural networks [Banino et al, 2018; Cueva & Wei, 2018]. It focuses entirely on path integration in 2D and 3D, from velocity inputs only, and it relies on two sub-networks: the motion model (an RNN) and the localization model (a feed-forward network). The avowed goal of the paper is to build a very simple and linear model for grid cells.\n\nBy linearly embedding the position x into a high-dimensional hidden vector v(x) (e.g., 96 elements), it can model motion using a linear model relying on matrix-vector multiplication: v(x + dx) = M(dx) v(x), where dx is the 2D or 3D displacement, v(.) is a vector and M(.) is a matrix. The embeddings v(.) are learnable and the paper assumes a square or cubic grid of N*N or N*N*N possible positions x (with N=40); these embeddings are also normalized to unit length and obey the kernel constraint that the dot-product between any two positions' vectors v(x) and v(y) is a Gaussian or an exponential function. The motion matrix is represented as block diagonal, where each block is a rotation of subvector v_k(x) into v_k(x + dx), where each block corresponds to a specific grid cell, and where the diagonal block is further expressed as a quadratic function of dx_1, dx_2, dx_3 elements of the displacement vector.\n\nThe strengths of the paper are that:\n1) The supervision of the localization subnetwork only depends on Euclidean proximity between two positions x and y and therefore uses relative positions, not absolute ones. Similarly, the path integration supervision of the motion model uses only relative displacements.\n2) The resulting rate maps of the hidden units seem perfect; the model exhibits multi-scale grid behaviour.\n3) The idea of using disentangled blocks, rather than injecting noise or using dropout and a softmax bottleneck as in [Banino et al, 2018], is interesting.\n4) The model accumulates little path integration error over 1000 step-long episodes.\n\nThe weakness of the paper is its simplicity:\n1) The assumption that A(x, y) can be modeled by a Gaussian or exponential (Laplacian?) kernel is limiting, in particular for positions x and y that are far apart.\n2) There is no discussion about what egocentric vs. allocentric referentials, and dx is assumed to be aligned with (x, y) axes (which are also the axes defining the bounding box of the area).\n3) Unlike the other work on learning path integration using an RNN, the linear matrix model can only handle allocentric displacements dx_1, dx_2 (and optional dx_3 in 3D).\n4) No consideration is given to non-square areas: would the network also exhibit grid-like behavior if the area was circular?\n5) What happens if the quadratic parameterisation of block diagonals is dropped?\n6) The paper did not use metrics accepted in the neuroscience community for computing a gridness score of the grid cells (although the grid cell nature is evident). There should however be metrics for quantifying how many units represent the different scales, offsets and orientations.\n7) The authors did not consider (but mentioned) embedding locations from vision, and did not consider ambiguous position embeddings.\n\nThe experiments about path planning are unconvincing. First of all, the algorithm requires to input absolute positions of every obstacle into equation (9) - (10), which assumes that there is perfect information about the map. Secondly, the search algorithm is greedy and it is not obvious how it would handle a complex maze with cul-de-sac. Saying that \"there is no need for reinforcement learning or sophisticated optimal control\" is very misleading: the problem here is simplified to the extreme, and fully observed, and any comparison with deep RL algorithms that can handle partial observations is just out of place.\n\nIn summary, the authors have introduced an interesting and elegant model for grid cells that suffers from simplifications. The part on path planning should be cut and replaced with more analysis of the grid cells and an explanation of how the model would handle egocentric velocity.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Review", "cdate": 1542234256405, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920464, "tmdate": 1552335920464, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkeifEYIhX", "original": null, "number": 1, "cdate": 1540949011262, "ddate": null, "tcdate": 1540949011262, "tmdate": 1543196575971, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Review", "content": {"title": "The motivation for this work needs to be clarified", "review": "\n\n=Major Comments=\nThe prior work on grid cells and deep learning makes it clear that the goal of the work is to demonstrate that a simple learning system equipped with representation learning will produce spatial representations that are grid-like. Finding grid-like representations is important because these representations occur in the mammalian brain. \n\nYour paper would be improved by making a similar argument, where you would need to draw much more explicitly on the neuroscience literature. Namely, the validation of your proposed representations for position and velocity are mostly validated by the fact that they yield grid-like representations, not that they are useful for downstream tasks.\n\nFurthermore, you should better justify why your simple model is better than prior work? What does the simplicity provide? Interpretability? Ease if optimization? Sample complexity for training?\n\nThis is important because otherwise it is unclear why you need to perform representation learning. The tasks you present (path integral and planning) could be easily performed in basic x-y coordinates. You wouldn\u2019t need to introduce a latent v. Furthermore, this would mprove your argument for the importance of the block-diagonal M, since it would be more clear why interpretability matters.\n\n\nFinally, you definitely need to discuss the literature on randomized approximations to RBF kernels (random Fourier features). Given the way you pose the representation learning objective, I expect that these would be optimal. With this, it is clear why grid-like patterns would emerge.\n\n=Additional Comments=\nWhat can you say about the quality of the path returned by (10)? Is it guaranteed to converge to a path that ends at y? Is it the globally optimal path? \n\nI don\u2019t agree with your statement that your approach enables simple planning by steepest descent. First of all, are the plans that your method outputs high-quality? Second, if you had solved (10) directly in x-y coordinates, you could have done this easily since it is an optimization problem in just 2 variables. That could be approximately solved by grid search.\n\nI would remove section 5.4. The latent vector v is a high-dimensional encoding of low-dimensional data, so of-course it is robust to corruptions. The corruptions you consider don\u2019t come from a meaningful noise process, however? I can imagine, for example, that the agent observes corrupted versions of (x,y), but why would v get corrupted?\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Review", "cdate": 1542234256405, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920464, "tmdate": 1552335920464, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1xoKkC_0Q", "original": null, "number": 11, "cdate": 1543196546956, "ddate": null, "tcdate": 1543196546956, "tmdate": 1543196546956, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "r1xO0OpuAX", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Thank for such a thorough response", "comment": "You have addressed my questions very well, and I appreciate that you have updated the document so much. I have raised my evaluation score."}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "r1xO0OpuAX", "original": null, "number": 10, "cdate": 1543194832405, "ddate": null, "tcdate": 1543194832405, "tmdate": 1543194832405, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "HJe9Ndau0m", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Reply to Reviewer 2 (part 2)", "comment": "Q4:\u201cWhat can you say about the quality of the path returned by (10)? Is it guaranteed to converge to a path that ends at y? Is it the globally optimal path? I don\u2019t agree with your statement that your approach enables simple planning by steepest descent. First of all, are the plans that your method outputs high-quality? Second, if you had solved (10) directly in x-y coordinates, you could have done this easily since it is an optimization problem in just 2 variables. That could be approximately solved by grid search. \u201d\n\nA4: The convergence of a path can be quantified by success rate. Specifically, we can define a path planning to be successful if the distance between the agent\u2019s end position and the target is less than 0.025, and the distance between each point on the path and the obstacle is larger than 0.025. When a = 0.5 and b = 6, the successful rate is larger than 99%. \n\nWe agree with your criticism of our statement. We have removed the statement that compares our method with reinforcement learning and path planning. We have re-positioned our work on path planning, only claiming that our system is capable of implementing a path planning algorithm that is similar to the potential field method in robotics, thus sharing the advantages and disadvantages of the latter. Please see the paragraph at the beginning of Section 5.4. \n\nWe agree with your comment about solving path planning in 2D coordinates. Our claim is only that our system based on (v(x), M(dx), A(x, y)) is capable of implementing path planning algorithms based on (x, dx, |x-y|) (here both x and y are 2D). This is actually non-trivial. A learned grid cells system that is capable of path integral is not necessarily capable of path planning. The fact that A(x, y) informs |x-y| in our system is important for path planning. \n\nAs to why the mammalian brain adopts the grid cells instead of directly representing the 2D coordinates (e.g., by two neurons), our explanation is that the high-dimensional v enables error correction. \n\n\nQ5: \u201cI would remove section 5.4. The latent vector v is a high-dimensional encoding of low-dimensional data, so of-course it is robust to corruptions. The corruptions you consider don\u2019t come from a meaningful noise process, however? I can imagine, for example, that the agent observes corrupted versions of (x,y), but why would v get corrupted?\u201d\n\nA5: Following your advice, we have moved the error correction part to the appendix. See Section D of the appendix. \n\nThe units in v are neurons, and they tend to be noisy in the biological system. The dropout may also be related to the asynchronous nature of neuron activities. Dropout may also be caused by the gradual loss of neurons due to aging or Alzheimer. \n\nError correction may provide a justification for the high-dimensional vector encoding of the two-dimensional coordinates. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "HJe9Ndau0m", "original": null, "number": 9, "cdate": 1543194673885, "ddate": null, "tcdate": 1543194673885, "tmdate": 1543194673885, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "SkeifEYIhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Reply to Reviewer 2 (part 1)", "comment": "Thank you for your helpful comments and suggestions. \n\nQ1:\u201cYour paper would be improved by making a similar argument, where you would need to draw much more explicitly on the neuroscience literature.\u201d\n\nA1: Your advice is followed. We have added a discussion in the related work (Section 2), indicating that the disentangled blocks assumption in our model is related to the \u201cmodules\u201d of grid cells. We have also added a quantitative analysis using measures from the neuroscience literature to analyze the spatial activity of the learned units. Please see Section B.3 of the appendix. \n\n\nQ2: \u201cFurthermore, you should better justify why your simple model is better than prior work? What does the simplicity provide? Interpretability? Ease if optimization? Sample complexity for training? This is important because otherwise it is unclear why you need to perform representation learning. The tasks you present (path integral and planning) could be easily performed in basic x-y coordinates. You wouldn\u2019t need to introduce a latent v. Furthermore, this would improve your argument for the importance of the block-diagonal M, since it would be more clear why interpretability matters.\u201d \n\nA2: Thanks for the thoughtful comments, which we agree. \n\nThe simplicity here is about explaining the patterns observed in grid cells, and simplicity is desired or even required of an explanation of an observed phenomenon. \n\nIn particular, in Section 5.1 of the revised version, we show that the emergence of the global hexagon patterns can be explained by a generic local kernel and a generic local motion model, both of which are very simple. \n\nIn our work, we show that this simple system is capable of path integral and path planning. \n\nWe agree with you that these tasks can be performed in the 2D coordinates. It is a deep question as to why the mammalian brain uses a latent v. The justification we can provide is that the system with a high-dimensional v is capable of error correction, considering the neural system is intrinsically noisy. But there may be deeper or stronger justifications. One speculation is that the neural system may prefer matrix-vector multiplication to addition and subtraction. \n\n\nQ3:\u201cFinally, you definitely need to discuss the literature on randomized approximations to RBF kernels (random Fourier features). Given the way you pose the representation learning objective, I expect that these would be optimal. With this, it is clear why grid-like patterns would emerge.\u201d\n\nA3:  Thanks for the reference and the insight. We have cited the related papers and compare them to our work at the end of Section 5.2. \n\nIn Section 5.1 of the revised version, we show that a local radial basis kernel and a local motion model are enough to explain the emergence of the global hexagon patterns of the grid cells. In Appendix A, we also provide a theoretical understanding. \n\nInspired by your comment, we have added an ablation study in Section B.4.1 of the appendix, we show that the motion model v(x+dx) = M(dx) v(x) is necessary for the emergence of the grid patterns. We cannot learn the grid patterns from the localization model A(x, y) = <v(x), v(y)> alone. \n\nCompared to random Fourier features, we learn the grid patterns without assuming Fourier basis, our RBF kernel is a local generic one based on the second order Taylor expansion, and we need a motion model for the emergence of grid patterns. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "SyxSaDadCQ", "original": null, "number": 8, "cdate": 1543194556625, "ddate": null, "tcdate": 1543194556625, "tmdate": 1543194580838, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "B1xEovpuRm", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Reply to Reviewer 3 (part 2)", "comment": "Q7: \u201cThe experiments about path planning are unconvincing. First of all, the algorithm requires to input absolute positions of every obstacle into equation (9) - (10), which assumes that there is perfect information about the map. Secondly, the search algorithm is greedy and it is not obvious how it would handle a complex maze with cul-de-sac. Saying that \"there is no need for reinforcement learning or sophisticated optimal control\" is very misleading: the problem here is simplified to the extreme, and fully observed, and any comparison with deep RL algorithms that can handle partial observations is just out of place.\u201d\n\nA7: We agree with your criticism. We have removed the statement about reinforcement learning and optimal control. We have re-positioned our work on path planning, by only claiming that our system is capable of implementing a path planning algorithm that is similar to the potential field method in robotics, thus sharing the advantages and disadvantages of the latter. Please see the paragraph at the beginning of Section 5.4.\n\nNow the purpose of this section is only to show that: our (v(x), M(dx), A(x, y)) system is capable of implementing path planning algorithms based on (v, dx, |x-y|), even though our system does not represent the 2D coordinates x = (x1, x2) explicitly. This is actually non-trivial. A learned system that is capable of path integral is not necessarily capable of path planning. The fact that A(x, y) informs |x-y| in our system is important for path planning. \n\nWe suspect that we need both path planning algorithm and a learned policy. The latter may be useful in a familiar environment, while the former may be necessary in an unfamiliar environment. During the path planning process, the grid cells are expected to be active even though the agent is not moving. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "B1xEovpuRm", "original": null, "number": 7, "cdate": 1543194524486, "ddate": null, "tcdate": 1543194524486, "tmdate": 1543194524486, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "S1ls-yVtnm", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Reply to Reviewer 3 (part 1)", "comment": "Thank you for the helpful comments and suggestions. \n\n\nQ1: \u201cThe assumption that A(x, y) can be modeled by a Gaussian or exponential (Laplacian?) kernel is limiting, in particular for positions x and y that are far apart.\u201d\n\nA1: We agree with your concern with the global adjacency. We have studied a generic local adjacency based on the second order Taylor expansion. Please see Sections 5.1, 5.2, and Section B of the appendix. \n\nThis generic local adjacency 1 \u2013 alpha |x-y|^2 appears to be the key for the emergence of the global hexagon grid pattern where alpha determines the metric. \n\nMeanwhile, the global adjacency is necessary for the following two reasons. (1) Regulate the metrics of multiple blocks of the hexagon grid units. (2) Inform |x-y| for the purpose of path planning. \n\n\nQ2: \u201cThere is no discussion about what egocentric vs. allocentric referentials, and dx is assumed to be aligned with (x, y) axes (which are also the axes defining the bounding box of the area). Unlike the other work on learning path integration using an RNN, the linear matrix model can only handle allocentric displacements dx_1, dx_2 (and optional dx_3 in 3D).\u201d\n\nA2: Inspired by your comment, we have added a section on egocentric model. Please see Section C of the appendix. \n\nThe model couples the grid system for head direction and the original grid system for self-position. The coupling is as follows: the vector of the head direction system determines the matrix of the self-position system via an attention or selection mechanism. We find this model quite interesting although we still need more work to refine it. \n\nThe head direction system can also be repurposed as a clock and timestamp system. \n\n\nQ3: \u201cNo consideration is given to non-square areas: would the network also exhibit grid-like behavior if the area was circular?\u201d\n\nA3: To answer your question, we learn the system in circular and triangular areas and the results are shown in Figure 7 of Section B.2.2 of the appendix. Hexagon patterns emerge in both cases. \n\n\nQ4: \u201cWhat happens if the quadratic parametetrisation of block diagonals is dropped?\u201d\n\nA4: To answer your question, we have added an ablation study in Section B.4.2 of the appendix, where we remove the block diagonal assumption and the quadratic parametrization, so that we learn a separate motion matrix for each displacement on the discretized 2D grid. With local adjacency, we can still learn hexagon grid patterns when the block size is relatively small. For global adjacency, we cannot learn hexagon grid patterns. \n\nQ5: \u201cThe paper did not use metrics accepted in the neuroscience community for computing a gridness score of the grid cells (although the grid cell nature is evident). There should however be metrics for quantifying how many units represent the different scales, offsets and orientations.\u201d\n\nA5: Following your advice, we have added a quantitative analysis in Section B.3 of the appendix, using the measures from the neuroscience literature, including gridness score, grid scale and orientation. 76 out of 96 units are classified as grid units according the gridness score. \n\nAn interesting result is that the scale measure is proportional to the metric (1/sqrt(alpha_k)) explicitly defined and automatically learned by our method. Please see Figure 8.d. \n\n\nQ6: \u201cThe authors did not consider (but mentioned) embedding locations from vision, and did not consider ambiguous position embeddings.\u201d\n\nA6: To address your comment, we have added the following paragraph in Section 3.2 to discuss embedding location for vision.\n\n \u201cOur system can be embedded into the SLAM (simultaneous localization and mapping) system (\\cite{whyte2006simultaneous}), which is based on a state space model that consists of a dynamic sub-model for self-position due to self-motion, and an observation sub-model for the observed visual image given the self-position. We can represent the dynamic sub-model by our system, or reformulate the whole model using our scheme. We leave it to future work. \u201d\n\nWe are currently pursuing this direction of research. \n\nFor ambiguous position embeddings, in Section D of the appendix, we consider errors in the units and show that our system is capable of error correction. We also added Section D.2 about noisy input of self-motion. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "Hke2XPpdC7", "original": null, "number": 6, "cdate": 1543194404136, "ddate": null, "tcdate": 1543194404136, "tmdate": 1543194404136, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "rJeFrS4J6X", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Reply to Reviewer 1", "comment": "We are very grateful for your positive review and insightful comments. \n\n\nQ1: \u201cBut I feel the paper also stops just a few steps short of developing a fuller theoretical understanding of what is going on.\u201d\n\nA1: Following your advice, we have added theoretical analysis. Please see Section 5.1.3 and Section A of the appendix. \n\nIn the theoretical analysis, we provide an analytical solution that combines three Fourier plane waves. The analysis is based on a tight frame in 2D. \n\nWe believe this analytical solution helps us understand the emergence of hexagon patterns. Meanwhile, our model assumes much less than the analytical solution. \n\n\nQ2: \u201cFor example the learned solution is quite Fourier like, and we know that Fourier transforms are good for representing position shift in terms of phase shift. That would correspond to block size of two (i.e., complex numbers) in terms of this model. So what's wrong with this solution (in terms of performance) and what is gained by having block size of six, beyond simply looking more grid like? It would be nice to go beyond phenomenology and look at what the grid-like solution is useful for.\u201d\n\nA2:  Thanks for the insight.  \n\nRestricting block size = 2 indeed enables us to learn Fourier plane waves. Please see Figure 7.a. \n\nFigure 3.c shows that the path integral error with block size 2 is bigger than other block sizes. \n\nIn terms of localization sub-model, a single pair of Fourier plane waves v(x) = exp(i<a,x>) in a block gives us an adjacency function <v(x), v(y)> = cos(<a, x-y>), which does not inform |x-y| very well due to the aperture problem, i.e., if x-y is perpendicular to a, the adjacency is always 1. \n\nIn our new result, if we assume a generic local kernel <v(x), v(y)> = 1 \u2013 alpha |x-y|^2, then we can always learn hexagon grid patterns as long as the block size is greater than or equal to 6, where alpha controls the metric of the block. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "B1l3yDTuCm", "original": null, "number": 5, "cdate": 1543194340180, "ddate": null, "tcdate": 1543194340180, "tmdate": 1543194340180, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "List of New Results", "comment": "Dear Reviewers, \n\nThank you for your very helpful reviews. We have tried our best to address all the points you have raised. Please find our detailed replies under your reviews respectively. \n\nWe have uploaded the third revision. The following is a summary of the new results we have added relative to the original submitted version. \n\n\n1.  Hexagon grid patterns and metrics. \n\nPlease see Sections 5.1, 5.2, and Section B of the appendix. \n\nBy introducing a generic local kernel based on the second order Taylor expansion, we are able to learn hexagon grid patterns with explicitly defined and automatically learned metrics. \n\n\n2. Theoretical analysis. \n\nPlease see Section 5.1.3 and Section A of the appendix. \n\n\n3. Egocentric model that couples two grid systems. \n\nPlease see Section C of the appendix. \n\nThe model couples the grid system for head direction and the original grid system for self-position. The vector of the head direction system selects or pays attention to the matrix of the self-position system. \n\nThe head direction system can also be repurposed as a clock and timestamp system. \n\n\n4. Evaluations in terms of gridness measures and non-square shapes of the region. \n\nPlease see Sections B.3 and B.2.2 of the appendix. \n\nAn interesting result is that the scale measure is proportional to the automatically learned metric. \n\n\n5. Ablation studies on model assumptions. \n\nPlease see Section B.4 of the appendix. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "Hkgw09yBaQ", "original": null, "number": 2, "cdate": 1541892815353, "ddate": null, "tcdate": 1541892815353, "tmdate": 1541892815353, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Surprising new result (continued): multiple hexagon blocks with automatically learned metrics", "comment": "Dear Reviewers, \n\nWe have uploaded the second revision that includes a new Subsection 5.2 on learning multiple blocks of grid cells where the metrics or grid sizes alpha_k are automatically learned. \n\nFigure 2.a shows the learned blocks and their learned metrics alpha_k. You can see that the learned blocks again show hexagon patterns, and different blocks have different metrics or grid sizes. The metrics are explicitly defined as the curvatures of the local kernels and are learned together with the vector and matrix representations. \n\nIn Figure 2.a, the number of cells in each block is 6. In Appendix D, we show the learned blocks with different numbers of cells. As long as the number is greater than or equal to 6, the hexagon patterns emerge (for smaller number, the learned cells tend to exhibit square lattice patterns). \n\nFigure 2.b shows the heat maps <v_k, v_k(x)> for inferring the location of v = (v_k, k = 1, ..., K). While individual heat maps have multiple firing locations, they add up to the Gaussian kernel with a unique location. The global Gaussian kernel is used to regulate the metrics of different constituent blocks, who vote for the inferred position by their heat maps. \n\nTo summarize, our model, while being very simple, explains the following aspects of grid cells at the computational (not necessarily neuroscience) level: (1) hexagon grid patterns. (2) metrics or grid sizes. (3) path integral. (4) path planning. (5) error correction. \n\nWe removed the original Subsection 5.2 on learning multiple blocks in the original version. We also shortened the discussion to stay within the page limit. \n\nWe will continue to revise our paper according to your advice, and we will reply to your comments soon. \n\nThank you for your consideration. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "rJxh001bTX", "original": null, "number": 1, "cdate": 1541631700347, "ddate": null, "tcdate": 1541631700347, "tmdate": 1541631700347, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "content": {"title": "Surprising new result: hexagon and metric, with theoretical analysis", "comment": "Dear Reviewers, \n\nThank you for your precious time and insightful comments. \n\nWe have uploaded the first revision to include a new result that we find surprising, interesting and important. Please see Subsection 5.1 of the first revision. \n\nTo summarize, we learn a single block of cells with a generic local kernel. Recall the adjacency A(x, y)=f(|x-y|). A second order Taylor expansion of f(r) at r = 0 gives us f(r) = 1-alpha r^2, for small r, where 2 alpha is the curvature of f(r) at 0. The first derivative is 0 because f(r) reaches maximum at 0. We use the localization loss term \n\n|<v(x), v(y)> - (1-alpha |x-y|^2)|^2,  for |x-y|^2 <= 1.5/alpha. \n\nTogether with the motion loss, our learning method unfailingly produces hexagon patterns, and alpha determines the metric or grid size. The hexagon patterns emerge as long as the number of units is greater than or equals to 6 (for smaller number we may learn rectangle patterns). \n\nWe also provide a theoretical solution when the number of units equals to 6, based on a tight frame in 2D. \n\nWe want to emphasize that both the localization loss and the motion loss are LOCAL, and yet the global hexagon patterns always emerge. Our loss function does not assume any global periodic pattern. It is perhaps the simplest loss function one can find: (1) a second order Taylor expansion for localization loss. (2) a matrix-vector product for motion loss. This is really minimalistic. That is, what we put in is far less than what we get out. \n\nWe believe this is the most important result of our work, because after all, the grid cells are characterized by hexagon patterns of different sizes. This is why they are called grid cells in the first place. We now can explain this crucial piece of puzzle. \n\nWe will incorporate this local kernel loss term into our original global kernel loss so that we will learn the metric alpha for each block automatically. \n\nTo save space, we moved the 3D path planning to appendix. We also added 1D result in appendix. The 1D result can be interpreted as time2vec or time stamp for events. \n\nA few key points: \n\nWe shall reply to your valuable comments soon and further revise our paper according to your comments and advice. But first please allow us to make a few key points here: \n\n(1)\tIn our representation scheme, we NEVER represent the coordinates x = (x1, x2) explicitly. We only represent the position by heat map or one-hot map. Without explicit coordinates, it is not a trivial task to do path planning, and it is very different from path planning in robotics based on explicit coordinates. \n(2)\tAbout path planning. Consider a rat leaves his home to forage. He needs path integration to know where he is. But MORE importantly, when he needs to go back home, he needs path planning. Even when he is standing still, his grid cells are changing during path planning, i.e., he is imagining or fantasizing the steps. Our proposed steepest ascent algorithm is of this nature.  The rat can also fantasize much bigger step sizes beyond his physical capability in path planning, and our method enables him to do that, see Figure 4(a) for straight path planning. \n(3)\tAbout error correction. When talking to people in CS and robotics, a common question is: how come the brain does not use two neurons to represent the two coordinates x = (x1, x2), and instead use many neurons to represent the position. Our error correction experiment may give a justification. The dropout experiment also points to the possibility that the grid cells can work asynchronously, which is typical of biological neural system. We shall explore this issue further. \n\nWe shall reply to your comments and upload the second revision soon. \n\nThank you for your consideration of our first revision and first reply. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621612283, "tddate": null, "super": null, "final": null, "reply": {"forum": "Syx0Mh05YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1317/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1317/Authors|ICLR.cc/2019/Conference/Paper1317/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers", "ICLR.cc/2019/Conference/Paper1317/Authors", "ICLR.cc/2019/Conference/Paper1317/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621612283}}}, {"id": "rJeFrS4J6X", "original": null, "number": 3, "cdate": 1541518656979, "ddate": null, "tcdate": 1541518656979, "tmdate": 1541533239673, "tddate": null, "forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1317/Official_Review", "content": {"title": "A simple and elegant approach to grid cells that begs for theoretical insight", "review": "This paper proposes a simple and elegant approach to learning \"grid-cell like\" representations that uses a high-dimensional encoding of position, together with a matrix for propagating position that involves only local connections among the elements of the vector.  The vectors are also constrained to have their inner products reflect positional similarity.  The paper also shows how such a representation may be used for path planning.\n\nBy stripping away the baggage and assumptions of previous approaches, I feel this paper starts to get at the essence of what drives the formation of grid cells.   It is still steps away from having direct ties to neurobiology, but is trying to get at the minimal components necessary for bringing about a grid cell like solution.  But I feel the paper also stops just a few steps short of developing a fuller theoretical understanding of what is going on.  For example the learned solution is quite Fourier like, and we know that Fourier transforms are good for representing position shift in terms of phase shift.  That would correspond to block size of two (i.e., complex numbers) in terms of this model.  So what's wrong with this solution (in terms of performance) and what is gained by having block size of six, beyond simply looking more grid like?  It would be nice to go beyond phenomenology and look at what the grid-like solution is useful for.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1317/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion", "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multi- plication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.", "keywords": [], "authorids": ["ruiqigao@ucla.edu", "jianwen@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "authors": ["Ruiqi Gao", "Jianwen Xie", "Song-Chun Zhu", "Ying Nian Wu"], "pdf": "/pdf/1553f8c38acacffb76f7dd1b0bb9bd75785a02f4.pdf", "paperhash": "gao|learning_grid_cells_as_vector_representation_of_selfposition_coupled_with_matrix_representation_of_selfmotion", "_bibtex": "@inproceedings{\ngao2018learning,\ntitle={Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion},\nauthor={Ruiqi Gao and Jianwen Xie and Song-Chun Zhu and Ying Nian Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Syx0Mh05YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1317/Official_Review", "cdate": 1542234256405, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Syx0Mh05YQ", "replyto": "Syx0Mh05YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1317/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920464, "tmdate": 1552335920464, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1317/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}